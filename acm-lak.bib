@inproceedings{10.1145/3706468.3706571,
author = {Deininger, Hannah and Parrisius, Cora and Lavelle-Hill, Rosa and Meurers, Detmar and Trautwein, Ulrich and Nagengast, Benjamin and Kasneci, Gjergji},
title = {Who Did What to Succeed? Individual Differences in Which Learning Behaviors Are Linked to Achievement},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706571},
doi = {10.1145/3706468.3706571},
abstract = {It is commonly assumed that digital learning environments such as intelligent tutoring systems facilitate learning and positively impact achievement. This study explores how different groups of students exhibit distinct relationships between learning behaviors and academic achievement in an intelligent tutoring system for English as a foreign language. We examined whether these differences are linked to students’ prior knowledge, personality traits, and motivation. We collected behavioral trace data from 507 German seventh-grade students during the 2021/22 school year and applied machine learning models to predict English performance based on learning behaviors (best-performing model’s R2 =.41). To understand the impact of specific behaviors, we applied the explainable AI method SHAP and identified three student clusters with distinct learning behavior patterns. Subsequent analyses revealed that these clusters also varied in prior knowledge and motivation: one with high prior knowledge and average motivation, another with low prior knowledge and average motivation, and a third with both low prior knowledge and low motivation. Our findings suggest that learning behaviors are linked differently to academic success across students and are closely tied to their prior knowledge and motivation. This hints towards the importance of personalizing learning systems to support individual learning needs better.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {771–782},
numpages = {12},
keywords = {Learning Analytics, Behavioral Trace Data, Academic Performance, Interindividual Differences},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706570,
author = {Barrett, Alex and Ke, Fengfeng and Zhang, Nuodi and Dai, Chih-Pu and Bhowmik, Saptarshi and Yuan, Xin},
title = {Pattern analysis of ambitious science talk between preservice teachers and AI-powered student agents},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706570},
doi = {10.1145/3706468.3706570},
abstract = {New frontiers in simulation-based teacher training have been unveiled with the advancement of artificial intelligence (AI). Integrating AI into virtual student agents increases the accessibility and affordability of teacher training simulations, but little is known about how preservice teachers interact with AI-powered student agents. This study analyzed the discourse behavior of 15 preservice teachers who undertook simulation-based training with AI-powered student agents. Using a framework of ambitious science teaching, we conducted a pattern analysis of teacher and student talk moves, looking for evidence of academically productive discourse. Comparisons are made with patterns found in real classrooms with professionally trained science teachers. Results indicated that preservice teachers generated academically productive discourse with AI-powered students by using ambitious talk moves. The pattern analysis also revealed coachable moments where preservice teachers succumbed to cycles of unproductive discourse. This study highlights the utility of analyzing classroom discourse to understand human-AI communication in simulation-based teacher training.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {761–770},
numpages = {10},
keywords = {Artificial intelligence, Discourse pattern analysis, Simulation, Teacher training},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706569,
author = {Oh, Hyunju and Liu, Zifeng and Xing, Wanli},
title = {Do Actions Speak Louder Than Words? Unveiling Linguistic Patterns in Online Learning Communities Using Cross Recurrence Quantification Analysis},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706569},
doi = {10.1145/3706468.3706569},
abstract = {This study explores the dynamics of engagement in online learning communities (OLCs), focusing on online math discussion forums. It employs Social Network Analysis (SNA) and Cross-Recurrence Quantification Analysis (CRQA) to examine interaction patterns and linguistic synchrony across participant clusters with varying levels of engagement. SNA reveals three distinct participant groups—core, intermediate, and peripheral—each exhibiting different interaction levels. The study’s findings highlight the significance of coordinated discourse in fostering collaborative learning and engagement in OLCs. This research contributes to the theoretical framework of Social Capital Theory by emphasizing the role of shared language in promoting cohesive communication. The results offer valuable insights for designing more effective online learning environments that encourage sustained student participation and knowledge construction.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {992–998},
numpages = {7},
keywords = {Online Learning Communities, Social Network Analysis, Cross-Recurrence Quantification Analysis},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706568,
author = {Woodrow, Juliette and Piech, Chris},
title = {Soft Grades: A Calibrated and Accurate Method for Course-Grade Estimation that Expresses Uncertainty},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706568},
doi = {10.1145/3706468.3706568},
abstract = {In traditional educational settings, students are often summarized by a single number—a final course grade—that reflects their performance. While final grades are convenient for reporting or comparison, they oversimplify a student’s true ability and do not express uncertainty. In this paper, we introduce a new item-response model for classroom settings that infers a distribution over student abilities and uses this to represent each student’s final grade as a probability distribution. This approach captures the uncertainty that comes from variations in both student performance and grading processes. Practical applications of our approach include enabling teachers to better understand grading confidence, impute missing assignment scores, and make informed decisions when curving final grades. For students, the model offers probabilistic estimates of their final course grades based on current performance, supporting informed academic decisions such as opting for Pass/Fail grading. We evaluate our model using real-world datasets, showing that the Soft Grades model is well-calibrated and surpasses the state-of-the-art polytomous IRT model in accurately predicting future scores. Additionally, we share a web application and Python scripts to make our model available to teachers and students.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {750–760},
numpages = {11},
keywords = {Item Response Theory, Grade Prediction, Soft Grades, Ability Inference},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706567,
author = {Yao, Chengyuan and Cortez, Carmen and Yu, Renzhe},
title = {Towards Fair and Privacy-Aware Transfer Learning for Educational Predictive Modeling: A Case Study on Retention Prediction in Community Colleges},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706567},
doi = {10.1145/3706468.3706567},
abstract = {Predictive analytics is a widely used application of learning analytics, but many resource-constrained institutions lack the capacity to develop their own predictive models or rely on proprietary models trained in different contexts with little transparency. In this context, transfer learning holds promise for expanding reliable and equitable access to predictive analytics, but this potential remains underexplored given existing legal and technical constraints. In this paper, we examine transfer learning strategies in the context of retention prediction at two-year community colleges in the United States, which enroll the most postsecondary students from underserved communities with higher dropout rates than selective universities. We envision a scenario where community colleges can collaborate with each other and with four-year universities to develop retention prediction models under privacy constraints, and evaluate the risks and potential improvement strategies of cross-institutional model transfer for different stakeholders. Using detailed administrative records from 4 research universities and 23 community colleges, which cover more than 800,000 students across 7 cohorts, we first identify performance and fairness degradation when source (external) models are deployed at a target institution without any localization. Fortunately, publicly available institution-level contextual information can be used to forecast these performance drops and offer early guidance for model portability. For model developers under data privacy regulations, sequential training that selects training institutions based on demographic similarities proves useful for enhancing the general fairness of resulting models without compromising performance. For target institutions without local data to fine-tune source models, we find that customizing evaluation thresholds for different sensitive groups is more successful than established transfer learning techniques at improving performance and fairness of deployed models. Our findings suggest the value of transfer learning for more accessible educational predictive modeling and call for judicious use of contextual information in model training, selection, and deployment to achieve reliable and equitable model transfer1.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {738–749},
numpages = {12},
keywords = {Predictive Analytics; Transfer Learning; Algorithmic Fairness; Privacy; Intersectionality; College Retention; Community Colleges; Higher Education},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706566,
author = {Yeung, Steven},
title = {A comparative study of rule-based, machine learning and large language model approaches in automated writing evaluation (AWE)},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706566},
doi = {10.1145/3706468.3706566},
abstract = {Automated Writing Evaluation (AWE) tools have proved beneficial to writing development. Research on AWE methods is essential for improving tool performance and further comparative studies are needed as new methods emerge. This study examines the performance of several AWE approaches, comparing rule-based and statistical methods, machine learning (ML) models, and a large language model (LLM). These three AWE methods were applied to a representative sample of academic essays from the TOEFL11 dataset to compare their assessment performance. Results show that the selected LLM, GPT-4, outperformed the other two approaches in terms of QWK and Pearson’s correlation coefficient, while the Support Vector Machine (SVM) model in the ML approach had the highest accuracy and the lowest mean absolute error. This paper provides a detailed comparison of these three approaches and discusses implications for educational practice and future research around AWE.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {984–991},
numpages = {8},
keywords = {Rule-based method, machine learning, large language model, automated writing evaluation, automated essay scoring, generative AI},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706565,
author = {Divjak, Bla\v{z}enka and Barthakur, Abhinava and Kovanovi\'{c}, Vitomir and Svetec, Barbi},
title = {The Impact of Learning Design on the Mastery of Learning Outcomes in Higher Education},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706565},
doi = {10.1145/3706468.3706565},
abstract = {Ensuring constructive alignment between learning outcomes (LOs) and assessment design is crucial to effective learning design (LD). While previous research has explored the alignment of LOs with assessments, there is a lack of empirical studies on how assessment design influences LO mastery, particularly the relationship between formative and summative assessments. To address this gap, we conducted an empirical study within an undergraduate mathematics course. First, we evaluated the course's learning design to identify potential gaps in constructive alignment. Then, using a sample of 169 students, we analysed their assessment results to explore how LO mastery is demonstrated through formative and summative assessments. This study provides a novel learning analytics (LA) methodology by combining cognitive diagnostic models, epistemic network analysis, and social network analysis to examine LO mastery and interdependencies. Our findings reveal a strong connection between the mastery of LOs through formative and summative assessments, underscoring the importance of well-constructed LD. The practical implications suggest that LA can serve as a critical tool for quality assurance by guiding the revision of LOs and optimising LD to foster deeper student engagement and mastery of critical concepts. These insights offer actionable pathways for more targeted, student-centered teaching practices.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {726–737},
numpages = {12},
keywords = {Assessment design, Assessment validity, Learning analytics, Learning design, Learning outcomes},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706564,
author = {Ramanathan, Sriram and Lim, Lisa-Angelique and Mottaghi, Nazanin Rezazadeh and Buckingham Shum, Simon},
title = {When the Prompt becomes the Codebook: Grounded Prompt Engineering (GROPROE) and its application to Belonging Analytics},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706564},
doi = {10.1145/3706468.3706564},
abstract = {With the emergence of generative AI, the field of Learning Analytics (LA) has increasingly embraced the use of Large Language Models (LLMs) to automate qualitative analysis. Deductive analysis requires theoretical or other conceptual grounding to inform coding. However, few studies detail the process of translating the literature into a codebook, and then into an effective LLM prompt. In this paper, we introduce Grounded Prompt Engineering (GROPROE) as a systematic process to develop a literature-grounded prompt for deductive analysis. We demonstrate our GROPROE process on a dataset of 860 written reflections, coding for students’ affective engagement and sense of belonging. To evaluate the quality of the coding we demonstrate substantial human/LLM Inter-Annotator Reliability (IAR). To evaluate the consistency of LLM coding, a subset of the data was analysed 60 times using the LLM Quotient showing how this stabilized for most codes. We discuss the dynamics of human-AI interaction when following GROPROE, foregrounding how the prompt took over as the iteratively revised codebook, and how the LLM provoked codebook revision. The contributions to the LA field are threefold: (i) GROPROE as a systematic prompt-design process for deductive coding grounded in literature, (ii) a detailed worked example showing its application to Belonging Analytics, and (iii) implications for human-AI interaction in automated deductive analysis.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {713–725},
numpages = {13},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706563,
author = {Ferguson, Rebecca and Gopalan, Yuveena and Buckingham Shum, Simon},
title = {What's the Value of a Doctoral Consortium? Analysing a Decade of LAK DCs as a Community of Practice},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706563},
doi = {10.1145/3706468.3706563},
abstract = {Since 2013, the Learning Analytics and Knowledge (LAK) conference has included a Doctoral Consortium (DC), supported by the Society for Learning Analytics Research (SoLAR). Given the LAK25 conference theme of ‘expanding the horizons of learning analytics’, it is timely to reflect on how well the LAK DC is meeting its objectives of building capacity in the field and developing the next generation. We frame the DC as a structured entry into the LAK community of practice (CoP), familiarising students with the domain of learning analytics, understanding its practices, and building connections with other members. CoPs generate five types of value for their members: immediate, potential, applied, realised and reframing. This study used a survey of the 92 DC students from the first decade (2013–22), supplemented with scientometric analysis of LAK publications, to address the questions: What value do students gain from attending the LAK doctoral consortium? and Do students gain the same value from face-to-face and virtual doctoral consortia? Thematic analysis of responses (N=37, a 40% response rate) showed that students gained a wide range of immediate and potential value from the DC, which in many cases also prompted changes in practice, performance improvement or redefinition of success. However, the value reported by a third of respondents who had attended virtually was more limited. We note that, despite the value already offered, the DC could provide clearer routes both to future community engagement and to extending goals that expand the horizons of learning analytics. This paper's contributions are (i) the first systematic documentation of student perceptions of LAK DCs, (ii) identification of ways in which doctoral consortia can be developed in the future, and (iii) specific attention to how virtual DCs can offer greater value for both participants and the host community of practice. The findings related to future development and value do not apply only to LAK but can be generalised to other doctoral consortia.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {702–712},
numpages = {11},
keywords = {applied value, communities of practice, doctoral consortium, immediate value, potential value, qualitative research, realized value, reframing value, thematic analysis},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706562,
author = {Kim, Jinwon and Li, Qiujie and Jiang, Zilu and Xu, Di},
title = {Not ALL Delay is Procrastination: Analyzing Subpatterns of Academic Delayers in Online Learning},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706562},
doi = {10.1145/3706468.3706562},
abstract = {In prior literature on using clickstream data to capture student behavior in virtual learning environments, procrastination is typically measured by the extent to which students delay their coursework. However, students may delay coursework under personal and environmental contexts and not all delays should be considered procrastination. Thus, this study aims to identify different types of delayers and examine how they differ in academic engagement and performance. We utilized learning management system (LMS) data from three online undergraduate courses. Specifically, using data from the first three weeks of the course, we classified delayers into three subgroups – high-achieving, low-achieving, and sporadic delayers – based on the timing of their coursework access and submission, the consistency of these behaviors, and their short-term course performance. Our findings reveal that the subgroups significantly differ in course engagement and long-term performance. Low-achieving delayers exhibited the lowest levels of engagement and performance. While sporadic delayers and high-achieving delayers demonstrated comparable levels of engagement, the latter received higher course grades. These findings challenge commonly used LMS measures for procrastination, highlight the complexity of academic delays, and reveal nuanced patterns of student behavior. The results contribute to discussions on future interventions and research related to distinct forms of delays.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {977–983},
numpages = {7},
keywords = {Academic Delays, Learning Management Systems, Procrastination, Learning Analytics, Student Performance, Online Learning, Higher Education},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706561,
author = {Tang, Joe and Gibson, Andrew and Bruza, Peter},
title = {Analysis of exploratory behaviour: A step towards modelling of curiosity},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706561},
doi = {10.1145/3706468.3706561},
abstract = {In this research, we analysed exploratory behaviour trace data for students engaging in learning tasks in a technology-enhanced data analytics course as the first step towards modelling curiosity in learning. Curiosity is a complex phenomenon that is not amenable to direct modelling, but it can be understood through related behaviours like exploration, which is critical to effective learning. We analysed trace data from 40 students using visualisation and network analysis techniques, focusing on their interactions with learning tasks within the JupyterLab environment. Our analysis found that providing sufficient exploration time before explicit instruction or answer revelation, and designing learning tasks that embrace errors as opportunities, encouraged behaviours associated with curiosity-driven learning. These findings highlight the importance of designing learning environments that foster curiosity and promote active exploration.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {690–701},
numpages = {12},
keywords = {exploratory behaviour, curiosity-driven learning, task-centric approach, trace data},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706560,
author = {Sun, Chen and Shute, Valerie and Stewart, Angela E.B. and D'Mello, Sidney K.},
title = {The Relationship between Collaborative Problem-Solving Skills and Group-to-Individual Learning Transfer in a Game-based Learning Environment},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706560},
doi = {10.1145/3706468.3706560},
abstract = {Collaborative problem solving (CPS) is viewed as an essential 21st century skill for the modern workforce. Accordingly, researchers have been investigating how to conceptualize, assess, and develop pedagogical approaches to improve CPS. These efforts require theoretically-grounded and empirically-validated frameworks of CPS which have been emerging over the past decade with various levels of validity data. The present paper focuses on validating the generalized competency model (GCM) of CPS with respect to predicting individual learning outcomes following CPS among triads. The GCM consists of three main facets–constructing shared knowledge, negotiation/coordination, and maintaining team function–mapped to behavioral indicators (i.e., observable evidence). It hypothesizes that scores on all three facets should positively predict CPS outcomes, including group-to-individual learning transfer. We tested this hypothesis in a study where 249 students who comprised 83 triads engaged in collaborative gameplay with the Physics Playground game environment remotely via videoconferencing. We found that the only CPS facet predicting individual physics learning was maintaining team function, after accounting for pretest scores, students’ perceptions of team collaboration, and their perceived physics self-efficacy. This facet was also the only significant predictor of individual learning regardless of how facet scores were computed (i.e., reverse coding of negative indicators, separating the sums of positive and negative indicators, and no reverse coding of negative indicators). Implications for the GCM and other CPS frameworks are discussed.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {680–689},
numpages = {10},
keywords = {collaborative problem solving, framework validation, physics learning outcome, game-based learning, triads},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706559,
author = {Li, Tongguang and Nath, Debarshi and Cheng, Yixin and Fan, Yizhou and Li, Xinyu and Rakovi\'{c}, Mladen and Khosravi, Hassan and Swiecki, Zachari and Tsai, Yi-Shan and Ga\v{s}evi\'{c}, Dragan},
title = {Turning Real-Time Analytics into Adaptive Scaffolds for Self-Regulated Learning Using Generative Artificial Intelligence},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706559},
doi = {10.1145/3706468.3706559},
abstract = {In computer-based learning environments (CBLEs), adopting effective self-regulated learning (SRL) strategies requires sophisticated coordination of multiple SRL processes. While various studies have proposed adaptive SRL scaffolds (i.e. real-time advice on adopting effective SRL processes) and embedded them in CBLEs to facilitate learners’ effective use of SRL strategies, two key research gaps remain. First, there is a lack of research on SRL scaffolds that are based on continuous assessment of both learners’ SRL processes and learning conditions (e.g., awareness of learning resources) to provide adaptive support. Second, current analytics-based scaffolding mechanisms lack the scalability needed to effectively address multiple learning conditions. Integration of analytics of SRL with generative artificial intelligence (GenAI) can provide scalable scaffolding for real-time SRL processes and evolving conditions. Yet, empirical studies implementing and evaluating effects of this integration remain scarce. To address these limitations, we conducted a randomized control trial, assigning participants to three groups (control, process only, and process with condition groups) to investigate the effects of using GenAI to turn insights from real-time analytics about students’ SRL processes and conditions into adaptive scaffolds. The results demonstrate that integrating real-time analytics with GenAI in adaptive SRL scaffolds – addressing both SRL processes and dynamic conditions – promotes more metacognitive learning patterns compared to the control and process-only groups. In addition, the learners showed varying levels of compliance with analytics-based GenAI scaffolds, and this was also reflected in how the learners coordinated their SRL processes, particularly in the performance phase of SRL. This study contributes to the literature by designing, implementing, and evaluating the impact of adaptive scaffolds on learners’ SRL processes using real-time analytics with GenAI.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {667–679},
numpages = {13},
keywords = {self-regulated learning, scaffolding compliance, GenAI, scaffolding, learning analytics},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706558,
author = {Thapa Magar, Abisha and Shakya, Anup and Fancsali, Stephen E. and Rus, Vasile and Murphy, April and Ritter, Steve and Venugopal, Deepak},
title = {"Can A Language Model Represent Math Strategies?": Learning Math Strategies from Big Data using BERT},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706558},
doi = {10.1145/3706468.3706558},
abstract = {AI models have shown a remarkable ability to perform representation learning using large-scale data. In particular, the emergence of Large Language Models (LLMs) attests to the capability of AI models to learn complex hidden structures in a bottom-up manner without requiring a lot of human expertise. In this paper, we leverage these models to learn Math learning strategies at scale. Specifically, we use student interaction data from the MATHia Intelligent Tutoring System to learn strategies based on sequences of actions performed by students. To do this, we develop an AI model based on BERT (Bidirectional Encoder Representations From Transformers) that has two main components. First, we pre-train BERT using an approach known as Masked Language Modeling to learn embeddings for strategies. The embeddings represent strategies in a vector form while preserving their semantics. Next, we fine-tune the model to predict if students are likely to apply a correct strategy to solve a novel problem. We demonstrate using a large dataset collected from 655 schools that our approach where we pre-train to learn strategies from a sample of schools can be fine-tuned with a small number of examples to make accurate predictions over student data collected from other schools.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {655–666},
numpages = {12},
keywords = {Math strategies, Representation learning, BERT, Big Data, Transformers},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706557,
author = {Hicks, Ben and Kitto, Kirsty},
title = {Game Theoretic Models of Intangible Learning Data},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706557},
doi = {10.1145/3706468.3706557},
abstract = {Learning Analytics is full of situations where features essential to understanding the learning process cannot be measured. The cognitive processes of students, their decisions to cooperate or cheat on an assessment, their interactions with class environments can all be critical contextual features of an educational system that are impossible to measure. This leaves an empty space where essential data is missing from our analysis. This paper proposes the use of Game Theoretic models as a way to explore that empty space and potentially even to generate synthetic data for our models. Cooperating or free-riding on the provisioning of feedback in a class activity is used as a case study. We show how our initially simple model can gradually be built up to help understand potential educator responses as new situations arise, using the emergence of GenAI in the classroom as a case in point.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {970–976},
numpages = {7},
keywords = {game theory, learning analytics, missing data, learning theory},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706556,
author = {Woollaston, Steve and Flanagan, Brendan and Ocheja, Patrick and Toyokawa, Yuko and Ogata, Hiroaki},
title = {ARCHIE: Exploring Language Learner Behaviors in LLM Chatbot-Supported Active Reading Log Data with Epistemic Network Analysis},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706556},
doi = {10.1145/3706468.3706556},
abstract = {With the increasing integration of technology in education, chatbots and e-readers have emerged as promising tools for enhancing language learning experiences. This study investigates how students engage with digital texts and a purpose-built chatbot designed to promote active reading for EFL students. We analysed student interactions and compared high-proficiency and low-proficiency English learners. Results indicate that while all students perceived the chatbot as easy to use, useful, and enjoyable, significant behavioural differences emerged between proficiency groups. High-proficiency students exhibited more frequent interactions with the chatbot, engaged in more active reading strategies like backtracking, and demonstrated less help seeking behaviours. Epistemic Network Analysis revealed distinct co-occurrence patterns, highlighting the stronger connection between navigation and review behaviours in the high-proficiency group. These findings underscore the potential of chatbot-assisted language learning and emphasise the importance of incorporating active reading strategies for improved comprehension.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {642–654},
numpages = {13},
keywords = {chatbot, e-readers, EFL, active reading, epistemic network analysis (ENA), log data, backtracking},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706555,
author = {Liu, Yiming and Ma, Zhengyang and Ng, Jeremy Tzi Dong and Hu, Xiao},
title = {Multimodal learning analytics for game-based assessment of collaborative problem solving skills among young students},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706555},
doi = {10.1145/3706468.3706555},
abstract = {Collaborative Problem Solving (CPS) has emerged as a key competence for the 21st century. In support of this, valid assessments of CPS skills have become critical. However, limited research has designed and developed CPS assessments for young students. Based on multimodal learning analytics, we aim to develop and validate a game-based assessment of CPS for primary school students. In this study, evidence centered design approach was used to design and develop the game-based CPS assessment. Specifically, we designed and developed a mobile multiplayer online 3D role-playing game on CPS and a coding scheme for coding students’ gameplay data (i.e., game logs and voice chat) based on the ATC21S CPS framework. A total of 32 primary 5 students participated in this study to play the game in a group of four and complete a questionnaire of CPS skills. The gameplay data were coded based on our coding scheme. Correlation analysis between the coded results and the CPS questionnaire data supported the criterion validity of our game-based assessment measure. Additionally, the results of expert interview facilitated our understanding of assessment design and data use. This study will make methodological and practical contributions to the integration of MMLA into game-based CPS assessments.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {963–969},
numpages = {7},
keywords = {Collaborative problem solving, Game-based assessment, K-12 education, Multimodal learning analytics},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706554,
author = {Spitzer, Markus Wolfgang Hermann and Bardach, Lisa and Strittmatter, Younes and Moeller, Korbinian},
title = {Usage and performance declines in a classroom-integrated digital learning software over the course of an academic year},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706554},
doi = {10.1145/3706468.3706554},
abstract = {In increasing numbers of classrooms worldwide, students use digital learning software. However, we know little about the trajectories of usage and the performance within such digital learning software over the academic year. This study analyzed real-world longitudinal data from a mathematics learning software used in classrooms in Germany and the Netherlands (6,000 students who worked on &gt;23 million problems). We evaluated students’ usage and performance trajectories across an academic year by examining the percentage of students using the software, worked-through problems, active days and weeks, as well as performance. Our results indicate a decline in both usage and performance over the course of the academic year, with overall lower usage in Germany than in the Netherlands. Our findings highlight the need for further research into the factors maintaining or increasing the usage of and performance in classroom-integrated digital learning software over extended periods.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {957–962},
numpages = {6},
keywords = {digital learning software, mathematics, academic performance, naturalistic data, expectancy-value theory, expectancy-value-cost theory},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706553,
author = {Prieto, Luis P. and Alfredo, Riordan and D\'{\i}az-Chavarr\'{\i}a, Henry Benjam\'{\i}n and Martinez-Maldonado, Roberto and Echeverr\'{\i}a, Vanessa},
title = {VALA/AID: A Method for Rapid, Participatory Value-sensitive Learning Analytics and Artificial Intelligence Design},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706553},
doi = {10.1145/3706468.3706553},
abstract = {The adoption of learning analytics (LA) and artificial intelligence (AI) in education has long been a challenge, in part due to the ethical issues it engenders (e.g., the alignment of technology-embedded and human stakeholder values). Value-sensitive design (VSD) is a human-centered and theory-grounded approach to technology design that explicitly elicits and accounts for human values. However, there is scant concrete guidance on how to involve students in co-designing LA technologies from a VSD perspective, in an efficient manner. This paper presents a novel method, called VALA/AID, to elicit student values, challenges and motivations, in the early stages of an LA design process. We briefly illustrate the application of the method and the kind of evidence and design insights that can be distilled from it, for a relatively under-explored context in LA research: doctoral education.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {950–956},
numpages = {7},
keywords = {Human-centered design, Value-sensitive design, Participatory design, Learning analytics, Artificial intelligence in education.},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706552,
author = {Gao, Ge and Leon, Amelia and Jetten, Andrea and Turner, Jasmine and Almoubayyed, Husni and Fancsali, Stephen and Brunskill, Emma},
title = {Predicting Long-Term Student Outcomes from Short-Term EdTech Log Data},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706552},
doi = {10.1145/3706468.3706552},
abstract = {Educational stakeholders are often particularly interested in sparse, delayed student outcomes, like end-of-year statewide exams. The rare occurrence of such assessments makes it harder to identify students likely to fail such assessments, as well as making it slow for researchers and educators to be able to assess the effectiveness of particular educational tools. Prior work has primarily focused on using logs from students full usage (e.g. year-long) of an educational product to predict outcomes, or considered predictive accuracy using a few minutes to predict outcomes after a short (e.g. 1 hour) session. In contrast, we investigate machine learning predictors using students’ logs during their first few hours of usage can provide useful predictive insight into those students’ end-of-school year external assessment. We do this on three diverse datasets: from students in Uganda using a literacy game product, and from students in the US using two mathematics intelligent tutoring systems. We consider various measures of the accuracy of the resulting predictors, including its ability to identify students at different parts along the assessment performance distribution. Our findings suggest that short-term log usage data, from 2-5 hours, can be used to provide valuable signal about students’ long term external performance.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {631–641},
numpages = {11},
keywords = {Long-term student outcomes prediction, Short-horizon data, Quantitative analysis, K-12 education, Data mining across educational contexts},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706551,
author = {Yun, Joy and Nie, Allen and Brunskill, Emma and Demszky, Dorottya},
title = {Exploring the Benefit of Customizing Feedback Interventions For Educators and Students With Offline Contextual Multi-Armed Bandits},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706551},
doi = {10.1145/3706468.3706551},
abstract = {Automated feedback to teachers powered by natural language processing has been successful at improving instruction and student outcomes across various learning contexts. However, existing one-size-fits-all feedback interventions may not be equally effective for all educators and students. Understanding whether and how customization might enhance the effectiveness of automated feedback is a timely issue. In this paper we investigate this using data from a randomized controlled trial conducted on a peer SAT math tutoring program where tutors and/or learners were provided with post session feedback on their discourse during tutoring. We employ a partially data-driven, partially expert knowledge driven, process to propose some potential context-specific intervention policies. We then use offline contextual multi-armed bandit policy evaluation measures to estimate the potential performance of these interventions compared to providing a single intervention designed to maximize overall average performance. Our preliminary results did not show substantial or significant gains, but suggest that there may be value in providing differentiated interventions. More generally, our results point to the potential for such analysis to be used as a hypothesis-generating tool for future empirical studies.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {944–949},
numpages = {6},
keywords = {Education, Automated Feedback, Reinforcement Learning, Offline Contextual Multi-Armed Bandit, Policy Evaluation},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706550,
author = {Liu, Naiming and Sonkar, Shashank and Basu Mallick, Debshila and Baraniuk, Richard and Chen, Zhongzhou},
title = {Atomic Learning Objectives and LLMs Labeling: A High-Resolution Approach for Physics Education},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706550},
doi = {10.1145/3706468.3706550},
abstract = {This paper introduces a novel approach to create a high-resolution “map" for physics learning: an "atomic" learning objectives (LOs) system designed to capture detailed cognitive processes and concepts required for problem solving in a college-level introductory physics course. Our method leverages Large Language Models (LLMs) for automated labeling of physics questions and introduces a comprehensive set of metrics to evaluate the quality of the labeling outcomes. The atomic LO system, covering nine chapters of an introductory physics course, uses a “subject-verb-object” structure to represent specific cognitive processes. We apply this system to 131 questions from expert-curated question banks and the OpenStax University Physics textbook. Each question is labeled with 1-8 atomic LOs across three chapters. Through extensive experiments using various prompting strategies and LLMs, we compare automated LOs labeling results against human expert labeling. Our analysis reveals both the strengths and limitations of LLMs, providing insight into LLMs reasoning processes for labeling LOs and identifying areas for improvement in LOs system design. Our work contributes to the field of learning analytics by proposing a more granular approach to mapping learning objectives with questions. Our findings have significant implications for the development of intelligent tutoring systems and personalized learning pathways in STEM education, paving the way for more effective “learning GPS” systems.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {620–630},
numpages = {11},
keywords = {Physics Education, Learning Objectives},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706549,
author = {Mun, Soyeon and Jo, Il-Hyun},
title = {Evolving the 4C/ID model through learning analytics approaches: Teaching and learning system design framework for supporting learners' complex problem-solving},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706549},
doi = {10.1145/3706468.3706549},
abstract = {In recent years, there has been a surge in studies focusing on learning analytics (LA) aimed to collecting and analyzing learning trace data from various digital learning platforms. However, there is a need for these platforms to be designed from the ground up by LA experts, ensuring that data collection and analysis procedures align with the objectives and activities of teaching and learning as informed by established instructional theories. As a result, we developed a novel framework that integrates the 4C/ID model with LA approaches to enhance learners’ complex problem-solving abilities within online and blended learning environments. In developing this framework, we focused on the four core components of the original 4C/ID model along with the P-A-S cycle to determine optimal timing and methods for data collection and analysis. Our aim is to propose a framework that not only revisits and reinterprets the 4C/ID model but also fosters the development of a LA system embedded within digital learning platforms and closely tied to educational goals and learning activities. The findings of this study can serve as a valuable resource for designing and constructing adaptive teaching and learning systems, ultimately supporting learners in effectively cultivating their problem-solving skills.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {611–619},
numpages = {9},
keywords = {4C/ID model, Adaptive learning system design, Complex problem-solving, Data-informed decision-making, Learning analytics},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706548,
author = {He, Xinyun and Shu, Qi and Zhang, Mo and Huang, Wei and Zhao, Han and Zhu, Mengxiao},
title = {Beyond Final Products: Multi-Dimensional Essay Scoring Using Keystroke Logs and Deep Learning},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706548},
doi = {10.1145/3706468.3706548},
abstract = {Essay assessment plays a crucial role in evaluating students’ abilities in logical reasoning, critical thinking, and creativity. However, traditional manual scoring methods often suffer from inefficiencies due to fatigue, bias, and emotional factors, compromising objectivity. Automated Essay Scoring (AES) systems offer a more efficient and impartial alternative, yet most existing systems focus primarily on evaluating the final written product, overlooking the valuable data captured during the writing process. To address this issue, we introduce an innovative model called KAES, which explores the potential of integrating writing process data to enhance AES performance. The KAES model leverages a variety of data sources, including text content, prompts, keystroke dynamics, and manually extracted features, to extract meaningful insights. It employs a multi-task learning approach to evaluate essays across both linguistic and argumentative dimensions. Extensive experiments on the real-world CBAL dataset demonstrate that the KAES model significantly outperforms traditional baseline models, highlighting the effectiveness of incorporating writing process data into AES tasks.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {601–610},
numpages = {10},
keywords = {Automated Essay Scoring, Keystroke Logs, Deep learning, Feature Engineering, Multi-Task Learning},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706547,
author = {Park, Seehee and Nixon, Nia and D'Mello, Sidney and Shariff, Danielle and Choi, Jaeyoon},
title = {Understanding Collaborative Learning Processes and Outcomes Through Student Discourse Dynamics},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706547},
doi = {10.1145/3706468.3706547},
abstract = {This study explores the relation between students’ discourse dynamics and performance during collaborative problem-solving activities utilizing Linguistic Inquiry Word Count (LIWC). We analyzed linguistic variables from students’ communications to explore social and cognitive behavior. Participants include 279 undergraduate students from two U.S. universities engaged in a controlled lab setting using the physics related educational game named Physics Playground. Findings highlight the relationship between social and cognitive linguistic variables and student’s physics performance outcome in a virtual collaborative learning context. This study contributes to a deeper understanding of how these discourse dynamics are related to learning outcomes in collaborative learning. It provides insights for optimizing educational strategies in collaborative remote learning environments. We further discuss the potential for conducting computational linguistic modeling on learner discourse and the role of natural language processing in deriving insights on learning behavior to support collaborative learning.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {938–943},
numpages = {6},
keywords = {Learning Analytics, Discourse Analytics, Natural Language Processing, Collaborative Problem Solving, Virtual Learning Environment},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706546,
author = {Liu, Qinyi and Deho, Oscar and Vadiee, Farhad and Khalil, Mohammad and Joksimovic, Srecko and Siemens, George},
title = {Can Synthetic Data be Fair and Private? A Comparative Study of Synthetic Data Generation and Fairness Algorithms},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706546},
doi = {10.1145/3706468.3706546},
abstract = {The increasing use of machine learning in learning analytics (LA) has raised significant concerns around algorithmic fairness and privacy. Synthetic data has emerged as a dual-purpose tool, enhancing privacy and improving fairness in LA models. However, prior research suggests an inverse relationship between fairness and privacy, making it challenging to optimize both. This study investigates which synthetic data generators can best balance privacy and fairness, and whether pre-processing fairness algorithms, typically applied to real datasets, are effective on synthetic data. Our results highlight that the DEbiasing CAusal Fairness (DECAF) algorithm achieves the best balance between privacy and fairness. However, DECAF suffers in utility, as reflected in its predictive accuracy. Notably, we found that applying pre-processing fairness algorithms to synthetic data improves fairness even more than when applied to real data. These findings suggest that combining synthetic data generation with fairness pre-processing offers a promising approach to creating fairer LA models.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {591–600},
numpages = {10},
keywords = {Privacy, Synthetic Data Generation, Algorithmic Fairness, Fairness Metrics, Classifiers},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706545,
author = {Jin, Yueqiao and Yang, Kaixun and Yan, Lixiang and Echeverria, Vanessa and Zhao, Linxuan and Alfredo, Riordan and Milesi, Mikaela and Fan, Jie Xiang and Li, Xinyu and Gasevic, Dragan and Martinez-Maldonado, Roberto},
title = {Chatting with a Learning Analytics Dashboard: The Role of Generative AI Literacy on Learner Interaction with Conventional and Scaffolding Chatbots},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706545},
doi = {10.1145/3706468.3706545},
abstract = {Learning analytics dashboards (LADs) simplify complex learner data into accessible visualisations, providing actionable insights for educators and students. However, their educational effectiveness has not always matched the sophistication of the technology behind them. Explanatory and interactive LADs, enhanced by generative AI (GenAI) chatbots, hold promise by enabling dynamic, dialogue-based interactions with data visualisations and offering personalised feedback through text. Yet, the effectiveness of these tools may be limited by learners’ varying levels of GenAI literacy, a factor that remains underexplored in current research. This study investigates the role of GenAI literacy in learner interactions with conventional (reactive) versus scaffolding (proactive) chatbot-assisted LADs. Through a comparative analysis of 81 participants, we examine how GenAI literacy is associated with learners’ ability to interpret complex visualisations and their cognitive processes during interactions with chatbot-assisted LADs. Results show that while both chatbots significantly improved learner comprehension, those with higher GenAI literacy benefited the most, particularly with conventional chatbots, demonstrating diverse prompting strategies. Findings highlight the importance of considering learners’ GenAI literacy when integrating GenAI chatbots in LADs and educational technologies. Incorporating scaffolding techniques within GenAI chatbots can be an effective strategy, offering a more guided experience that reduces reliance on learners’ GenAI literacy.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {579–590},
numpages = {12},
keywords = {learning analytics dashboard, generative AI literacy, generative AI chatbots, data visualisation},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706544,
author = {Yang, Kaixun and Rakovi\'{c}, Mladen and Liang, Zhiping and Yan, Lixiang and Zeng, Zijie and Fan, Yizhou and Ga\v{s}evi\'{c}, Dragan and Chen, Guanliang},
title = {Modifying AI, Enhancing Essays: How Active Engagement with Generative AI Boosts Writing Quality},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706544},
doi = {10.1145/3706468.3706544},
abstract = {Students are increasingly relying on Generative AI (GAI) to support their writing—a key pedagogical practice in education. In GAI-assisted writing, students can delegate core cognitive tasks (e.g., generating ideas and turning them into sentences) to GAI while still producing high-quality essays. This creates new challenges for teachers in assessing and supporting student learning, as they often lack insight into whether students are engaging in meaningful cognitive processes during writing or how much of the essay’s quality can be attributed to those processes. This study aimed to help teachers better assess and support student learning in GAI-assisted writing by examining how different writing behaviors, especially those indicative of meaningful learning versus those that are not, impact essay quality. Using a dataset of 1,445 GAI-assisted writing sessions, we applied the cutting-edge method, X-Learner, to quantify the causal impact of three GAI-assisted writing behavioral patterns (i.e., seeking suggestions but not accepting them, seeking suggestions and accepting them as they are, and seeking suggestions and accepting them with modification) on four measures of essay quality (i.e., lexical sophistication, syntactic complexity, text cohesion, and linguistic bias). Our analysis showed that writers who frequently modified GAI-generated text—suggesting active engagement in higher-order cognitive processes—consistently improved the quality of their essays in terms of lexical sophistication, syntactic complexity, and text cohesion. In contrast, those who often accepted GAI-generated text without changes, primarily engaging in lower-order processes, saw a decrease in essay quality. Additionally, while human writers tend to introduce linguistic bias when writing independently, incorporating GAI-generated text—even without modification—can help mitigate this bias.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {568–578},
numpages = {11},
keywords = {GAI-assisted Writing, Causal Inference, Writing Quality, Linguistic Bias},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706543,
author = {Ahrar, Arash and Doroodian, Mohammadreza and Hatala, Marek},
title = {Exploring Eye-tracking Features to Understand Students' Sensemaking of Learning Analytics Dashboards},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706543},
doi = {10.1145/3706468.3706543},
abstract = {Learning analytics dashboards (LADs) are widely used in learning analytics as visual tools to present information about learning activities and outcomes. However, only few studies have explored how students make sense from LAD elements and what cognitive processes follow after viewing each element. In this study, we explore how eye-tracking data can help researchers to identify salient LAD elements critical to students’ sensemaking process. Our findings reveal that the eye-tracking derived features, including fixation duration and eye movement patterns, are highly indicative of students’ social comparison tendencies and offer valuable insights into their sensemaking processes.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {931–937},
numpages = {7},
keywords = {learning analytics dashboards, eye tracking, sensemaking, motivation},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706542,
author = {Li, Fanjie and Wise, Alyssa Friend},
title = {From Filling Gaps to Amplifying Strengths: Exploring an Asset-Based Approach to Learning Analytics},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706542},
doi = {10.1145/3706468.3706542},
abstract = {This paper explores how an asset-based lens can help expand the design space of learning analytics beyond a deficit-oriented approach focused mainly on identifying and remedying gaps to one that elevates every student’s strengths and potentials. To do so, we draw on the rich history of asset-based pedagogies to consider expansive possibilities for the kinds of information that analytics uncover, the processes that analytics support, and the outcomes that analytics seek to engender. To explore the value and feasibility of an asset-based lens for learning analytics, this paper instantiates the approach with a proof-of-concept prototype designed to support teachers’ noticing of student contributions to discussions in a K-12 science learning classroom. The illustrative case demonstrates ways in which the analytic capabilities of large language models can be leveraged to make visible, and create opportunities for teachers to build upon, the funds of knowledge that students bring to the classroom, thereby creating spaces for minoritized students’ agentive engagement.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {924–930},
numpages = {7},
keywords = {Educational equity, Generative AI, Value-sensitive design, Classroom discourse analytics},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706541,
author = {Claassen, Alrike and Mirriahi, Negin and Kovanovi\'{c}, Vitomir and Dawson, Shane},
title = {From Data to Design: Integrating Learning Analytics into Educational Design for Effective Decision-Making: From Data to Design},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706541},
doi = {10.1145/3706468.3706541},
abstract = {Learning Analytics (LA) aims to provide university instructors with meaningful data and insights that can be used to improve courses. However, instructors are often met with challenges that arise when wanting to use LA to inform their educational design decisions. For instance, there may be a misalignment between instructors’ needs and the data and insights LA systems provide. Further research is required to understand instructors’ expectations of LA and how it can support the diversity of educational designs. This case study addresses this gap by investigating the role of LA in instructors’ educational decision-making processes. The study employs self-determination theory's constructs to examine instructors’ existing practices when using LA to support their decision-making. The study reveals that LA enables instructors to make data-informed iterative educational design decisions, supporting their need for competence and relatedness. The emotional aspect of LA is an important consideration that can easily lead to demotivation and avoidance of LA. Support is needed to address instructors’ psychological needs so instructors can fully utilise LA to make effective educational design decisions. The findings inform a framework for considering how instructors’ data-informed educational decision-making can be understood. The implications of our findings and opportunities for the future are discussed.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {558–567},
numpages = {10},
keywords = {Learning analytics, data-informed decision-making, educational design, higher education, self-determination theory},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706540,
author = {Choi, Jaeyoon and Karumbaiah, Shamya and Matayoshi, Jeffrey},
title = {Bias or Insufficient Sample Size? Improving Reliable Estimation of Algorithmic Bias for Minority Groups},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706540},
doi = {10.1145/3706468.3706540},
abstract = {Despite the prevalent use of predictive models in learning analytics, several studies have demonstrated that these models can show disparate performance across different demographic groups of students. The first step to audit for and mitigate bias is to accurately estimate it. However, the current practice of identifying and measuring group bias faces reliability issues. In this paper, we use simulations and real-world data analysis to explore statistical factors that impact the reliability of bias estimation and suggest approaches to account for it. Our analysis revealed that small group sizes lead to high variability in group bias estimation due to sampling error – an issue that is more likely to impact students from historically marginalized communities. We then suggest statistical approaches, such as bootstrapping, to construct confidence intervals for a more reliable estimation of group bias. Based on our findings, we encourage future learning analytics research to ensure sufficiently large group sizes, construct confidence intervals, use at least two metrics, and move beyond the dichotomy of the presence or absence of bias for a more comprehensive evaluation of group bias.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {547–557},
numpages = {11},
keywords = {Algorithmic bias, Fairness, Group bias, Estimation of bias, Reliability, Predictive models},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706539,
author = {Zhu, Wangda and Xing, Wanli and Lyu, Bailing and Li, Chenglu and Zhang, Fan and Li, Hai},
title = {Bridging the Gender Gap: The Role of AI-Powered Math Story Creation in Learning Outcomes},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706539},
doi = {10.1145/3706468.3706539},
abstract = {Addressing the gender gap in K-12 math education is essential for providing equitable learning opportunities, as historical disparities in engagement, performance, and confidence between male and female students in mathematics are often linked to educational biases. Integrating Generative AI (GAI) into math education shows promise for bridging the gender gap in K12 math learning. This study proposes an innovative pedagogy and platform that enables students to create math stories powered by GAI, enhancing their conceptual understanding of key mathematical ideas. The platform was implemented in two K5 schools to evaluate its effectiveness and mechanism (N = 86). Pre- and post-intervention surveys and usage logs indicated significant improvements in students’ learning outcomes regarding Math Question (MQ) skills and Math Story (MS) skills. Bayes SEM further modeled the mechanism: students’ creating math stories powered by GAI significantly improves MS, which further improves MQ. We further found female students were significantly more engaged in creating stories on this platform and gained more improvement on MQ than male students. The results suggest that AI-powered math story creation can be an effective tool for deepening students’ mathematical learning outcomes and has the potential to mitigate the gender gap.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {918–923},
numpages = {6},
keywords = {Gender gap, Generative AI, Math story, Learning outcomes},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706538,
author = {Wang, Yeyu and Carpenter, Zack and Swiecki, Zachari and Shaffer, David Williamson},
title = {Qualitative Parameter Triangulation: A Conceptual and Methodological Framework for Event-Based Temporal Models},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706538},
doi = {10.1145/3706468.3706538},
abstract = {Learning is a complex process that occurs over time. To represent this complex process, interests has been rising in conceptualizing and integrating temporality into model constructions. However, the construction of an event-based temporal model is challenging. Specifically, researchers struggle with translating qualitative heuristics and theoretical hypotheses into quantifiable temporal parameters. Existing methods of parameter derivation also suffer from issues of model transparency and oversimplification of learning contexts. Thus, we proposed a conceptual and methodological framework, Qualitative Parameter Triangulation (QPT), to center human interpretation in model construction. Based on human interpretations, QPT constructs a qualitative loss function and derives temporal parameters using an automatical optimization algorithm. The final step is to check consistency between a global representation with local qualitative evidence given specific learning moments. By presenting a worked example of QPT, we demonstrated the process of maintaining pairwise alignments across interpretation, systematization, and approxi-gation. As a proof of concept, QPT is a feasible framework for determining temporal parameters and constructing event-based temporal models.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {537–546},
numpages = {10},
keywords = {Temporal Analysis, Interpretivity, Methodology, Event-based Modeling.},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706537,
author = {Li, Hai and Xing, Wanli and Li, Chenglu and Zhu, Wangda and Lyu, Bailing and Zhang, Fan and Liu, Zifeng},
title = {Who Should Be My Tutor? Analyzing the Interactive Effects of Automated Text Personality Styles Between Middle School Students and a Mathematics Chatbot},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706537},
doi = {10.1145/3706468.3706537},
abstract = {Engaging with instructors through question-and-response techniques is an efficient method for delivering mathematics instruction to middle school learners. The flexible nature and sophisticated functionality of large language models (LLMs) have fueled interest in automating this process to strengthen students’ mathematical understanding, with the chatbot’s personality serving as an essential aspect of its design. While much research has explored students’ preferences for chatbot personalities, preferences in the context of learning gains, considering students’ own personalities, remain unclear. This study draws on QA dialogue logs between middle school students and a chatbot from a U.S.-based online mathematics learning platform. An automated feature extraction framework was designed to analyze text style from a personality perspective, extracting features including emotional polarity (reflecting emotional arousal), subjectivity (degree of subjective-neutral expression), and the big five personality traits (indicating potential personality tendencies). Linear regression was then used to analyze the relationship between these features and students’ learning gains in mathematics. Our findings support the complementary hypothesis from interpersonal interaction theory, which posits that students prefer chatbot personalities that complement their own. We discuss the implications for instructional design. Our analysis contributes to the development of more effective conversational AI applications in educational technology.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {910–917},
numpages = {8},
keywords = {conversational AI, chatbot personality, text style},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706536,
author = {Pan, Hongchen and Araujo Oliveira, Eduardo and Ferreira Mello, Rafael},
title = {Exploring Human-AI Collaboration in Educational Contexts: Insights from Writing Analytics and Authorship Attribution},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706536},
doi = {10.1145/3706468.3706536},
abstract = {This research investigates the characteristics of student essays written with and without generative AI assistance, using stylometric analysis and deep learning techniques to explore human-AI collaboration in academic writing. To address three research questions, the study examines: (1) patterns in vocabulary diversity, sentence structure, and readability in AI-generated versus student-written essays; (2) the development of a stylometry-based BERT model for authorship attribution, focusing on linguistic features to accurately distinguish between student and AI-generated content; and (3) the application of this model to measure AI involvement at the sentence level in collaborative essays. Using a dataset of student and AI-assisted essays, we observed distinct stylistic differences, with AI-generated content exhibiting higher lexical diversity and readability scores. The BERT model demonstrated high accuracy (85%), precision (79%), and F1-scores (74%) in identifying AI contributions, surpassing the adopted baseline. While limitations such as dataset imbalance and variability in AI outputs remain, this study highlights the potential of stylometric analysis in improving authorship attribution and quantifying AI involvement in academic writing. These findings provide educators with tools to monitor student progress, offer personalised feedback, and maintain academic integrity in the face of growing AI usage in education.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {903–909},
numpages = {7},
keywords = {Generative AI, Authorship Attribution, Writing Analytics},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706535,
author = {Williamson, Kimberly and Kizilcec, Rene and Fath, Sean and Heffernan, Neil},
title = {Algorithm Appreciation in Education: Educators Prefer Complex over Simple Algorithms},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706535},
doi = {10.1145/3706468.3706535},
abstract = {Algorithm aversion among educators can pose a challenge to the adoption of AI tools in education, especially when complex algorithms are involved. This study investigates how providing explanations for a complex algorithm in an intelligent tutoring system (ITS) affects educators’ attitudes, trust, and willingness to adopt the tool. In two randomized experiments (N = 570), we compare educator preferences between a simple heuristic algorithm and a complex (Bayesian Knowledge Tracing) algorithm, focusing on how explanations for the complex algorithm can improve attitudes and adoption. Surprisingly, we found that educators generally preferred the complex over the simple algorithm, and explanations did not improve attitudes or adoption intentions, even when educators had to explain the complex algorithm’s predictions. The complex algorithm scored lower on informational fairness than the simple one, considering it is less transparent, and the explanation was insufficient to overcome this. Overall, the findings suggest that widespread algorithm aversion may have evolved into algorithm appreciation, at least in the context of widely used technologies like ITS.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {527–536},
numpages = {10},
keywords = {Algorithm Aversion, Algorithm Appreciation, AI Literacy, XAI},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706534,
author = {Zhao, Linxuan and Rakovi\'{c}, Mladen and B. Cloude, Elizabeth and Li, Xinyu and Ga\v{s}evi\'{c}, Dragan and Bardach, Lisa},
title = {The Effect of Sequential Transition of Self-Regulated Learning Processes on Performance: Insights from Ordered Network Analysis},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706534},
doi = {10.1145/3706468.3706534},
abstract = {Productively engaging in SRL is challenging for learners since it involves coordinating multiple motivational, affective, cognitive, and metacognitive processes. Researchers have investigated methods to adaptively scaffold learners’ productive engagement using SRL processes automatically captured by SRL detectors. However, most previous studies relied solely on the frequency of SRL processes to drive adaptive scaffolds (e.g., feedback, hints), possibly missing the sequential characteristics inherent to self-regulation, a crucial dimension of productive SRL. To address this gap, this study analysed the impact of sequential transitions between multiple SRL processes on learners’ performance on a reading-writing task with a hypermedia environment called Flora. A sample of 66 secondary-school learners completed the task and trace data were collected. Grounded in the COPES model of SRL, a rule-based SRL detector was employed to capture SRL processes from collected trace data. We employed a method combining logistic regression with ordered network analysis (ONA) to analyse the transitions between the detected SRL processes. This exploratory study revealed several influential transitions to learners’ performance in different temporal learning blocks of self-regulation. The implications suggest the potential of using COPES SRL process transitions to drive adaptive scaffolds to facilitate engagement in productive SRL, benefiting performance outcomes in hypermedia environments.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {516–526},
numpages = {11},
keywords = {Self-regulated learning, Learning analytics, Ordered network analysis, Learning strategies},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706533,
author = {Ortega-Arranz, Alejandro and Topali, Paraskevi and Molenaar, Inge},
title = {Configuring and Monitoring Students' Interactions with Generative AI Tools: Supporting Teacher Autonomy},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706533},
doi = {10.1145/3706468.3706533},
abstract = {The widespread use of Generative Artificial Intelligence (GenAI) tools, such as ChatGPT, has come along with multiple benefits in education (e.g., 24h teacher, augmenting student monitoring). However, at the same time, these tools hinder teachers’ autonomy, limiting the capacity and freedom to exert control over students’ actions and their learning process. Additionally, the generic character of the GenAI output usually lacks contextualization (e.g., course curriculum, students’ age), thus hampering the successful attainment of the course goals. To address these issues, this paper proposes the development of a system mediating between the GenAI interfaces and their back-ends. This system allows teachers to monitor the students’ interactions and align the given answers with the course learning objectives and teaching methods. This research follows the Systems Development Research methodology, and within the first iteration, we developed a system prototype that was evaluated with 8 secondary-school teachers. Results showed a high perceived usefulness of the system for monitoring students’ interactions; for alerting the teachers to take specific actions (e.g., suspicious copy-paste behaviours), and for having control over the GenAI outputs. Additionally, while most teachers perceived a higher autonomy level within the given scenarios, some teachers did not. The evaluation also served to collect further requirements and usability features to keep improving the tool in the next methodological iterations.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {895–902},
numpages = {8},
keywords = {Generative AI, Learning Analytics, GenAI Analytics, Human-Centred Design, Teachers},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706532,
author = {Lyu, Bailing and Li, Chenglu and Li, Hai and Oh, Hyunju and Song, Yukyeong and Zhu, Wangda and Xing, Wanli},
title = {Exploring the Role of Teachable AI Agents’ Personality Traits in Shaping Student Interaction and Learning in Mathematics Education},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706532},
doi = {10.1145/3706468.3706532},
abstract = {With advances in artificial intelligence (AI), educational researchers have integrated AI into mathematics education to offer scalable instructional practices and personalized learning. One such innovation is teachable AI agents, designed as learners to facilitate learning by teaching. Previous research has evidenced the benefits of learning by teaching, and its effectiveness depends on the quality of tutor-tutee interaction. However, few studies have explored how features of teachable agents, particularly personality traits, influence student interactions and the agents’ effectiveness. Given the documented importance of personality traits in student learning, this empirical study examines the relationship between teachable AI agents’ personality traits and students’ math learning experiences in a naturalistic setting. Results indicated that students provided more cognitive support when interacting with teachable agents characterized by neuroticism, openness, and conscientiousness, while more affect management and non-responsive behaviors were observed with agents displaying extraversion. These interaction patterns impacted the effectiveness of the teachable agents, providing implications for the integration of AI systems into education.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {887–894},
numpages = {8},
keywords = {Teachable agent, mathematics education, personality traits},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706531,
author = {Thomas, Danielle R and Borchers, Conrad and Kakarla, Sanjit and Lin, Jionghao and Bhushan, Shambhavi and Guo, Boyuan and Gatz, Erin and Koedinger, Kenneth R},
title = {Do Tutors Learn from Equity Training and Can Generative AI Assess It?},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706531},
doi = {10.1145/3706468.3706531},
abstract = {Equity is a core concern of learning analytics. However, applications that teach and assess equity skills, particularly at scale are lacking, often due to barriers in evaluating language. Advances in generative AI via large language models (LLMs) are being used in a wide range of applications, with this present work assessing its use in the equity domain. We evaluate tutor performance within an online lesson on enhancing tutors’ skills when responding to students in potentially inequitable situations. We apply a mixed-method approach to analyze the performance of 81 undergraduate remote tutors. We find marginally significant learning gains with increases in tutors’ self-reported confidence in their knowledge in responding to middle school students experiencing possible inequities from pretest to posttest. Both GPT-4o and GPT-4-turbo demonstrate proficiency in assessing tutors ability to predict and explain the best approach. Balancing performance, efficiency, and cost, we determine that few-shot learning using GPT-4o is the preferred model. This work makes available a dataset of lesson log data, tutor responses, rubrics for human annotation, and generative AI prompts. Future work involves leveling the difficulty among scenarios and enhancing LLM prompts for large-scale grading and assessment.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {505–515},
numpages = {11},
keywords = {Tutor Training, Generative AI, Large Language Models, Assessment, Equity},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706530,
author = {Thomas, Danielle R and Borchers, Conrad and Kakarla, Sanjit and Lin, Jionghao and Bhushan, Shambhavi and Guo, Boyuan and Gatz, Erin and Koedinger, Kenneth R},
title = {Does Multiple Choice Have a Future in the Age of Generative AI? A Posttest-only RCT},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706530},
doi = {10.1145/3706468.3706530},
abstract = {The role of multiple-choice questions (MCQs) as effective learning tools has been debated in past research. While MCQs are widely used due to their ease in grading, open response questions are increasingly used for instruction, given advances in large language models (LLMs) for automated grading. This study evaluates MCQs effectiveness relative to open-response questions, both individually and in combination, on learning. These activities are embedded within six tutor lessons on advocacy. Using a posttest-only randomized control design, we compare the performance of 234 tutors (790 lesson completions) across three conditions: MCQ only, open response only, and a combination of both. We find no significant learning differences across conditions at posttest, but tutors in the MCQ condition took significantly less time to complete instruction. These findings suggest that MCQs are as effective, and more efficient, than open response tasks for learning when practice time is limited. To further enhance efficiency, we autograded open responses using GPT-4o and GPT-4-turbo. GPT models demonstrate proficiency for purposes of low-stakes assessment, though further research is needed for broader use. This study contributes a dataset of lesson log data, human annotation rubrics, and LLM prompts to promote transparency and reproducibility.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {494–504},
numpages = {11},
keywords = {Tutoring, Generative AI, Human-AI tutoring, AI-assisted tutoring, Assessment},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706529,
author = {Hassany, Mohammad and Brusilovsky, Peter and Savelka, Jaromir and Lekshmi Narayanan, Arun Balajiee and Akhuseyinoglu, Kamil and Agarwal, Arav and Hendrawan, Rully Agus},
title = {Generating Effective Distractors for Introductory Programming Challenges: LLMs vs Humans},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706529},
doi = {10.1145/3706468.3706529},
abstract = {As large language models (LLMs) show great promise in generating a wide spectrum of educational materials, robust yet cost-effective assessment of the quality and effectiveness of such materials becomes an important challenge. Traditional approaches, including expert-based quality assessment and student-centered evaluation, are resource-consuming, and do not scale efficiently. In this work, we explored the use of pre-existing student learning data as a promising approach to evaluate LLM-generated learning materials. Specifically, we used a dataset where students were completing the program construction challenges by picking the correct answers among human-authored distractors to evaluate the quality of LLM-generated distractors for the same challenges. The dataset included responses from 1,071 students across 22 classes taught from Fall 2017 to Spring 2023. We evaluated five prominent LLMs (OpenAI-o1, GPT-4, GPT-4o, GPT-4o-mini, and Llama-3.1-8b) across three different prompts to see which combinations result in more effective distractors, i.e., those that are plausible (often picked by students), and potentially based on common misconceptions. Our results suggest that GPT-4o was the most effective model, matching close to 50% of the functional distractors originally authored by humans. At the same time, all of the evaluated LLMs generated many novel distractors, i.e., those that did not match the pre-existing human-authored ones. Our preliminary analysis shows that those appear to be promising. Establishing their effectiveness in real-world classroom settings is left for future work.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {484–493},
numpages = {10},
keywords = {Large Language Models (LLMs), Distractor Generation and Evaluation, Student Learning Data, Introductory Programming, GPT, LLaMA},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706528,
author = {Fang, Yu and Huang, Shihong and Ogan, Amy},
title = {A Cross-Cultural Confusion Model for Detecting and Evaluating Students’ Confusion In a Large Classroom},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706528},
doi = {10.1145/3706468.3706528},
abstract = {In traditional lecture delivery setting, it is very challenging to identify which part of the lecture material that students are struggling with. One approach to identify difficult concepts is to capture students’ confusion during class time. However, most existing confusion detectors focus on an individual student rather than a classroom, and only on a single ethnicity group which could propagate bias when developing pedagogical technologies. In this paper, we leverage two existing ‘Confused’ facial expression datasets (DAiSEE and DevEmo) with an East Asian ‘Confused’ facial expression dataset that we collected. Through model performance and explainableAI, we address potential cultural biases in detecting emotions, particularly in confusion, and identified culturally-specific features that align with prior research. As a proof-of-concept, we deployed this cross-cultural confusion machine learning model in a live semester-long class. This work to integrate cross-cultural facial features highlights the importance of fostering inclusivity in educational technologies.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {473–483},
numpages = {11},
keywords = {Cross-cultural models, Confusion, Affective computing, Retrieval-augmented generation},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706527,
author = {Se\ss{}ler, Kathrin and F\"{u}rstenberg, Maurice and B\"{u}hler, Babette and Kasneci, Enkelejda},
title = {Can AI grade your essays? A comparative analysis of large language models and teacher ratings in multidimensional essay scoring},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706527},
doi = {10.1145/3706468.3706527},
abstract = {The manual assessment and grading of student writing is a time-consuming yet critical task for teachers. Recent developments in generative AI offer potential solutions to facilitate essay-scoring tasks for teachers. In our study, we evaluate the performance (e.g. alignment and reliability) of both open-source and closed-source LLMs in assessing German student essays, comparing their evaluations to those of 37 teachers across 10 pre-defined criteria (i.e., plot logic, expression). A corpus of 20 real-world essays from Year 7 and 8 students was analyzed using five LLMs: GPT-3.5, GPT-4, o1-preview, LLaMA 3-70B, and Mixtral 8x7B, aiming to provide in-depth insights into LLMs’ scoring capabilities. Closed-source GPT models outperform open-source models in both internal consistency and alignment with human ratings, particularly excelling in language-related criteria. The o1 model outperforms all other LLMs, achieving Spearman’s r =.74 with human assessments in the Overall score, and an internal consistency of ICC =.80, though biased towards higher scores. These findings indicate that LLM-based assessment can be a useful tool to reduce teacher workload by supporting the evaluation of essays, especially with regard to language-related criteria. However, due to their tendency to overrate and their remaining issues to capture the content quality, the models require further refinement.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {462–472},
numpages = {11},
keywords = {Large Language Models, Automated Essay Scoring, Learning Analytics, Education},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706526,
author = {Huang, Kevin and Ferreira Mello, Rafael and Pereira Junior, Cleon and Rodrigues, Luiz and Baars, Martine and Viberg, Olga},
title = {That's What RoBERTa Said: Explainable Classification of Peer Feedback},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706526},
doi = {10.1145/3706468.3706526},
abstract = {Peer feedback (PF) is essential for improving student learning outcomes, particularly in Computer-Supported Collaborative Learning (CSCL) settings. When using digital tools for PF practices, student data (e.g., PF text entries) is generated automatically. Analyzing these large datasets can enhance our understanding of how students learn and help improve their learning. However, manually processing these large datasets is time-intensive, highlighting the need for automation. This study investigates the use of six machine learning models to classify PF messages from 231 students in a large university course. The models include Multi-Layer Perceptron (MLP), Decision Tree, BERT, RoBERTa, DistilBERT, and ChatGPT4o. The models were evaluated based on Cohen’s accuracy and F1-score. Preprocessing involved removing stop words, and the impact of this on model performance was assessed. Results showed that only the Decision Tree model improved with stop-word removal, while performance decreased in the other models. RoBERTa consistently outperformed the others across all metrics. Explainable AI was used to understand RoBERTa’s decisions by identifying the most predictive words. This study contributes to the automatic classification of peer feedback which is crucial for scaling learning analytics efforts aiming to provide better in-time support to students in CSCL settings.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {880–886},
numpages = {7},
keywords = {Peer feedback, Higher Education, Machine Learning, Explainable artificial intelligence, Computer Supported Collaborative Learning.},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706525,
author = {Li, Zaibei and Yamaguchi, Shunpei and Spikol, Daniel},
title = {OpenMMLA: an IoT-based Multimodal Data Collection Toolkit for Learning Analytics},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706525},
doi = {10.1145/3706468.3706525},
abstract = {Multimodal Learning Analytics (MMLA) expands traditional learning analytics into the digital and physical learning environment, using diverse sensors and systems to collect information about education in more real-world environments. Challenges remain in making these technologies practical for capturing data in authentic learning situations. With the advent of readily accessible powerful artificial intelligence that includes multimodal large language models, new opportunities are available. However, few approaches allow access to these technologies, and most systems are developed for specific environments. Recent work has begun to make toolkits with access to collecting data from sensors, processing, and analyzing, yet these tools are challenging to integrate into a system. This paper introduces OpenMMLA, a toolkit approach that provides programming interfaces for harnessing these technologies into an MMLA platform with prebuilt pipelines, including the audio analyzer, indoor positioning, and video frame analyzer, offering multimodal data collection and visualizations and analytics. The paper provides an initial evaluation of the functionalities of the toolkit in data capturing and the implemented pipelines’ performances.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {872–879},
numpages = {8},
keywords = {Multimodal Learning Analytics, Group Work, Internet of Thing, Smart Badges},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706524,
author = {Russell, Jae-Eun and Smith, Anna Marie and George, Salim and Pratt, Jonah and Fodale, Brian and Monk, Cassandra and Brummett, Adam},
title = {Unlocking Insights: Investigating Student AI Tutor Interactions in a Large Introductory STEM Course},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706524},
doi = {10.1145/3706468.3706524},
abstract = {This study explored the use of an AI tutor and its relationship to performance outcomes in a large introductory undergraduate STEM course, where the AI tutor was integrated into the online homework system. The course included 13 weekly homework assignments, comprising 221 questions that contributed 19.5% to the final grade. Results showed that students predominantly completed homework problems without AI tutor assistance, using it selectively to address specific challenges. Patterns of AI interaction varied at both the problem and student levels, with demographic factors having little to no relationship to AI usage. Notably, the frequency of AI use was not linked to exam performance. A multi-level cluster analysis identified distinct patterns in students’ use of the AI tutor during problem-solving. These patterns of use had more significant associations with performance than frequency of use alone. This paper explores these interaction patterns in depth and discusses the study’s limitations and implications.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {451–461},
numpages = {11},
keywords = {AI tutors, Higher Education, STEM},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706523,
author = {Khalil, Mohammad and Vadiee, Farhad and Shakya, Ronas and Liu, Qinyi},
title = {Creating Artificial Students that Never Existed: Leveraging Large Language Models and CTGANs for Synthetic Data Generation},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706523},
doi = {10.1145/3706468.3706523},
abstract = {In this study, we explore the growing potential of AI and deep learning technologies, particularly Generative Adversarial Networks (GANs) and Large Language Models (LLMs), for generating synthetic tabular data. Access to quality students’ data is critical for advancing learning analytics, but privacy concerns and stricter data protection regulations worldwide limit their availability and usage. Synthetic data offers a promising alternative. We investigate whether synthetic data can be leveraged to create artificial students for serving learning analytics models. Using the popular GAN model- CTGAN and three LLMs- GPT2, DistilGPT2, and DialoGPT, we generate synthetic tabular student data. Our results demonstrate the strong potential of these methods to produce high-quality synthetic datasets that resemble real students’ data. To validate our findings, we apply a comprehensive set of utility evaluation metrics to assess the statistical and predictive performance of the synthetic data and compare the different generator models used, specially the performance of LLMs. Our study aims to provide the learning analytics community with valuable insights into the use of synthetic data, laying the groundwork for expanding the field’s methodological toolbox with new innovative approaches for learning analytics data generation.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {439–450},
numpages = {12},
keywords = {Synthetic Data Generation; Artificial data; Learning Analytics (LA); Artificial Intelligence for Education (AIED); Large Language Models (LLMs); Conditional Tabular GAN (CTGAN); Deep Learning},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706522,
author = {Acosta, Halim and Hong, Daeun and Lee, Seung and Min, Wookhee and Mott, Bradford and Hmelo-Silver, Cindy and Lester, James},
title = {Collaborative Game-based Learning Analytics: Predicting Learning Outcomes from Game-based Collaborative Problem Solving Behaviors},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706522},
doi = {10.1145/3706468.3706522},
abstract = {Skills in collaborative problem solving (CPS) are essential for the 21st century, enabling students to solve complex problems effectively. As the demand for these skills rises, understanding their development and manifestation becomes increasingly important. To address this need, we present a data-driven framework that identifies behavioral patterns associated with CPS practices and can assess students’ learning outcomes. It provides explainable insights into the relationship between students’ behaviors and learning performance. We employ embedding and clustering techniques to categorize similar trace logs and apply Latent Dirichlet allocation to generate meaningful descriptors. To capture the temporal evolution of student behaviors, we introduce a graph-based representation of transitions between behavior patterns extracted using constraint-based pattern mining. We map behavioral patterns to a CPS ontology by analyzing how action sequences correspond to specific CPS practices. Analysis of semi-structured trace log data from 61 middle school students engaged in collaborative game-based learning reveals that the extracted behavioral patterns significantly predict student learning gains using generalized additive models. Our analysis identifies patterns that provide insights into the relationship between student use of CPS practices and learning outcomes.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {429–438},
numpages = {10},
keywords = {Collaborative Game-Based Learning, Collaborative Problem Solving, Trace Log Analysis, Explainable Machine Learning},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706521,
author = {Lee, HaeJin and Belitz, Clara and Nasiar, Nidhi and Bosch, Nigel},
title = {XAI Reveals the Causes of Attention Deficit Hyperactivity Disorder (ADHD) Bias in Student Performance Prediction},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706521},
doi = {10.1145/3706468.3706521},
abstract = {Uncovering algorithmic bias related to sensitive attributes is crucial. However, understanding the underlying causes of bias is even more important to ensure fairer outcomes. This study investigates bias associated with Attention Deficit Hyperactivity Disorder (ADHD) in a machine learning model predicting students’ test scores. While fairness metrics did not reveal significant bias, potential subtle bias indicated by variations in model performance for students with ADHD was observed. To uncover causes of this potential bias, we correlated SHapley Additive exPlanations (SHAP) values with the model’s prediction errors, identifying the features most strongly associated with increasing prediction errors. Behavioral and self-reported survey features designed to measure students’ use of effective learning strategies were identified as potential causes of the model underestimating test grades for students with ADHD. Behavioral features had a stronger correlation between absolute SHAP values and prediction errors (up to r =.354, p =.013) for students with ADHD than for those without ADHD. Students with ADHD often use unique yet effective approaches to studying in online learning environments—approaches that may not be fully captured by traditional measures of typical student behaviors. These insights suggest adjusting feature design to better account for students with ADHD and mitigate bias.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {418–428},
numpages = {11},
keywords = {Explainable AI, Algorithmic bias, Machine Learning, Self-regulated Learning, Attention Deficit Hyperactivity Disorder},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706520,
author = {Albuquerque, Josmario and Rienties, Bart and Divjak, Bla\v{z}enka},
title = {Decoding Learning Design Decisions: A Cluster Analysis of 12,749 Teaching and Learning Activities},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706520},
doi = {10.1145/3706468.3706520},
abstract = {Substantial progress has been made in how educators can be supported to implement effective learning design (LD) with learning analytics (LA). However, how educators make micro-decisions about designing individual teaching and learning activities (TLAs) and how these are related to wider pedagogical approaches has received limited empirical support. This study explored how 165 educators designed and integrated 12,749 TLA in 218 LDs using clustering, pattern-mining, and correlational analysis. The findings suggest most educators use a combination of four common LD TLAs (i.e., Collaboration, Generating independent learning, Assessment, and Traditional classroom activities). The four common TLAs could be used to develop LA and Generative Artificial Intelligence (Gen-AI) approaches to support educators in making more informed and evidence-based design decisions for effective learning and teaching.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {407–417},
numpages = {11},
keywords = {Learning Design, Learning Analytics, Cluster Analysis, Teaching and Learning Activities, Artificial Intelligence},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706519,
author = {Aguinalde, Anna Pauline and Shin, Jinnie},
title = {Talking in Sync: How Linguistic Synchrony Shapes Teacher-Student Conversation in English as a Second Language Tutoring Environment},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706519},
doi = {10.1145/3706468.3706519},
abstract = {Linguistic synchrony, or alignment, has been shown to be critical for student learning, particularly for L2 students (second language learners), whose patterns of synchrony often differ from fluent speakers due to proficiency constraints. While many studies have explored various dimensions of synchrony in global language tutoring contexts, there is a gap in understanding how linguistic synchrony evolves dynamically over the course of a tutoring session and how tutors’ pedagogical strategies influence this process. This study incorporates three dimensions of synchrony—lexical, syntactic, and semantic—along with tutors’ dialogue acts to evaluate their association with student performance using multivariate time-series analysis. Results indicate that lower-performing L2 students tend to lexically align with their tutor more consistently in the long term and with higher intensity in the short term. In contrast, higher-performing students demonstrate greater alignment with the tutor in syntactic and semantic dimensions. Furthermore, the dialogue acts of eliciting, scaffolding, and enquiry were found to play the strongest roles in influencing synchrony and impacting learning outcomes.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {395–406},
numpages = {12},
keywords = {Linguistic Synchrony, Dialogue acts, Language learner, Tutoring Conversation, Time-series analysis},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706518,
author = {Hern\'{a}ndez-Campos, M\'{o}nica and Hilliger, Isabel and Garc\'{\i}a-Pe\~{n}alvo, Francisco-Jos\'{e}},
title = {Evaluating Learning Outcomes Through Curriculum Analytics: Actionable Insights for Curriculum Decision-making: A Design-based research approach to assess learning outcomes in higher education},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706518},
doi = {10.1145/3706468.3706518},
abstract = {Learning analytics (LA) emerged with the promise of improving student learning outcomes (LOs), however, its effectiveness in informing actionable insights remains a challenge. Curriculum analytics (CA), a subfield of LA, seeks to address this by using data to inform curriculum development. This study explores using CA to evaluate LOs through direct standardized measures at the subject level, examining how this process informs curriculum decision-making. Conducted at an engineering-focused higher education institution, the research involved 32 administrators and 153 faculty members, serving 9.906 students across nine programs. By utilizing the Integrative Learning Design Framework, we conducted three phases of this framework and present key results. Findings confirm the importance of stakeholder involvement throughout different design phases, highlighting the need for ongoing training and support. Among the actionable insights that emerged from LOs assessments, we identified faculty reflections regarding the need to incorporate active learning strategies, improve course planning, and acknowledge the need for education-specific training for faculty development. Although the study does not demonstrate whether these insights lead to improvements in LOs, this paper contributes to the CA field by offering a practical approach to evaluating LOs and translating these assessments into actionable improvements within an actual-world educational context.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {384–394},
numpages = {11},
keywords = {Additional Keywords and Phrases},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706516,
author = {Venugopalan, Devika and Yan, Ziwen and Borchers, Conrad and Lin, Jionghao and Aleven, Vincent},
title = {Combining Large Language Models with Tutoring System Intelligence: A Case Study in Caregiver Homework Support},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706516},
doi = {10.1145/3706468.3706516},
abstract = {Caregivers (i.e., parents and members of a child’s caring community) are underappreciated stakeholders in learning analytics. Although caregiver involvement can enhance student academic outcomes, many obstacles hinder involvement, most notably knowledge gaps with respect to modern school curricula. An emerging topic of interest in learning analytics is hybrid tutoring, which includes instructional and motivational support. Caregivers assert similar roles in homework, yet it is unknown how learning analytics can support them. Our past work with caregivers suggested that conversational support is a promising method of providing caregivers with the guidance needed to effectively support student learning. We developed a system that provides instructional support to caregivers through conversational recommendations generated by a Large Language Model (LLM). Addressing known instructional limitations of LLMs, we use instructional intelligence from tutoring systems while conducting prompt engineering experiments with the open-source Llama 3 LLM. This LLM generated message recommendations for caregivers supporting their child’s math practice via chat. Few-shot prompting and combining real-time problem-solving context from tutoring systems with examples of tutoring practices yielded desirable message recommendations. These recommendations were evaluated with ten middle school caregivers, who valued recommendations facilitating content-level support and student metacognition through self-explanation. We contribute insights into how tutoring systems can best be merged with LLMs to support hybrid tutoring settings through conversational assistance, facilitating effective caregiver involvement in tutoring systems.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {373–383},
numpages = {11},
keywords = {large language models, tutoring systems, hybrid tutoring, K-12, mathematics education, caregivers},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706515,
author = {Rodrigues, Luiz and Xavier, Cleon and Costa, Newarney and Batista, Hyan and Silva, Luiz Felipe Bagnhuk and Chaleghi de Melo, Weslei and Gasevic, Dragan and Ferreira Mello, Rafael},
title = {LLMs Performance in Answering Educational Questions in Brazilian Portuguese: A Preliminary Analysis on LLMs Potential to Support Diverse Educational Needs},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706515},
doi = {10.1145/3706468.3706515},
abstract = {Question-answering systems facilitate adaptive learning and respond to student queries, making education more responsive. Despite that, challenges such as natural language understanding and context management complicate their widespread adoption, where Large Language Models (LLMs) offer a promising solution. However, existing research is predominantly focused on English, proprietary models, and often limited to a single question type, subject, or skill, leaving a gap in understanding LLMs’ performance in languages like Brazilian Portuguese and across questions of various characteristics. This study investigates how LLMs could be integrated in an educational question-answering system efficiently to answer different question types (multiple-choice, cloze, open-ended), subjects (mathematics and Portuguese language), and skills (summation/subtraction, multiplication, interpretation, and grammar), evaluating answers by GPT-4 - the main LLM at the time of writing - and Sabi\'{a} - the open-source Brazilian Portuguese LLM - based on grades assigned by two experienced teachers. Overall, both LLMs demonstrated strong overall performance, with mean scores close to 9.8 out of 10. However, specific challenges emerged, with distinct strengths and weaknesses observed for each model, such as GPT-4’s error in a multiple-choice subtraction question and Sabi\'{a}’s misinterpretation of a cloze question.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {865–871},
numpages = {7},
keywords = {GPT, Sabi\'{a}, Question-Answering, Chatbot.},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706514,
author = {Imhof, Christof and Hlosta, Martin and Bergamin, Per},
title = {Will they or won't they make it in time? The role of contextual and behavioral predictors in reaching deadlines of mandatory assignments},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706514},
doi = {10.1145/3706468.3706514},
abstract = {Procrastination and other forms of irrational delay are widespread among university students, leading to an array of potential negative consequences. While the reasons for this type of behavior are manifold and many facilitating factors have been identified, which of these factors are able to predict dilatory behavior in online/distance education has received comparatively little attention in the literature so far. In this study, we intended to compare the performance of two sets of objective predictors of delay, namely contextual variables based on characteristics of the assignment, and behavioral variables based on log data. Using historical data drawn from our university's learning management system, we calculated Bayesian multilevel models. The strongest and most consistent predictors of dilatory behavior turned out to be interval between the first click on the assignment and its deadline, the interval between the start of a block and the first click on the assignment, the number of clicks on the assignment, and the deadline type. The combination of both sets of predictors slightly improved the model's performance.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {362–372},
numpages = {11},
keywords = {deadlines, dilatory behavior, log data, predictive models, procrastination},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706513,
author = {Saqr, Mohammed and L\'{o}pez-Pernas, Sonsoles and T\"{o}rm\"{a}nen, Tiina and Kaliisa, Rogers and Misiejuk, Kamila and Tikka, Santtu},
title = {Transition Network Analysis: A Novel Framework for Modeling, Visualizing, and Identifying the Temporal Patterns of Learners and Learning Processes},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706513},
doi = {10.1145/3706468.3706513},
abstract = {This paper presents a novel learning analytics method: Transition Network Analysis (TNA), a method that integrates Stochastic Process Mining and probabilistic graph representation to model, visualize, and identify transition patterns in the learning process data. Combining the relational and temporal aspects into a single lens offers capabilities beyond either framework, including centralities to capture important learning events, community detection to identify behavior patterns, and clustering to reveal temporal patterns. Furthermore, TNA introduces several significance tests that go beyond either method and add rigor to the analysis. Here, we introduce the theoretical and mathematical foundations of TNA and we demonstrate the functionalities of TNA with a case study where students (n=191) engaged in small-group collaboration to map patterns of group dynamics using the theories of co-regulation and socially-shared regulated learning. The analysis revealed that TNA can map the regulatory processes as well as identify important events, patterns, and clusters. Bootstrap validation established the significant transitions and eliminated spurious transitions. As such, TNA can capture learning dynamics and provide a robust framework for investigating the temporal evolution of learning processes. Future directions include —inter alia— expanding estimation methods, reliability assessment, and building longitudinal TNA.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {351–361},
numpages = {11},
keywords = {Transition Network Analysis, Process Mining, Social Network Analysis, Learning process, Learning analytics},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706512,
author = {Shah, Mamta and Tan, Yuanru and Eagan, Brendan and Chabalowski, Brittny and Chen, Yahan},
title = {A Dual-Method Examination of Nursing Students’ Teamwork in Simulation-Based Learning: Combining CORDTRA and Ordered Network Analysis to Reveal Patterns and Dynamics},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706512},
doi = {10.1145/3706468.3706512},
abstract = {This study examines nursing students’ teamwork during a simulated pediatric scenario by combining Chronologically Ordered Representations of Discourse and Tool-Related Activity (CORDTRA) with Ordered Network Analysis (ONA). CORDTRA revealed each dyad's progression and critical moments during the scenario, while ONA illustrated how roles were divided. Our findings show that patient and parent interactions, education, and assessments were typically shared between students, whereas technical tasks such as dosage calculations were led by one student with support from the other. These findings highlight the nuanced ways in which manikin-based simulations foster essential teamwork skills, such as communication, task delegation, and problem-solving. This study highlights the methodological benefit of integrating CORDTRA and ONA to capture both temporal and relational dynamics, along with the practical implication that targeted feedback and debriefing informed by these approaches can enhance nursing students’ individual and team performance, and by extension their practice readiness.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {858–864},
numpages = {7},
keywords = {CORDTRA, Learning Analytics, Nursing Education, Ordered Network Analysis, Simulations, Visualization Techniques},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706511,
author = {Song, Yige and Oliveira, Eduardo and de Barba, Paula and Kirley, Michael and Thompson, Pauline},
title = {Investigating Validity and Generalisability in Trace-Based Measurement of Self-Regulated Learning: A Multidisciplinary Study},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706511},
doi = {10.1145/3706468.3706511},
abstract = {Self-regulated learning (SRL) skills are critical for effective learning and academic success. With the growing availability of trace data from students’ online learning activities, researchers are increasingly leveraging this data to infer SRL processes. However, challenges remain regarding the validity of these inferences and their generalisability across diverse learning contexts. This study presents a structured approach to investigate these challenges by examining SRL behaviours in a multidisciplinary university cohort. The dataset includes 76 baseline survey responses, over 300 daily SRL survey submissions, and more than 6,000 sequences of recorded learning actions as trace data. Using mixed linear models and sequence mining, the analysis is grounded in SRL theory and evaluated through machine learning performance metrics. Our findings indicate consistent within-person patterns of SRL and online learning behaviours, supporting the concept of transferable, holistic skill development. Additionally, the results validate the trace-based detection of SRL engagement but highlight limitations in accurately detecting planning and reflection phases. These findings underscore the potential of automating SRL engagement detection while emphasising the need for multi-modal approaches to capture the full spectrum of SRL processes comprehensively.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {339–350},
numpages = {12},
keywords = {Learning Analytics, Blended Learning, Trace data, Self-Regulated Learning, Trace-SRL, Mixed Linear Model, Sequence Mining},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706510,
author = {Martins Van Jaarsveld, Gabrielle and Wong, Jacqueline and Baars, Martine and Specht, Marcus and Paas, Fred},
title = {Scaling goal-setting interventions in higher education using a conversational agent: Examining the effectiveness of guidance and adaptive feedback},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706510},
doi = {10.1145/3706468.3706510},
abstract = {Goal setting is the first and driving stage of the self-regulated learning cycle. Studies have shown that supporting goal setting is an effective means of improving academic performance among higher education students. However, doing so can be complex and resource intensive. In this study, a goal-setting conversational agent was designed and deployed to support higher education students in setting academic goals. Across 5-weeks, we tested the effects of goal-setting prompts (guided vs. unguided) and adaptive feedback (with vs. without) when delivered via a goal-setting conversational agent. We explored the effects of these supports (i.e., guidance and feedback) on students’ 1) goal quality and 2) goal attainment. Findings showed that guidance and feedback combined had the largest positive effect on goal quality. They also revealed that guidance alone produced initially high-quality goals which decreased in quality overtime, whereas feedback had a delayed but cumulative effect on quality across multiple goal setting iterations. However, neither guidance nor feedback had significant effects on goal attainment, and there was no significant relationship between goal quality and attainment. This study provides insights into how a goal-setting conversational agent and adaptive feedback can be used to support the academic goal setting process for higher education students.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {328–338},
numpages = {11},
keywords = {Adaptive Support, Conversational Agents, Feedback, Self-Regulated Learning},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706509,
author = {Ko, Pakon and Liu, Cong and Law, Nancy and Tan, Yuanru and Shaffer, David Williamson},
title = {Exploring students’ epistemic orientation, learning trajectories, and outcomes},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706509},
doi = {10.1145/3706468.3706509},
abstract = {The influence of students’ epistemic orientations on their learning behavior and outcomes is well-documented. However, limited research explores students’ epistemic orientations in terms of conceptual engagement and learning outcomes. This study, set within the context of higher education, examined the patterns of conceptual engagement among two performance groups and identifies differences in their epistemic orientations. Both epistemic network analysis (ENA) and ordered network analysis (ONA) methods were used. The results from the ENA revealed distinct trajectories and patterns of conceptual engagement between high-performing and low-performing students during different periods in their learning journey. High-performing students were able to establish a more interconnected and distributed epistemic network earlier than their low-performing counterparts. ONA results revealed that (1) high-performing students were more inclined to employ abstract theoretical concepts to address empirical concerns, doing so more frequently and earlier; and (2) low-performing students benefitted from forum interactions with high-performing students to expand their knowledge resources and engagement with theoretical constructs over time. These discoveries contribute to our comprehension of epistemic orientations in different learners. The implications of this study could help generate learning analytics that monitor students’ conceptual engagement in forum discussion and provide feedback to guide the design of learning.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {317–327},
numpages = {11},
keywords = {ENA, ONA, epistemic orientation, learning trajectory},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706508,
author = {Herodotou, Christothea and Carr, Jessica and Shrestha, Sagun and Comfort, Catherine and Bayer, Vaclav and Maguire, Claire and Lee, John and Mulholland, Paul and Fernandez, Miriam},
title = {Prescriptive analytics motivating distance learning students to take remedial action: A case study of a student-facing dashboard},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706508},
doi = {10.1145/3706468.3706508},
abstract = {Student-facing learning analytics dashboards aim to help students to monitor their study progress, achieve learning goals and develop self-regulation skills. Only few of them present personalised data visualisations and aim to develop agentic students who take remedial action to improve their study habits, learning and performance. In this paper, a student-facing dashboard, designed following principles of participatory research, was tested with 30 undergraduate students, who engaged with it over a period of 4 to 15 weeks and while studying an online course. This is one of the few dashboards available that presents all different types of analytics to students: descriptive, predictive and prescriptive. A mixed methods approach was used to assess its usefulness and impact on motivation to study and take remedial action to support learning. Data analysis showcased that such a dashboard can be “a roadmap to success” by motivating students to study more and improve their performance, in addition to helping with monitoring, planning and reflection. While all dashboard features were perceived as being useful, special value was placed on prescriptive elements, in particular material recommendations and contacting tutors and university support teams, emphasizing the significance of making explicit on a dashboard the actions students should take to improve their performance. Implications for future studies are discussed.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {306–316},
numpages = {11},
keywords = {learning analytics dashboards, student-facing dashboards, prescriptive analytics},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706507,
author = {Xiao, Changrong and Ma, Wenxing and Song, Qingping and Xu, Sean Xin and Zhang, Kunpeng and Wang, Yufang and Fu, Qi},
title = {Human-AI Collaborative Essay Scoring: A Dual-Process Framework with LLMs},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706507},
doi = {10.1145/3706468.3706507},
abstract = {Receiving timely and personalized feedback is essential for second-language learners, especially when human instructors are unavailable. This study explores the effectiveness of Large Language Models (LLMs), including both proprietary and open-source models, for Automated Essay Scoring (AES). Through extensive experiments with public and private datasets, we find that while LLMs do not surpass conventional state-of-the-art (SOTA) grading models in performance, they exhibit notable consistency, generalizability, and explainability. We propose an open-source LLM-based AES system, inspired by the dual-process theory. Our system offers accurate grading and high-quality feedback, at least comparable to that of fine-tuned proprietary LLMs, in addition to its ability to alleviate misgrading. Furthermore, we conduct human-AI co-grading experiments with both novice and expert graders. We find that our system not only automates the grading process but also enhances the performance and efficiency of human graders, particularly for essays where the model has lower confidence. These results highlight the potential of LLMs to facilitate effective human-AI collaboration in the educational context, potentially transforming learning experiences through AI-generated feedback.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {293–305},
numpages = {13},
keywords = {LLM Application, Automatic Essay Scoring, AI-assisted Learning},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706506,
author = {Xiang, Mengtong and Zhang, Jingjing and Saqr, Mohammed and Jiang, Han and Liu, Wei},
title = {Capturing The Temporal Dynamics of Learner Interactions In Moocs: A Comprehensive Approach With Longitudinal And Inferential Network Analysis},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706506},
doi = {10.1145/3706468.3706506},
abstract = {While research on social network analysis is abundant and less frequently so temporal network analysis, research that uses inferential temporal network methods is barely existent. This paper aims to fill this gap by conducting a comparative analysis of temporal networks and inferential longitudinal network methods in the context of learner interactions in Massive Open Online Courses (MOOCs). We focus on three prominent methods: Temporal Network Analysis (TNA), Temporal Exponential Random Graph Models (TERGM) and Simulation Investigation for Empirical Network Analysis (SIENA). Using a five-week Nature Education MOOC as a case study, we compared the features, metrics of each method as well as their understanding of using network to analyze learner interactions. TNA focuses on describing and visualizing temporal changes in network structure, while TERGM and SIENA view networks as evolving systems influenced by individual behaviors and structural dependencies. TERGM treats network changes as a joint of random processes, while SIENA emphasizes the agency of learners and analyzes continuous network evolution. The findings provide guidelines for researchers and educators to select appropriate network analysis methods for temporal studies in educational contexts.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {851–857},
numpages = {7},
keywords = {MOOCs, Network Analysis, SIENA, TERGM, TNA, Temporal Dynamics},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706505,
author = {Butler, Darren and Borchers, Conrad and Asher, Michael and Lee, Yongmin and Karnataki, Sonya and Dangi, Sameeksha and Athreya, Samyukta and Stamper, John and Ogan, Amy and Carvalho, Paulo},
title = {Does the Doer Effect Generalize To Non-WEIRD Populations? Toward Analytics in Radio and Phone-Based Learning},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706505},
doi = {10.1145/3706468.3706505},
abstract = {The Doer Effect states that completing more active learning activities, like practice questions, is more strongly related to positive learning outcomes than passive learning activities, like reading, watching, or listening to course materials. Although broad, most evidence has emerged from practice with tutoring systems in Western, Industrialized, Rich, Educated, and Democratic (WEIRD) populations in North America and Europe. Does the Doer Effect generalize beyond WEIRD populations, where learners may practice in remote locales through different technologies? Through learning analytics, we provide evidence from N = 234 Ugandan students answering multiple-choice questions via phones and listening to lectures via community radio. Our findings support the hypothesis that active learning is more associated with learning outcomes than passive learning. We find this relationship is weaker for learners with higher prior educational attainment. Our findings motivate further study of the Doer Effect in diverse populations. We offer considerations for future research in designing and evaluating contextually relevant active and passive learning opportunities including leveraging familiar technology, increasing the number of practice opportunities, and aligning multiple data sources.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {844–850},
numpages = {7},
keywords = {doer effect, learning by doing, replication, mobile learning, global south, distance learning, equity},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706504,
author = {Alghamdi, Saleh Ramadhan and Rakovi\'{c}, Mladen and Yang, Kaixun and Fan, Yizhou and Ga\v{s}evi\'{c}, Dragan and Chen, Guanliang},
title = {Analytics of Temporal Patterns of Self-regulated Learners: A Time Series Approach},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706504},
doi = {10.1145/3706468.3706504},
abstract = {Temporal patterns play a significant role in understanding dynamic changes in Self-regulated Learning (SRL) engagement over time. Several previous studies have proposed approaches for automated detection of SRL strategies through analysis of temporal patterns. However, these approaches are mostly focused on the analysis of patterns in sequential ordering of SRL processes. This offers a useful yet limited temporal perspective to SRL. As noted in the literature, temporality of SRL has two dimensions – passage of time and ordering of events. To address this gap, this paper specifically proposes a time series approach that can automatically detect SRL strategies by accounting for both dimensions of temporality. Our approach also explores when specific processes occur and how learners engage metacognitively or cognitively with learning tasks. In particular, this study investigated SRL engagement as students composed essays using multiple sources within a 120-minute time frame. The results indicated that five distinct strategies with varying levels of engagement were detected. The correlation between these identified strategies and students’ scores was not statistically significant; however, further exploration revealed that students who adopted a specific strategy could outperform other groups based on obtained scores. We also noticed additional factors that had a positive effect on learners’ performance.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {283–292},
numpages = {10},
keywords = {Self-regulated Learning (SRL), Time Series, Clustering, Learning Strategies, Learning Analytics.},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706503,
author = {Zhong, Lina and Lang, Weijie and Rong, Jia and Chen, Guanliang and Fan, Miao},
title = {Enhancing Motivation and Learning in Primary School History Classrooms: The Impact of Virtual Reality},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706503},
doi = {10.1145/3706468.3706503},
abstract = {Conventional classroom instruction often struggles to effectively convey cultural heritage due to constraints in spatial and temporal dimensions, limiting students’ ability to fully engage with and appreciate historical content. In contrast, virtual reality (VR) technology offers a human-centered, immersive way to present cultural heritage, creating a dynamic digital experience particularly beneficial when physical access to heritage sites is unavailable. This study investigates whether VR-based learning can enhance students’ performance in cultural education compared to traditional teaching methods. A sample of 228 primary school students from Grades 5 and 6 was randomly assigned to one of two groups: a high-visual engagement group (VR with 360° video) or a low-visual engagement group (static video and textbook). The findings revealed that students in the high-visual engagement group achieved higher levels of intrinsic motivation and demonstrated greater learning improvements than their counterparts in the low-visual engagement group. Furthermore, the study identified negative user experiences as a significant factor moderating the connection between intrinsic motivation and learning outcomes. These results highlight the value of integrating VR into conventional teaching practices, showcasing its potential to enhance student engagement and improve educational outcomes in history and cultural studies.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {272–282},
numpages = {11},
keywords = {Virtual reality, Cultural education, K-6 learning, Structural equation modeling, Fornell-Larcker criterion, PLS-SEM model, Motivation, Engagement},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706502,
author = {Cheng, Yixin and Guan, Rui and Li, Tongguang and Rakovi\'{c}, Mladen and Li, Xinyu and Fan, Yizhou and Jin, Flora and Tsai, Yi-Shan and Ga\v{s}evi\'{c}, Dragan and Swiecki, Zachari},
title = {Self-regulated Learning Processes in Secondary Education: A Network Analysis of Trace-based Measures},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706502},
doi = {10.1145/3706468.3706502},
abstract = {While the capacity to self-regulate has been found to be crucial for secondary school students, prior studies often rely on self-report surveys and think-aloud protocols that present notable limitations in capturing self-regulated learning (SRL) processes. This study advances the understanding of SRL in secondary education by using trace data to examine SRL processes during multi-source writing tasks, with higher education participants included for comparison. We collected fine-grained trace data from 66 secondary school students and 59 university students working on the same writing tasks within a shared SRL-oriented learning environment. The data were labelled using Bannert’s validated SRL coding scheme to reflect specific SRL processes, and we examined the relationship between these processes, essay performance, and educational levels. Using epistemic network analysis (ENA) to model and visualise the interconnected SRL processes in Bannert’s coding scheme, we found that: (a) secondary school students predominantly engaged in three SRL processes—Orientation, Re-reading, and Elaboration/Organisation; (b) high-performing secondary students engaged more in Re-reading, while low-performing students showed more Orientation process; and (c) higher education students exhibited more diverse SRL processes such as Monitoring and Evaluation than their secondary education counterparts, who heavily relied on following task instructions and rubrics to guide their writing. These findings highlight the necessity of designing scaffolding tools and developing teacher training programs to enhance awareness and development of SRL skills for secondary school learners.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {260–271},
numpages = {12},
keywords = {Self-regulated learning, K-12 education, Epistemic network analysis, Secondary education},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706501,
author = {Scarlatos, Alexander and Baker, Ryan S. and Lan, Andrew},
title = {Exploring Knowledge Tracing in Tutor-Student Dialogues using LLMs},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706501},
doi = {10.1145/3706468.3706501},
abstract = {Recent advances in large language models (LLMs) have led to the development of artificial intelligence (AI)-powered tutoring chatbots, showing promise in providing broad access to high-quality personalized education. Existing works have studied how to make LLMs follow tutoring principles, but have not studied broader uses of LLMs for supporting tutoring. Up until now, tracing student knowledge and analyzing misconceptions has been difficult and time-consuming to implement for open-ended dialogue tutoring. In this work, we investigate whether LLMs can be supportive of this task: we first use LLM prompting methods to identify the knowledge components/skills involved in each dialogue turn, i.e., a tutor utterance posing a task or a student utterance that responds to it. We also evaluate whether the student responds correctly to the tutor and verify the LLM’s accuracy using human expert annotations. We then apply a range of knowledge tracing (KT) methods on the resulting labeled data to track student knowledge levels over an entire dialogue. We conduct experiments on two tutoring dialogue datasets, and show that a novel yet simple LLM-based method, LLMKT, significantly outperforms existing KT methods in predicting student response correctness in dialogues. We perform extensive qualitative analyses to highlight the challenges in dialogueKT and outline multiple avenues for future work.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {249–259},
numpages = {11},
keywords = {Knowledge Components, Knowledge Tracing, Large Language Models, Tutoring dialogues},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706500,
author = {Duan, Zhangqi and Fernandez, Nigel and Hicks, Alexander and Lan, Andrew},
title = {Test Case-Informed Knowledge Tracing for Open-ended Coding Tasks},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706500},
doi = {10.1145/3706468.3706500},
abstract = {Open-ended coding tasks, which ask students to construct programs according to certain specifications, are common in computer science education. Student modeling can be challenging since their open-ended nature means that student code can be diverse. Traditional knowledge tracing (KT) models that only analyze response correctness may not fully capture nuances in student knowledge from student code. In this paper, we introduce Test case-Informed Knowledge Tracing for Open-ended Coding (TIKTOC), a framework to simultaneously analyze and predict both open-ended student code and whether the code passes each test case. We augment the existing CodeWorkout dataset with the test cases used for a subset of the open-ended coding questions, and propose a multi-task learning KT method to simultaneously analyze and predict 1) whether a student’s code submission passes each test case and 2) the student’s open-ended code, using a large language model as the backbone. We quantitatively show that these methods outperform existing KT methods for coding that only use the overall score a code submission receives. We also qualitatively demonstrate how test case information, combined with open-ended code, helps us gain fine-grained insights into student knowledge.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {238–248},
numpages = {11},
keywords = {Computer Science Education, Large Language Models, Open-ended Coding Questions, Test Cases},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706499,
author = {Lee, Melissa and Huang, Chun-Wei and Collins, Kelly and Feng, Mingyu},
title = {Examining the Relationship between Math Anxiety, Effort, and Learning Outcomes Using Latent Class Analysis},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706499},
doi = {10.1145/3706468.3706499},
abstract = {Math anxiety has been found to negatively correlate with math achievement, affecting students’ choices to take fewer math classes and avoid math educational opportunities. Educational technology tools can ameliorate some of the negative effects of math anxiety. We examined students’ math anxiety, effort in an educational technology platform, and their relationship with students’ math achievement. Multilevel latent class analysis was used to identify student profiles of math anxiety. Regression analysis was used to examine how students of different profiles interacted with MathSpring, an adaptive intelligent tutor that provides affective supports to students during math problem-solving. The student's math achievement was measured by a standardized test. Our analysis indicated heterogeneity in math anxiety, and students could fall into one of three groups: Highly Anxious, Performance Anxious, and Calm. Highly Anxious students tended to give up more often when solving questions in MathSpring and had the lowest math achievement outcomes. For these students, using hints to solve problems in MathSpring was significantly associated with increased math outcomes. These findings have implications for the field's understanding of how students of different math anxiety profiles can demonstrate varying efforts in math educational technology platforms, and different math learning outcomes.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {227–237},
numpages = {11},
keywords = {latent class analysis, math anxiety, math learning, social-emotional learning},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706498,
author = {Borchers, Conrad and Baker, Ryan S.},
title = {ABROCA Distributions For Algorithmic Bias Assessment: Considerations Around Interpretation},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706498},
doi = {10.1145/3706468.3706498},
abstract = {Algorithmic bias continues to be a key concern of learning analytics. We study the statistical properties of the Absolute Between-ROC Area (ABROCA) metric. This fairness measure quantifies group-level differences in classifier performance through the absolute difference in ROC curves. ABROCA is particularly useful for detecting nuanced performance differences even when overall Area Under the ROC Curve (AUC) values are similar. We sample ABROCA under various conditions, including varying AUC differences and class distributions. We find that ABROCA distributions exhibit high skewness dependent on sample sizes, AUC differences, and class imbalance. When assessing whether a classifier is biased, this skewness inflates ABROCA values by chance, even when data is drawn (by simulation) from populations with equivalent ROC curves. These findings suggest that ABROCA requires careful interpretation given its distributional properties, especially when used to assess the degree of bias and when classes are imbalanced.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {837–843},
numpages = {7},
keywords = {algorithmic bias, algorithmic fairness, ABROCA, AUC ROC, simulation, classification, prediction},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706497,
author = {Fang, Zheng and Wang, Weiqing and Chen, Guanliang and Swiecki, Zachari},
title = {The Company You Keep: Refining Neural Epistemic Network Analysis},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706497},
doi = {10.1145/3706468.3706497},
abstract = {Collaborative problem-solving (CPS) is defined as an inherently sociocognitive phenomena. Despite this, extant learning analytic techniques tend to focus on either the social or cognitive aspects without explicitly considering their interaction. Prior work developed Neural Epistemic Network Analysis (NENA), which used a combination of deep learning methods to simultaneously model the social and cognitive aspects of CPS; however, the method had several limitations. The refined version of NENA presented here addresses these limitations by (a) introducing a simplified autoencoder deep learning architecture; (b) using a combination of social and epistemic networks as input to preserve interpretability in terms of social and cognitive factors; and (c) introducing an isometry loss function to ensure downstream statistical tests are meaningful. We found that the refined version of NENA is able to achieve high performance on criteria we would expect from a network analytic technique in the context of learning analytics: interpretability, goodness of fit, orthogonality and isometry; and discriminatory power. We also demonstrated that this method was comparable in performance to a more traditional learning analytic technique, Epistemic Network Analysis (ENA), while providing information that ENA did not. The results suggest that NENA could be a useful method for exploring the cognitive interactions of a given individual’s social network and thus the influences their network exerts upon them.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {216–226},
numpages = {11},
keywords = {Collaborative Problem-Solving, Epistemic Network Analysis, Social Network Analysis, Graph Neural Networks},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706496,
author = {Ocumpaugh, Jaclyn and Liu, Xiner and Zambrano, Andres Felipe},
title = {Language Models and Dialect Differences},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706496},
doi = {10.1145/3706468.3706496},
abstract = {The advancements in automatic language processing being ushered in by Large Language Models suggest enormous potential for better personalization during student learning. However, this potential can be best exploited if we know that LLMs are equally capable of interacting with students who speak or write in a range of different dialects. This case study uses systematically manipulated student essays, previously evaluated by human raters, to examine how ChatGPT responds to and addresses specific dialect differences. Results point to important concerns about the potential biases and limitations of both LLMs and humans when evaluating and providing feedback to students who use minoritized dialects. Addressing these concerns is critical for the field of learning analytics, as it seeks to ensure equity and asset-based approaches to learning analytics.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {204–215},
numpages = {12},
keywords = {African American Language, automatic writing assessment, equity, large language models (LLMs)},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706495,
author = {Ocumpaugh, Jaclyn and Nasiar, Nidhi and Zambrano, Andres Felipe and Goslen, Alex and Vandenberg, Jessica and Esiason, Jordan and Rowe, Jonathan and Hutt, Stephen},
title = {Refocusing the lens through which we view affect dynamics: The Skills, Difficulty, Value, Efficacy and Time Model},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706495},
doi = {10.1145/3706468.3706495},
abstract = {For more than a decade, a handful of theoretical models have shaped a substantial amount of the research related to students’ emotional experiences during learning. This research has been productive, but articulating the underlying implicit assumptions in existing theories and their implications in our empirical interpretations can help to better investigate the reciprocal relationships between learning and emotion, and subsequently, to develop better interventions. This paper expands upon the existing theoretical frameworks, increasing the types of questions we ask about affect dynamics. We do so within the context of Crystal Island, a virtual world that allows middle school students to investigate microbiology questions. Specifically, we use this data to examine and revise the assumptions that are implicit in these models and the methods we use to investigate them.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {192–203},
numpages = {12},
keywords = {Affect Dynamics, Control Value Theory, Game-based learning, Self-Efficacy},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706494,
author = {Jansen, Thorben and Horbach, Andrea and Meyer, Jennifer},
title = {Feedback from Generative AI: Correlates of Student Engagement in Text Revision from 655 Classes from Primary and Secondary School},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706494},
doi = {10.1145/3706468.3706494},
abstract = {Writing is fundamental in knowledge-based societies, and engaging students in text revision through feedback is critical for developing students’ writing skills. Automated feedback offers a promising solution to teachers’ time constraints creating feedback. However, prior research indicates that 20 to 71 percent of students receiving feedback do not engage in any text revision. Despite these concerning figures, students’ non-engagement has not received widespread attention, likely due to fragmented evidence from a few grade levels and writing tasks disconnected from regular teaching. Further, whether the issue persists when generative AI generates the feedback is unclear. The present study investigates what percentage of students behaviorally engage with feedback from generative AI in authentic classroom learning contexts. We analyzed data from an educational technology company, including 655 teacher-generated writing tasks involving 14,236 students across grades 1-12. Our findings show that around half of the students did not revise a single character in the text after receiving feedback. The percentage was similar across grade levels, task types, or feedback characteristics. We discuss the importance of including the percentage of engaged students as an additional metric in feedback research to achieve the goal that no student is left behind.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {831–836},
numpages = {6},
keywords = {student engagement, automated feedback, writing, generative AI},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706493,
author = {Liu, Qinyi and Shakya, Ronas and Khalil, Mohammad and Jovanovic, Jelena},
title = {Advancing privacy in learning analytics using differential privacy},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706493},
doi = {10.1145/3706468.3706493},
abstract = {This paper addresses the challenge of balancing learner data privacy with the use of data in learning analytics (LA) by proposing a novel framework by applying Differential Privacy (DP). The need for more robust privacy protection keeps increasing, driven by evolving legal regulations and heightened privacy concerns, as well as traditional anonymization methods being insufficient for the complexities of educational data. To address this, we introduce the first DP framework specifically designed for LA and provide practical guidance for its implementation. We demonstrate the use of this framework through a LA usage scenario and validate DP in safeguarding data privacy against potential attacks through an experiment on a well-known LA dataset. Additionally, we explore the trade-offs between data privacy and utility across various DP settings. Our work contributes to the field of LA by offering a practical DP framework that can support researchers and practitioners in adopting DP in their works.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {181–191},
numpages = {11},
keywords = {Differential Privacy (DP), Learning Analytics, Privacy-Enhanced Technologies, Privacy-Preserving, framework},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706491,
author = {Wang, Zuo and Lin, Weiyue and Hu, Xiao},
title = {Self-service Teacher-facing Learning Analytics Dashboard with Large Language Models},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706491},
doi = {10.1145/3706468.3706491},
abstract = {With the rise of online learning platforms, the need for effective learning analytics (LA) has become critical for teachers. However, the development of traditional LA dashboards often requires technical expertise and a certain level of data literacy, preventing many teachers from integrating LA dashboards effectively and flexibly into their teaching practice. This paper explores the development of a self-service teacher-facing learning analytics dashboard powered by large language models (LLMs), for improving teaching practices. By leveraging LLMs, the self-service system aims to simplify the implementation of data queries and visualizations, allowing teachers to create personalized LA dashboards using natural languages. This study also investigates the capabilities of LLMs in generating charts for LA dashboards and evaluates the effectiveness of the self-service system through usability tests with 15 teachers. Preliminary findings suggest that LLMs demonstrate high capabilities in generating charts for LA dashboards, and the LLM-powered self-service system can effectively address participating teachers’ pedagogical needs for LA. This research contributes to the ongoing research on the intersection of LLMs and education, emphasizing the potential of self-service systems to empower teachers in daily teaching practices.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {824–830},
numpages = {7},
keywords = {data visualization, large language models, learning analytics dashboard, self-service learning analytics},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706490,
author = {Strugatski, Alona and Alexandron, Giora},
title = {Applying IRT to Distinguish Between Human and Generative AI Responses to Multiple-Choice Assessments},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706490},
doi = {10.1145/3706468.3706490},
abstract = {Generative AI is transforming the educational landscape, raising significant concerns about cheating. Despite the widespread use of multiple-choice questions (MCQs) in assessments, the detection of AI cheating in MCQ-based tests has been almost unexplored, in contrast to the focus on detecting AI-cheating on text-rich student outputs. In this paper, we propose a method based on the application of Item Response Theory (IRT) to address this gap. Our approach operates on the assumption that artificial and human intelligence exhibit different response patterns, with AI cheating manifesting as deviations from the expected patterns of human responses. These deviations are modeled using Person-Fit Statistics (PFS). We demonstrate that this method effectively highlights the differences between human responses and those generated by premium versions of leading chatbots (ChatGPT, Claude, and Gemini), but that it is also sensitive to the amount of AI cheating in the data. Furthermore, we show that the chatbots differ in their reasoning profiles. Our work provides both a theoretical foundation and empirical evidence for the application of IRT to identify AI cheating in MCQ-based assessments.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {817–823},
numpages = {7},
keywords = {Cheating with AI, separating AI from humans, Item-response theory, person-fit statistics},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706489,
author = {Khalil, Mohammad and Prinsloo, Paul},
title = {The lack of generalisability in learning analytics research: why, how does it matter, and where to?},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706489},
doi = {10.1145/3706468.3706489},
abstract = {Concerns about the lack of impact of learning analytics (LA) research has been part of the evolution of the field since its emergence as a research focus and practice in 2011. The preponderance of small-scale and exploratory nature of much of LA research are well-documented as contributing factors to the lack of generalisability, transferability, replicability and scalability. Through an analysis of 144 full research papers published in the conference proceedings of the Learning Analytics &amp; Knowledge (LAK) Conference '22, 23 and 24, this paper provides an overview of the extent and contours of the lack of generalisability in LA research and pointers for making LA research more generalisable. The inductive and deductive analysis of the recent three LAK conferences provide evidence that a significant percentage (46%) of the corpus papers do not refer at all to generalisability or transferability, while few papers report on the scalability of their research findings. While the crisis of replicability/reproducibility is a wider concern in the broader context of research, considering and reporting on generalisability and transferability is integral to the scientific rigour. We conclude our paper with a range of pointers for addressing the lack of generalisability in LA research including, but not limited to expanding data, methodological adaptation and the potential of open science.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {170–180},
numpages = {11},
keywords = {generalisability, impact, learning analytics, replicability, scalability, transferability},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706488,
author = {Shin, Insub and Hwang, Su Bhin and Yoo, Yun Joo and Bae, Sooan and Kim, Rae Yeong},
title = {Comparing Student Preferences for AI-Generated and Peer-Generated Feedback in AI-driven Formative Peer Assessment},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706488},
doi = {10.1145/3706468.3706488},
abstract = {Formative assessment can enhance student learning and improve teaching practices by identifying areas for growth and providing feedback. However, practical obstacles remain, such as time constraints and students’ passive participation and the low quality of peer feedback. Artificial intelligence (AI) has been explored for its potential to automate grading and provide timely feedback, making it a valuable tool in formative assessment. Nevertheless, there is still limited research on how AI can be used effectively in the context of formative peer assessment. In this study, we conducted an AI-driven formative peer assessment with 108 secondary school students. During the peer assessment process, students not only evaluated peers’ responses and received peer-generated feedback, but also evaluated AI-generated responses and received AI-generated feedback. This research focused on analyzing the differences in preference between AI-generated and peer-generated feedback using trace data and dispositional data. In scenarios where student participation was low or the quality of peer feedback was insufficient, students showed a higher preference for AI-generated feedback, demonstrating its potential utility. However, students with high Math Confidence and AI Interest preferred peer-generated feedback. Based on these findings, we will propose practical strategies for implementing AI-driven formative peer assessment.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {159–169},
numpages = {11},
keywords = {Formative Peer Assessment, Feedback, Generative Artificial Intelligence, Trace Data, Dispositional Data},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706487,
author = {Li, Ziqing and Cukurova, Mutlu and Bulathwela, Sahan},
title = {A Novel Approach to Scalable and Automatic Topic-Controlled Question Generation in Education},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706487},
doi = {10.1145/3706468.3706487},
abstract = {The development of Automatic Question Generation (QG) models has the potential to significantly improve educational practices by reducing the teacher workload associated with creating educational content. This paper introduces a novel approach to educational question generation that controls the topical focus of questions. The proposed Topic-Controlled Question Generation (T-CQG) method enhances the relevance and effectiveness of the generated content for educational purposes. Our approach uses fine-tuning on a pre-trained T5-small model, employing specially created datasets tailored to educational needs. The research further explores the impacts of pre-training strategies, quantisation, and data augmentation on the model’s performance. We specifically address the challenge of generating semantically aligned questions with paragraph-level contexts, thereby improving the topic specificity of the generated questions. In addition, we introduce and explore novel evaluation methods to assess the topical relatedness of the generated questions. Our results, validated through rigorous offline and human-backed evaluations, demonstrate that the proposed models effectively generate high-quality, topic-focused questions. These models have the potential to reduce teacher workload and support personalised tutoring systems by serving as bespoke question generators. With its relatively small number of parameters, the proposals not only advance the capabilities of question generation models for handling specific educational topics but also offer a scalable solution that reduces infrastructure costs. This scalability makes them feasible for widespread use in education without reliance on proprietary large language models like ChatGPT.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {148–158},
numpages = {11},
keywords = {Educational Question Generation, Formative Assessment, Summative Assessment, Personalised Testing, Natural Language Processing},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706486,
author = {Yang, Tianyuan and Ren, Baofeng and Gu, Chenghao and Ma, Boxuan and He, Tianjia and Konomi, Shin'Ichi},
title = {Towards Better Course Recommendations: Integrating Multi-Perspective Meta-Paths and Knowledge Graphs},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706486},
doi = {10.1145/3706468.3706486},
abstract = {Course recommender systems demonstrate their potential in assisting students with course selection and effectively alleviating the problem of information overload. Current course recommender systems focus predominantly on collaborative information and fail to consider the multi-perspective information and the bi-directional relationship between students and courses. This paper introduces a novel Multi-perspective Aware Explainable Course Recommendation model (MAECR) that leverages knowledge graphs and multi-perspective meta-paths to enhance both the accuracy and explainability of course recommendations. By the dual-side modeling from both the student and the course for each meta-path, MAECR can identify and understand the interests and needs of students in each course, as well as evaluate the attractiveness and suitability of the courses for individual students. Following the dual-side modeling for each meta-path, we aggregate multi-perspective meta-paths of each student and course using a carefully designed attention mechanism. The attention weights generated by this mechanism serve as explanations for the recommendation results, representing the preference score for each perspective. MAECR thus provides personalized and explainable recommendations. Comprehensive experiments are implemented to demonstrate the effectiveness and improved interpretability of the proposed model.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {137–147},
numpages = {11},
keywords = {Course recommendation, Explainable recommender systems, Knowledge graphs},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706485,
author = {\v{S}v\'{a}bensk\'{y}, Valdemar and Borchers, Conrad and Cloude, Elizabeth B. and Shimada, Atsushi},
title = {Evaluating the Impact of Data Augmentation on Predictive Model Performance},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706485},
doi = {10.1145/3706468.3706485},
abstract = {In supervised machine learning (SML) research, large training datasets are essential for valid results. However, obtaining primary data in learning analytics (LA) is challenging. Data augmentation can address this by expanding and diversifying data, though its use in LA remains underexplored. This paper systematically compares data augmentation techniques and their impact on prediction performance in a typical LA task: prediction of academic outcomes. Augmentation is demonstrated on four SML models, which we successfully replicated from a previous LAK study based on AUC values. Among 21 augmentation techniques, SMOTE-ENN sampling performed the best, improving the average AUC by 0.01 and approximately halving the training time compared to the baseline models. In addition, we compared 99 combinations of chaining 21 techniques, and found minor, although statistically significant, improvements across models when adding noise to SMOTE-ENN (+0.014). Notably, some augmentation techniques significantly lowered predictive performance or increased performance fluctuation related to random chance. This paper’s contribution is twofold. Primarily, our empirical findings show that sampling techniques provide the most statistically reliable performance improvements for LA applications of SML, and are computationally more efficient than deep generation methods with complex hyperparameter settings. Second, the LA community may benefit from validating a recent study through independent replication.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {126–136},
numpages = {11},
keywords = {learning analytics, prediction, supervised learning, data generation, synthetic data, replication},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706484,
author = {Borchers, Conrad and Ooge, Jeroen and Peng, Cindy and Aleven, Vincent},
title = {How Learner Control and Explainable Learning Analytics About Skill Mastery Shape Student Desires to Finish and Avoid Loss in Tutored Practice},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706484},
doi = {10.1145/3706468.3706484},
abstract = {Personalized problem selection enhances student practice in tutoring systems. Prior research has focused on transparent problem selection that supports learner control but rarely engages learners in selecting practice materials. We explored how different levels of control (i.e., full AI control, shared control, and full learner control), combined with showing learning analytics on skill mastery and visual what-if explanations, can support students in practice contexts requiring high degrees of self-regulation, such as homework. Semi-structured interviews with six middle school students revealed three key insights: (1)&nbsp;participants highly valued learner control for an enhanced learning experience and better self-regulation, especially because most wanted to avoid losses in skill mastery; (2)&nbsp;only seeing their skill mastery estimates often made participants base problem selection on their weaknesses; and (3)&nbsp;what-if explanations stimulated participants to focus more on their strengths and improve skills until they were mastered. These findings show how explainable learning analytics could shape students’ selection strategies when they have control over what to practice. They suggest promising avenues for helping students learn to regulate their effort, motivation, and goals during practice with tutoring systems.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {810–816},
numpages = {7},
keywords = {explainable AI, mastery learning, intelligent tutoring systems, design-based research, K-12, self-regulated learning},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706483,
author = {Hedlin, Elias and Estling, Ludwig and Wong, Jacqueline and Demmans Epp, Carrie and Viberg, Olga},
title = {Got It! Prompting Readability Using ChatGPT to Enhance Academic Texts for Diverse Learning Needs},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706483},
doi = {10.1145/3706468.3706483},
abstract = {Reading skills are crucial for students' success in education and beyond. However, reading proficiency among K-12 students has been declining globally, including in Sweden, leaving many underprepared for post-secondary education. Additionally, an increasing number of students have reading disorders, such as dyslexia, which require support. Generative artificial intelligence (genAI) technologies, like ChatGPT, may offer new opportunities to improve reading practices by enhancing the readability of educational texts. This study investigates whether ChatGPT-4 can simplify academic texts and which prompting strategies are most effective. We tasked ChatGPT to re-write 136 academic texts using four prompting approaches: Standard, Meta, Roleplay, and Chain-of-Thought. All four approaches improved text readability, with Meta performing the best overall and the Standard prompt sometimes creating texts that were less readable than the original. This study found variability in the simplified texts, suggesting that different strategies should be used based on the specific needs of individual learners. Overall, the findings highlight the potential of genAI tools, like ChatGPT, to improve the accessibility of academic texts, offering valuable support for students with reading difficulties and promoting more equitable learning opportunities.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {115–125},
numpages = {11},
keywords = {Analytics, Equity, Large language models, Literacy, Prompt engineering, Readability},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706482,
author = {Garg, Manika and Goel, Anita},
title = {Towards Fair Assessments: A Machine Learning-based Approach for Detecting Cheating in Online Assessments},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706482},
doi = {10.1145/3706468.3706482},
abstract = {Academic cheating poses a significant challenge to conducting fair online assessments. One common way is collusion, where students unethically share answers during the assessment. While several researchers proposed solutions, there is lack of clarity regarding the specific types they target among the different types of collusion. Researchers have used statistical techniques to analyze basic attributes collected by the platforms, for collusion detection. Only few works have used machine learning, considering two or three attributes only; the use of limited features leading to reduced accuracy and increased risk of false accusations.In this work, we focus on In-Parallel Collusion, where students simultaneously work together on an assessment. For data collection, a quiz tool is improvised to capture clickstream data at a finer level of granularity. We use feature engineering to derive seven features and create a machine learning model for collusion detection. The results show: 1) Random Forest exhibits the best accuracy (98.8%), and 2) In contrast to less features as used in earlier works, the full feature set provides the best result; showing that considering multiple facets of similarity enhance the model accuracy. The findings provide platform designers and teachers with insights into optimizing quiz platforms and creating cheat-proof assessments.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {104–114},
numpages = {11},
keywords = {Academic dishonesty, Cheating, Feature engineering, Integrity, Machine learning, Online education},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706481,
author = {Ferreira Mello, Rafael and Pereira Junior, Cleon and Rodrigues, Luiz and Pereira, Filipe Dwan and Cabral, Luciano and Costa, Newarney and Ramalho, Geber and Gasevic, Dragan},
title = {Automatic Short Answer Grading in the LLM Era: Does GPT-4 with Prompt Engineering beat Traditional Models?},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706481},
doi = {10.1145/3706468.3706481},
abstract = {Assessing short answers in educational settings is challenging due to the need for scalability and accuracy, which led to the field of Automatic Short Answer Grading (ASAG). Traditional machine learning models, such as ensemble and embeddings, have been widely researched in ASAG, but they often suffer from generalizability issues. Recently, Large Language Models (LLMs) emerged as an alternative to optimize ASAG systems. However, previous research has failed to present a comprehensive analysis of LLMs’ performance powered by prompt engineering strategies and compare its capabilities to traditional models. This study presents a comparative analysis between traditional machine learning models and GPT-4 in the context of ASAG. We investigated the effectiveness of different models and text representation techniques and explored prompt engineering strategies for LLMs. The results indicate that traditional machine learning models outperform LLMs. However, GPT-4 showed promising capabilities, especially when configured with optimized prompt components, such as few-shot examples and clear instructions. This study contributes to the literature by providing a detailed evaluation of LLM performance compared to traditional machine learning models in a multilingual ASAG context, offering insights for developing more efficient automatic grading systems.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {93–103},
numpages = {11},
keywords = {Automatic short answer grading, Natural Language Processing, Assessment, LLM, GPT},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706480,
author = {Nguyen, Ha and Park, Saerok},
title = {Providing Automated Feedback on Formative Science Assessments: Uses of Multimodal Large Language Models},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706480},
doi = {10.1145/3706468.3706480},
abstract = {Formative assessment in science education often involves multimodality and combines textual and visual representations. We evaluate the capacity of multimodal large language models (MLLMs), including Anthropic’s Claude 3.5 Sonnet, Google’s Gemini 1.5 Flash, and OpenAI’s GPT-4o and GPT-4 Turbo, to score and provide feedback on multimodal science assessments. Overall, the MLLMs can accurately transcribe students’ hand-written text. The best performing models (Claude and GPT4-o) show moderate to substantial agreement with human evaluators in assessing students’ scientific reasoning. MLLMs provided with example responses, scores, and explanations (few-shot learning) generally perform better than those without examples (zero-shot learning). Thematic analysis reveals cases where the models misevaluate the depth in students’ answers, add details not included in the input (i.e., hallucinate), or show incorrect numerical reasoning. Findings demonstrate the feasibility of and considerations for using MLLMs to provide in-time feedback for science assessments. Such feedback can help to revise students’ understanding and inform teachers’ instructional practices.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {803–809},
numpages = {7},
keywords = {multimodal large language model, science assessment, automated evaluation},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706479,
author = {R\"{u}dian, Sylvio and Podelo, Julia and Ku\v{z}\'{\i}lek, Jakub and Pinkwart, Niels},
title = {Feedback on Feedback: Student’s Perceptions for Feedback from Teachers and Few-Shot LLMs},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706479},
doi = {10.1145/3706468.3706479},
abstract = {Large language models (LLMs) can be a valuable resource for generating texts and performing various instruction-based tasks. In this paper, we explored the use of LLMs, particularly for generating feedback for students in higher education. More precisely, we conducted an experiment to examine students’ perceptions regarding LLM-generated feedback. This has the overall aim of assisting teachers in the feedback creation process. First, we examine the different student perceptions regarding the feedback that students got without being aware of whether it was created by their teacher or an LLM. Our results reveal that the feedback source has not impacted how it was perceived by the students, except in cases where repetitive content has been generated, which is a known limitation of LLMs. Second, students have been asked to identify whether the feedback comes from an LLM or the teacher. The results demonstrate, that students were unable to identify the feedback source. A small subset of indicators has been identified, that clearly revealed from whom the feedback comes from. Third, student perceptions are analyzed while knowing that feedback has been auto-generated. This examination indicates that generated feedback is likely to be met with resistance. It contradicts the findings of the first examination. This emphasizes the need of a teacher-in-the-loop approach when employing auto-generated feedback in higher education.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {82–92},
numpages = {11},
keywords = {Large Language Models, Prompt Engineering, Feedback Indicators, Language Learning},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706478,
author = {Baker, Ryan and Hutt, Stephen},
title = {MORF: A Post-Mortem},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706478},
doi = {10.1145/3706468.3706478},
abstract = {There has been increasing interest in data enclaves in recent years, both in education and other fields. Data enclaves make it possible to conduct analysis on large-scale and higher-risk data sets, while protecting the privacy of the individuals whose data is included in the data sets, thus mitigating risks around data disclosure. In this article, we provide a post-mortem on the MORF (MOoc Replication Framework) 2.1 infrastructure, a data enclave expected to sunset and be replaced in the upcoming years, reviewing the core factors that reduced its usefulness for the community. We discuss challenges to researchers in terms of usability, including challenges involving learning to use core technologies, working with data that cannot be directly viewed, debugging, and working with restricted outputs. Our post-mortem discusses possibilities for ways that future infrastructures could get past these challenges.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {797–802},
numpages = {6},
keywords = {Data enclave, MORF, Privacy},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706477,
author = {Baker, Ryan and Mills, Caitlin and Choi, Jaeyoon},
title = {The Difficulty of Achieving High Precision with Low Base Rates for High-Stakes Intervention},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706477},
doi = {10.1145/3706468.3706477},
abstract = {Automated detectors are routinely used in learning analytics for high-stakes, high-risk interventions. Such interventions depend on detectors with a low rate of false positives (i.e., predicting the construct is present when it is not present) in order to avoid giving an intervention where it is not needed, especially when such interventions can be costly or even harmful. This in turn suggests that such a detector needs to have high precision at the cut-off used by the detector for decision-making.  However, high precision is difficult to achieve for the common case where the base rate of the target construct is low. In this paper, we demonstrate the difficulty of achieving high precision for low base rates, and demonstrate how other metrics (such as F1, Kappa, Specificity, and AUC ROC) are insufficient for this specific use case and situation, despite their merits and advantages for other use cases and situations.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {790–796},
numpages = {7},
keywords = {Automated Detection, Precision, Prediction Model, Unbalanced Data},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706476,
author = {Yan, Lixiang and Gasevic, Dragan and Echeverria, Vanessa and Jin, Yueqiao and Zhao, Linxuan and Martinez-Maldonado, Roberto},
title = {From Complexity to Parsimony: Integrating Latent Class Analysis to Uncover Multimodal Learning Patterns in Collaborative Learning},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706476},
doi = {10.1145/3706468.3706476},
abstract = {Multimodal Learning Analytics (MMLA) leverages advanced sensing technologies and artificial intelligence to capture complex learning processes, but integrating diverse data sources into cohesive insights remains challenging. This study introduces a novel methodology for integrating latent class analysis (LCA) within MMLA to map monomodal behavioural indicators into parsimonious multimodal ones. Using a high-fidelity healthcare simulation context, we collected positional, audio, and physiological data, deriving 17 monomodal indicators. LCA identified four distinct latent classes: Collaborative Communication, Embodied Collaboration, Distant Interaction, and Solitary Engagement, each capturing unique monomodal patterns. Epistemic network analysis compared these multimodal indicators with the original monomodal indicators and found that the multimodal approach was more parsimonious while offering higher explanatory power regarding students’ task and collaboration performances. The findings highlight the potential of LCA in simplifying the analysis of complex multimodal data while capturing nuanced, cross-modality behaviours, offering actionable insights for educators and enhancing the design of collaborative learning interventions. This study proposes a pathway for advancing MMLA, making it more parsimonious and manageable, and aligning with the principles of learner-centred education.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {70–81},
numpages = {12},
keywords = {multimodal learning analytics, collaborative learning, healthcare simulation, latent class analysis},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706475,
author = {Alfredo, Riordan and Mejia-Domenzain, Paola and Echeverria, Vanessa and Rahayu, Dwi and Zhao, Linxuan and Alajlan, Haya and Swiecki, Zachari and K\"{a}ser, Tanja and Ga\v{s}evi\'{c}, Dragan and Martinez-Maldonado, Roberto},
title = {TeamTeachingViz: Benefits, Challenges, and Ethical Considerations of Using a Multimodal Analytics Dashboard to Support Team Teaching Reflection},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706475},
doi = {10.1145/3706468.3706475},
abstract = {Team teaching in higher education can be challenging, especially for educators managing large classes with limited pedagogical training and few opportunities to reflect on their practices. Emerging sensing technologies and analytics can capture and analyse patterns of collaboration, communication, and movement of team teaching. Yet, few studies have presented these data to educators for reflection. To address this gap, we examine the benefits, challenges, and concerns of presenting multimodal teaching data (positional, audio, and spatial pedagogy observations) to educators via the TeamTeachingViz dashboard. We evaluated TeamTeachingViz in an authentic classroom context where educators explored their own data and team teaching strategies. Multimodal data was collected from 36 in-the-wild classroom sessions involving 12 educators grouped in various combinations over 4 weeks, followed by semi-structured interviews to reflect on their practices. Findings suggest that educators improved their self-awareness by using data-driven insights to understand their movements and interactions, enabling continuous improvement in team teaching. However, they noted the need for additional data, such as student behaviours and speech content, to better contextualise these insights.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {58–69},
numpages = {12},
keywords = {teaching analytics, LA dashboard, multimodal learning analytics, co-teaching, teaching reflection, spatial pedagogy, in-the-wild},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706474,
author = {Yin, Stella Xin and Liu, Zhengyuan and Goh, Dion Hoe-Lian and Quek, Choon Lang and Chen, Nancy F.},
title = {Scaling Up Collaborative Dialogue Analysis: An AI-driven Approach to Understanding Dialogue Patterns in Computational Thinking Education},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706474},
doi = {10.1145/3706468.3706474},
abstract = {Pair programming is a collaborative activity that enhances students’ computational thinking (CT) skills. Analyzing students’ interactions during pair programming provides valuable insights into effective learning. However, interpreting classroom dialogues is a challenging and complex task. Due to the simultaneous interaction between interlocutors and other ambient noise in collaborative learning contexts, previous work heavily relied on manual transcription and coding, which is labor-intensive and time-consuming. Recent advancements in speech and language processing offer promising opportunities to automate and scale up dialogue analysis. Besides, previous work mainly focused on task-related interactions, with little attention to social interactions. To address these gaps, we conducted a four-week CT course with 26 fifth-grade primary school students. We recorded their discussions, transcribed them with speech processing models, and developed a coding scheme and applied LLMs for annotation. Our AI-driven pipeline effectively analyzed classroom recordings with high accuracy and efficiency. After identifying the dialogue patterns, we investigated the relationships between these patterns and CT performance. Four clusters of dialogue patterns have been identified: Inquiry, Constructive Collaboration, Disengagement, and Disputation. We observed that Inquiry and Constructive Collaboration patterns were positively related to students’ CT skills, while Disengagement and Disputation patterns were associated with lower CT performance. This study contributes to the understanding of how dialogue patterns relate to CT performance and provides implications for both research and educational practice in CT learning.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {47–57},
numpages = {11},
keywords = {Collaborative learning, Computational thinking, Dialogue analysis, Large language models, Pair programming, Speech and language processing},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706473,
author = {Hui, Bowen and Adeyemi, Opey and Phan, Kiet and Schoenit, Justin and Akins, Seth and Khademi, Keyvan},
title = {Diversity Considerations in Team Formation Design, Algorithm, and Measurement},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706473},
doi = {10.1145/3706468.3706473},
abstract = {Building teams that foster equitable interaction provides the foundation for a positive collaborative learning experience. Existing literature shows that many context-specific algorithms exist to help instructors form teams automatically in large classes, but the field lacks general guidelines for selecting a suitable algorithm in a given pedagogical context and lacks a general evaluation approach that allows for the methodological comparison of these algorithms. This paper presents a general-purpose team formation algorithm that considers diversity and inclusion in its design. We also describe an evaluation framework with diversity metrics to assess team compositions using synthetically generated student data and real class data. Our simulation and classroom experiments show that our algorithm performs competitively against three state-of-the-art algorithms. We hope this work contributes to building a more equitable and collaborative learning environment for students.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {36–46},
numpages = {11},
keywords = {Team formation, evaluation, diversity metrics, hill climbing},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706472,
author = {Jaiyeola, Grace D. and Wong, Aaron Y. and Bryck, Richard L. and Mills, Caitlin and Hutt, Stephen},
title = {One Size Does Not Fit All: Considerations when using Webcam-Based Eye Tracking to Models of Neurodivergent Learners’ Attention and Comprehension},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706472},
doi = {10.1145/3706468.3706472},
abstract = {This study investigates the use of webcam-based eye tracking to model attention and comprehension in both neurotypical and neurodivergent learners. Leveraging the WebGazer, a previously used online data collection tool, we collected gaze and interaction data (N=354) during online reading tasks to explore task unrelated thought (TUT) and comprehension in an ecologically valid setting. Our findings challenge the "one size fits all" approach to learner modeling by demonstrating distinct differences in indicators of both constructs between neurotypical and neurodivergent learners. We compared general models trained on the entire population with tailored models specific to neurodivergent and neurotypical groups. Results indicate that diagnosis-specific models provide more accurate predictions (AUROC's .59-.70 vs. .57 for the general model), and through SHAPley analysis, we note that the strongest indicators of each construct vary as the training population is refined, highlighting the limitations of generalized approaches. This work supports the scalability of webcam-based cognitive modeling and underscores the potential for personalized learning analytics and modeling to better support diverse learning needs.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {24–35},
numpages = {12},
keywords = {Comprehension, Eye tracking, Learner modeling, Neurodivergent learners, Task Unrelated Thought (TUT)},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706471,
author = {Musabirov, Ilya and Reza, Mohi and Song, Haochen and Moore, Steven and Chen, Pan and Kumar, Harsh and Li, Tong and Stamper, John and Bier, Norman and Rafferty, Anna and Price, Thomas and Deliu, Nina and Durand, Audrey and Liut, Michael and Williams, Joseph Jay},
title = {Platform-based Adaptive Experimental Research in Education: Lessons Learned from The Digital Learning Challenge},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706471},
doi = {10.1145/3706468.3706471},
abstract = {Adaptive Experimentation is one of the most promising approaches to support complex decision-making in learning experience design and delivery. This paper reports on our experience with a real-world, multi-experimental evaluation of an adaptive experimentation platform within the XPRIZE Digital Learning Challenge framework, and summarizes data-driven lessons learned and best practices for Adaptive Experimentation in education. We outline key scenarios of the applicability of platform-supported experiments and reflect on lessons learned from this two-year project, focusing on implications relevant to platform developers, researchers, practitioners, and policy stakeholders to integrate Adaptive Experiments in real-world courses.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {13–23},
numpages = {11},
keywords = {adaptive experiments, posterior sampling, experimentation platforms, educational technology, human-computer interaction},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706470,
author = {Ooge, Jeroen and Vanneste, Arno and Szymanski, Maxwell and Verbert, Katrien},
title = {Designing Visual Explanations and Learner Controls to Engage Adolescents in AI-Supported Exercise Selection},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706470},
doi = {10.1145/3706468.3706470},
abstract = {E-learning platforms that personalise content selection with AI are often criticised for lacking transparency and controllability. Researchers have therefore proposed solutions such as open learner models and letting learners select from ranked recommendations, which engage learners before or after the AI-supported selection process. However, little research has explored how learners – especially adolescents – could engage during such AI-supported decision-making. To address this open challenge, we iteratively designed and implemented a control mechanism that enables learners to steer the difficulty of AI-compiled exercise series before practice, while interactively analysing their control’s impact in a what-if visualisation. We evaluated our prototypes through four qualitative studies involving adolescents, teachers, EdTech professionals, and pedagogical experts, focusing on different types of visual explanations for recommendations. Our findings suggest that why explanations do not always meet the explainability needs of young learners but can benefit teachers. Additionally, what-if explanations were well-received for their potential to boost motivation. Overall, our work illustrates how combining learner control and visual explanations can be operationalised on e-learning platforms for adolescents. Future research can build upon our designs for why and what-if explanations and verify our preliminary findings.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {1–12},
numpages = {12},
keywords = {explainable artificial intelligence, learner control, human-centred design, education, K-12, adaptive learning, self-regulated learning},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706469,
author = {Le Tallec, Julie and Prihar, Ethan and K\"{a}ser, Tanja},
title = {The Effect of Different Support Strategies on Student Affect},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706469},
doi = {10.1145/3706468.3706469},
abstract = {Within many online learning platforms, struggling students are provided with support to guide them through challenging material. Support comes in many forms, and is typically evaluated based on its ability to improve students’ performance on future tasks. However, there is little experimentation to evaluate how these supports impact students’ emotional states. Student’s emotional state, or affect, significantly impacts their motivation to engage with learning material and persist through challenges. Positive emotions can foster intrinsic engagement and deeper commitment, whereas negative emotions may lead to disengagement and avoidance of challenging tasks. In this work, we use publicly available data from online experiments and affect modeling to causally evaluate the impact that different support strategies have on students’ affect. Through analysis of 25 experiments with 6,463 total participants, we find multiple significant positive and negative changes in students’ affect when receiving hints, examples, or scaffolding questions, despite all three having a positive impact on performance, revealing the need for more nuanced evaluations of support strategies to uncover their impact beyond just performance. The code for this project is available at https://osf.io/74dgx.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {783–789},
numpages = {7},
keywords = {Online Tutoring, Affect Detection, Randomized Controlled Experiments},
location = {
},
series = {LAK '25}
}

@proceedings{10.1145/3706468,
title = {LAK '25: Proceedings of the 15th International Learning Analytics and Knowledge Conference},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@inproceedings{10.1145/3636555.3636939,
author = {Song, Yige and Oliveira, Eduardo and Kirley, Michael and Thompson, Pauline},
title = {A Case Study on University Student Online Learning Patterns Across Multidisciplinary Subjects},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636939},
doi = {10.1145/3636555.3636939},
abstract = {This case study explores the online learning patterns of a cohort of first-year university students in two subjects: a compulsory science subject and an introductory programming subject, by analysing trace data from the Learning Management Systems (LMS). The methodology extends existing learning analytics techniques to incorporate temporal aspects of students’ learning, such as session duration and weekly online behaviours. By examining over 82,000 learning actions, the research unveils significant variations in students’ online learning strategies between subjects, offering deeper insights into these differences and their associated challenges. The study seeks to initiate broader discussions in learning analytics, emphasising the need to comprehend students’ diverse online learning experiences and encouraging further exploration in future research.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {936–942},
numpages = {7},
keywords = {Blended learning, Learning analytics, Learning strategy, Learning tactic, Trace data},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636938,
author = {Kitto, Kirsty and Gibson, Andrew},
title = {Places to intervene in complex learning systems},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636938},
doi = {10.1145/3636555.3636938},
abstract = {Responding to recent questioning of Learning Analytics (LA) as a field that is achieving its aim of understanding and optimising learning and the environments in which it occurs, this paper argues that there is a need to genuinely embrace the complexity of learning when considering the impact of LA. Rather than focusing upon ‘optimisation’, we propose that LA should seek to understand and improve the complex socio-technical system in which it operates. We adopt a framework from systems theory to propose 12 different intervention points for learning systems, and apply it to two case studies. We conclude with an invitation to the community to critique and extend this proposed framework.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {929–935},
numpages = {7},
keywords = {Complex Systems, Intervention, Learning, Theory},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636937,
author = {Booth, Brandon M. and Jacobs, Jennifer and Bush, Jeffrey B. and Milne, Brent and Fischaber, Tom and DMello, Sidney K.},
title = {Human-tutor Coaching Technology (HTCT): Automated Discourse Analytics in a Coached Tutoring Model},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636937},
doi = {10.1145/3636555.3636937},
abstract = {High-dosage tutoring has become an effective strategy for bolstering K-12 academic performance and combating education declines accelerated by the COVID-19 pandemic. To achieve high-dosage tutoring at scale, tutoring programs often rely on paraprofessional tutors—recruited tutors with college degrees who lack formal training in education—however, these tutors may require consistent and targeted feedback from instructional coaches for improvement. Accordingly, we developed a human-tutor coaching technology (HTCT) system to automatically extract discourse analytics pertaining to accountable talk moves (or academically productive talk) from tutoring sessions and provide feedback visualizations to coaches to aid their coaching sessions with tutors. We deployed HTCT in a user study using a virtual tutoring platform with 11 real coaches, 40 tutors, and their students to investigate coaches’ usage patterns with HTCT, perceptions of its utility, and changes in tutors’ talk. Overall, we found that coaches had positive perceptions of the system. We also observed an increase in accountable talk from tutors whose coaches used HTCT compared to tutors whose coaches did not. We discuss implications for AI-based applications which offer coaches a promising way to provide personalized, automated, and data-driven feedback to scale high-dosage tutoring.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {725–735},
numpages = {11},
keywords = {coached tutoring, discourse analytics, in situ user study, natural language processing},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636936,
author = {Xu, Zhen and Olson, Joseph and Pochinki, Nicole and Zheng, Zhijian and Yu, Renzhe},
title = {Contexts Matter but How? Course-Level Correlates of Performance and Fairness Shift in Predictive Model Transfer},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636936},
doi = {10.1145/3636555.3636936},
abstract = {Learning analytics research has highlighted that contexts matter for predictive models, but little research has explicated how contexts matter for models’ utility. Such insights are critical for real-world applications where predictive models are frequently deployed across instructional and institutional contexts. Building upon administrative records and behavioral traces from 37,089 students across 1,493 courses, we provide a comprehensive evaluation of performance and fairness shifts of predictive models when transferred across different course contexts. We specifically quantify how differences in various contextual factors moderate model portability. Our findings indicate an average decline in model performance and inconsistent directions in fairness shifts, without a direct trade-off, when models are transferred across different courses within the same institution. Among the course-to-course contextual differences we examined, differences in admin features account for the largest portion of both performance and fairness loss. Differences in student composition can simultaneously amplify drops in performance and fairness while differences in learning design have a greater impact on performance degradation. Given these complexities, our results highlight the importance of considering multiple dimensions of course contexts and evaluating fairness shifts in addition to performance loss when conducting transfer learning of predictive models in education.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {713–724},
numpages = {12},
keywords = {Algorithmic Fairness, Higher Education, Intersectionality, Learning Management System, Predictive Analytics, Transfer Learning},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636935,
author = {Wang, Zuo and Ng, Jeremy Tzi Dong and Que, Ying and Hu, Xiao},
title = {Unveiling Synchrony of Learners’ Multimodal Data in Collaborative Maker Activities},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636935},
doi = {10.1145/3636555.3636935},
abstract = {While current evaluation of maker activities has rarely explored students’ learning processes, the multi-perspective and multi-level nature of collaboration adds complexity to learning processes of collaborative maker activities. In terms of group dynamics as an important indicator of collaboration quality, extant studies have shown the benefits of synchrony between learners’ actions during collaborative learning processes. However, synchrony of learners’ cognitive processes and visual attention in collaborative maker activities remains under-explored. Leveraging the multimodal learning analytics (MMLA) approach, this pilot study examines learners’ synchrony patterns from multiple modalities of data in the collaborative maker activity of virtual reality (VR) content creation. We conducted a user experiment with five pairs of students, and collected and analyzed their electroencephalography (EEG) signals, eye movement and system log data. Results showed that the five pairs of collaborators demonstrated diverse synchrony patterns. We also discovered that, while some groups exhibited synchrony in one modality of data before becoming not synchronized in another modality, other groups started with a lack of synchrony followed by maintaining synchrony. This study is expected to make methodological and practical contributions to MMLA research and assessment of collaborative maker activities.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {922–928},
numpages = {7},
keywords = {Computer-supported collaborative learning, Maker activities, Multimodal learning analytics},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636934,
author = {Rakovi\'{c}, Mladen and Li, Yuheng and Foumani, Navid Mohammadi and Salehi, Mahsa and Kuhlmann, Levin and Mackellar, Geoffrey and Martinez-Maldonado, Roberto and Haffari, Gholamreza and Swiecki, Zachari and Li, Xinyu and Chen, Guanliang and Ga\v{s}evi\'{c}, Dragan},
title = {Measuring Affective and Motivational States as Conditions for Cognitive and Metacognitive Processing in Self-Regulated Learning},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636934},
doi = {10.1145/3636555.3636934},
abstract = {Even though the engagement in self-regulated learning (SRL) has been shown to boost academic performance, SRL skills of many learners remain underdeveloped. They often struggle to productively navigate multiple cognitive, affective, metacognitive and motivational (CAMM) processes in SRL. To provide learners with the required SRL support, it is essential to understand how learners enact CAMM processes as they study. More research is needed to advance the measurement of affective and motivational processes within SRL, and investigate how these processes influence learners’ cognition and metacognition. With this in mind, we conducted a lab study involving 22 university students who worked on a 45-minute reading and writing task in digital learning environment. We used a wearable electroencephalogram device to record learner academic emotional and motivational states, and digital trace data to record learner cognitive and metacognitive processes. We harnessed time series prediction and explainable artificial intelligence methods to examine how learner’s emotional and motivational states influence their choice of cognitive and metacognitive processes. Our results indicate that emotional and motivational states can predict learners’ use of low cognitive, high cognitive and metacognitive processes with considerable classification accuracy (F1 &gt; 0.73), and that higher values of interest, engagement and excitement promote cognitive processing.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {701–712},
numpages = {12},
keywords = {electroencephalography, multi-modal learning analytics, self-regulated learning, time-series classification},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636933,
author = {Szymanski, Maxwell and Ooge, Jeroen and De Croon, Robin and Vanden Abeele, Vero and Verbert, Katrien},
title = {Feedback, Control, or Explanations? Supporting Teachers With Steerable Distractor-Generating AI},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636933},
doi = {10.1145/3636555.3636933},
abstract = {Recent advancements in Educational AI have focused on models for automatic question generation. Yet, these advancements face challenges: (1) their "black-box" nature limits transparency, thereby obscuring the decision-making process; and (2) their novelty sometimes causes inaccuracies due to limited feedback systems. Explainable AI (XAI) aims to address the first limitation by clarifying model decisions, while Interactive Machine Learning (IML) emphasises user feedback and model refinement. However, both XAI and IML solutions primarily serve AI experts, often neglecting novices like teachers. Such oversights lead to issues like misaligned expectations and reduced trust. Following the user-centred design method, we collaborated with teachers and ed-tech experts to develop an AI-aided system for generating multiple-choice question distractors, which incorporates feedback, control, and visual explanations. Evaluating these through semi-structured interviews with 12 teachers, we found a strong preference for the feedback feature, enabling teacher-guided AI improvements. Control and explanations’ usefulness was largely dependent on model performance: they were valued when the model performed well. If the model did not perform well, teachers sought context over AI-centric explanations, suggesting a tilt towards data-centric explanations. Based on these results, we propose guidelines for creating tools that enable teachers to steer and interact with question-generating AI models.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {690–700},
numpages = {11},
keywords = {Interactive Machine Learning, XAI, automated question generation, user control, user studies},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636932,
author = {Wang, Chao and Hu, Xiao and Hern\'{a}ndez L\'{o}pez, Nora Patricia and Ng, Jeremy Tzi Dong},
title = {Needs Analysis of Learning Analytics Dashboard for College Teacher Online Professional Learning in an International Training Initiative for the Global South},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636932},
doi = {10.1145/3636555.3636932},
abstract = {Online courses enable wide access to educational resources and thus provide a feasible platform for cross-regional teacher professional learning. Learning analytics dashboards (LAD) can support online learners by providing fine-grained feedback generated from learners’ interactions with platforms. Nevertheless, most studies on teacher online professional learning focus on resource-rich and technology-advanced regions, with scarce attention to the Global South. Furthermore, existing studies on LAD design mainly target students’ learning, rather than teachers’ professional learning. Therefore, it is much needed to develop LAD for teacher-learners online professional learning in the Global South. Contextualized in an international online professional training initiative, this study conducted in-depth interviews with 42 teacher-learners from 19 countries in the Global South, aiming to identify their needs for 1) support on their self-regulated learning (SRL), and 2) potential LA components in dashboards. Findings indicated that teacher-learners needed support for self-regulated learning strategies, including motivation maintenance, time management, environment structuring, help-seeking, and self-evaluation. Nine LA features were identified to design the LADs to support SRL preliminarily. This co-designed LAD study with interviewees improved our understanding on the needs of college teachers in the Global South for LA support during their online professional learning, generating practical insights into needs-driven LAD designs.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {915–921},
numpages = {7},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636931,
author = {Rashid, M Parvez and Gehringer, Edward and Khosravi, Hassan},
title = {Navigating (Dis)agreement: AI Assistance to Uncover Peer Feedback Discrepancies},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636931},
doi = {10.1145/3636555.3636931},
abstract = {Engaging students in the peer review process has been recognized as a valuable educational tool. It not only nurtures a collaborative learning environment where reviewees receive timely and rich feedback but also enhances the reviewer’s critical thinking skills and encourages reflective self-evaluation. However, a common concern arises when students encounter misaligned or conflicting feedback. Not only can such feedback confuse students; but it can also make it difficult for the instructor to rely on the reviews when assigning a score to the work. Addressing this pressing issue, our paper introduces an innovative, AI-assisted approach that is designed to detect and highlight disagreements within formative feedback. We’ve harnessed extensive data from 170 students, analyzing 15,500 instances of peer feedback from a software development course. By utilizing clustering techniques coupled with sophisticated natural language processing (NLP) models, we transform feedback into distinct feature vectors to pinpoint disagreements. The findings from our study underscore the effectiveness of our approach in enhancing text representations to significantly boost the capability of clustering algorithms in discerning disagreements in feedback. These insights bear implications for educators and software development courses, offering a promising route to streamline and refine the peer review process for the betterment of student learning outcomes.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {907–914},
numpages = {8},
keywords = {NLP, clustering, disagreement, feedback, peer-assessment},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636930,
author = {Fernandez-Nieto, Gloria Milena and Martinez-Maldonado, Roberto and Echeverria, Vanessa and Kitto, Kirsty and Ga\v{s}evi\'{c}, Dragan and Buckingham Shum, Simon},
title = {Data Storytelling Editor: A Teacher-Centred Tool for Customising Learning Analytics Dashboard Narratives},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636930},
doi = {10.1145/3636555.3636930},
abstract = {Dashboards are increasingly used in education to provide teachers and students with insights into learning. Yet, existing dashboards are often criticised for their failure to provide the contextual information or explanations necessary to help students interpret these data. Data Storytelling (DS) is emerging as an alternative way to communicate insights providing guidance and context to facilitate students’ interpretations. However, while data stories have proven effective in prompting students’ reflections, to date, it has been necessary for researchers to craft the stories rather than enabling teachers to do this by themselves. This can make this approach more feasible and scalable while also respecting teachers’ agency. Based on the notion of DS, this paper presents a DS editor for teachers. A study was conducted in two universities to examine whether the editor could enable teachers to create stories adapted to their learning designs. Results showed that teachers appreciated how the tool enabled them to contextualise automated feedback to their teaching needs, generating data stories to support student reflection.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {678–689},
numpages = {12},
keywords = {LA Dashboards, data storytelling, teacher-centered tool},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636929,
author = {Pradhan, Siddhartha and Gurung, Ashish and Ottmar, Erin},
title = {Gamification and Deadending: Unpacking Performance Impacts in Algebraic Learning},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636929},
doi = {10.1145/3636555.3636929},
abstract = {This study explores the effects of varying problem-solving strategies on students’ future performance within the gamified algebraic learning platform From Here To There! (FH2T). The study focuses on the procedural pathways students adopted, transitioning from a start state to a goal state in solving algebraic problems. By dissecting the nature of these pathways—optimal, sub-optimal, incomplete, and dead-end—we sought correlations with post-test outcomes. A striking observation was that students who frequently engaged in what we term ‘regular dead-ending behavior’, were significantly correlated with higher post-test performance. This finding underscores the potential of exploratory learner behavior within a low-stakes gamified framework in bolstering algebraic comprehension. The implications of our findings are twofold: they accentuate the significance of tailoring gamified platforms to student behaviors and highlight the potential benefits of fostering an environment that promotes exploration without retribution. Moreover, our insights hint at the notion that fostering exploratory behavior could be instrumental in cultivating mathematical flexibility.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {899–906},
numpages = {8},
keywords = {algebraic learning, gamification, math flexibility, networks, procedural pathways},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636928,
author = {Dwivedi, Deep and Garg, Ritik and Baghel, Shiva and Thareja, Rushil and Kulshrestha, Ritvik and Mohania, Mukesh and Shukla, Jainendra},
title = {Effecti-Net: A Multimodal Framework and Database for Educational Content Effectiveness Analysis},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636928},
doi = {10.1145/3636555.3636928},
abstract = {Amid the evolving landscape of education, evaluating the impact of educational video content on students remains a challenge. Existing methods for assessment often rely on heuristics and self-reporting, leaving room for subjectivity and limited insight. This study addresses this issue by leveraging physiological sensor data to predict student-perceived content effectiveness. Within the realm of educational content evaluation, prior studies focused on conventional approaches, leaving a gap in understanding the nuanced responses of students to educational materials. To bridge this gap, our research introduces a novel perspective, building upon previous work in multimodal physiological data analysis. Our primary contributions encompass two key elements. First, we present the ’Effecti-Net’ architecture, a sophisticated deep learning model that integrates data from multiple sensor modalities, including Electroencephalogram (EEG), Eye Tracker, Galvanic Skin Response (GSR), and Photoplethysmography (PPG). Second, we introduce the ’DECEP’ dataset, a repository comprising 597 minutes of multimodal sensor data. To assess the effectiveness of our approach, we benchmark it against conventional methods. Remarkably, our model achieves a lowest MSE of 0.1651 and MAE of 0.3544 on the DECEP dataset. It offers educators and content creators a comprehensive framework that promotes the development of more engaging educational content.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {667–677},
numpages = {11},
keywords = {Affective Datasets, Deep Learning, Machine Learning, Multi-Modal frameworks, Physiological Data Analysis, User Studies},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636927,
author = {Iqbal, Sehrish and Rakovic, Mladen and Chen, Guanliang and Li, Tongguang and Bajaj, Jasmine and Mello, Rafael Ferreira and Fan, Yizhou and Aljohani, Naif Radi and Gasevic, Dragan},
title = {Towards Improving Rhetorical Categories Classification and Unveiling Sequential Patterns in Students' Writing},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636927},
doi = {10.1145/3636555.3636927},
abstract = {To meet the growing demand for future professionals who can present information to an audience and create quality written products, educators are increasingly assigning writing assignments that require students to gather information from multiple sources, reorganise and reinterpret knowledge from source materials, and plan for rhetorical structure goals in order to meet the task requirements. When evaluating an essay coherence, scorers manually look for the presence of required rhetorical categories, which takes time. Supervised Machine Learning (ML) techniques have proven to be an effective tool for automatic detection of rhetorical categories that approximate students’ cognitive engagement with source information. Previous studies that addressed this problem used relatively small datasets and reported relatively low kappa scores for accuracy, limiting the use of such models in real-world scenarios. Moreover, to empower educators to effectively evaluate the overall quality of students’ writing, the associations between the sequential patterns of rhetorical categories in students’ writing and writing performance must be examined, which remains largely unexplored in educational domain. Therefore, to fill these gaps, our study aimed to i) investigate the impact of data augmentation approaches on the performance of deep learning algorithms in classifying rhetorical categories in student essays according to Bloom‘s taxonomy ii) and explore the sequential patterns of rhetorical categories in students’ writing that can influence writing performance. Our findings showed that deep learning-based model BERT on Easy Data Augmentation (EDA) based augmented data achieved 20% higher Cohen’s kappa than normal (non-augmented) data, and we discovered that students in different performance groups were statistically different in terms of rhetorical patterns. Our proposed study is valuable in terms of building a data analytic foundation that can be used to create formative feedback on students’ writings based on the patterns of rhetorical categories to improve essay quality.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {656–666},
numpages = {11},
keywords = {Rhetorical structure, automatic classification, deep learning, essay analysis, ordered network analysis},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636926,
author = {Nath, Debarshi and Gasevic, Dragan and Fan, Yizhou and Rajendran, Ramkumar},
title = {CTAM4SRL: A Consolidated Temporal Analytic Method for Analysis of Self-Regulated Learning},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636926},
doi = {10.1145/3636555.3636926},
abstract = {Temporality in Self-Regulated Learning (SRL) has two perspectives: one as a passage of time and the other as an ordered sequence of events. Each of these conceptions is distinct and requires independent considerations. Only a single analytic method is not sufficient in adequately capturing both these facets of temporality. Yet, most research uses a single method in temporally-focused SRL research, and those that use multiple methods do not address both aspects of temporality. We propose CTAM4SRL, a consolidated temporal analytic method which combines advanced data visualisation, network analysis and pattern mining to capture both facets of temporality. We employ CTAM4SRL in a cohort of 36 learners engaged in a reading-writing activity. Using CTAM4SRL, we were able to provide a rich temporal explanation of the interplay of the self-regulatory processes of the learners. We were further able to identify differences in SRL behaviours in high and low performers in terms of their approach to learning comprising deep and surface strategies. High performers were able to more selectively and strategically combine deep and surface learning strategies when compared to low scorers– a behaviour which was only hypothesised in SRL literature previously, but now has empirical support provided by our consolidated analytic method.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {645–655},
numpages = {11},
keywords = {Learning Analytics, Ordered Network Analysis, Pattern Mining, Self-Regulated Learning, Visualisation Techniques},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636924,
author = {Demszky, Dorottya and Wang, Rose and Geraghty, Sean and Yu, Carol},
title = {Does Feedback on Talk Time Increase Student Engagement? Evidence from a Randomized Controlled Trial on a Math Tutoring Platform},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636924},
doi = {10.1145/3636555.3636924},
abstract = {Providing ample opportunities for students to express their thinking is pivotal to their learning of mathematical concepts. We introduce the Talk Meter, which provides in-the-moment automated feedback on student-teacher talk ratios. We conduct a randomized controlled trial on a virtual math tutoring platform (n=742 tutors) to evaluate the effectiveness of the Talk Meter at increasing student talk. In one treatment arm, we show the Talk Meter only to the tutor, while in the other arm we show it to both the student and the tutor. We find that the Talk Meter increases student talk ratios in both treatment conditions by 13-14%; this trend is driven by the tutor talking less in the tutor-facing condition, whereas in the student-facing condition it is driven by the student expressing significantly more mathematical thinking. Through interviews with tutors, we find the student-facing Talk Meter was more motivating to students, especially those with introverted personalities, and was effective at encouraging joint effort towards balanced talk time. These results demonstrate the promise of in-the-moment joint talk time feedback to both teachers and students as a low cost, engaging, and scalable way to increase students’ mathematical reasoning.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {632–644},
numpages = {13},
keywords = {automated feedback, joint feedback to students and teachers, math tutoring, randomized controlled trial, student engagement, talk time},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636923,
author = {Asatryan, Hayk and Tousside, Basile and Mohr, Janis and Neugebauer, Malte and Bijl, Hildo and Spiegelberg, Paul and Frohn-Schauf, Claudia and Frochte, J\"{o}rg},
title = {Exploring Student Expectations and Confidence in Learning Analytics},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636923},
doi = {10.1145/3636555.3636923},
abstract = {Learning Analytics (LA) is nowadays ubiquitous in many educational systems, providing the ability to collect and analyze student data in order to understand and optimize learning and the environments in which it occurs. On the other hand, the collection of data requires to comply with the growing demand regarding privacy legislation. In this paper, we use the Student Expectation of Learning Analytics Questionnaire (SELAQ) to analyze the expectations and confidence of students from different faculties regarding the processing of their data for Learning Analytics purposes. This allows us to identify four clusters of students through clustering algorithms: Enthusiasts, Realists, Cautious and Indifferents. This structured analysis provides valuable insights into the acceptance and criticism of Learning Analytics among students.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {892–898},
numpages = {7},
keywords = {Clustering, Data Protection, Learning Analytics, Survey},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636922,
author = {Samadi, Mohammad Amin and Jaquay, Spencer and Lin, Yiwen and Tajik, Elham and Park, Seehee and Nixon, Nia},
title = {Minds and Machines Unite: Deciphering Social and Cognitive Dynamics in Collaborative Problem Solving with AI},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636922},
doi = {10.1145/3636555.3636922},
abstract = {We investigated the feasibility of automating the modeling of collaborative problem-solving skills encompassing both social and cognitive aspects. Leveraging a diverse array of cutting-edge techniques, including machine learning, deep learning, and large language models, we embarked on the classification of qualitatively coded interactions within groups. These groups were composed of four undergraduate students, each randomly assigned to tackle a decision-making challenge. Our dataset comprises contributions from 514 participants distributed across 129 groups. Employing a suite of prominent machine learning methods such as Random Forest, Support Vector Machines, Naive Bayes, Recurrent and Convolutional Neural Networks, BERT, and GPT-2 language models, we undertook the intricate task of classifying peer interactions. Notably, we introduced a novel task-based train-test split methodology, allowing us to assess classification performance independently of task-related context. This research carries significant implications for the learning analytics field by demonstrating the potential for automated modeling of collaborative problem-solving skills, offering new avenues for understanding and enhancing group learning dynamics.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {885–891},
numpages = {7},
keywords = {Artificial Intelligence, CPS, Learning Analytics, Machine Learning, NLP},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636921,
author = {Liu, Qinyi and Khalil, Mohammad and Jovanovic, Jelena and Shakya, Ronas},
title = {Scaling While Privacy Preserving: A Comprehensive Synthetic Tabular Data Generation and Evaluation in Learning Analytics},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636921},
doi = {10.1145/3636555.3636921},
abstract = {Privacy poses a significant obstacle to the progress of learning analytics (LA), presenting challenges like inadequate anonymization and data misuse that current solutions struggle to address. Synthetic data emerges as a potential remedy, offering robust privacy protection. However, prior LA research on synthetic data lacks thorough evaluation, essential for assessing the delicate balance between privacy and data utility. Synthetic data must not only enhance privacy but also remain practical for data analytics. Moreover, diverse LA scenarios come with varying privacy and utility needs, making the selection of an appropriate synthetic data approach a pressing challenge. To address these gaps, we propose a comprehensive evaluation of synthetic data, which encompasses three dimensions of synthetic data quality, namely resemblance, utility, and privacy. We apply this evaluation to three distinct LA datasets, using three different synthetic data generation methods. Our results show that synthetic data can maintain similar utility (i.e., predictive performance) as real data, while preserving privacy. Furthermore, considering different privacy and data utility requirements in different LA scenarios, we make customized recommendations for synthetic data generation. This paper not only presents a comprehensive evaluation of synthetic data but also illustrates its potential in mitigating privacy concerns within the field of LA, thus contributing to a wider application of synthetic data in LA and promoting a better practice for open science.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {620–631},
numpages = {12},
keywords = {Generative adversarial network, Learning analytics, Privacy Preserving Technologies, Synthetic data generation},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636920,
author = {Li, Lin and Srivastava, Namrata and Rong, Jia and Pianta, Gina and Varanasi, Raju and Ga\v{s}evi\'{c}, Dragan and Chen, Guanliang},
title = {Unveiling Goods and Bads: A Critical Analysis of Machine Learning Predictions of Standardized Test Performance in Early Childhood Education},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636920},
doi = {10.1145/3636555.3636920},
abstract = {Learning analytics (LA) holds a promise to transform education by utilizing data for evidence-based decision-making. Yet, its application in early childhood education (ECE) remains relatively under-explored. ECE plays a crucial role in fostering fundamental numeracy and literacy skills. While standardized tests was intended to be used to monitor student progress, they have been increasingly assumed summative and high-stake due to the substantial impact. The pressures in succeeding in such standardized tests have been well-documented to negatively affect both students and teachers. Attempting to ease such stress and better support students and teachers, the current study delved into the LA potential for predicting standardized test performance using formative assessments. Beyond predictive accuracy, the study addressed ethical considerations related to fairness to uncover potential risks associated with LA adoption. Our findings revealed a promising opportunity to empower teachers and schools with more time and room to help students better prepared based on predictions obtained earlier before standardized tests. Notably, bias can be significantly observed in predictions for students with disabilities even they have same actual competence compared to students without disabilities. In addition, we noticed that inclusion of demographic attribute had no significant impact on the predictive accuracy, and not necessarily exacerbate the overall predictive bias, but may significantly affect the predictions received by certain demographic subgroups (e.g., students with different types of disability).},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {608–619},
numpages = {12},
keywords = {bias, early childhood education, fairness, learning analytics, machine learning, standardized tests},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636919,
author = {Sheel, Shreya and Anastasopoulos, Ioannis and Pardos, Zach A.},
title = {Comparing Authoring Experiences with Spreadsheet Interfaces vs GUIs},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636919},
doi = {10.1145/3636555.3636919},
abstract = {There is little consensus over whether graphical user interfaces (GUIs) or programmatic systems are better for word processing. Even less is known about each interfaces’ affordances and limitations in the context of creating content for adaptive tutoring systems. In order to afford instructors the use of such systems with their own or adapted pedagogies, we must study their experiences in inputting their content. In this study, we conduct a between-subjects A/B test with two content authoring interfaces, a GUI and spreadsheet, to explore 32 instructors’ experiences in authoring algebra content with hints, scaffolds, images, and special characters. We study their experiences by measuring time taken, accuracy, and their perceptions of each interfaces’ usability. Our findings indicate no significant relationship between interface used and time taken authoring problems but significantly more accuracy in authoring problems in the spreadsheet interface over the GUI. Although both interfaces performed reasonably well in time taken and accuracy, both were perceived as average to low in usability, highlighting a dissonance between instructors’ perceptions and actual performances. Since both interfaces are reasonable in authoring content, other factors can be explored, such as cost and author incentive, when deciding which interface approach to take for authoring tutor content.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {598–607},
numpages = {10},
keywords = {A/B testing, adaptive tutoring systems, content authoring, human-computer interaction, usability},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636918,
author = {Feng, Shihui and Yan, Lixiang and Zhao, Linxuan and Maldonado, Roberto Martinez and Ga\v{s}evi\'{c}, Dragan},
title = {Heterogenous Network Analytics of Small Group Teamwork: Using Multimodal Data to Uncover Individual Behavioral Engagement Strategies},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636918},
doi = {10.1145/3636555.3636918},
abstract = {Individual behavioral engagement is an important indicator of active learning in collaborative settings, encompassing multidimensional behaviors mediated through various interaction modes. Little existing work has explored the use of multimodal process data to understand individual behavioral engagement in face-to-face collaborative learning settings. In this study we bridge this gap, for the first time, introducing a heterogeneous tripartite network approach to analyze the interconnections among multimodal process data in collaborative learning. Students’ behavioral engagement strategies are analyzed based on their interaction patterns with various spatial locations and verbal communication types using a heterogeneous tripartite network. The multimodal collaborative learning process data were collected from 15 teams of four students. We conducted stochastic blockmodeling on a projection of the heterogeneous tripartite network to cluster students into groups that shared similar spatial and oral engagement patterns. We found two distinct clusters of students, whose characteristic behavioural engagement strategies were identified by extracting interaction patterns that were statistically significant relative to a multinomial null model. The two identified clusters also exhibited a statistically significant difference regarding students’ perceived collaboration satisfaction and teacher-assessed team performance level. This study advances collaboration analytics methodology and provides new insights into personalized support in collaborative learning.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {587–597},
numpages = {11},
keywords = {collaborative learning, heterogeneous networks, individual engagement, multimodal learning analytics},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636917,
author = {Chandler, Chelsea and Breideband, Thomas and Reitman, Jason G. and Chitwood, Marissa and Bush, Jeffrey B. and Howard, Amanda and Leonhart, Sarah and Foltz, Peter W. and Penuel, William R. and D'Mello, Sidney K.},
title = {Computational Modeling of Collaborative Discourse to Enable Feedback and Reflection in Middle School Classrooms},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636917},
doi = {10.1145/3636555.3636917},
abstract = {Collaboration analytics has the potential to empower teachers and students with valuable insights to facilitate more meaningful and engaging collaborative learning experiences. Towards this end, we developed computational models of student speech during small group work, identifying instances of uplifting behavior related to three Community Agreements: community building, moving thinking forward, and being respectful. Pre-trained RoBERTa language models were fine-tuned and evaluated on human annotated data (N = 9,607 student utterances from 100 unique 5-minute classroom recordings). The models achieved moderate accuracies (AUROCs between 0.67-0.84) and were robust to speech recognition errors. Preliminary generalizability studies indicated that the models generalized well to two other domains (transfer ratios between 0.46-0.85; with 1.0 indicating perfect transfer). We also developed four approaches to provide qualitative feedback in the form of noticings (i.e., specific exemplars) of positive instances of the Community Agreements, finding moderate alignment with human ratings. This research contributes to the computational modeling of the relationship dimension of collaboration from noisy classroom data, selection of positive examples for qualitative feedback, and towards the empowerment of teachers to support diverse learners during collaborative learning.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {576–586},
numpages = {11},
keywords = {Collaboration analytics, Natural language processing},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636916,
author = {Ramanathan, Sriram and Buckingham Shum, Simon and Lim, Lisa-Angelique},
title = {To what extent do responses to a single survey question provide insights into students' sense of belonging?},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636916},
doi = {10.1145/3636555.3636916},
abstract = {A student's “sense of belonging” is critical to retention and success in higher education. However, belonging is a multifaceted and dynamic concept, making monitoring and supporting it with timely action challenging. Conventional approaches to researching belonging depend on lengthy surveys and/or focus groups, and while often insightful, these are resource-intensive, slow, and cannot be repeated too often. “Belonging Analytics” is an emerging concept pointing to the potential of learning analytics to address this challenge, and to illustrate this concept, this paper investigates the feasibility of asking students a single question about what promotes their sense of belonging. To validate this, responses were analysed using a form of topic modelling, and these were triangulated by examining alignment with (i) students’ responses to Likert scale items in a belonging scale and (ii)&nbsp;the literature on the drivers of belonging. These alignments support our proposal that this is a practical tool to gain timely insight into a cohort's sense of belonging. Reflecting our focus on practical tools, the approach is implemented using analytics products readily available to educational institutions — Linguistic Inquiry Word Count (LIWC) and Statistical Program for Social Sciences (SPSS).},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {878–884},
numpages = {7},
keywords = {Belonging, LIWC, Meaning Extraction Method, Natural Language Processing, Topic Modelling},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636915,
author = {Osakwe, Ikenna and Chen, Guanliang and Fan, Yizhou and Rakovic, Mladen and Singh, Shaveen and Molenaar, Inge and Ga\v{s}evi\'{c}, Dragan},
title = {Measurement of Self-regulated Learning: Strategies for mapping trace data to learning processes and downstream analysis implications},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636915},
doi = {10.1145/3636555.3636915},
abstract = {Trace data provides opportunities to study self-regulated learning (SRL) processes as they unfold. However, raw trace data must be translated into meaningful SRL constructs to enable analysis. This typically involves developing a pattern dictionary that maps trace sequences to SRL processes, and a trace parser to implement the mappings. While much attention focuses on the pattern dictionary, trace parsing methodology remains under-investigated. This study explores how trace parsers affect extracted processes and downstream analysis. Four methods were compared: Disconnected, Connected, Lookahead, and Synonym Matching. Statistical analysis of medians and process mining networks showed parsing choices significantly impacted SRL process identification and sequencing. Disconnected parsing isolated metacognitive processes while Connected approaches showed greater connectivity between meta-cognitive and cognitive events. Furthermore, Connected methods provided process maps more aligned with cyclical theoretical models of SRL. The results demonstrate trace parser design critically affects the validity of extracted SRL processes, with implications for SRL measurement in learning analytics.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {563–575},
numpages = {13},
keywords = {learning analytics, learning strategies, self-regulated learning},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636914,
author = {Jung, Yeonji and Wise, Alyssa Friend},
title = {Probing Actionability in Learning Analytics: The Role of Routines, Timing, and Pathways},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636914},
doi = {10.1145/3636555.3636914},
abstract = {Actionability is a critical, but understudied, issue in learning analytics for driving impact on learning. This study investigated access and action-taking of 91 students in an online undergraduate statistics course who received analytics designed for actionability twice a week for five weeks in the semester. Findings showed high levels of access, but little direct action through the provided links. The major contribution of the study was the identification of unexpected indirect actions taken by students in response to the analytics which requires us to think (and look for evidence of impact) more broadly than has been done previously. The study also found that integrating analytics into existing learning tools and routines can increase access rates to the analytics, but may not guarantee meaningful engagement without better strategies to manage analytic timing. Together, this study advances an understanding of analytic actionability, calling for a broader examination of both direct and indirect actions within a larger learning ecosystem.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {871–877},
numpages = {7},
keywords = {Actionability, data-informed learning, formative feedback, learning analytics implementation},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636913,
author = {Taylor, Megan and Barthakur, Abhinava and Azad, Arslan and Joksimovic, Srecko and Zhang, Xuwei and Siemens, George},
title = {Quantifying Collaborative Complex Problem Solving in Classrooms using Learning Analytics},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636913},
doi = {10.1145/3636555.3636913},
abstract = {Complex problem solving (CPS) is a critical skill with far-reaching implications for personal success and professional development. While CPS research has made extensive progress, additional investigation is needed to explore CPS processes beyond online contexts and performance outcomes. This study, conducted with Year 9 students aged between thirteen and fourteen, focuses on collaborative CPS. It utilises audio and video recordings to capture group communications during a CPS classroom activity. To analyse these interactions, we introduce a novel CPS framework as a dynamic, cognitive and social process involving interrelated main skills, sub-skills, and indicators. Through sequential pattern mining, we identify recurring subskill patterns that reflect CPS processes in an educational environment. Our research underscores the importance of employing diverse patterns before plan execution, particularly building shared knowledge, planning, and negotiation. We uncover patterns related to groups going off-task and highlight the significance of effective communication and maintaining focus for keeping groups on track. Furthermore, we indicate patterns following the detection of emergent issues, recognising the value of cultivating clarity and adaptability among team members. Our CPS framework, combined with our research results, offers practical implications for teaching, learning, and assessment approaches in educational, professional and industry sectors.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {551–562},
numpages = {12},
keywords = {Collaborative learning, Complex problem solving, Sequential pattern mining, problem-solving strategies},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636912,
author = {Snyder, Caitlin and Hutchins, Nicole M and Cohn, Clayton and Fonteles, Joyce Horn and Biswas, Gautam},
title = {Analyzing Students Collaborative Problem-Solving Behaviors in Synergistic STEM+C Learning},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636912},
doi = {10.1145/3636555.3636912},
abstract = {This study introduces a methodology to investigate students’ collaborative behaviors as they work in pairs to build computational models of scientific processes. We expand the Self-Regulated Learning (SRL) framework—specifically, Planning, Enacting, and Reflection—proposed in the literature, applying it to examine students’ collaborative problem-solving (CPS) behaviors in a computational modeling task. We analyze these behaviors by employing a Markov Chain (MC) modeling approach that scrutinizes students’ model construction and model debugging behaviors during CPS. This involves interpreting their actions in the system collected through computer logs and analyzing their conversations using a Large Language Model (LLM) as they progress through their modeling task in segments. Our analytical framework assesses the behaviors of high- and low-performing students by evaluating their proficiency in completing the specified computational model for a kinematics problem. We employ a mixed-methods approach, combining Markov Chain analysis of student problem-solving transitions with qualitative interpretations of their conversation segments. The results highlight distinct differences in behaviors between high- and low-performing groups, suggesting potential for developing adaptive scaffolds in future work to enhance support for students in collaborative problem-solving.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {540–550},
numpages = {11},
keywords = {SRL, STEM, collaboration, learning analytics},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636911,
author = {Borchers, Conrad and Zhang, Jiayi and Baker, Ryan S. and Aleven, Vincent},
title = {Using Think-Aloud Data to Understand Relations between Self-Regulation Cycle Characteristics and Student Performance in Intelligent Tutoring Systems},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636911},
doi = {10.1145/3636555.3636911},
abstract = {Numerous studies demonstrate the importance of self-regulation during learning by problem-solving. Recent work in learning analytics has largely examined students’ use of SRL concerning overall learning gains. Limited research has related SRL to in-the-moment performance differences among learners. The present study investigates SRL behaviors in relationship to learners’ moment-by-moment performance while working with intelligent tutoring systems for stoichiometry chemistry. We demonstrate the feasibility of labeling SRL behaviors based on AI-generated think-aloud transcripts, identifying the presence or absence of four SRL categories (processing information, planning, enacting, and realizing errors) in each utterance. Using the SRL codes, we conducted regression analyses to examine how the use of SRL in terms of presence, frequency, cyclical characteristics, and recency relate to student performance on subsequent steps in multi-step problems. A model considering students’ SRL cycle characteristics outperformed a model only using in-the-moment SRL assessment. In line with theoretical predictions, students’ actions during earlier, process-heavy stages of SRL cycles exhibited lower moment-by-moment correctness during problem-solving than later SRL cycle stages. We discuss system re-design opportunities to add SRL support during stages of processing and paths forward for using machine learning to speed research depending on the assessment of SRL based on transcription of think-aloud data.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {529–539},
numpages = {11},
keywords = {intelligent tutoring systems, process analysis, self-regulated learning, think-aloud method},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636910,
author = {Hou, Chenyu and Zhu, Gaoxia and Zheng, Juan and Zhang, Lishan and Huang, Xiaoshan and Zhong, Tianlong and Li, Shan and Du, Hanxiang and Ker, Chin Lee},
title = {Prompt-based and Fine-tuned GPT Models for Context-Dependent and -Independent Deductive Coding in Social Annotation},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636910},
doi = {10.1145/3636555.3636910},
abstract = {GPT has demonstrated impressive capabilities in executing various natural language processing (NLP) and reasoning tasks, showcasing its potential for deductive coding in social annotations. This research explored the effectiveness of prompt engineering and fine-tuning approaches of GPT for deductive coding of context-dependent and context-independent dimensions. Coding context-dependent dimensions (i.e., Theorizing, Integration, Reflection) requires a contextualized understanding that connects the target comment with reading materials and previous comments, whereas coding context-independent dimensions (i.e., Appraisal, Questioning, Social, Curiosity, Surprise) relies more on the comment itself. Utilizing strategies such as prompt decomposition, multi-prompt learning, and a codebook-centered approach, we found that prompt engineering can achieve fair to substantial agreement with expert-labeled data across various coding dimensions. These results affirm GPT's potential for effective application in real-world coding tasks. Compared to context-independent coding, context-dependent dimensions had lower agreement with expert-labeled data. To enhance accuracy, GPT models were fine-tuned using 102 pieces of expert-labeled data, with an additional 102 cases used for validation. The fine-tuned models demonstrated substantial agreement with ground truth in context-independent dimensions and elevated the inter-rater reliability of context-dependent categories to moderate levels. This approach represents a promising path for significantly reducing human labor and time, especially with large unstructured datasets, without sacrificing the accuracy and reliability of deductive coding tasks in social annotation. The study marks a step toward optimizing and streamlining coding processes in social annotation. Our findings suggest the promise of using GPT to analyze qualitative data and provide detailed, immediate feedback for students to elicit deepening inquiries.&nbsp;},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {518–528},
numpages = {11},
keywords = {Context-Dependent, Fine-tuning, GPT, Prompt Engineering, Social Annotation, deductive coding},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636909,
author = {Lee, Morgan and Siedahmed, Abubakir and Heffernan, Neil},
title = {Expert Features for a Student Support Recommendation Contextual Bandit Algorithm},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636909},
doi = {10.1145/3636555.3636909},
abstract = {Contextual multi-armed bandits have previously been used to personalize student support messages given to learners by supplying a model with relevant context about the user, problem, and available student supports. In this work, we propose using careful feature selection with relevant domain knowledge to improve the quality of student support recommendations. By providing Bayesian Knowledge Tracing mastery estimates to a contextual multi-armed bandit as user-level context in a simulated environment, we demonstrate that using domain knowledge to engineer contextual features results in higher average cumulative reward, and significant improvement over randomly selecting student supports. The data used to simulate sequential recommendations are available at https://osf.io/sfyzv/?view_only=351fb8781d2c4f3bbc9d7486762d563a.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {864–870},
numpages = {7},
keywords = {Feature Engineering, Multi-Armed Bandit, Personalized Learning, Reinforcement Learning},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636908,
author = {Gurung, Ashish and Vanacore, Kirk and Mcreynolds, Andrew A. and Ostrow, Korinn S. and Worden, Eamon and Sales, Adam C. and Heffernan, Neil T.},
title = {Multiple Choice vs. Fill-In Problems: The Trade-off Between Scalability and Learning},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636908},
doi = {10.1145/3636555.3636908},
abstract = {Learning experience designers consistently balance the trade-off between open and close-ended activities. The growth and scalability of Computer Based Learning Platforms (CBLPs) have only magnified the importance of these design trade-offs. CBLPs often utilize close-ended activities (i.e. Multiple-Choice Questions [MCQs]) due to feasibility constraints associated with the use of open-ended activities. MCQs offer certain affordances, such as immediate grading and the use of distractors, setting them apart from open-ended activities. Our current study examines the effectiveness of Fill-In problems as an alternative to MCQs for middle school mathematics. We report on a randomized study conducted from 2017 to 2022, with a total of 6,768 students from middle schools across the US. We observe that, on average, Fill-In problems lead to better post-test performance than MCQs; albeit deeper explorations indicate differences between the two design paradigms to be more nuanced. We find evidence that students with higher math knowledge benefit more from Fill-In problems than those with lower math knowledge.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {507–517},
numpages = {11},
keywords = {Causal Inference, Fill-In Problems, Learning Experience Design, Learning Outcomes, Multiple Choice Questions},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636907,
author = {Wang, Karen D. and Liu, Haoyu and DeLiema, David and Haber, Nick and Salehi, Shima},
title = {Discovering Players’ Problem-Solving Behavioral Characteristics in a Puzzle Game through Sequence Mining},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636907},
doi = {10.1145/3636555.3636907},
abstract = {Digital games offer promising platforms for assessing student higher-order competencies such as problem-solving. However, processing and analyzing the large volume of interaction log data generated in these platforms to uncover meaningful behavioral patterns remain a complex research challenge. In this study, we employ sequence mining and clustering techniques to examine students’ log data in an interactive puzzle game that requires player to change rules to win the game. Our goal is to identify behavioral characteristics associated with the problem-solving practices adopted by individual students. The findings indicate that the most effective problem solvers made fewer rule changes and took longer time to make those changes across both an introductory and a more advanced level of the game. Conversely, rapid rule change actions were linked to ineffective problem-solving. This research underscores the potential of sequence mining and cluster analysis as generalizable methods for understanding student higher-order competencies through log data in digital gaming and learning environments. It also suggests future directions on how to provide just-in-time, in-game feedback to enhance student problem-solving competences.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {498–506},
numpages = {9},
keywords = {cluster analysis, digital games, log data, problem-solving, sequence mining},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636906,
author = {Glandorf, Dominik and Lee, Hye Rin and Orona, Gabe Avakian and Pumptow, Marina and Yu, Renzhe and Fischer, Christian},
title = {Temporal and Between-Group Variability in College Dropout Prediction},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636906},
doi = {10.1145/3636555.3636906},
abstract = {Large-scale administrative data is a common input in early warning systems for college dropout in higher education. Still, the terminology and methodology vary significantly across existing studies, and the implications of different modeling decisions are not fully understood. This study provides a systematic evaluation of contributing factors and predictive performance of machine learning models over time and across different student groups. Drawing on twelve years of administrative data at a large public university in the US, we find that dropout prediction at the end of the second year has a 20% higher AUC than at the time of enrollment in a Random Forest model. Also, most predictive factors at the time of enrollment, including demographics and high school performance, are quickly superseded in predictive importance by college performance and in later stages by enrollment behavior. Regarding variability across student groups, college GPA has more predictive value for students from traditionally disadvantaged backgrounds than their peers. These results can help researchers and administrators understand the comparative value of different data sources when building early warning systems and optimizing decisions under specific policy goals.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {486–497},
numpages = {12},
keywords = {administrative data, college dropout prediction, machine learning, student heterogeneity, temporal dynamics},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636905,
author = {Suraworachet, Wannapon and Seon, Jennifer and Cukurova, Mutlu},
title = {Predicting challenge moments from students' discourse: A comparison of GPT-4 to two traditional natural language processing approaches},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636905},
doi = {10.1145/3636555.3636905},
abstract = {Effective collaboration requires groups to strategically regulate themselves to overcome challenges. Research has shown that groups may fail to regulate due to differences in members’ perceptions of challenges which may benefit from external support. In this study, we investigated the potential of leveraging three distinct natural language processing models: an expert knowledge rule-based model, a supervised machine learning (ML) model and a Large Language model (LLM), in challenge detection and challenge dimension identification (cognitive, metacognitive, emotional and technical/other challenges) from student discourse, was investigated. The results show that the supervised ML and the LLM approaches performed considerably well in both tasks, in contrast to the rule-based approach, whose efficacy heavily relies on the engineered features by experts. The paper provides an extensive discussion of the three approaches’ performance for automated detection and support of students’ challenge moments in collaborative learning activities. It argues that, although LLMs provide many advantages, they are unlikely to be the panacea to issues of the detection and feedback provision of socially shared regulation of learning due to their lack of reliability, as well as issues of validity evaluation, privacy and confabulation. We conclude the paper with a discussion on additional considerations, including model transparency to explore feasible and meaningful analytical feedback for students and educators using LLMs.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {473–485},
numpages = {13},
keywords = {Challenge moments, Collaborative learning, Discourse analysis, Natural language processing},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636904,
author = {Vanacore, Kirk and Gurung, Ashish and Sales, Adam and Heffernan, Neil T.},
title = {The Effect of Assistance on Gamers: Assessing The Impact of On-Demand Hints &amp; Feedback Availability on Learning for Students Who Game the System},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636904},
doi = {10.1145/3636555.3636904},
abstract = {Gaming the system, characterized by attempting to progress through a learning activity without engaging in essential learning behaviors, remains a persistent problem in computer-based learning platforms. This paper examines a simple intervention to mitigate the harmful effects of gaming the system by evaluating the impact of immediate feedback on students prone to gaming the system. Using a randomized controlled trial comparing two conditions - one with immediate hints and feedback and another with delayed access to such resources - this study employs a Fully Latent Principal Stratification model to determine whether students inclined to game the system would benefit more from the delayed hints and feedback. The results suggest differential effects on learning, indicating that students prone to gaming the system may benefit from restricted or delayed access to on-demand support. However, removing immediate hints and feedback did not fully alleviate the learning disadvantage associated with gaming the system. Additionally, this paper highlights the utility of combining detection methods and causal models to comprehend and effectively respond to students’ behaviors. Overall, these findings contribute to our understanding of effective intervention design that addresses gaming the system behaviors, consequently enhancing learning outcomes in computer-based learning platforms.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {462–472},
numpages = {11},
keywords = {Causal Inference, Computer-Based Learning Platforms, Feedback, Gaming the System Detection, Hints},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636903,
author = {Xu, Yinuo and Pardos, Zach A.},
title = {Extracting Course Similarity Signal using Subword Embeddings},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636903},
doi = {10.1145/3636555.3636903},
abstract = {Several studies have shown the utility of neural network models in learning course similarities and providing insightful course recommendations from enrollment data. In this study, we explore if additional signals can be found in the morphological structure of course names. We train skip-gram, FastText, and other combination models on these course sequence data from the past nine years and compare results with state-of-the-art models. We find a 97.95% improvement in model performance (as measured by recall @ 10 in similarity-based course recommendations) from skip-gram to FastText, and 80.75% improvement from the current best combination model to the previous state-of-the-art model, indicating that the naming convention of courses (e.g., PHYS_H101) carries valuable signals. We define attributes with which to categorize course pairs from our validation set and present an analysis of which models are strongest and weakest at predicting the similarity of which categories of course pairs. Additionally, we also explore course-taking culture, analyzing if courses with the same demographic features are learned to be more similar. Our approach could help students find alternatives to full courses, improve existing course recommendation systems and course articulations between institutions, and assist institutions in course policy-making.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {857–863},
numpages = {7},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636902,
author = {Baucks, Frederik and Schmucker, Robin and Wiskott, Laurenz},
title = {Gaining Insights into Course Difficulty Variations Using Item Response Theory},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636902},
doi = {10.1145/3636555.3636902},
abstract = {Curriculum analytics (CA) studies curriculum structure and student data to ensure the quality of educational programs. To gain statistical robustness, most existing CA techniques rely on the assumption of time-invariant course difficulty, preventing them from capturing variations that might occur over time. However, ensuring low temporal variation in course difficulty is crucial to warrant fairness in treating individual student cohorts and consistency in degree outcomes. We introduce item response theory (IRT) as a CA methodology that enables us to address the open problem of monitoring course difficulty variations over time. We use statistical criteria to quantify the degree to which course performance data meets IRT’s theoretical assumptions and verify validity and reliability of IRT-based course difficulty estimates. Using data from 664 Computer Science and 1,355 Mechanical Engineering undergraduate students, we show how IRT can yield valuable CA insights: First, by revealing temporal variations in course difficulty over several years, we find that course difficulty has systematically shifted downward during the COVID-19 pandemic. Second, time-dependent course difficulty and cohort performance variations confound conventional course pass rate measures. We introduce IRT-adjusted pass rates as an alternative to account for these factors. Our findings affect policymakers, student advisors, accreditation, and course articulation.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {450–461},
numpages = {12},
keywords = {curriculum analytics, fairness., item response theory},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636901,
author = {Van Campenhout, Rachel and Kimball, Murray and Clark, Michelle and Dittel, Jeffrey S. and Jerome, Bill and Johnson, Benny G.},
title = {An Investigation of Automatically Generated Feedback on Student Behavior and Learning},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636901},
doi = {10.1145/3636555.3636901},
abstract = {Decades of research have focused on the feedback delivered to students after answering questions—when to deliver feedback and what kind of feedback is most beneficial for learning. While there is a well-established body of research on feedback, new advances in technology have led to new methods for developing feedback and large-scale usage provides new data for understanding how feedback impacts learners. This paper focuses on feedback that was developed using artificial intelligence for an automatic question generation system. The automatically generated questions were placed alongside text as a formative learning tool in an e-reader platform. Three types of feedback were randomized across the questions: outcome feedback, context feedback, and common answer feedback. In this study, we investigate the effect of different feedback types on student behavior. This analysis is significant to the expanding body of research on automatic question generation, as little research has been reported on automatically generated feedback specifically, as well as the additional insights that microlevel data can reveal on the relationship between feedback and student learning behaviors.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {850–856},
numpages = {7},
keywords = {Automatic question generation, Automatically generated feedback, Feedback, Formative practice, Learning by doing, Student behavior},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636900,
author = {Li, Tongguang and Fan, Yizhou and Srivastava, Namrata and Zeng, Zijie and Li, Xinyu and Khosravi, Hassan and Tsai, Yi-Shan and Swiecki, Zachari and Ga\v{s}evi\'{c}, Dragan},
title = {Analytics of Planning Behaviours in Self-Regulated Learning: Links with Strategy Use and Prior Knowledge},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636900},
doi = {10.1145/3636555.3636900},
abstract = {A sophisticated grasp of self-regulated learning (SRL) skills has become essential for learners in computer-based learning environment (CBLE). One aspect of SRL is the plan-making process, which, although emphasized in many SRL theoretical frameworks, has attracted little research attention. Few studies have investigated the extent to which learners complied with their planned strategies, and whether making a strategic plan is associated with actual strategy use. Limited studies have examined the role of prior knowledge in predicting planned and actual strategy use. In this study, we developed a CBLE to collect trace data, which were analyzed to investigate learners’ plan-making process and its association with planned and actual strategy use. Analysis of prior knowledge and trace data of 202 participants indicated that 1) learners tended to adopt strategies that significantly deviated from their planned strategies, 2) the level of prior knowledge was associated with planned strategies, and 3) neither the act of plan-making nor prior knowledge predicted actual strategy use. These insights bear implications for educators and educational technologists to recognise the dynamic nature of strategy adoption and to devise approaches that inspire students to continually revise and adjust their plans, thereby strengthening SRL.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {438–449},
numpages = {12},
keywords = {learning analytics, learning strategies, self-regulated learning, strategic planning},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636899,
author = {Leon, Amelia and Nie, Allen and Chandak, Yash and Brunskill, Emma},
title = {Estimating the Causal Treatment Effect of Unproductive Persistence},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636899},
doi = {10.1145/3636555.3636899},
abstract = {There has been considerable work in classifying and predicting unproductive persistence, but much less in understanding its causal impact on downstream outcomes of interest, like external assessments. In general, it is experimentally challenging to understand the causal impact because, unlike in many other settings, we cannot directly intervene (to conduct a randomized control trial) and cause students to struggle unproductively in an authentic manner. In this work, we use data from a prior study that used virtual reality headsets to alert teacher’s attention to students who were unproductively struggling. We show that we can use this as an instrumental variable, and use a two-stage least squares analysis to provide a causal estimate of the treatment effect of unproductive persistence on post-test performance. Our results further strengthen the importance of unproductive struggle and highlight the potential of leveraging instruments to identify causal treatment effects of student behaviors during the use of educational technology.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {843–849},
numpages = {7},
keywords = {Causal Inference, Education, Instrumental Variable, Unproductive Persistence},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636898,
author = {Frej, Jibril and Shah, Neel and Knezevic, Marta and Nazaretsky, Tanya and K\"{a}ser, Tanja},
title = {Finding Paths for Explainable MOOC Recommendation: A Learner Perspective},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636898},
doi = {10.1145/3636555.3636898},
abstract = {The increasing availability of Massive Open Online Courses (MOOCs) has created a necessity for personalized course recommendation systems. These systems often combine neural networks with Knowledge Graphs (KGs) to achieve richer representations of learners and courses. While these enriched representations allow more accurate and personalized recommendations, explainability remains a significant challenge which is especially problematic for certain domains with significant impact such as education and online learning. Recently, a novel class of recommender systems that uses reinforcement learning and graph reasoning over KGs has been proposed to generate explainable recommendations in the form of paths over a KG. Despite their accuracy and interpretability on e-commerce datasets, these approaches have scarcely been applied to the educational domain and their use in practice has not been studied. In this work, we propose an explainable recommendation system for MOOCs that uses graph reasoning. To validate the practical implications of our approach, we conducted a user study examining user perceptions of our new explainable recommendations. We demonstrate the generalizability of our approach by conducting experiments on two educational datasets: COCO and Xuetang.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {426–437},
numpages = {12},
keywords = {Explainable AI, MOOCs, Recommendation, User study},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636897,
author = {Wang, Karen D. and Chen, Zhongzhou and Wieman, Carl},
title = {Can Crowdsourcing Platforms Be Useful for Educational Research?},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636897},
doi = {10.1145/3636555.3636897},
abstract = {A growing number of social science researchers, including educational researchers, have turned to online crowdsourcing platforms such as Prolific and MTurk for their experiments. However, there is a lack of research investigating the quality of data generated by online subjects and how they compare with traditional subject pools of college students in studies that involve cognitively demanding tasks. Using an interactive problem-solving task embedded in an educational simulation, we compare the task engagement and performance based on the interaction log data of college students recruited from Prolific to those from an introductory physics course. Results show that Prolific participants performed on par with participants from the physics class in obtaining the correct solutions. Furthermore, the physics course students who submitted incorrect answers were more likely than Prolific participants to make rushed cursory attempts to solve the problem. These results suggest that with thoughtful study design and advanced learning analytics and data mining techniques, crowdsourcing platforms can be a viable tool for conducting research on teaching and learning in higher education.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {416–425},
numpages = {10},
keywords = {crowdsourcing research, log data, online experiments, postsecondary STEM education, problem solving},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636896,
author = {Thomas, Danielle R and Lin, Jionghao and Gatz, Erin and Gurung, Ashish and Gupta, Shivang and Norberg, Kole and Fancsali, Stephen E and Aleven, Vincent and Branstetter, Lee and Brunskill, Emma and Koedinger, Kenneth R},
title = {Improving Student Learning with Hybrid Human-AI Tutoring: A Three-Study Quasi-Experimental Investigation},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636896},
doi = {10.1145/3636555.3636896},
abstract = {Artificial intelligence (AI) applications to support human tutoring have potential to significantly improve learning outcomes, but engagement issues persist, especially among students from low-income backgrounds. We introduce an AI-assisted tutoring model that combines human and AI tutoring and hypothesize this synergy will have positive impacts on learning processes. To investigate this hypothesis, we conduct a three-study quasi-experiment across three urban and low-income middle schools: 1) 125 students in a Pennsylvania school; 2) 385 students (50% Latinx) in a California school, and 3) 75 students (100% Black) in a Pennsylvania charter school, all implementing analogous tutoring models. We compare learning analytics of students engaged in human-AI tutoring compared to students using math software only. We find human-AI tutoring has positive effects, particularly in student’s proficiency and usage, with evidence suggesting lower achieving students may benefit more compared to higher achieving students. We illustrate the use of quasi-experimental methods adapted to the particulars of different schools and data-availability contexts so as to achieve the rapid data-driven iteration needed to guide an inspired creation into effective innovation. Future work focuses on improving the tutor dashboard and optimizing tutor-student ratios, while maintaining annual costs per student of approximately $700 annually.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {404–415},
numpages = {12},
keywords = {AI-assisted tutoring, Design-based research, Human-AI tutoring, Tutoring},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636895,
author = {Li, Chenglu and Zhu, Wangda and Xing, Wanli and Guo, Rui},
title = {Analyzing Student Attention and Acceptance of Conversational AI for Math Learning: Insights from a Randomized Controlled Trial},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636895},
doi = {10.1145/3636555.3636895},
abstract = {The significance of nurturing a deep conceptual understanding in math learning cannot be overstated. Grounded in the pedagogical strategies of induction, concretization, and exemplification (ICE), we designed and developed a conversational AI using both rule- and generation-based techniques to facilitate math learning. Serving as a preliminary step, this study employed an experimental design involving 151 U.S.-based college students to reveal students’ attention patterns, technology acceptance model, and qualitative feedback when using the developed ConvAI. Our findings suggest that participants in the ConvAI group generally exhibit higher attention levels than those in the control group, aside from the initial stage where the control group was more attentive. Meanwhile, participants appreciated their experience with the ConvAI, particularly valuing the ICE support features. Finally, qualitative analysis of participants’ feedback was conducted to inform future refinement and to inspire educational researchers and practitioners.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {836–842},
numpages = {7},
keywords = {Conversational AI, Large language models, Math learning, Technology design and development},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636894,
author = {Zhou, Qi and Suraworachet, Wannapon and Cukurova, Mutlu},
title = {Harnessing Transparent Learning Analytics for Individualized Support through Auto-detection of Engagement in Face-to-Face Collaborative Learning},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636894},
doi = {10.1145/3636555.3636894},
abstract = {Using learning analytics to investigate and support collaborative learning has been explored for many years. Recently, automated approaches with various artificial intelligence approaches have provided promising results for modelling and predicting student engagement and performance in collaborative learning tasks. However, due to the lack of transparency and interpretability caused by the use of “black box” approaches in learning analytics design and implementation, guidance for teaching and learning practice may become a challenge. On the one hand, the black box created by machine learning algorithms and models prevents users from obtaining educationally meaningful learning and teaching suggestions. On the other hand, focusing on group and cohort level analysis only can make it difficult to provide specific support for individual students working in collaborative groups. This paper proposes a transparent approach to automatically detect student's individual engagement in the process of collaboration. The results show that the proposed approach can reflect student's individual engagement and can be used as an indicator to distinguish students with different collaborative learning challenges (cognitive, behavioural and emotional) and learning outcomes. The potential of the proposed collaboration analytics approach for scaffolding collaborative learning practice in face-to-face contexts is discussed and future research suggestions are provided.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {392–403},
numpages = {12},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636893,
author = {Shingjergji, Krist and Urlings, Corrie and Iren, Deniz and Klemke, Roland},
title = {Shaping and evaluating a system for affective computing in online higher education using a participatory design and the system usability scale},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636893},
doi = {10.1145/3636555.3636893},
abstract = {Online learning’s popularity has surged. However, teachers face the challenge of the lack of non-verbal communication with students, making it difficult to perceive their learning-centered affective states (LCAS), leading to missed intervention opportunities. Addressing this challenge requires a system that detects students’ LCAS from their non-verbal cues and informs teachers in an actionable way. To design such a system, it is essential to explore field experts’ needs and requirements. Therefore, we conducted design-based research focus groups with teachers to determine which LCAS they find important to know during online lectures and their preferred communication methods. The results indicated that confusion, engagement, boredom, frustration, and curiosity are the most important LCAS and that the proposed system should take into account teachers’ cognitive load and give them autonomy in the choice of content and frequency of the information. Considering the obtained feedback, a prototype of two versions was developed. The prototype was evaluated by teachers utilizing the System Usability Scale (SUS). Results indicated an average SUS score of 80.5 and 74.5 for each version, suggesting acceptable usability. These findings can guide the design and development of a system that can help teachers recognize students’ LCAS, thus improving synchronous online learning.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {382–391},
numpages = {10},
keywords = {affective computing, emotions in learning, focus groups, learning-centered affective states, online learning, participatory design, prototype evaluation, system usability},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636892,
author = {Borchers, Conrad and Wang, Yeyu and Karumbaiah, Shamya and Ashiq, Muhammad and Shaffer, David Williamson and Aleven, Vincent},
title = {Revealing Networks: Understanding Effective Teacher Practices in AI-Supported Classrooms using Transmodal Ordered Network Analysis},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636892},
doi = {10.1145/3636555.3636892},
abstract = {Learning analytics research increasingly studies classroom learning with AI-based systems through rich contextual data from outside these systems, especially student-teacher interactions. One key challenge in leveraging such data is generating meaningful insights into effective teacher practices. Quantitative ethnography bears the potential to close this gap by combining multimodal data streams into networks of co-occurring behavior that drive insight into favorable learning conditions. The present study uses transmodal ordered network analysis to understand effective teacher practices in relationship to traditional metrics of in-system learning in a mathematics classroom working with AI tutors. Incorporating teacher practices captured by position tracking and human observation codes into modeling significantly improved the inference of how efficiently students improved in the AI tutor beyond a model with tutor log data features only. Comparing teacher practices by student learning rates, we find that students with low learning rates exhibited more hint use after monitoring. However, after an extended visit, students with low learning rates showed learning behavior similar to their high learning rate peers, achieving repeated correct attempts in the tutor. Observation notes suggest conceptual and procedural support differences can help explain visit effectiveness. Taken together, offering early conceptual support to students with low learning rates could make classroom practice with AI tutors more effective. This study advances the scientific understanding of effective teacher practice in classrooms learning with AI tutors and methodologies to make such practices visible.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {371–381},
numpages = {11},
keywords = {AI-supported classrooms, multimodal learning analytics, quantitative ethnography, teacher practices},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636891,
author = {Cho, Ji Yong and Tao, Yan and Yeomans, Michael and Tingley, Dustin and Kizilcec, Ren\'{e} F.},
title = {Which Planning Tactics Predict Online Course Completion?},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636891},
doi = {10.1145/3636555.3636891},
abstract = {Planning is a self-regulated learning strategy and widely used behavior change technique that can help learners achieve academic goals (e.g., pass an exam, apply to college, or complete an online course). Numerous studies have tested the effects of planning interventions, but few have examined the content of learners’ plans and how it relates to their academic outcomes. Building on a large-scale intervention study, we conducted a qualitative content analysis of 650 learner plans sampled from 15 massive open online courses (MOOCs). We identified a number of planning tactics, compared their prevalence, and examined which ones significantly predict course progress and completion using regression analyses. We found that learners whose plans specify a time of day (e.g., morning, afternoon, night) are significantly more likely to complete a MOOC, but only 25% of the learners in our sample used this tactic. The high degree of variation in the effectiveness of planning tactics may contribute to mixed intervention findings in scale-up studies. Models of plan effectiveness can be used to provide feedback on the quality of learners’ plans and encourage them to use effective tactics to achieve their learning goals.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {360–370},
numpages = {11},
keywords = {MOOCs, content analysis, online learning, planning intervention},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636890,
author = {Zambrano, Andres Felipe and Zhang, Jiayi and Baker, Ryan S.},
title = {Investigating Algorithmic Bias on Bayesian Knowledge Tracing and Carelessness Detectors},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636890},
doi = {10.1145/3636555.3636890},
abstract = {In today's data-driven educational technologies, algorithms have a pivotal impact on student experiences and outcomes. Therefore, it is critical to take steps to minimize biases, to avoid perpetuating or exacerbating inequalities. In this paper, we investigate the degree to which algorithmic biases are present in two learning analytics models: knowledge estimates based on Bayesian Knowledge Tracing (BKT) and carelessness detectors. Using data from a learning platform used across the United States at scale, we explore algorithmic bias following three different approaches: 1) analyzing the performance of the models on every demographic group in the sample, 2) comparing performance across intersectional groups of these demographics, and 3) investigating whether the models trained using specific groups can be transferred to demographics that were not observed during the training process. Our experimental results show that the performance of these models is close to equal across all the demographic and intersectional groups. These findings establish the feasibility of validating educational algorithms for intersectional groups and indicate that these algorithms can be fairly used for diverse students at scale.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {349–359},
numpages = {11},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636889,
author = {Sonkar, Shashank and Chen, Xinghe and Le, Myco and Liu, Naiming and Basu Mallick, Debshila and Baraniuk, Richard},
title = {Code Soliloquies for Accurate Calculations in Large Language Models},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636889},
doi = {10.1145/3636555.3636889},
abstract = {High-quality conversational datasets are crucial for the successful development of Intelligent Tutoring Systems (ITS) that utilize a Large Language Model (LLM) backend. Synthetic student-teacher dialogues, generated using advanced GPT-4 models, are a common strategy for creating these datasets. However, subjects like physics that entail complex calculations pose a challenge. While GPT-4 presents impressive language processing capabilities, its limitations in fundamental mathematical reasoning curtail its efficacy for such subjects. To tackle this limitation, we introduce in this paper an innovative stateful prompt design. Our design orchestrates a mock conversation where both student and tutorbot roles are simulated by GPT-4. Each student response triggers an internal monologue, or ‘code soliloquy’ in the GPT-tutorbot, which assesses whether its subsequent response would necessitate calculations. If a calculation is deemed necessary, it scripts the relevant Python code and uses the Python output to construct a response to the student. Our approach notably enhances the quality of synthetic conversation datasets, especially for subjects that are calculation-intensive. The preliminary Subject Matter Expert evaluations reveal that our Higgs model, a fine-tuned LLaMA model, effectively uses Python for computations, which significantly enhances the accuracy and computational reliability of Higgs’ responses.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {828–835},
numpages = {8},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636888,
author = {Brezack, Natalie and Chan, Wynnie and Feng, Mingyu},
title = {Student Effort and Progress Learning Analytics Data Inform Teachers’ SEL Discussions in Math Class},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636888},
doi = {10.1145/3636555.3636888},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {338–348},
numpages = {11},
keywords = {learning analytics data, math instruction, online math problem-solving, socioemotional learning},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636887,
author = {Saint, John and Fan, Yizhou and Gasevic, Dragan},
title = {Analytics of scaffold compliance for self-regulated learning},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636887},
doi = {10.1145/3636555.3636887},
abstract = {The shift toward digitally-based education has emphasised the need for learners to have strong skills for self-regulated learning (SRL). The use of scaffolding prompts is seen as an effective way to stimulate SRL and enhance academic outcomes. A key aspect of SRL scaffolding prompts is the degree to which they are complied to by students. Compliance is a complex concept, one that is further complicated by the nature of scaffold design in the context of adaptability. These nuances notwithstanding, scaffold compliance demands specific exploration. To that end, we conducted a study in which we: 1) focused specifically on scaffolding interaction behaviour in a timed online assessment task, as opposed to the broader interaction with non-scaffolding artefacts; 2) identified distinct scaffold interaction patterns in the context of compliance and non-compliance to scaffold design; 3) analysed how groups of learners traverse compliant and non-compliant interaction behaviours and engage in SRL processes in response to a sequence of timed and personalised SRL-informed scaffold prompts. We found that scaffold interactions fell into two categories of compliance and non-compliance, and whilst there was a healthy engagement with compliance, it does ebb and flow during an online timed assessment.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {326–337},
numpages = {12},
keywords = {Clustering, Learning Analytics, Process Mining, Scaffolding, Scaffolding Compliance, Self-Regulated Learning},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636886,
author = {Alcock, Sarah and Rienties, Bart and Aristeidou, Maria and Kouadri Most\'{e}faoui, Soraya},
title = {How do visualizations and automated personalized feedback engage professional learners in a Learning Analytics Dashboard?},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636886},
doi = {10.1145/3636555.3636886},
abstract = {Learning Analytics Dashboards (LAD) are the subject of research in a multitude of schools and higher education institutions, but a lack of research into learner-facing dashboards in professional learning has been identified. This study took place in an authentic professional learning context and aims to contribute insights into LAD design by using an academic approach in a practice-based environment. An existing storytelling LAD created to support 81 accountants was evaluated using Technology Acceptance Model, finding a learner expectation for clarity, conciseness, understanding and guidance on next steps. High usage levels and a ‘take what you need’ approach was identified, with all visualizations and automated personalized feedback being considered useful although to varying degrees. Professional learners in this study focus on understanding and acting upon weaknesses rather than celebrating strengths. The lessons for LAD design are to offer choice and create elements which support learners to take action to improve performance at a multitude of time points and levels of success.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {316–325},
numpages = {10},
keywords = {LAD, Learning Analytics Dashboard, Technology Acceptance Model, accountancy, assessment, data storytelling, feedback, personalization, professional learning},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636885,
author = {Mangaroska, Katerina and Larssen, Kristine and Amundsen, Andreas and Vesin, Boban and Giannakos, Michail},
title = {Understanding engagement through game learning analytics and design elements: Insights from a word game case study},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636885},
doi = {10.1145/3636555.3636885},
abstract = {Educational games have become an efficient and engaging way to enhance learning. Analytics have played a critical role in designing contemporary educational games, with most game design elements leveraging analytics produced during gameplay and learning. The presented study tackles the complex construct of engagement, which has been the central piece behind the success of educational games, by investigating the role of analytics-driven game elements on players’ engagement. To do so, we implemented a casual word game incorporating game design elements relevant to learning and conducted a within-subjects study where 39 participants played the game for two weeks. We found that the frequency of use of different game elements contributed to different dimensions of engagement. Our findings show that five of the eight game elements implemented in the word game engage players on an emotional, motivational, and cognitive level, thus emphasizing the importance of engagement as a multidimensional construct in designing educational casual games that offer highly engaging experiences.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {305–315},
numpages = {11},
keywords = {Cognitive evaluation theory, analytics-driven game design elements},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636884,
author = {Kaliisa, Rogers and Misiejuk, Kamila and L\'{o}pez-Pernas, Sonsoles and Khalil, Mohammad and Saqr, Mohammed},
title = {Have Learning Analytics Dashboards Lived Up to the Hype? A Systematic Review of Impact on Students' Achievement, Motivation, Participation and Attitude},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636884},
doi = {10.1145/3636555.3636884},
abstract = {While learning analytics dashboards (LADs) are the most common form of LA intervention, there is limited evidence regarding their impact on students’ learning outcomes. This systematic review synthesizes the findings of 38 research studies to investigate the impact of LADs on students' learning outcomes, encompassing achievement, participation, motivation, and attitudes. As we currently stand, there is no evidence to support the conclusion that LADs have lived up to the promise of improving academic achievement. Most studies reported negligible or small effects, with limited evidence from well-powered controlled experiments. Many studies merely compared users and non-users of LADs, confounding the dashboard effect with student engagement levels. Similarly, the impact of LADs on motivation and attitudes appeared modest, with only a few exceptions demonstrating significant effects. Small sample sizes in these studies highlight the need for larger-scale investigations to validate these findings. Notably, LADs showed a relatively substantial impact on student participation. Several studies reported medium to large effect sizes, suggesting that LADs can promote engagement and interaction in online learning environments. However, methodological shortcomings, such as reliance on traditional evaluation methods, self-selection bias, the assumption that access equates to usage, and a lack of standardized assessment tools, emerged as recurring issues. To advance the research line for LADs, researchers should use rigorous assessment methods and establish clear standards for evaluating learning constructs. Such efforts will advance our understanding of the potential of LADs to enhance learning outcomes and provide valuable insights for educators and researchers alike.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {295–304},
numpages = {10},
keywords = {Learning analytics dashboards (LADs), impact, learning outcomes, systematic review},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636883,
author = {Xu, Austin and Monroe, Will and Bicknell, Klinton},
title = {Large language model augmented exercise retrieval for personalized language learning},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636883},
doi = {10.1145/3636555.3636883},
abstract = {We study the problem of zero-shot exercise retrieval in the context of online language learning, to give learners the ability to explicitly request personalized exercises via natural language. Using real-world data collected from language learners, we observe that vector similarity approaches poorly capture the relationship between exercise content and the language that learners use to express what they want to learn. This semantic gap between queries and content dramatically reduces the effectiveness of general-purpose retrieval models pretrained on large scale information retrieval datasets like MS MARCO&nbsp;[2]. We leverage the generative capabilities of large language models to bridge the gap by synthesizing hypothetical exercises based on the learner’s input, which are then used to search for relevant exercises. Our approach, which we call mHyER, overcomes three challenges: (1) lack of relevance labels for training, (2) unrestricted learner input content, and (3) low semantic similarity between input and retrieval candidates. mHyER outperforms several strong baselines on two novel benchmarks created from crowdsourced data and publicly available data.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {284–294},
numpages = {11},
keywords = {large language models, online language learning, personalization, zero-shot exercise retrieval},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636882,
author = {Dunder, Nora and Lundborg, Saga and Wong, Jacqueline and Viberg, Olga},
title = {Kattis vs ChatGPT: Assessment and Evaluation of Programming Tasks in the Age of Artificial Intelligence},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636882},
doi = {10.1145/3636555.3636882},
abstract = {AI-powered education technologies can support students and teachers in computer science education. However, with the recent developments in generative AI, and especially the increasingly emerging popularity of ChatGPT, the effectiveness of using large language models for solving programming tasks has been underexplored. The present study examines ChatGPT’s ability to generate code solutions at different difficulty levels for introductory programming courses. We conducted an experiment where ChatGPT was tested on 127 randomly selected programming problems provided by Kattis, an automatic software grading tool for computer science programs, often used in higher education. The results showed that ChatGPT independently could solve 19 out of 127 programming tasks generated and assessed by Kattis. Further, ChatGPT was found to be able to generate accurate code solutions for simple problems but encountered difficulties with more complex programming tasks. The results contribute to the ongoing debate on the utility of AI-powered tools in programming education.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {821–827},
numpages = {7},
keywords = {Academic Integrity, Automated Grading, ChatGPT, Programming Education},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636881,
author = {Ain, Qurat Ul and Chatti, Mohamed Amine and Meteng Kamdem, Paul Arthur and Alatrash, Rawaa and Joarder, Shoeb and Siepmann, Clara},
title = {Learner Modeling and Recommendation of Learning Resources using Personal Knowledge Graphs},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636881},
doi = {10.1145/3636555.3636881},
abstract = {Educational recommender systems (ERS) are playing a pivotal role in providing recommendations of personalized resources and activities to students, tailored to their individual learning needs. A fundamental part of generating recommendations is the learner modeling process that identifies students’ knowledge state. Current ERSs, however, have limitations mainly related to the lack of transparency and scrutability of the learner models as well as capturing the semantics of learner models and learning materials. To address these limitations, in this paper we empower students to control the construction of their personal knowledge graphs (PKGs) based on the knowledge concepts that they actively mark as ’did not understand (DNU)’ while interacting with learning materials. We then use these PKGs to build semantically-enriched learner models and provide personalized recommendations of external learning resources. We conducted offline experiments and an online user study (N=31), demonstrating the benefits of a PKG-based recommendation approach compared to a traditional content-based one, in terms of several important user-centric aspects including perceived accuracy, novelty, diversity, usefulness, user satisfaction, and use intentions. In particular, our results indicate that the degree of control students are able to exert over the learner modeling process, has positive consequences on their satisfaction with the ERS and their intention to accept its recommendations.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {273–283},
numpages = {11},
keywords = {Educational Recommender System, Learner Modeling, Learning Analytics, MOOC, Open Learner Model, Personal Knowledge Graph, Sentence Encoder},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636880,
author = {Valle Torre, Manuel and Oertel, Catharine and Specht, Marcus},
title = {The Sequence Matters in Learning - A Systematic Literature Review},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636880},
doi = {10.1145/3636555.3636880},
abstract = {Describing and analysing learner behaviour using sequential data and analysis is becoming more and more popular in Learning Analytics. Nevertheless, we found a variety of definitions of learning sequences, as well as choices regarding data aggregation and the methods implemented for analysis. Furthermore, sequences are used to study different educational settings and serve as a base for various interventions. In this literature review, the authors aim to generate an overview of these aspects to describe the current state of using sequence analysis in educational support and learning analytics. The 74 included articles were selected based on the criteria that they conduct empirical research on an educational environment using sequences of learning actions as the main focus of their analysis. The results enable us to highlight different learning tasks where sequences are analysed, identify data mapping strategies for different types of sequence actions, differentiate techniques based on purpose and scope, and identify educational interventions based on the outcomes of sequence analysis.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {263–272},
numpages = {10},
keywords = {Learning Analytics, Learning Sequences, Literature Review, Sequence Analysis},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636879,
author = {Garg, Ryan and Han, Jaeyoung and Cheng, Yixin and Fang, Zheng and Swiecki, Zachari},
title = {Automated Discourse Analysis via Generative Artificial Intelligence},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636879},
doi = {10.1145/3636555.3636879},
abstract = {Coding discourse data is critical to many learning analytics studies. To code their data, researchers may use manual techniques, automated techniques, or a combination thereof. Manual coding can be time-consuming and error prone; automated coding can be difficult to implement for non-technical users. Generative artificial intelligence (GAI) offers a user friendly alternative to automated discourse coding via prompting and APIs. We assessed the ability of GAI, specifically the GPT class of models, at automatically coding discourse in the context of a learning analytics study using a variety of prompting and training strategies. We found that fine-tuning approaches produced the best results; however, no results achieved standard thresholds for reliability in our field.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {814–820},
numpages = {7},
keywords = {Automated Discourse Coding, Generative Artificial Intelligence},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636878,
author = {Poquet, Oleksandra and Joksimovic, Srecko and Brams, Pernille},
title = {The Role of Gender in Citation Practices of Learning Analytics Research},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636878},
doi = {10.1145/3636555.3636878},
abstract = {Mounting evidence indicates that modern citation practices contribute to inequalities in who receives citations. In response to this evidence, our paper investigates citation practices in learning analytics (LA). We analyse citations in papers published over ten years at the Learning Analytics and Knowledge conference (LAK). Our analysis examines the gender composition of authored and cited papers in LA, estimating various factors that explain why one paper cites another, and if the citation rates differ across different author teams. Results indicate an overall increase in the number of women authors at LAK, while the ratio of men to women remains stable. Citation patterns in LAK are influenced by the seniority of authors, paper age, topic, and team size. We found that LAK papers with women as the last author are under-cited, but papers where the first author is a woman and the last author is a man are over-cited. Author teams with different gender composition also vary in who they over- and under-cite. Upon presenting the empirical results, the paper reflects on the role of mindful citation practices and reviews existing measures proposed to promote diversity in citations.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {807–813},
numpages = {7},
keywords = {citations, equity, gender, learning analytics},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636877,
author = {Chejara, Pankaj and Kasepalu, Reet and Prieto, Luis and Rodr\'{\i}guez-Triana, Mar\'{\i}a Jes\'{u}s and Ruiz-Calleja, Adolfo},
title = {Bringing Collaborative Analytics using Multimodal Data to Masses: Evaluation and Design Guidelines for Developing a MMLA System for Research and Teaching Practices in CSCL},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636877},
doi = {10.1145/3636555.3636877},
abstract = {The Multimodal Learning Analytics (MMLA) research community has significantly grown in the past few years. Researchers in this field have harnessed diverse data collection devices such as eye-trackers, motion sensors, and microphones to capture rich multimodal data about learning. This data, when analyzed, has been proven highly valuable for understanding learning processes across a variety of educational settings. Notwithstanding this progress, an ubiquitous use of MMLA in education is still limited by challenges such as technological complexity, high costs, etc. In this paper, we introduce CoTrack, a MMLA system for capturing the multimodality of a group’s interaction in terms of audio, video, and writing logs in online and co-located collaborative learning settings. The system offers a user-friendly interface, designed to cater to the needs of teachers and students without specialized technical expertise. Our usability evaluation with 2 researchers, 2 teachers and 24 students has yielded promising results regarding the system’s ease of use. Furthermore, this paper offers design guidelines for the development of more user-friendly MMLA systems. These guidelines have significant implications for the broader aim of making MMLA tools accessible to a wider audience, particularly for non-expert MMLA users.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {800–806},
numpages = {7},
keywords = {CSCL, MMLA, Multimodal Learning Analytics},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636876,
author = {Sch\"{u}tt, Anan and Huber, Tobias and Nasir, Jauwairia and Conati, Cristina and Andr\'{e}, Elisabeth},
title = {Does Difficulty even Matter? Investigating Difficulty Adjustment and Practice Behavior in an Open-Ended Learning Task},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636876},
doi = {10.1145/3636555.3636876},
abstract = {Difficulty adjustment in practice exercises has been shown to be beneficial for learning. However, previous research has mostly investigated close-ended tasks, which do not offer the students multiple ways to reach a valid solution. Contrary to this, in order to learn in an open-ended learning task, students need to effectively explore the solution space as there are multiple ways to reach a solution. For this reason, the effects of difficulty adjustment could be different for open-ended tasks. To investigate this, as our first contribution, we compare different methods of difficulty adjustment in a user study conducted with 86 participants. Furthermore, as the practice behavior of the students is expected to influence how well the students learn, we additionally look at their practice behavior as a post-hoc analysis. Therefore, as a second contribution, we identify different types of practice behavior and how they link to students’ learning outcomes and subjective evaluation measures as well as explore the influence the difficulty adjustment methods have on the practice behaviors. Our results suggest the usefulness of taking into account the practice behavior in addition to only using the practice performance to inform adaptive intervention and difficulty adjustment methods.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {253–262},
numpages = {10},
keywords = {Adaptive Practice, Clustering, Difficulty Adjustment, Educational Data Mining},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636875,
author = {Cloude, Elizabeth B. and Munshi, Anabil and Andres, J. M. Alexandra and Ocumpaugh, Jaclyn and Baker, Ryan S. and Biswas, Gautam},
title = {Exploring Confusion and Frustration as Non-linear Dynamical Systems},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636875},
doi = {10.1145/3636555.3636875},
abstract = {Numerous studies aim to enhance learning in digital environments through emotionally-sensitive interventions. The D’Mello and Graesser (2012) model of affect dynamics hypothesizes that when a learner encounters confusion, the degree to which it is prolonged (and transitions into frustration) or resolved, significantly affects their learning outcomes in digital environments. However, studies yield inconclusive results regarding relations between confusion, frustration, and learning. More research is needed to explore how confusion and frustration manifest during learning and its relation to outcomes. We go beyond past work looking at the rate, duration, and transitions of confusion and frustration by treating these affective states as non-linear dynamical systems consisting of expressive and behavioral components. We examined the frequency and recurrence of facial expressions associated with basic emotions (as automatically labeled by AffDex, a standard tool for analyzing emotions with video data) during confused and frustrated states (as automatically labeled with BROMP-based detectors applied to students’ interaction data). We compare these co-occurring patterns to learning outcomes (pre-tests, post-tests, and learning gains) within a digital learning environment, Betty’s Brain. Results showed that the frequency and recurrence rate of basic emotions expressed during confusion and frustration are complex and remain incompletely understood. Specifically, we show that confusion and frustration have different relationships with learning outcomes, depending on which basic emotion expressions they co-occur with. Implications of this study open avenues for better understanding these emotions as complex and non-linear dynamical systems, in the long-term enabling personalized feedback and emotional support within digital learning environments that enhance learning outcomes.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {241–252},
numpages = {12},
keywords = {Confusion, Digital Learning Environments, Frustration, Learning Outcomes, Non-linear Dynamical Systems},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636874,
author = {Dang, Belle and Nguyen, Andy and J\"{a}rvel\"{a}, Sanna},
title = {The Unspoken Aspect of Socially Shared Regulation in Collaborative Learning: AI-Driven Learning Analytics Unveiling ‘Silent Pauses’},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636874},
doi = {10.1145/3636555.3636874},
abstract = {Socially Shared Regulation (SSRL) contributes to collaborative learning success. Recent advancements in Artificial Intelligence (AI) and Learning Analytics (LA) have enabled examination of this phenomenon’s temporal and cyclical complexities. However, most of these studies focus on students’ verbalised interactions, not accounting for the intertwined ’silent pauses’ that can index learners’ internal cognitive and emotional processes, potentially offering insight into regulation’s core mental processes. To address this gap, we employed AI-driven LA to explore the deliberation tactics among ten triads of secondary students during a face-to-face collaborative task (2,898 events). Discourse was coded for deliberative interactions for SSRL. With the micro-annotation of ‘silent pause’ added, sequences were analysed with the Optimal Matching algorithm, Ward’s Clustering and Lag Sequential Analysis. Three distinct deliberation tactics with different patterns and characteristics involving silent pauses emerged: i) Elaborated deliberation, ii) Coordinated deliberation, and iii) Solitary deliberation. Our findings highlight the role of ‘silent pauses’ in revealing not only the pattern but also the dynamics and characteristics of each deliberative interaction. This study illustrates the potential of AI-driven LA to tap into granular data points that enrich discourse analysis, presenting theoretical, methodological, and practical contributions and implications.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {231–240},
numpages = {10},
keywords = {Artificial Intelligence, Collaborative Learning, Deliberative Interactions, Learning Analytics, Socially Shared Regulation},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636873,
author = {Cohausz, Lea and Kappenberger, Jakob and Stuckenschmidt, Heiner},
title = {What Fairness Metrics Can Really Tell You: A Case Study in the Educational Domain},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636873},
doi = {10.1145/3636555.3636873},
abstract = {Recently, discussions on fairness and algorithmic bias have gained prominence in the learning analytics and educational data mining communities. To quantify algorithmic bias, researchers and practitioners often use popular fairness metrics, e.g., demographic parity, without discussing their choices. This can be considered problematic, as the choices should strongly depend on the underlying data generation mechanism, the potential application, and normative beliefs. Likewise, whether and how one should deal with the indicated bias depends on these aspects. This paper presents and discusses several theoretical cases to highlight precisely this. By providing a set of examples, we hope to facilitate a practice where researchers discuss potential fairness concerns by default.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {792–799},
numpages = {8},
keywords = {causal models, education, fairness},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636872,
author = {Li, Zaibei and Jensen, Martin Thoft and Nolte, Alexander and Spikol, Daniel},
title = {Field report for Platform mBox: Designing an Open MMLA Platform},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636872},
doi = {10.1145/3636555.3636872},
abstract = {Multimodal Learning Analytics (MMLA) is an evolving sector within learning analytics that has become increasingly useful for examining complex learning and collaboration dynamics for group work across all educational levels. The availability of low-cost sensors and affordable computational power allows researchers to investigate different modes of group work. However, the field faces challenges stemming from the complexity and specialization of the systems required for capturing diverse interaction modalities, with commercial systems often being expensive or narrow in scope and researcher-developed systems needing to be more specialized and difficult to deploy. Therefore, more user-friendly, adaptable, affordable, open-source, and easy-to-deploy systems are needed to advance research and application in the MMLA field. The paper presents a field report on the design of mBox that aims to support group work across different contexts. We share the progress of mBox, a low-cost, easy-to-use platform grounded on learning theories to investigate collaborative learning settings. Our approach has been guided by iterative design processes that let us rapidly prototype different solutions for these settings.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {785–791},
numpages = {7},
keywords = {Multimodal Learning Analytics, Prototyping, Sociometric Wearable Devices},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636871,
author = {Paik, Jae H. and Himelfarb, Igor and Yoo, Seung Hee and Yoo, KyoungMi and Ha, Hoyong},
title = {The relationships among school engagement, students' emotions, and academic performance in an elementary online learning},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636871},
doi = {10.1145/3636555.3636871},
abstract = {This study investigated the relationship among school engagement, students’ emotions, and academic performance of students in grades 3-6 in South Korea. A random sampling approach was used to extract data from 1,075 students out of a total of 141,926 students who used the educational learning platform, I-TokTok, adapted as the primary Learning Management System (LMS) at the provincial level. The present study aimed to identify dimensions of school engagement by exploring the behaviors consistent with IMS Caliper Analytics Specifications, a common standard utilized for collecting learning data from digital resources. Exploratory and Confirmatory Factor Analyses revealed a three-factor model of school engagement among the 13 learning behavioral indicators: behavioral engagement, social engagement, and cognitive engagement.&nbsp;Students’ emotions were measured through voluntary daily activities in the platform involving reflecting on, recognizing, and recording of their emotions. Students’ academic performance was assessed with performance in math tests administered within the platform. Consistent with current literature, results demonstrated that dimensions of school engagement (i.e., behavioral and social engagement) and students’ emotions positively predicted their math performance. Lastly, school engagement mediated the relationship between students’ emotions and math performance. The present study emphasizes the importance of investigating the underlying mechanisms through which elementary students emotions and school engagement predict academic achievement in an online learning environment. This relatively new area of educational research deserves attention in the field of learning analytics. We highlight the importance of considering ways to improve both students’ emotions and their school engagement to maximize the student learning outcomes.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {219–230},
numpages = {12},
keywords = {Academic Performance, Elementary Education, Learning Management System, School Engagement, Students’ Emotions},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636870,
author = {Xiang, Mengtong and Zhang, Jingjing and Li, Yue},
title = {Understanding Knowledge Convergence in a Cross-cultural Online Context: An Individual and Collective Approach},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636870},
doi = {10.1145/3636555.3636870},
abstract = {The concept of knowledge convergence refers to building a shared cognitive understanding among individuals through social interaction. It is considered as a crucial aspect of collaborative learning and plays a significant role in the process of consensus building. However, there is a lack of research exploring knowledge convergence in the context of online learning, especially in cross-cultural settings. Collaborative learning primarily focuses on constructing cognitive knowledge representations at the individual level, while online learning emphasizes the social mechanism of knowledge diffusion and flow at the collective level. This study aims to investigate individual online knowledge convergence through content analysis of social annotations within a cross-cultural course and using Simulation Investigation for Empirical Network Analysis (SIENA) to depict the collective social interaction. The findings reveal that online knowledge convergence exhibits distinct characteristics, quick consensus building could foster a harmonious community and similar experiences compensated for limited interactions, triggering deep consensus. Individual convergence leads to emergent properties such as reciprocity and transitivity within a dynamic collective interactive network, which can serve as novel indicators for evaluating knowledge convergence at the collective level. By approaching knowledge convergence from multifaceted perspectives, this study contributes to a comprehensive understanding of the concept across diverse learning contexts.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {779–784},
numpages = {6},
keywords = {Knowledge convergence, collaborative learning, complex network analysis, cross-cultural online course},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636869,
author = {Belitz, Clara and Lee, HaeJin and Nasiar, Nidhi and Fancsali, Stephen E. and Ritter, Steve and Almoubayyed, Husni and Baker, Ryan S. and Ocumpaugh, Jaclyn and Bosch, Nigel},
title = {Hierarchical Dependencies in Classroom Settings Influence Algorithmic Bias Metrics},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636869},
doi = {10.1145/3636555.3636869},
abstract = {Measuring algorithmic bias in machine learning has historically focused on statistical inequalities pertaining to specific groups. However, the most common metrics (i.e., those focused on individual- or group-conditioned error rates) are not currently well-suited to educational settings because they assume that each individual observation is independent from the others. This is not statistically appropriate when studying certain common educational outcomes, because such metrics cannot account for the relationship between students in classrooms or multiple observations per student across an academic year. In this paper, we present novel adaptations of algorithmic bias measurements for regression for both independent and nested data structures. Using hierarchical linear models, we rigorously measure algorithmic bias in a machine learning model of the relationship between student engagement in an intelligent tutoring system and year-end standardized test scores. We conclude that classroom-level influences had a small but significant effect on models. Examining significance with hierarchical linear models helps determine which inequalities in educational settings might be explained by small sample sizes rather than systematic differences.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {210–218},
numpages = {9},
keywords = {Algorithmic bias, Intelligent tutoring systems, Interactive learning environments, Predictive analytics},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636868,
author = {Jiang, Lan and Belitz, Clara and Bosch, Nigel},
title = {Synthetic Dataset Generation for Fairer Unfairness Research},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636868},
doi = {10.1145/3636555.3636868},
abstract = {Recent research has made strides toward fair machine learning. Relatively few datasets, however, are commonly examined to evaluate these fairness-aware algorithms, and even fewer in education domains, which can lead to a narrow focus on particular types of fairness issues. In this paper, we describe a novel dataset modification method that utilizes a genetic algorithm to induce many types of unfairness into datasets. Additionally, our method can generate an unfairness benchmark dataset from scratch (thus avoiding data collection in situations that might exploit marginalized populations), or modify an existing dataset used as a reference point. Our method can increase the unfairness by 156.3% on average across datasets and unfairness definitions while preserving AUC scores for models trained on the original dataset (just 0.3% change, on average). We investigate the generalization of our method across educational datasets with different characteristics and evaluate three common unfairness mitigation algorithms. The results show that our method can generate datasets with different types of unfairness, large and small datasets, different types of features, and which affect models trained with different classifiers. Datasets generated with this method can be used for benchmarking and testing for future research on the measurement and mitigation of algorithmic unfairness.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {200–209},
numpages = {10},
keywords = {data generation, datasets, fair machine learning, student data},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636867,
author = {Henricks, Genevieve and Perry, Michelle and Bhat, Suma},
title = {The Relation Among Gender, Language, and Posting Type in Online Chemistry Course Discussion Forums},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636867},
doi = {10.1145/3636555.3636867},
abstract = {This study explored gendered language used in an online chemistry course’s discussion forums, to understand how using gendered language might help or hinder learning outcomes, while considering the goal of various posting structures required in the course. Findings revealed that although gendered-language use did not differ between men and women, gendered forms of language were widely used throughout the forums. The use of gendered language appeared strategic, however, and reliably varied by the goal of the discussion post (i.e., posting a solution to a homework problem, asking a question, or answering a question). Ultimately, gender, language and posting type were found to be related to final grade.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {189–199},
numpages = {11},
keywords = {Gendered language, Online discussion, STEM education},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636866,
author = {Cheng, Yixin and Lyons, Kayley and Chen, Guanliang and Ga\v{s}evi\'{c}, Dragan and Swiecki, Zachari},
title = {Evidence-centered Assessment for Writing with Generative AI},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636866},
doi = {10.1145/3636555.3636866},
abstract = {We propose a learning analytics-based methodology for assessing the collaborative writing of humans and generative artificial intelligence. Framed by the evidence-centered design, we used elements of knowledge-telling, knowledge transformation, and cognitive presence to identify assessment claims; we used data collected from the CoAuthor writing tool as potential evidence for these claims; and we used epistemic network analysis to make inferences from the data about the claims. Our findings revealed significant differences in the writing processes of different groups of CoAuthor users, suggesting that our method is a plausible approach to assessing human-AI collaborative writing.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {178–188},
numpages = {11},
keywords = {Assessment, Epistemic Network Analysis, Evidence-centered Design, Generative Artificial Intelligence},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636865,
author = {Milesi, Mikaela Elizabeth and Martinez-Maldonado, Roberto},
title = {Data Storytelling in Learning Analytics? A Qualitative Investigation into Educators’ Perceptions of Benefits and Risks},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636865},
doi = {10.1145/3636555.3636865},
abstract = {Emerging research has begun to explore the incorporation of data storytelling (DS) elements to enhance the design of learning analytics (LA) dashboards. This involves using visual features, such as text annotations and visual highlights, to help educators and learners focus their attention on key insights derived from data and act upon them. Previous studies have often overlooked the perspectives of educators and other stakeholders on the potential value and risks associated with implementing DS in LA to guide attention. We address this gap by presenting a case study examining how educators perceive the: i) potential value of DS features for teaching and learning design; ii) role of the visualisation designer in delivering a contextually appropriate data story; and iii) ethical implications of utilising DS to communicate insights. We asked educators from a first-year undergraduate program to explore and discuss DS and the visualisation designer by reviewing sample data stories using their students’ data and crafting their own data stories. Our findings suggest that educators were receptive to DS features, especially meaningful use of annotations and highlighting important data points to easily identify critical information. Every participant acknowledged the potential for DS features to be exploited for harmful or self-serving purposes.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {167–177},
numpages = {11},
keywords = {data storytelling, information visualisation, learning analytics},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636864,
author = {Fang, Zheng and Wang, Weiqing and Chen, Guanliang and Swiecki, Zachari},
title = {Neural Epistemic Network Analysis: Combining Graph Neural Networks and Epistemic Network Analysis to Model Collaborative Processes},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636864},
doi = {10.1145/3636555.3636864},
abstract = {We report on the design and evaluation of a novel technique for analysing the sociocognitive nature of collaborative problem-solving—neural epistemic network analysis (NENA). NENA combines the computational power and representational ability of graph neural networks (GNNs) to naturally incorporate social and cognitive features in the analysis with the interpretative advantages of epistemic network analysis (ENA). Comparing NENA and ENA on two datasets from collaborative problem-solving contexts, we found that NENA improves upon ENA’s ability to distinguish between known subgroups in CPS data, while also improving the interpretability and explainability of GNN results.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {157–166},
numpages = {10},
keywords = {Collaborative Problem-Solving, Epistemic Network Analysis, Graph Neural Networks, Social Network Analysis},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636863,
author = {Song, Yukyeong and Li, Chenglu and Xing, Wanli and Li, Shan and Lee, Hakeoung Hannah},
title = {A Fair Clustering Approach to Self-Regulated Learning Behaviors in a Virtual Learning Environment},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636863},
doi = {10.1145/3636555.3636863},
abstract = {While virtual learning environments (VLEs) are widely used in K-12 education for classroom instruction and self-study, young students’ success in VLEs highly depends on their self-regulated learning (SRL) skills. Therefore, it is important to provide personalized support for SRL. One important precursor of designing personalized SRL support is to understand students’ SRL behavioral patterns. Extensive studies have clustered SRL behaviors and prescribed personalized support for each cluster. However, limited attention has been paid to the algorithm bias and fairness of clustering results. In this study, we “fairly” clustered the behavioral patterns of SRL using fair-capacitated clustering (FCC), an algorithm that incorporates constraints to ensure fairness in the assignment of data points. We used data from 14,251 secondary school learners in a virtual math learning environment. The results of FCC showed that it could capture six clusters of SRL behaviors in a fair way; three clusters belonging to high-performing (i.e., H-1. Help-provider, H-2) Active SRL learner, H-3) Active onlooker), and three clusters in low-performing groups (i.e., L-1) Quiz-taker, L-2) Dormant learner, and L-3) Inactive onlooker). The findings provide a better understanding of SRL patterns in online learning and can potentially guide the design of personalized support for SRL.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {771–778},
numpages = {8},
keywords = {Fair clustering, Self-regulated learning, Virtual learning environment},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636862,
author = {Pishtari, Gerti and Sarmiento-M\'{a}rquez, Edna and Rodr\'{\i}guez-Triana, Mar\'{\i}a Jes\'{u}s and Wagner, Marlene and Ley, Tobias},
title = {Mirror mirror on the wall, what is missing in my pedagogical goals? The Impact of an AI-Driven Feedback System on the Quality of Teacher-Created Learning Designs},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636862},
doi = {10.1145/3636555.3636862},
abstract = {Given the rising prominence of Artificial Intelligence (AI) in education, understanding its impact on teacher practices is essential. This paper presents an ABAB reversal design study conducted during a teacher training, where an AI-driven feedback system helped 19 teachers to create learning designs. It investigates the impact that the AI-driven feedback had on the quality of designs and assesses pre- and post-training shifts in teachers’ perceptions of the technology. We observed statistical differences between designs crafted without (in phase A1) and with AI (B1). Notably, a small positive influence persisted even after AI withdrawal (A2). This hints that specialized AI algorithms for learning design can assist teachers in effectively achieving their design objectives. Despite noticeable shifts in teachers’ perceived understanding and usefulness of AI, their trust and intention to use remained unchanged. For a successful teacher-AI partnership, future research should explore the long-term impact that AI usage can have on learning design practices and strategies to nurture trust.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {145–156},
numpages = {12},
keywords = {Artificial Intelligence, Design Analytics, Inquiry-Based Learning, Learning Design, Mobile Learning, Teacher Training},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636861,
author = {Steinbeck, Hendrik and Elhayany, Mohamed and Meinel, Christoph},
title = {Millions of Views, But Does It Promote Learning? Analyzing Popular SciComm Production Styles Regarding Learning Success, User Behavior and Perception},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636861},
doi = {10.1145/3636555.3636861},
abstract = {With a rising amount of highly successful educational content on major video platforms, science communication (SciComm) can be considered mainstream. Although the success in terms of social media metrics (e.g. followers and watch time) is undoubtedly given, the learning mechanisms of these production styles is under-researched. Through a between-subject-design of 980 adult learners in a MOOC about data science, this study analyzes how much of a difference four popular SciComm production styles about relational databases make in regard to perceived quality, learning success and technical user behavior. Testing the isolated effect showed no statistical difference in the grand scheme of things. Additionally, a multivariate regression model, estimating the overall course points with robust standard errors showed six significant variables: The time spend with the material and the number of exercise submissions are particular noteworthy. Based on our results, an underlying (video) script is more relevant than the actual production style. Prioritizing the preparation of this material instead following a specific, pre-existing video production style is recommended.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {134–144},
numpages = {11},
keywords = {field-experiment, science communication, video production styles, video-based education, youtube},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636860,
author = {Li, Hai and Li, Chenglu and Xing, Wanli and Baral, Sami and Heffernan, Neil},
title = {Automated Feedback for Student Math Responses Based on Multi-Modality and Fine-Tuning},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636860},
doi = {10.1145/3636555.3636860},
abstract = {Open-ended mathematical problems are a commonly used method for assessing students’ abilities by teachers. In previous automated assessments, natural language processing focusing on students’ textual answers has been the primary approach. However, mathematical questions often involve answers containing images, such as number lines, geometric shapes, and charts. Several existing computer-based learning systems allow students to upload their handwritten answers for grading. Yet, there are limited methods available for automated scoring of these image-based responses, with even fewer multi-modal approaches that can simultaneously handle both texts and images. In addition to scoring, another valuable scaffolding to procedurally and conceptually support students while lacking automation is comments. In this study, we developed a multi-task model to simultaneously output scores and comments using students’ multi-modal artifacts (texts and images) as inputs by extending BLIP, a multi-modal visual reasoning model. Benchmarked with three baselines, we fine-tuned and evaluated our approach on a dataset related to open-ended questions as well as students’ responses. We found that incorporating images with text inputs enhances feedback performance compared to using texts alone. Meanwhile, our model can effectively provide coherent and contextual feedback in mathematical settings.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {763–770},
numpages = {8},
keywords = {auto-scoring, automated comment, fine-tuning, image response, multi-modality, open-ended response},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636859,
author = {Wong, Cheryl Sze Yin and Ramasamy, Savitha},
title = {Architectural Adaptation and Regularization of Attention Networks for Incremental Knowledge Tracing},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636859},
doi = {10.1145/3636555.3636859},
abstract = {EdTech platforms continuously refresh their database with new questions and concepts with evolving course syllabus. The state-of-the-art knowledge tracing models are unable to adapt to these changes, as the size of the question embedding layers is typically fixed. In this work, we propose an incremental learning algorithm for knowledge tracing that is capable of adapting itself to growing pool of concepts and questions, through its architectural adaptation and regularization strategies. The algorithm, referred as, "Architectural adaptation and Regularization of Attention network for Incremental Knowledge Tracing (ARAIKT)", is capable of adapting the embeddings with increasing concepts and question bank, while preserving representations of the previous concepts and question banks. Furthermore, they are robust to distributional drifts in the data, and are capable of preserving privacy of data across study centers and EdTech platforms. We demonstrate the effectiveness of the ARAIKT by evaluating its performance on subsets of study centers/academic years within ASSISTment2009 and ASSISTment2017 data sets, respectively.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {757–762},
numpages = {6},
keywords = {Incremental, Knowledge Tracing},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636858,
author = {Kandemir, Erva Nihan and Vie, Jill-J\^{e}nn and Sanchez-Ayte, Adam and Palombi, Olivier and Ramus, Franck},
title = {Adaptation of the Multi-Concept Multivariate Elo Rating System to Medical Students' Training Data},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636858},
doi = {10.1145/3636555.3636858},
abstract = {Accurate estimation of question difficulty and prediction of student performance play key roles in optimizing educational instruction and enhancing learning outcomes within digital learning platforms. The Elo rating system is widely recognized for its proficiency in predicting student performance by estimating both question difficulty and student ability while providing computational efficiency and real-time adaptivity. This paper presents an adaptation of a multi-concept variant of the Elo rating system to the data collected by a medical training platform—a platform characterized by a vast knowledge corpus, substantial inter-concept overlap, a huge question bank with significant sparsity in user-question interactions, and a highly diverse user population, presenting unique challenges. Our study is driven by two primary objectives: firstly, to comprehensively evaluate the Elo rating system’s capabilities on this real-life data, and secondly, to tackle the issue of imprecise early-stage estimations when implementing the Elo rating system for online assessments. Our findings suggest that the Elo rating system exhibits comparable accuracy to the well-established logistic regression model in predicting final exam outcomes for users within our digital platform. Furthermore, results underscore that initializing Elo rating estimates with historical data remarkably reduces errors and enhances prediction accuracy, especially during the initial phases of student interactions.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {123–133},
numpages = {11},
keywords = {Elo-based learning model, knowledge tracing, logistic regression},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636857,
author = {Echeverria, Vanessa and Yan, Lixiang and Zhao, Linxuan and Abel, Sophie and Alfredo, Riordan and Dix, Samantha and Jaggard, Hollie and Wotherspoon, Rosie and Osborne, Abra and Buckingham Shum, Simon and Gasevic, Dragan and Martinez-Maldonado, Roberto},
title = {TeamSlides: a Multimodal Teamwork Analytics Dashboard for Teacher-guided Reflection in a Physical Learning Space},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636857},
doi = {10.1145/3636555.3636857},
abstract = {Advancements in Multimodal Learning Analytics (MMLA) have the potential to enhance the development of effective teamwork skills and foster reflection on collaboration dynamics in physical learning environments. Yet, only a few MMLA studies have closed the learning analytics loop by making MMLA solutions immediately accessible to educators to support reflective practices, especially in authentic settings. Moreover, deploying MMLA solutions in authentic settings can bring new challenges beyond logistic and privacy issues. This paper reports the design and use of TeamSlides, a multimodal teamwork analytics dashboard to support teacher-guided reflection. We conducted an in-the-wild classroom study involving 11 teachers and 138 students. Multimodal data were collected from students working in team healthcare simulations. We examined how teachers used the dashboard in 22 debrief sessions to aid their reflective practices. We also interviewed teachers to discuss their perceptions of the dashboard’s value and the challenges faced during its use. Our results suggest that the dashboard effectively reinforced discussions and augmented teacher-guided reflection practices. However, teachers encountered interpretation conflicts, sometimes leading to mistrust or misrepresenting the information. We discuss the considerations needed to overcome these challenges in MMLA research.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {112–122},
numpages = {11},
keywords = {MMLA, dashboards, reflection, team dynamics, teamwork analytics, visualisation},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636856,
author = {Yan, Lixiang and Martinez-Maldonado, Roberto and Gasevic, Dragan},
title = {Generative Artificial Intelligence in Learning Analytics: Contextualising Opportunities and Challenges through the Learning Analytics Cycle},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636856},
doi = {10.1145/3636555.3636856},
abstract = {Generative artificial intelligence (GenAI), exemplified by ChatGPT, Midjourney, and other state-of-the-art large language models and diffusion models, holds significant potential for transforming education and enhancing human productivity. While the prevalence of GenAI in education has motivated numerous research initiatives, integrating these technologies within the learning analytics (LA) cycle and their implications for practical interventions remain underexplored. This paper delves into the prospective opportunities and challenges GenAI poses for advancing LA. We present a concise overview of the current GenAI landscape and contextualise its potential roles within Clow’s generic framework of the LA cycle. We posit that GenAI can play pivotal roles in analysing unstructured data, generating synthetic learner data, enriching multimodal learner interactions, advancing interactive and explanatory analytics, and facilitating personalisation and adaptive interventions. As the lines blur between learners and GenAI tools, a renewed understanding of learners is needed. Future research can delve deep into frameworks and methodologies that advocate for human-AI collaboration. The LA community can play a pivotal role in capturing data about human and AI contributions and exploring how they can collaborate most effectively. As LA advances, it is essential to consider the pedagogical implications and broader socioeconomic impact of GenAI for ensuring an inclusive future.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {101–111},
numpages = {11},
keywords = {ChatGPT, Midjourney, educational technology, generative artificial intelligence, human-AI collaboration, learning analytics},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636855,
author = {Zhao, Linxuan and Echeverria, Vanessa and Swiecki, Zachari and Yan, Lixiang and Alfredo, Riordan and Li, Xinyu and Gasevic, Dragan and Martinez-Maldonado, Roberto},
title = {Epistemic Network Analysis for End-users: Closing the Loop in the Context of Multimodal Analytics for Collaborative Team Learning},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636855},
doi = {10.1145/3636555.3636855},
abstract = {Effective collaboration and team communication are critical across many sectors. However, the complex dynamics of collaboration in physical learning spaces, with overlapping dialogue segments and varying participant interactions, pose assessment challenges for educators and self-reflection difficulties for students. Epistemic network analysis (ENA) is a relatively novel technique that has been used in learning analytics (LA) to unpack salient aspects of group communication. Yet, most LA works based on ENA have primarily sought to advance research knowledge rather than directly aid teachers and students by closing the LA loop. We address this gap by conducting a study in which we i) engaged teachers in designing human-centred versions of epistemic networks; ii) formulated an NLP methodology to code physically distributed dialogue segments of students based on multimodal (audio and positioning) data, enabling automatic generation of epistemic networks; and iii) deployed the automatically generated epistemic networks in 28 authentic learning sessions and investigated how they can support teaching. The results indicate the viability of completing the analytics loop through the design of streamlined epistemic network representations that enable teachers to support students’ reflections.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {90–100},
numpages = {11},
keywords = {Collaborative learning, Human-centred, Learning Analytics, Multimodality, Teamwork},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636854,
author = {Que, Ying and Ng, Jeremy Tzi Dong and Hu, Xiao and Mak, Mitchell Kam Fai and Yip, Peony Tsz Yan},
title = {Using Multimodal Learning Analytics to Examine Learners’ Responses to Different Types of Background Music during Reading Comprehension},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636854},
doi = {10.1145/3636555.3636854},
abstract = {Previous studies have evaluated the affordances and challenges of performing cognitively demanding learning tasks with background music (BGM), yet the effects of various types of BGM on learning still remain an open question. This study aimed to examine the impacts of different music genres and fine-grained music characteristics on learners’ emotional, physiological, and pupillary responses during reading comprehension. Leveraging multimodal learning analytics (MmLA) methods of collecting data in multiple modalities from learners, a user experiment was conducted on 102 participants, with half of them reading with self-selected BGM (i.e., the experimental group), while the other half reading without BGM (i.e., the control group). Results of statistical analyses and interviews revealed significant differences between the two groups in their self-reported emotions and automatically measured physiological responses when the experimental group was exposed to classical, easy-listening, rebellious and rhythmic music. Fine-grained music characteristics (e.g., instrumentation, tempo) could predict learners’ emotions, pupillary, and physiological responses during reading comprehension. The expected contributions of this study include: 1) providing empirical evidence for understanding affective dimensions of learning with BGM, 2) applying MmLA methods for examining the impacts of BGM on learning, and 3) yielding practical implications on how to improve learning with BGM.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {749–756},
numpages = {8},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636853,
author = {Singh, Anjali and Brooks, Christopher and Wang, Xu and Li, Warren and Kim, Juho and Wilson, Deepti},
title = {Bridging Learnersourcing and AI: Exploring the Dynamics of Student-AI Collaborative Feedback Generation},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636853},
doi = {10.1145/3636555.3636853},
abstract = {This paper explores the space of optimizing feedback mechanisms in complex domains such as data science, by combining two prevailing approaches: Artificial Intelligence (AI) and learnersourcing. Towards addressing the challenges posed by each approach, this work compares traditional learnersourcing with an AI-supported approach. We report on the results of a randomized controlled experiment conducted with 72 Master’s level students in a data visualization course, comparing two conditions: students writing hints independently versus revising hints generated by GPT-4. The study aimed to evaluate the quality of learnersourced hints, examine the impact of student performance on hint quality, gauge learner preference for writing hints with versus without AI support, and explore the potential of the student-AI collaborative exercise in fostering critical thinking about LLMs. Based on our findings, we provide insights for designing learnersourcing activities leveraging AI support and optimizing students’ learning as they interact with LLMs.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {742–748},
numpages = {7},
keywords = {Data Visualization, Feedback Generation, GPT-4, Learnersourcing},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636852,
author = {Barthakur, Abhinava and Jovanovic, Jelena and Zamecnik, Andrew and Kovanovic, Vitomir and Xu, Gongjun and Dawson, Shane},
title = {Towards Comprehensive Monitoring of Graduate Attribute Development: A Learning Analytics Approach in Higher Education},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636852},
doi = {10.1145/3636555.3636852},
abstract = {In response to the evolving demands of the contemporary workplace, higher education (HE) institutions are increasingly emphasising the development of transversal skills and graduate attributes (GAs). The development of GAs, such as effective communication, collaboration, and lifelong learning, are non-linear and follow distinct trajectories for individual learners. The ability to trace and measure the progression of GA remains a significant challenge. While previous studies have focused on empirical methods for measuring GAs in individual courses, a notable gap exists in understanding their longitudinal development within HE programs. To address this research gap, our study focuses on measuring and tracing the development of GAs in an Initial Teacher Education (ITE) undergraduate program at a large public university in Australia. By combining learning analytics (LA) with psychometric models, we analysed students’ assessment grades to measure learners’ GA development in each year of the ITE program. The resulting measurements enabled the identification of distinct profiles of GA attainment, as demonstrated by learners and their distinct pathways. The overall approach allows for a comprehensive representation of a learner's progress throughout the program of study. As such, the developed approach sets the grounds for more personalised learning support, program evaluation, and improvement of students’ GA attainment.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {78–89},
numpages = {12},
keywords = {Higher education, graduate attributes, learner profile, monitoring progression},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636851,
author = {Zambrano, Andres Felipe and Baker, Ryan S.},
title = {Long-Term Prediction from Topic-Level Knowledge and Engagement in Mathematics Learning},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636851},
doi = {10.1145/3636555.3636851},
abstract = {During middle school, students' learning experiences begin to influence their future decisions about college enrollment and career selection. Prior research indicates that both knowledge gained and the disengagement and affect experienced during this period are predictors of these future outcomes. However, this past research has investigated affect, disengagement, and knowledge in an overall fashion – looking at the average manifestation of these constructs across all topics studied across a year of mathematics. It may be that some mathematics topics are more associated with these outcomes than others. In this study, we use data from middle school students interacting with a digital mathematics learning platform, to analyze the interplay of these features across different topic areas. Our findings show that mastering Functions is the most important predictor of both college enrollment and STEM career selection, while the importance of knowing other topic areas varies across the two outcomes. Furthermore, while subject knowledge tends to be the most relevant predictor for general college enrollment, affective states, especially confusion and engaged concentration, become more important for predicting STEM career selection.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {66–77},
numpages = {12},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636850,
author = {Hutt, Stephen and DePiro, Allison and Wang, Joann and Rhodes, Sam and Baker, Ryan S and Hieb, Grayson and Sethuraman, Sheela and Ocumpaugh, Jaclyn and Mills, Caitlin},
title = {Feedback on Feedback: Comparing Classic Natural Language Processing and Generative AI to Evaluate Peer Feedback},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636850},
doi = {10.1145/3636555.3636850},
abstract = {Peer feedback can be a powerful tool as it presents learning opportunities for both the learner receiving feedback as well as the learner providing feedback. Despite its utility, it can be difficult to implement effectively, particularly for younger learners, who are often novices at providing feedback. It can be difficult for students to learn what constitutes “good” feedback – particularly in open-ended problem-solving contexts. To address this gap, we investigate both classical natural language processing techniques and large language models, specifically ChatGPT, as potential approaches to devise an automated detector of feedback quality (including both student progress towards goals and next steps needed). Our findings indicate that the classical detectors are highly accurate and, through feature analysis, we elucidate the pivotal elements influencing its decision process. We find that ChatGPT is less accurate than classical NLP but illustrate the potential of ChatGPT in evaluating feedback, by generating explanations for ratings, along with scores. We discuss how the detector can be used for automated feedback evaluation and to better scaffold peer feedback for younger learners.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {55–65},
numpages = {11},
keywords = {Generative AI, Language Analytics, Large Language Models, Natural Language Processing, Peer Feedback},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636849,
author = {Barrett, Jake and Day, Alasdair and Gal, Kobi},
title = {Improving Model Fairness with Time-Augmented Bayesian Knowledge Tracing},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636849},
doi = {10.1145/3636555.3636849},
abstract = {Modelling student performance is an increasingly popular goal in the learning analytics community. A common method for this task is Bayesian Knowledge Tracing (BKT), which predicts student performance and topic mastery using the student’s answer history. While BKT has strong qualities and good empirical performance, like many machine learning approaches it can be prone to bias. In this study we demonstrate an inherent bias in BKT with respect to students’ income support levels and gender, using publicly available data. We find that this bias is likely a result of the model’s ‘slip’ parameter disregarding answer speed when deciding if a student has lost mastery status. We propose a new BKT model variation that directly considers answer speed, resulting in a significant fairness increase without sacrificing model performance. We discuss the role of answer speed as a potential cause of BKT model bias, as well as a method to minimise bias in future implementations.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {46–54},
numpages = {9},
keywords = {fairness, knowledge tracing},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636848,
author = {Cloude, Elizabeth B. and Kumar, Pranshu and Baker, Ryan S. and Fouh, Eric},
title = {Novice programmers inaccurately monitor the quality of their work and their peers’ work in an introductory computer science course},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636848},
doi = {10.1145/3636555.3636848},
abstract = {A student’s ability to accurately evaluate the quality of their work holds significant implications for their self-regulated learning and problem-solving proficiency in introductory programming. A widespread cognitive bias that frequently impedes accurate self- assessment is overconfidence, which often stems from a misjudgment of contextual and task-related cues, including students’ judgment of their peers’ competencies. Little research has explored the role of overconfidence on novice programmers’ ability to accurately monitor their own work in comparison to their peers’ work and its impact on performance in introductory programming courses. The present study examined whether novice programmers exhibited a common cognitive bias called the "hard-easy effect", where students believe their work is better than their peers on easier tasks (overplace) but worse than their peers on harder tasks (underplace). Results showed a reversal of the hard-easy effect, where novices tended to overplace themselves on harder tasks, yet underplace themselves on easier ones. Remarkably, underplacers performed better on an exam compared to overplacers. These findings advance our understanding of relationships between the hard-easy effect, monitoring accuracy across multiple tasks, and grades within introductory programming. Implications of this study can be used to guide instructional decision making and design to improve novices’ metacognitive awareness and performance in introductory programming courses.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {35–45},
numpages = {11},
keywords = {CS1, Hard-easy Effect, Metacognition, Overconfidence},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636847,
author = {Alfredo, Riordan and Echeverria, Vanessa and Jin, Yueqiao and Swiecki, Zachari and Ga\v{s}evi\'{c}, Dragan and Martinez-Maldonado, Roberto},
title = {SLADE: A Method for Designing Human-Centred Learning Analytics Systems},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636847},
doi = {10.1145/3636555.3636847},
abstract = {There is a growing interest in creating Learning Analytics (LA) systems that incorporate student perspectives. Yet, many LA systems still lean towards a technology-centric approach, potentially overlooking human values and the necessity of human oversight in automation. Although some recent LA studies have adopted a human-centred design stance, there is still limited research on establishing safe, reliable, and trustworthy systems during the early stages of LA design. Drawing from a newly proposed framework for human-centred artificial intelligence, we introduce SLADE, a method for ideating and identifying features of human-centred LA systems that balance human control and computer automation. We illustrate SLADE’s application in designing LA systems to support collaborative learning in healthcare. Twenty-one third-year students participated in design sessions through SLADE’s four steps: i) identifying challenges and corresponding LA systems; ii) prioritising these LA systems; iii) ideating human control and automation features; and iv) refining features emphasising safety, reliability, and trustworthiness. Our results demonstrate SLADE’s potential to assist researchers and designers in: 1) aligning authentic student challenges with LA systems through both divergent ideation and convergent prioritisation; 2) understanding students’ perspectives on personal agency and delegation to teachers; and 3) fostering discussions about the safety, reliability, and trustworthiness of LA solutions.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {24–34},
numpages = {11},
keywords = {Design Thinking, Double Diamond, Human-centered AI, Human-centered learning analytics},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636846,
author = {Phung, Tung and P\u{a}durean, Victor-Alexandru and Singh, Anjali and Brooks, Christopher and Cambronero, Jos\'{e} and Gulwani, Sumit and Singla, Adish and Soares, Gustavo},
title = {Automating Human Tutor-Style Programming Feedback: Leveraging GPT-4 Tutor Model for Hint Generation and GPT-3.5 Student Model for Hint Validation},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636846},
doi = {10.1145/3636555.3636846},
abstract = {Generative AI and large language models hold great promise in enhancing programming education by automatically generating individualized feedback for students. We investigate the role of generative AI models in providing human tutor-style programming hints to help students resolve errors in their buggy programs. Recent works have benchmarked state-of-the-art models for various feedback generation scenarios; however, their overall quality is still inferior to human tutors and not yet ready for real-world deployment. In this paper, we seek to push the limits of generative AI models toward providing high-quality programming hints and develop a novel technique, GPT4HINTS-GPT3.5VAL. As a first step, our technique leverages GPT-4 as a “tutor” model to generate hints – it boosts the generative quality by using symbolic information of failing test cases and fixes in prompts. As a next step, our technique leverages GPT-3.5, a weaker model, as a “student” model to further validate the hint quality – it performs an automatic quality validation by simulating the potential utility of providing this feedback. We show the efficacy of our technique via extensive evaluation using three real-world datasets of Python programs covering a variety of concepts ranging from basic algorithms to regular expressions and data analysis using pandas library.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {12–23},
numpages = {12},
keywords = {ChatGPT, Feedback Generation, GPT4, Generative AI, Programming Education},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636845,
author = {Bourguet, Marie-Luce},
title = {Demonstrating the impact of study regularity on academic success using learning analytics},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636845},
doi = {10.1145/3636555.3636845},
abstract = {Students can be described as self-regulated learners when they are meta-cognitively, motivationally, and behaviourally active participants in their own learning. Flipping the classroom requires from students good self-regulated learning skills, primarily time management and study regularity, as they must have engaged in learning activities prior to attending live classes. In this short paper, we describe our approach of using learning analytics to demonstrate the impact of study regularity on academic success in a flipped learning environment. Our key contribution is the definition of a measure of study regularity that can uncover various students’ learning profiles during flipped learning. We are showing that the regularity measure correlate strongly with academic success and that it can be used to predict students’ performance. We then discuss how such a measure can also be used to raise student’s awareness about their learning behaviour and lack of appropriate strategy, to nudge the students into modifying their learning behaviour, and to monitor class behaviour, such as detecting a worrying students’ disengagement trend.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {736–741},
numpages = {6},
keywords = {Flipped learning, Grade predictions, Learning pathways, Learning traces, Self-regulated learning, Study regularity},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636844,
author = {Sloan-Lynch, Jay and Morse, Robert},
title = {Equity-Forward Learning Analytics: Designing a Dashboard to Support Marginalized Student Success},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636844},
doi = {10.1145/3636555.3636844},
abstract = {Student outcomes in US higher education exhibit deep and persistent inequities. The continued underperformance of historically marginalized students remains a serious concern across higher education, reflected in increasing efforts among institutions to infuse diversity, equity, and inclusion into their academic and social communities. Yet despite widespread recognition of these inequities, few studies in the learning analytics literature engage in practical ways with issues of educational equity or DEI considerations. In this paper, we share our work supporting a large college's strategic DEI goals through the creation of a Course Diversity Dashboard informed by research into how students’ study behaviors and performance interact with their gender and ethnic identities to impact course outcomes. The dashboard enables users to explore inequalities in course outcomes and take concrete actions to improve student study strategies, time management, and prior knowledge. Results from our research revealed the existence of previously hidden learner inequities in all courses included in our study as well as critical differences in underrepresented minority students’ prior knowledge. And while we did not find evidence of meaningful differences in the study behaviors of student subgroups, our findings further validate the effectiveness of evidence-informed study strategies in an authentic educational setting.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {1–11},
numpages = {11},
keywords = {DEI, educational equity, learning analytics dashboards, study strategies},
location = {Kyoto, Japan},
series = {LAK '24}
}

@proceedings{10.1145/3636555,
title = {LAK '24: Proceedings of the 14th Learning Analytics and Knowledge Conference},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Kyoto, Japan}
}

@inproceedings{10.1145/3576050.3576153,
author = {Franco, Andrea and Holzer, Adrian},
title = {Fostering Privacy Literacy among High School Students by Leveraging Social Media Interaction and Learning Traces in the Classroom},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576153},
doi = {10.1145/3576050.3576153},
abstract = {With daily social media consumption among teens exceeding eight hours, it becomes increasingly important to raise awareness about the digital traces they leave behind. However, concepts of privacy literacy such as data and metadata can seem abstract and difficult to grasp. In this short research paper, we tackle this issue by designing, implementing and presenting an evaluation of a novel technology-enhanced pedagogical scenario for high school students. The scenario covers two main sessions. In a first session the SpeakUp social-media-like classroom interaction app is used to support a digitally mediated debate. In the second session, the actual learning traces from the digitally mediated debate are used as an object of study to enable students to reflect on the traces they leave behind on social media platforms. In order to enable this scenario we extended the existing SpeakUp app to the specifics of the context. The scenario was implemented and evaluated in real classrooms during a semester-long course on digital skills with 45 high school students. Our results show that the learning scenario is appreciated by students and even though non-STEM students might require more onboarding to be fully engaged in the digitally mediated debate, students from both STEM and non-STEM classes learn effectively. We discuss shortcomings and future research avenues.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {538–544},
numpages = {7},
keywords = {SpeakUp, classroom interaction app, learning traces, metadata, privacy literacy, social media},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576152,
author = {Oliveira, Hil\'{a}rio and Ferreira Mello, Rafael and Barreiros Rosa, Bruno Alexandre and Rakovic, Mladen and Miranda, Pericles and Cordeiro, Thiago and Isotani, Seiji and Bittencourt, Ig and Gasevic, Dragan},
title = {Towards explainable prediction of essay cohesion in Portuguese and English},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576152},
doi = {10.1145/3576050.3576152},
abstract = {Textual cohesion is an essential aspect of a formally written text, related to linguistic mechanisms that connect elements such as words, sentences, and paragraphs. Several studies have proposed approaches to estimate textual cohesion in essays automatically. There is limited research that aims to study the extent to which the use of machine learning approaches can predict the textual cohesion of essays written in different languages (not just English). This paper reports on the findings of a study that aimed to propose and evaluate approaches that automatically estimate the cohesion of essays in Portuguese and English. The study proposed regression-based models grounded in conventional feature-based machine learning methods and deep learning-based pre-trained language models. The study also examined the explainability of automated approaches to scrutinize their predictions. We analyzed two datasets composed of 4,570 (Portuguese) and 7,101 (English) essays. The results demonstrate that a deep learning-based model achieved the best performance on both datasets with a moderate Pearson correlation with human-rated cohesion scores. However, the explainability of the automatic cohesion estimations based on conventional machine learning models offered a stronger potential than that of the deep learning model.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {509–519},
numpages = {11},
keywords = {Essay analysis, explainable artificial intelligence, regression models, textual cohesion},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576149,
author = {Cock, Jade Mai and Bilal, Muhammad and Davis, Richard and Marras, Mirko and Kaser, Tanja},
title = {Protected Attributes Tell Us Who, Behavior Tells Us How: A Comparison of Demographic and Behavioral Oversampling for Fair Student Success Modeling},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576149},
doi = {10.1145/3576050.3576149},
abstract = {Algorithms deployed in education can shape the learning experience and success of a student. It is therefore important to understand whether and how such algorithms might create inequalities or amplify existing biases. In this paper, we analyze the fairness of models which use behavioral data to identify at-risk students and suggest two novel pre-processing approaches for bias mitigation. Based on the concept of intersectionality, the first approach involves intelligent oversampling on combinations of demographic attributes. The second approach does not require any knowledge of demographic attributes and is based on the assumption that such attributes are a (noisy) proxy for student behavior. We hence propose to directly oversample different types of behaviors identified in a cluster analysis. We evaluate our approaches on data from (i) an open-ended learning environment and (ii) a flipped classroom course. Our results show that both approaches can mitigate model bias. Directly oversampling on behavior is a valuable alternative, when demographic metadata is not available. Source code and extended results are provided in https://github.com/epfl-ml4ed/behavioral-oversampling.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {488–498},
numpages = {11},
keywords = {Behavioral data, Bias, Fairness, Machine Learning, Oversampling, Student success},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576148,
author = {Galici, Roberta and Kaser, Tanja and Fenu, Gianni and Marras, Mirko},
title = {Do Not Trust a Model Because It is Confident: Uncovering and Characterizing Unknown Unknowns to Student Success Predictors in Online-Based Learning},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576148},
doi = {10.1145/3576050.3576148},
abstract = {Student success models might be prone to develop weak spots, i.e., examples hard to accurately classify due to insufficient representation during model creation. This weakness is one of the main factors undermining users’ trust, since model predictions could for instance lead an instructor to not intervene on a student in need. In this paper, we unveil the need of detecting and characterizing unknown unknowns in student success prediction in order to better understand when models may fail. Unknown unknowns include the students for which the model is highly confident in its predictions, but is actually wrong. Therefore, we cannot solely rely on the model’s confidence when evaluating the predictions quality. We first introduce a framework for the identification and characterization of unknown unknowns. We then assess its informativeness on log data collected from flipped courses and online courses using quantitative analyses and interviews with instructors. Our results show that unknown unknowns are a critical issue in this domain and that our framework can be applied to support their detection. The source code is available at https://github.com/epfl-ml4ed/unknown-unknowns.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {441–452},
numpages = {12},
keywords = {Fairness, Machine Learning, Student Success, Trust, Uncertainty, Unknown Unknowns.},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576147,
author = {Swamy, Vinitra and Du, Sijia and Marras, Mirko and Kaser, Tanja},
title = {Trusting the Explainers: Teacher Validation of Explainable Artificial Intelligence for Course Design},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576147},
doi = {10.1145/3576050.3576147},
abstract = {Deep learning models for learning analytics have become increasingly popular over the last few years; however, these approaches are still not widely adopted in real-world settings, likely due to a lack of trust and transparency. In this paper, we tackle this issue by implementing explainable AI methods for black-box neural networks. This work focuses on the context of online and blended learning and the use case of student success prediction models. We use a pairwise study design, enabling us to investigate controlled differences between pairs of courses. Our analyses cover five course pairs that differ in one educationally relevant aspect and two popular instance-based explainable AI methods (LIME and SHAP). We quantitatively compare the distances between the explanations across courses and methods. We then validate the explanations of LIME and SHAP with 26 semi-structured interviews of university-level educators regarding which features they believe contribute most to student success, which explanations they trust most, and how they could transform these insights into actionable course design decisions. Our results show that quantitatively, explainers significantly disagree with each other about what is important, and qualitatively, experts themselves do not agree on which explanations are most trustworthy. All code, extended results, and the interview protocol are provided at https://github.com/epfl-ml4ed/trusting-explainers.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {345–356},
numpages = {12},
keywords = {Counterfactuals, Explainable AI, LIME, LSTMs, MOOCs, SHAP, Student Performance Prediction},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576146,
author = {Poellhuber, Louis-Vincent and Poellhuber, Bruno and Desmarais, Michel and Leger, Christian and Roy, Normand and Manh-Chien Vu, Mathieu},
title = {Cluster-Based Performance of Student Dropout Prediction as a Solution for Large Scale Models in a Moodle LMS},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576146},
doi = {10.1145/3576050.3576146},
abstract = {Learning management systems provide a wide breadth of data waiting to be analyzed and utilized to enhance student and faculty experience in higher education. As universities struggle to support students’ engagement, success and retention, learning analytics is being used to build predictive models and develop dashboards to support learners and help them stay engaged, to help teachers identify students needing support, and to predict and prevent dropout. Learning with Big Data has its challenges, however: managing great quantities of data requires time and expertise. To predict students at risk, many institutions use machine learning algorithms with LMS data for a given course or type of course, but only a few are trying to make predictions for a large subset of courses. This begs the question: “How can student dropout be predicted on a very large set of courses in an institution Moodle LMS?” In this paper, we use automation to improve student dropout prediction for a very large subset of courses, by clustering them based on course design and similarity, then by automatically training, testing, and selecting machine learning algorithms for each cluster. We developed a promising methodology that outlines a basic framework that can be adjusted and optimized in many ways and that further studies can easily build on and improve.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {592–598},
numpages = {7},
keywords = {Moodle LMS, dropout prediction, engagement, learning analytics},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576145,
author = {Li, Xinyu and Yan, Lixiang and Zhao, Linxuan and Martinez-Maldonado, Roberto and Gasevic, Dragan},
title = {CVPE: A Computer Vision Approach for Scalable and Privacy-Preserving Socio-spatial, Multimodal Learning Analytics},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576145},
doi = {10.1145/3576050.3576145},
abstract = {Capturing data on socio-spatial behaviours is essential in obtaining meaningful educational insights into collaborative learning and teamwork in co-located learning contexts. Existing solutions, however, have limitations regarding scalability and practicality since they rely largely on costly location tracking systems, are labour-intensive, or are unsuitable for complex learning environments. To address these limitations, we propose an innovative computer-vision-based approach – Computer Vision for Position Estimation (CVPE) – for collecting socio-spatial data in complex learning settings where sophisticated collaborations occur. CVPE is scalable and practical with a fast processing time and only needs low-cost hardware (e.g., cameras and computers). The built-in privacy protection modules also minimise potential privacy and data security issues by masking individuals’ facial identities and provide options to automatically delete recordings after processing, making CVPE a suitable option for generating continuous multimodal/classroom analytics. The potential of CVPE was evaluated by applying it to analyse video data about teamwork in simulation-based learning. The results showed that CVPE extracted socio-spatial behaviours relatively reliably from video recordings compared to indoor positioning data. These socio-spatial behaviours extracted with CVPE uncovered valuable insights into teamwork when analysed with epistemic network analysis. The limitations of CVPE for effective use in learning analytics are also discussed.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {175–185},
numpages = {11},
keywords = {collaborative learning, computer vision, epistemic network, learning analytics, multimodal},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576144,
author = {Chejara, Pankaj and Prieto, Luis P. and Rodriguez-Triana, Maria Jesus and Kasepalu, Reet and Ruiz-Calleja, Adolfo and Shankar, Shashi Kant},
title = {How to Build More Generalizable Models for Collaboration Quality? Lessons Learned from Exploring Multi-Context Audio-Log Datasets using Multimodal Learning Analytics},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576144},
doi = {10.1145/3576050.3576144},
abstract = {Multimodal learning analytics (MMLA) research for building collaboration quality estimation models has shown significant progress. However, the generalizability of such models is seldom addressed. In this paper, we address this gap by systematically evaluating the across-context generalizability of collaboration quality models developed using a typical MMLA pipeline. This paper further presents a methodology to explore modelling pipelines with different configurations to improve the generalizability of the model. We collected 11 multimodal datasets (audio and log data) from face-to-face collaborative learning activities in six different classrooms with five different subject teachers. Our results showed that the models developed using the often-employed MMLA pipeline degraded in terms of Kappa from Fair (.20 &lt; Kappa &lt; .40) to Poor (Kappa &lt; .20) when evaluated across contexts. This degradation in performance was significantly ameliorated with pipelines that emerged as high-performing from our exploration of 32 pipelines. Furthermore, our exploration of pipelines provided statistical evidence that often-overlooked contextual data features improve the generalizability of a collaboration quality model. With these findings, we make recommendations for the modelling pipeline which can potentially help other researchers in achieving better generalizability in their collaboration quality estimation models.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {111–121},
numpages = {11},
keywords = {Collaboration Quality, Generalizability, Machine Learning, MultiModal Learning Analytics},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576143,
author = {Chejara, Pankaj and Prieto, Luis P. and Rodriguez-Triana, Maria Jesus and Ruiz-Calleja, Adolfo and Khalil, Mohammad},
title = {Impact of window size on the generalizability of collaboration quality estimation models developed using Multimodal Learning Analytics},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576143},
doi = {10.1145/3576050.3576143},
abstract = {Multimodal Learning Analytics (MMLA) has been applied to collaborative learning, often to estimate collaboration quality with the use of multimodal data, which often have uneven time scales. The difference in time scales is usually handled by dividing and aggregating data using a fixed-size time window. So far, the current MMLA research lacks a systematic exploration of whether and how much window size affects the generalizability of collaboration quality estimation models. In this paper, we investigate the impact of different window sizes (e.g., 30 seconds, 60s, 90s, 120s, 180s, 240s) on the generalizability of classification models for collaboration quality and its underlying dimensions (e.g., argumentation). Our results from an MMLA study involving the use of audio and log data showed that a 60 seconds window size enabled the development of more generalizable models for collaboration quality (AUC 61%) and argumentation (AUC 64%). In contrast, for modeling dimensions focusing on coordination, interpersonal relationship, and joint information processing, a window size of 180 seconds led to better performance in terms of across-context generalizability (on average from 56% AUC to 63% AUC). These findings have implications for the eventual application of MMLA in authentic practice.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {559–565},
numpages = {7},
keywords = {Collaboration Quality, Generalizability, Machine Learning, MultiModal Learning Analytics, Temporal Window},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576142,
author = {Kizilcec, Ren\'{e} F. and Viberg, Olga and Jivet, Ioana and Martinez Mones, Alejandra and Oh, Alice and Hrastinski, Stefan and Mutimukwe, Chantal and Scheffel, Maren},
title = {The Role of Gender in Students’ Privacy Concerns about Learning Analytics: Evidence from five countries},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576142},
doi = {10.1145/3576050.3576142},
abstract = {The protection of students’ privacy in learning analytics (LA) applications is critical for cultivating trust and effective implementations of LA in educational environments around the world. However, students’ privacy concerns and how they may vary along demographic dimensions that historically influence these concerns have yet to be studied in higher education. Gender differences, in particular, are known to be associated with people's information privacy concerns, including in educational settings. Building on an empirically validated model and survey instrument for student privacy concerns, their antecedents and their behavioral outcomes, we investigate the presence of gender differences in students’ privacy concerns about LA. We conducted a survey study of students in higher education across five countries (N = 762): Germany, South Korea, Spain, Sweden and the United States. Using multiple regression analysis, across all five countries, we find that female students have stronger trusting beliefs and they are more inclined to engage in self-disclosure behaviors compared to male students. However, at the country level, these gender differences are significant only in the German sample, for Bachelor's degree students, and for students between the ages of 18 and 24. Thus, national context, degree program, and age are important moderating factors for gender differences in student privacy concerns.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {545–551},
numpages = {7},
keywords = {Gender, Learning Analytics, Privacy Concerns, Students},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576141,
author = {Divjak, Blazenka and Svetec, Barbi and Horvat, Damir},
title = {Learning analytics dashboards: What do students actually ask for?},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576141},
doi = {10.1145/3576050.3576141},
abstract = {Learning analytics (LA) has been opening new opportunities to support learning in higher education (HE). LA dashboards are an important tool in providing students with insights into their learning progress, and predictions, leading to reflection and adaptation of learning plans and habits. Based on a human-centered approach, we present a perspective of students, as essential stakeholders, on LA dashboards. We describe a longitudinal study, based on survey methodology. The study included two iterations of a survey, conducted with second-year ICT students in 2017 (N = 222) and 2022 (N = 196). The study provided insights into the LA dashboard features the students find the most useful to support their learning. The students highly appreciated features related to short-term planning and organization of learning, while they were cautious about comparison and competition with other students, finding such features possibly demotivating. We compared the 2017 and 2022 results to establish possible changes in the students’ perspectives with the COVID-19 pandemic. The students’ awareness of the benefits of LA has increased, which may be related to the strong focus on online learning during the pandemic. Finally, a factor analysis yielded a dashboard model with five underlying factors: comparison, planning, predictions, extracurricular, and teachers.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {44–56},
numpages = {13},
keywords = {dashboard, higher education, human-centered, learning analytics, students’ perspectives},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576139,
author = {Marinho, Wemerson and Clua, Esteban Walter and Mart\'{\i}, Luis and Marinho, Karla},
title = {Predicting Item Response Theory Parameters Using Question Statements Texts},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576139},
doi = {10.1145/3576050.3576139},
abstract = {Recently, new Neural Language Models pre-trained on a massive corpus of texts are available. These models encode statistical features of the languages through their parameters, creating better word vector representations that allow the training of neural networks with smaller sample sets. In this context, we investigate the application of these models to predict Item Response Theory parameters in multiple choice questions. More specifically, we apply our models for the Brazilian National High School Exam (ENEM) questions using the text of their statements and propose a novel optimization target for regression: Item Characteristic Curve. The architecture employed could predict the difficulty parameter b of the ENEM 2020 and 2021 items with a mean absolute error of 70 points. Calculating the IRT score in each knowledge area of the exam for a sample of 100,000 students, we obtained a mean absolute below 40 points for all knowledge areas. Considering only the top quartile, the exam’s main target of interest, the average error was less than 30 points for all areas, being the majority lower than 15 points. Such performance allows predicting parameters on newly created questions, composing mock tests for student training, and analyzing their performance with excellent precision, dispensing with the need for costly item calibration pre-test step.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {1–10},
numpages = {10},
keywords = {ENEM, Item response theory, Question difficulty prediction, Text regression},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576122,
author = {Lee, Yoon and Specht, Marcus},
title = {Can We Empower Attentive E-reading with a Social Robot? An Introductory Study with a Novel Multimodal Dataset and Deep Learning Approaches},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576122},
doi = {10.1145/3576050.3576122},
abstract = {Reading on digital devices has become more commonplace, while it often poses challenges to learners’ attention. In this study, we hypothesized that allowing learners to reflect on their reading phases with an empathic social robot companion might enhance learners’ attention in e-reading. To verify our assumption, we collected a novel dataset (SKEP) in an e-reading setting with social robot support. It contains 25 multimodal features from various sensors and logged data that are direct and indirect cues of attention. Based on the SKEP dataset, we comprehensively compared the difference between HRI-based (treatment) and GUI-based (control) feedback and obtained insights for intervention design. Based on the human annotation of the nearly 40 hours of video data streams from 60 subjects, we developed a machine learning model to capture attention-regulation behaviors in e-reading. We exploited a two-stage framework to recognize learners’ observable self-regulatory behaviors and conducted attention analysis. The proposed system showed a promising performance with high prediction results of e-reading with HRI, such as 72.97% accuracy in recognizing attention regulation behaviors, 74.29% accuracy in predicting knowledge gain, 75.00% for perceived interaction experience, and 75.00% for perceived social presence. We believe our work can inspire the future design of HRI-based e-reading and its analysis.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {520–530},
numpages = {11},
keywords = {Attention Self-regulation, Deep Learning, E-reading, Human-Robot Interaction, Novel dataset},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576119,
author = {Li, Lin and Sha, Lele and Li, Yuheng and Rakovi\'{c}, Mladen and Rong, Jia and Joksimovic, Srecko and Selwyn, Neil and Ga\v{s}evi\'{c}, Dragan and Chen, Guanliang},
title = {Moral Machines or Tyranny of the Majority? A Systematic Review on Predictive Bias in Education},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576119},
doi = {10.1145/3576050.3576119},
abstract = {Machine Learning (ML) techniques have been increasingly adopted to support various activities in education, including being applied in important contexts such as college admission and scholarship allocation. In addition to being accurate, the application of these techniques has to be fair, i.e., displaying no discrimination towards any group of stakeholders in education (mainly students and instructors) based on their protective attributes (e.g., gender and age). The past few years have witnessed an explosion of attention given to the predictive bias of ML techniques in education. Though certain endeavors have been made to detect and alleviate predictive bias in learning analytics, it is still hard for newcomers to penetrate. To address this, we systematically reviewed existing studies on predictive bias in education, and a total of 49 peer-reviewed empirical papers published after 2010 were included in this study. In particular, these papers were reviewed and summarized from the following three perspectives: (i) protective attributes, (ii) fairness measures and their applications in various educational tasks, and (iii) strategies for enhancing predictive fairness. These findings were summarized into recommendations to guide future endeavors in this strand of research, e.g., collecting and sharing more quality data containing protective attributes, developing fairness-enhancing approaches which do not require the explicit use of protective attributes, validating the effectiveness of fairness-enhancing on students and instructors in real-world settings.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {499–508},
numpages = {10},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576117,
author = {Davalos, Eduardo and Vatral, Caleb and Cohn, Clayton and Horn Fonteles, Joyce and Biswas, Gautam and Mohammed, Naveeduddin and Lee, Madison and Levin, Daniel},
title = {Identifying Gaze Behavior Evolution via Temporal Fully-Weighted Scanpath Graphs},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576117},
doi = {10.1145/3576050.3576117},
abstract = {Eye-tracking technology has expanded our ability to quantitatively measure human perception. This rich data source has been widely used to characterize human behavior and cognition. However, eye-tracking analysis has been limited in its applicability, as contextualizing gaze to environmental artifacts is non-trivial. Moreover, the temporal evolution of gaze behavior through open-ended environments where learners are alternating between tasks often remains unclear. In this paper, we propose temporal fully-weighted scanpath graphs as a novel representation of gaze behavior and combine it with a clustering scheme to obtain high-level gaze summaries that can be mapped to cognitive tasks via network metrics and cluster mean graphs. In a case study with nurse simulation-based team training, our approach was able to explain changes in gaze behavior with respect to key events during the simulation. By identifying cognitive tasks via gaze behavior, learners’ strategies can be evaluated to create online performance metrics and personalized feedback.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {476–487},
numpages = {12},
keywords = {eye-tracking, learning analytics, network analysis, simulation-based training, temporal},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576116,
author = {Kim, Yunsung and Piech, Chris},
title = {The Student Zipf Theory: Inferring Latent Structures in Open-Ended Student Work To Help Educators},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576116},
doi = {10.1145/3576050.3576116},
abstract = {Are there structures underlying student work that are universal across every open-ended task? We demonstrate that, across many subjects and assignment types, the probability distribution underlying student-generated open-ended work is close to Zipf’s Law. Inferring this latent structure for classroom assignments can help learning analytics researchers, instruction designers, and educators understand the landscape of various student approaches, assess the complexity of assignments, and prioritise pedagogical attention. However, typical classrooms are way too small to witness even the contour of the Zipfian pattern, and it is generally impossible to perform inference for Zipf’s law from such small number of samples. We formalise this difficult task as the Zipf Inference Challenge: (1) Infer the ordering of student-generated works by their underlying probabilities, and (2) Estimate the shape parameter of the underlying distribution in a typical-sized classroom. Our key insight in addressing this challenge is to leverage the densities of the student response landscapes represented by semantic similarity. We show that our “Semantic Density Estimation” method is able to do a much better job at inferring the latent Zipf shape and the probability-ordering of student responses for real world education datasets.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {464–475},
numpages = {12},
keywords = {Constructed Response, Open-Ended Response, Probabilistic Modeling, Student Work, Zipf’s Law},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576115,
author = {Wong, Aaron Y. and Bryck, Richard L. and Baker, Ryan S. and Hutt, Stephen and Mills, Caitlin},
title = {Using a Webcam Based Eye-tracker to Understand Students’ Thought Patterns and Reading Behaviors in Neurodivergent Classrooms},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576115},
doi = {10.1145/3576050.3576115},
abstract = {Previous learning analytics efforts have attempted to leverage the link between students’ gaze behaviors and learning experiences to build effective real-time interventions. Historically, however, these technologies have not been scalable due to the high cost of eye-tracking devices. Further, such efforts have been almost exclusively focused on neurotypical students, despite recent work that suggests a “one size fits many” approach can disadvantage neurodivergent students. Here we attempt to address these limitations by examining the validity and applicability of using scalable, webcam-based eye tracking as a basis for adaptively responding to neurodivergent students in an educational setting. Forty-three neurodivergent students read a text and answered questions about their in-situ thought patterns while a webcam-based eye tracker assessed their gaze locations. Results indicate that eye-tracking measures were sensitive to: 1) moments when students experienced difficulty disengaging from their own thoughts and 2) students’ familiarity with the text. Our findings highlight the fact that a free, open-source, webcam-based eye-tracker can be used to assess differences in reading patterns and online thought patterns. We discuss the implications and possible applications of these results, including the idea that webcam-based eye tracking may be a viable solution for designing real-time interventions for neurodivergent student populations.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {453–463},
numpages = {11},
keywords = {Educational technology, Eye-Tracking, Neurodivergence, Webcam-based tracking},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576113,
author = {Moulder, Robert and Booth, Brandon and Abitino, Angelina and D'Mello, Sidney},
title = {Recurrence Quantification Analysis of Eye Gaze Dynamics During Team Collaboration},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576113},
doi = {10.1145/3576050.3576113},
abstract = {Shared visual attention between team members facilitates collaborative problem solving (CPS), but little is known about how team-level eye gaze dynamics influence the quality and successfulness of CPS. To better understand the role of shared visual attention during CPS, we collected eye gaze data from 279 individuals solving computer-based physics puzzles while in teams of three. We converted eye gaze into discrete screen locations and quantified team-level gaze dynamics using recurrence quantification analysis (RQA). Specifically, we used a centroid-based auto-RQA approach, a pairwise team member cross-RQAs approach, and a multi-dimensional RQA approach to quantify team-level eye gaze dynamics from the eye gaze data of team members. We find that teams differing in composition based on prior task knowledge, gender, and race show few differences in team-level eye gaze dynamics. We also find that RQA metrics of team-level eye gaze dynamics were predictive of task success (all ps &lt; .001). However, the same metrics showed different patterns of feature importance depending on predictive model and RQA type, suggesting some redundancy in task-relevant information. These findings signify that team-level eye gaze dynamics play an important role in CPS and that different forms of RQA pick up on unique aspects of shared attention between team-members.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {430–440},
numpages = {11},
keywords = {eye gaze dynamics, recurrence quantification analysis, shared attention, team collaboration, team dynamics},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576112,
author = {Iqbal, Sehrish and Rakovic, Mladen and Chen, Guanliang and Li, Tongguang and Ferreira Mello, Rafael and Fan, Yizhou and Fiorentino, Giuseppe and Radi Aljohani, Naif and Gasevic, Dragan},
title = {Towards Automated Analysis of Rhetorical Categories in Students Essay Writings using Bloom’s Taxonomy},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576112},
doi = {10.1145/3576050.3576112},
abstract = {Essay writing has become one of the most common learning tasks assigned to students enrolled in various courses at different educational levels, owing to the growing demand for future professionals to effectively communicate information to an audience and develop a written product (i.e. essay). Evaluating a written product requires scorers who manually examine the existence of rhetorical categories, which is a time-consuming task. Machine Learning (ML) approaches have the potential to alleviate this challenge. As a result, several attempts have been made in the literature to automate the identification of rhetorical categories using Rhetorical Structure Theory (RST). However, RST do not provide information regarding students’ cognitive level, which motivates the use of Bloom’s Taxonomy. Therefore, in this research we propose to: i) investigate the extent to which classification of rhetorical categories can be automated based on Bloom’s taxonomy by comparing the traditional ML classifiers with the pre-trained language model BERT, ii) explore the associations between rhetorical categories and writing performance. Our results showed that BERT model outperformed the traditional ML-based classifiers with 18% better accuracy, indicating it can be used in future analytics tool. Moreover, we found a statistical difference between the associations of rhetorical categories in low-achiever, medium-achiever and high-achiever groups which implies that rhetorical categories can be predictive of writing performance.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {418–429},
numpages = {12},
keywords = {Rhetorical structure, epistemic network analysis, essay analysis, machine learning},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576111,
author = {Ng, Jeremy T. D. and Liu, Yiming and Chui, Didier S. Y. and Man, Jack C. H. and Hu, Xiao},
title = {Leveraging LMS Logs to Analyze Self-Regulated Learning Behaviors in a Maker-based Course},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576111},
doi = {10.1145/3576050.3576111},
abstract = {Existing learning analytics (LA) studies on self-regulated learning (SRL) have rarely focused on maker education that emphasizes student autonomy in their learning process. Towards using LA methods for generating evidence of SRL in maker-based courses, this study leverages logs of a learning management system (LMS) with its activity design aligned with the maker-based pedagogy. We explored frequencies and sequential patterns of students’ SRL behaviors as reflected in the LMS logs and their relations with learning performance. Adopting a mixed method approach, we collected and triangulated both quantitative (i.e., system logs, performance scores) and qualitative (i.e., student-written reflections) data sources from 104 students. Based on current LA-based SRL research, we developed an LMS log-based analytic framework to define the SRL phases and behaviors applicable to maker activities. Statistical, data mining, and qualitative analysis methods were conducted on 48,602 logged events and 131 excerpts extracted from student reflections. Results reveal that high-performing students demonstrated some SRL behaviors (e.g., Making Personal Plans, Evaluation) more frequently than their low-performing counterparts, yet the two groups showcased fairly similar sequences of SRL behaviors. Theoretical, methodological and pedagogical implications are drawn for LA-based SRL research and maker education.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {670–676},
numpages = {7},
keywords = {Learning Management System, Maker education, Self-regulated learning},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576110,
author = {Lang, Charles and Davis, Laura},
title = {Learning Analytics and Stakeholder Inclusion: What do We Mean When We Say "Human-Centered"?},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576110},
doi = {10.1145/3576050.3576110},
abstract = {Given the growth in interest in human-centeredness within the learning analytics community - a workshop at LAK, a special issue in the Journal of Learning Analytics and multiple papers published on the topic - it seems an appropriate time to critically evaluate the popular design approach. Using a corpus of 165 publications that have substantial reference to both learning analytics and human-centeredness, the following paper delineates what is meant by "human-centered" and then discusses what the implications are for this approach. The conclusion reached through this analysis is that when authors refer to human-centeredness in learning analytics they are largely referring to stakeholder inclusion and the means by which this can be achieved (methodologically, politically and logistically). Furthermore, the justification for stakeholder inclusion is often coached in terms of its ability to develop more effective learning analytics applications along several dimensions (efficiency, efficacy, impact). With reference to human-centered design in other fields a discussion follows of the issues with such an approach and a prediction that LA will likely move toward a more neutral stance on stakeholder inclusion, as has occurred in both human-centered design and stakeholder engagement research in the past. A more stakeholder-neutral stance is defined as one in which stakeholder inclusion is one of many tools utilized in developing learning analytics applications.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {411–417},
numpages = {7},
keywords = {Human-centered design, co-design, participatory, user-centered design},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576109,
author = {Gurung, Ashish and Baral, Sami and Vanacore, Kirk P. and Mcreynolds, Andrew A. and Kreisberg, Hilary and Botelho, Anthony F. and Shaw, Stacy T. and Hefferna, Neil T.},
title = {Identification, Exploration, and Remediation: Can Teachers Predict Common Wrong Answers?},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576109},
doi = {10.1145/3576050.3576109},
abstract = {Prior work analyzing tutoring sessions provided evidence that highly effective tutors, through their interaction with students and their experience, can perceptively recognize incorrect processes or “bugs” when students incorrectly answer problems. Researchers have studied these tutoring interactions examining instructional approaches to address incorrect processes and observed that the format of the feedback can influence learning outcomes. In this work, we recognize the incorrect answers caused by these buggy processes as Common Wrong Answers (CWAs). We examine the ability of teachers and instructional designers to identify CWAs proactively. As teachers and instructional designers deeply understand the common approaches and mistakes students make when solving mathematical problems, we examine the feasibility of proactively identifying CWAs and generating Common Wrong Answer Feedback (CWAFs) as a formative feedback intervention for addressing student learning needs. As such, we analyze CWAFs in three sets of analyses. We first report on the accuracy of the CWAs predicted by the teachers and instructional designers on the problems across two activities. We then measure the effectiveness of the CWAFs using an intent-to-treat analysis. Finally, we explore the existence of personalization effects of the CWAFs for the students working on the two mathematics activities.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {399–410},
numpages = {12},
keywords = {Automated Feedback, Buggy Messages, Causal Inference, Common Wrong Answers},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576108,
author = {Murali, Rohit and Conati, Cristina and Azevedo, Roger},
title = {Predicting Co-occurring Emotions in MetaTutor when Combining Eye-Tracking and Interaction Data from Separate User Studies},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576108},
doi = {10.1145/3576050.3576108},
abstract = {Learning can be improved by providing personalized feedback adapting to the emotions that the learner may be experiencing. There is initial evidence that co-occurring emotions can be predicted during learning in Intelligent Tutoring Systems (ITS) through eye-tracking and interaction data. Predicting co-occurring emotions is a complex task and merging datasets has the potential to improve predictive performance. In this paper, we combine data from two user studies with an ITS, and analyze whether there is an improvement in predictive performance of co-occurring emotions, despite the user studies using different eye-trackers. In the pursuit towards developing real affect-aware ITS, we look at whether we can isolate classifiers that perform better than a baseline. In this regard we perform a series of statistical analyses and test out the predictive performance of standard machine learning models as well as an ensemble classifier for the task of predicting co-occurring emotions.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {388–398},
numpages = {11},
keywords = {Co-occurring emotions, eye-tracking, interaction},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576107,
author = {Aghaei, Kimia and Hatala, Marek and Mogharrab, Alireza},
title = {How Students’ Emotion and Motivation Changes After Viewing Dashboards with Varied Social Comparison Group: A Qualitative Study},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576107},
doi = {10.1145/3576050.3576107},
abstract = {The need to personalize learning analytics dashboards (LADs) is getting more recognized in learning analytics research community. In order to study the impact of these dashboards on learners, various types of prototypes have been designed and deployed in different settings. Applying Weiner’s attribution theory, our goal in this study was to understand the effect of dashboard information content on learners. We wanted to understand how elements of assignment grade, time spent on an assignment, assignment view, and proficiency in the dashboard affect students’ attribution of achievement and motivation for future work. We designed a qualitative study in which we analyzed participants’ responses and indicated behavioural changes after viewing the dashboard. Through in-depth interviews, we aimed to understand students’ interpretations of the designed dashboard, and to what extent social comparison impacts their judgments of learning. Students used multiple dimensions to attribute their success or failure to their ability and effort. Our results indicate that to maximize the benefits of dashboards as a vehicle for motivating change in students learning, the dashboard should promote effort in both personal and social comparison capacities.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {663–669},
numpages = {7},
keywords = {attribution theory, learning analytics dashboard, motivation, qualitative analysis, social comparison},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576106,
author = {Cloude, Elizabeth B. and Baker, Ryan S. and Fouh, Eric},
title = {Online help-seeking occurring in multiple computer-mediated conversations affects grades in an introductory programming course},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576106},
doi = {10.1145/3576050.3576106},
abstract = {Computing education researchers often study the impact of online help-seeking behaviors that occur across multiple online resources in isolation. Such separation fails to capture the interconnected nature of online help-seeking behaviors that occur across multiple online resources and its affect on course grades. This is particularly important for programming education, which arguably has more online resources to seek help from other people (e.g., computer-mediated conversations) than other majors. Using data from an introductory programming course (CS1) at a large US university, we found that students (n=301) sought help in multiple computer-mediated conversations, both Q&amp;A forum and online office hours (OHQ), differently. Results showed the more prior knowledge about programming students had, the more they sought help in the Q&amp;A compared to students with less prior knowledge. In general, higher-performing students sought help online in the Q&amp;A more than the lower-performing groups on all the homework assignments, but not for the OHQ. By better understanding how students seek help online across multiple modalities of computer-mediated conversations and the relationship between help-seeking and grades, we can re-design online resources that best support all students in introductory programming courses at scale.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {378–387},
numpages = {10},
keywords = {CS1, Learning analytics at scale, Online help-seeking, Programming},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576105,
author = {Almoubayyed, Husni and Fancsali, Stephen E. and Ritter, Steve},
title = {Instruction-Embedded Assessment for Reading Ability in Adaptive Mathematics Software},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576105},
doi = {10.1145/3576050.3576105},
abstract = {Adaptive educational software is likely to better support broader and more diverse sets of learners by considering more comprehensive views (or models) of such learners. For example, recent work proposed making inferences about “non-math” factors like reading comprehension while students used adaptive software for mathematics to better support and adapt to learners. We build on this proposed approach to more comprehensive learning modeling by providing an empirical basis for making inferences about students’ reading ability from their performance on activities in adaptive software for mathematics. We lay out an approach to predicting middle school students’ reading ability using their performance on activities within Carnegie Learning’s MATHia, a widely used intelligent tutoring system for mathematics. We focus on how performance in an early, introductory activity as an especially powerful place to consider instruction-embedded assessment of non-math factors like reading comprehension to guide adaptation based on factors like reading ability. We close by discussing opportunities to extend this work by focusing on particular knowledge components or skills tracked by MATHia that may provide important “levers” for driving adaptation based on students’ reading ability while they learn and practice mathematics.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {366–377},
numpages = {12},
keywords = {assessments, intelligent tutoring systems, machine learning, predictive modeling},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576104,
author = {Zhang, Fan and Xing, Wanli and Li, Chenglu},
title = {Predicting Students’ Algebra I Performance using Reinforcement Learning with Multi-Group Fairness},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576104},
doi = {10.1145/3576050.3576104},
abstract = {Numerous studies have successfully adopted learning analytics techniques such as machine learning (ML) to address educational issues. However, limited research has addressed the problem of algorithmic bias in ML. In the few attempts to develop strategies to concretely mitigate algorithmic bias in education, the focus has been on debiasing ML models with single group membership. This study aimed to propose an algorithmic strategy to mitigate bias in a multi-group context. The results showed that our proposed model could effectively reduce algorithmic bias in a multi-group setting while retaining competitive accuracy. The findings implied that there could be a paradigm shift from focusing on debiasing a single group to multiple groups in educational attempts on ML.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {657–662},
numpages = {6},
keywords = {fair AI, math achievement prediction, multi-group fairness, reinforcement learning},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576103,
author = {Van Campenhout, Rachel and Jerome, Bill and Dittel, Jeffrey S. and Johnson, Benny G.},
title = {The Doer Effect at Scale: Investigating Correlation and Causation Across Seven Courses},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576103},
doi = {10.1145/3576050.3576103},
abstract = {The future of digital learning should be focused on methods proven to be effective by learning science and learning analytics. One such method is learning by doing—combining formative practice with expository content so students actively engage with their learning resource. This generates the doer effect: the principle that students who do practice while they read have higher outcomes than those who only read [9]. Research on the doer effect has shown it to be causal to learning [10], and these causal findings have previously been replicated in a single course [19]. This study extends the replication of the doer effect by analyzing 15.2 million data events from 18,546 students in seven courses at an online higher education institution, the most students and courses known to date. Furthermore, we analyze each course five ways by using different outcomes, accounting for prior knowledge, and doing both correlational and causal analyses. By performing the doer effect analyses five ways on seven courses, new insights are gained on how this method of learning analytics can contribute to our interpretation of this learning science principle. Practical implications of the doer effect for students are discussed, and future research goals are established.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {357–365},
numpages = {9},
keywords = {Doer effect, causal discovery, course effectiveness, courseware, external validity, learn by doing, learning outcomes, replication},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576101,
author = {Emerson, Andrew and Min, Wookhee and Rowe, Jonathan and Azevedo, Roger and Lester, James},
title = {Multimodal Predictive Student Modeling with Multi-Task Transfer Learning},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576101},
doi = {10.1145/3576050.3576101},
abstract = {Game-based learning environments have the distinctive capacity to promote learning experiences that are both engaging and effective. Recent advances in sensor-based technologies (e.g., facial expression analysis and eye gaze tracking) and natural language processing have introduced the opportunity to leverage multimodal data streams for learning analytics. Learning analytics and student modeling informed by multimodal data captured during students’ interactions with game-based learning environments hold significant promise for designing effective learning environments that detect unproductive student behaviors and provide adaptive support for students during learning. Learning analytics frameworks that can accurately predict student learning outcomes early in students’ interactions hold considerable promise for enabling environments to dynamically adapt to individual student needs. In this paper, we investigate a multimodal, multi-task predictive student modeling framework for game-based learning environments. The framework is evaluated on two datasets of game-based learning interactions from two student populations (n=61 and n=118) who interacted with two versions of a game-based learning environment for microbiology education. The framework leverages available multimodal data channels from the datasets to simultaneously predict student post-test performance and interest. In addition to inducing models for each dataset individually, this work investigates the ability to use information learned from one source dataset to improve models based on another target dataset (i.e., transfer learning using pre-trained models). Results from a series of ablation experiments indicate the differences in predictive capacity among a combination of modalities including gameplay, eye gaze, facial expressions, and reflection text for predicting the two target variables. In addition, multi-task models were able to improve predictive performance compared to single-task baselines for one target variable, but not both. Lastly, transfer learning showed promise in improving predictive capacity in both datasets.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {333–344},
numpages = {12},
keywords = {Game-Based Learning, Multimodal Learning Analytics, Predictive Student Modeling},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576100,
author = {Hutchins, Nicole and Biswas, Gautam},
title = {Using Teacher Dashboards to Customize Lesson Plans for a Problem-Based, Middle School STEM Curriculum},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576100},
doi = {10.1145/3576050.3576100},
abstract = {Keeping K-12 teachers engaged during students’ learning and problem solving in technology-enhanced, integrated problem-based learning (PBL) has been shown to support deeper student involvement, and, therefore, better success learning difficult science, computing, and engineering concepts and practices. However, students’ learning processes and corresponding difficulties are not easily noticed by teachers as students learn from these environments as processes are captured through mouse clicks, drag and drop actions, and other low-level activities. As such, teachers find it difficult to set up meaningful interactions with students while also maintaining the focus on student-centered learning. Little research has examined dashboard-supported responsive teaching practices for K-12 PBL. This study examined 8 teachers as they used a co-designed teacher dashboard to assess and respond to students’ learning and strategies during an integrated, PBL STEM curriculum. Teachers completed a series of 5 “planning period simulations” leveraging the dashboard and think-aloud protocols were implemented, supported by semi-structured interview questions, to enable the teachers to verbalize their thought and evaluation processes. Content analysis and epistemic network analysis were conducted to analyze the simulations. Understanding how teachers use dashboards to support evidence-based teaching practices during technology-enhanced curricula is critical for improving teacher support and preparation.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {324–332},
numpages = {9},
keywords = {co-design, computational modeling, responsive teaching, teacher dashboards},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576099,
author = {Rajarathinam, Robin Jephthah and D'Angelo, Cynthia M.},
title = {Turn-taking analysis of small group collaboration in an engineering discussion classroom},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576099},
doi = {10.1145/3576050.3576099},
abstract = {This preliminary study focuses on using voice activity detection (VAD) algorithms to extract turn information of small group work detected from recorded individual audio stream data from undergraduate engineering discussion sections. Video data along with audio were manually coded for collaborative behavior of students and teacher-student interaction. We found that individual audio data can be used to obtain features that can describe group work in noisy classrooms. We observed patterns in student turn taking and talk duration during various sections of the classroom which matched with the video coded data. Results show that high quality individual audio data can be effective in describing collaborative processes that occurs in the classroom. Future directions on using prosodic features and implications on how we can conceptualize collaborative group work using audio data are discussed.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {650–656},
numpages = {7},
keywords = {audio analysis, collaborative problem solving, discussion patterns, voice activity detection},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576098,
author = {Morris, Wesley and Crossley, Scott and Holmes, Langdon and Trumbore, Anne},
title = {Using Transformer Language Models to Validate Peer-Assigned Essay Scores in Massive Open Online Courses (MOOCs)},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576098},
doi = {10.1145/3576050.3576098},
abstract = {Massive Open Online Courses (MOOCs) such as those offered by Coursera are popular ways for adults to gain important skills, advance their careers, and pursue their interests. Within these courses, students are often required to compose, submit, and peer review written essays, providing a valuable pedagogical experience for the student and a wealth of natural language data for the educational researcher. However, the scores provided by peers do not always reflect the actual quality of the text, generating questions about the reliability and validity of the scores. This study evaluates methods to increase the reliability of MOOC peer-review ratings through a series of validation tests on peer-reviewed essays. Reliability of reviewers was based on correlations between text length and essay quality. Raters were pruned based on score variance and the lexical diversity observed in their comments to create sub-sets of raters. Each subset was then used as training data to finetune distilBERT large language models to automatically score essay quality as a measure of validation. The accuracy of each language model for each subset was evaluated. We find that training language models on data subsets produced by more reliable raters based on a combination of score variance and lexical diversity produce more accurate essay scoring models. The approach developed in this study should allow for enhanced reliability of peer-reviewed scoring in MOOCS affording greater credibility within the systems.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {315–323},
numpages = {9},
keywords = {moocs, natural language processing, rater reliability, transformers},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576097,
author = {Barboza, Luiz and Mello, Rafael and Modell, Micah and Teixeira, Erico Souza},
title = {Blockly-DS: Blocks Programming for Data Science with Visual, Statistical, Descriptive and Predictive Analysis},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576097},
doi = {10.1145/3576050.3576097},
abstract = {Interest in data science has been growing across industries - both STEM and non-STEM. Non-STEM students often have difficulties with programming and data analysis tools. These entry barriers can be minimized, and these concepts can be easily absorbed when using visual tools. Thus, for this specific audience, the use of visual tools has been essential for teaching data science. Several of these tools are available, but they all have limitations. This work presents Blockly-DS: a new tool capable of assisting in teaching data science to a non-STEM audience. The Blockly-DS tool is being tested in two Brazilian higher education institutions, one, IBMEC, a business undergraduate university, and the other, FIAP, a STEM school that offers an MBA as well as corporate and undergraduate courses. The preliminary results presented in this article refers to a validation with two groups of training sessions for junior financial analysts of a major Brazilian bank in partnership with FIAP.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {644–649},
numpages = {6},
keywords = {block-based programming, data science, visual programming tool},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576096,
author = {Kitto, Kirsty and Manly, Catherine A. and Ferguson, Rebecca and Poquet, Oleksandra},
title = {Towards more replicable content analysis for learning analytics},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576096},
doi = {10.1145/3576050.3576096},
abstract = {Content analysis (CA) is a method frequently used in the learning sciences and so increasingly applied in learning analytics (LA). Despite this ubiquity, CA is a subtle method, with many complexities and decision points affecting the outcomes it generates. Although appearing to be a neutral quantitative approach, coding CA constructs requires an attention to decision making and context that aligns it with a more subjective, qualitative interpretation of data. Despite these challenges, we increasingly see the labels in CA-derived datasets used as training sets for machine learning (ML) methods in LA. However, the scarcity of widely shareable datasets means research groups usually work independently to generate labelled data, with few attempts made to compare practice and results across groups. A risk is emerging that different groups are coding constructs in different ways, leading to results that will not prove replicable. We report on two replication studies using a previously reported construct. A failure to achieve high inter-rater reliability suggests that coding of this scheme is not currently replicable across different research groups. We point to potential dangers in this result for those who would use ML to automate the detection of various educationally relevant constructs in LA.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {303–314},
numpages = {12},
keywords = {content analysis, labelled data, methodology, reproducibility},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576095,
author = {Nazeri, Sina and Hatala, Marek and Salehian Kia, Fatemeh},
title = {When to Intervene? Utilizing Two Facets of Temporality in Students’ SRL Processes in a Programming Course},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576095},
doi = {10.1145/3576050.3576095},
abstract = {This study explored two aspects of temporality in students’ SRL behaviours to understand the dynamics of SRL phase transitions. In the first aspect, which refers to the temporal order of Self-regulated learning (SRL) phases, we characterized four types of SRL processes based on phase transitions and the cyclical nature of SRL. The SRL types were mapped into the kinds of iterative behaviours over SRL phases which correspond to the theorized self-regulatory behaviours of students at different levels of SRL skills. We found a significant association between SRL types and the assignment grades that suggests the higher achieved learning outcomes, i.e., programming skills demonstrated in the assignments, being associated with more advanced SRL processes. This study also focused on the second aspect of temporality, which refers to the instance of time. We revealed the temporal dynamics between SRL phase transitions by analyzing time profiles for transitions in each SRL process type. Next, we showed that a two-day interval is a threshold by which most students iteratively transition from adapting to enactment phases, which provides a suitable time to intervene if the transition is not observed.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {293–302},
numpages = {10},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576094,
author = {Liu, Qianhui and Paquette, Luc},
title = {Using submission log data to investigate novice programmers’ employment of debugging strategies},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576094},
doi = {10.1145/3576050.3576094},
abstract = {Debugging is a distinct subject in programming that is both comprehensive and challenging for novice programmers. However, instructors have limited opportunities to gain insights into the difficulties students encountered in isolated debugging processes. While qualitative studies have identified debugging strategies that novice programmers use and how they relate to theoretical debugging frameworks, limited larger scale quantitative analyses have been conducted to investigate how students’ debugging behaviors observed in log data align with the identified strategies and how they relate to successful debugging. In this study, we used submission log data to understand how the existing debugging strategies are employed by students in an introductory CS course when solving homework problems. We identified strategies from existing debugging literature that can be observed with trace data and extracted features to reveal how efficient debugging is associated with debugging strategy usage. Our findings both align with and contradict past assumptions from previous studies by suggesting that minor code edition can be a beneficial strategy and that width and depth aggregations of the same debugging behavior can reveal opposite effects on debugging efficiency.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {637–643},
numpages = {7},
keywords = {Computer Science education, Debugging strategies, Submission log data},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576093,
author = {Lewis, Armanda and Ochoa, Xavier and Qamra, Rohini},
title = {Instructor-in-the-Loop Exploratory Analytics to Support Group Work},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576093},
doi = {10.1145/3576050.3576093},
abstract = {This case study examines an interactive, low barrier process, termed instructor-in-the-loop, by which an instructor defines and makes meaning from exploratory metrics and visualizations, and uses this multimodal information to improve a course iteratively. We present potentials for course improvement based on automated learning analytics insights related to students’ participation in small active learning sessions associated with a large lecture course. Automated analytics processes are essential for larger courses where engaging smaller groups is important to ensure participation and understanding, but monitoring a large total number of groups throughout an instructional experience becomes untenable for the instructor. Of interest is providing instructors with easy-to-digest summaries of group performance that do not require complex set up and knowledge of more advanced algorithmic approaches. We explore synthesizing metrics and visualizations as ways to engage instructors in meaning making of complex learning environments, but in a low barrier manner that provides insights quickly.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {284–292},
numpages = {9},
keywords = {automated detection, group work, multimodal learning analytics},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576092,
author = {Quick, Joshua Dallas and Motz, Benjamin and Morrone, Anastasia},
title = {Lost in Translation: Determining the Generalizability of Temporal Models across Course Contexts&nbsp;},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576092},
doi = {10.1145/3576050.3576092},
abstract = {A common activity in learning analytics research is to demonstrate a new analytical technique by applying it to data from a single course.&nbsp; We explore whether the value of an analytical approach might generalize across course contexts.&nbsp; Accordingly, we conduct a conceptual replication of a well-cited temporal modeling study using self-regulated learning (SRL) taxonomies. We attempt to conceptually replicate this previous work through the analysis of 411 students across 19 courses’ trace event data. Using established SRL categorizations, learner actions are sequenced to identify regular clusters of interaction through hierarchical clustering methods. These clusters are then compared with the entire data corpus and each other through the development of first-order Markov models to develop process maps. Our findings indicate that, although some general patterns of SRL can generalize, these results are more limited at higher scales. Comparing these clusters of interaction along students’ performance in courses also indicates some relationships between activity and outcomes, though this finding is also limited in relation to the complexity introduced by scaling out these methods. We discuss how these temporal models should be viewed when making descriptive and qualitative inferences about students’ activity in digital learning environments.&nbsp;&nbsp;},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {273–283},
numpages = {11},
keywords = {Generalizability, Process Modeling, Replication, Self-regulated Learning, Trace Data},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576091,
author = {Kong, ByeongJo and Hemberg, Erik and Bell, Ana and O'Reilly, Una-May},
title = {Investigating Student's Problem-solving Approaches in MOOCs using Natural Language Processing},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576091},
doi = {10.1145/3576050.3576091},
abstract = {Problem-solving approaches are an essential part of learning. Knowing how students approach solving problems can help instructors improve their instructional designs and effectively guide the learning process of students. We propose a natural language processing (NLP) driven method to capture online learners’ problem-solving approaches at scale while using Massive Open Online Courses (MOOCs) as a learning platform. We employ an online survey to gather data, NLP techniques, and existing educational theories to investigate this in the lens of both computer science and education. The paper shows how NLP techniques, i.e. preprocessing, topic modeling, and text summarization, must be tuned to extract information from a large-scale text corpus. The proposed method discovered 18 problem-solving approaches from the text data, such as using pen and paper, peer learning, trial and error, etc. We also observed topics that appear over the years, such as clarifying code logic, watching videos, etc. We observed that students heavily rely on "tools" for solving programming problems and can expect that such selection of methods can vary depending on the type of task.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {262–272},
numpages = {11},
keywords = {MOOCs, problem-solving methods, text summarization, topic modeling},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576090,
author = {Hur, Paul and Machaka, Nessrine and Krist, Christina and Bosch, Nigel},
title = {Informing Expert Feature Engineering through Automated Approaches: Implications for Coding Qualitative Classroom Video Data},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576090},
doi = {10.1145/3576050.3576090},
abstract = {While classroom video data are detailed sources for mining student learning insights, their complex and unstructured nature makes them less than straightforward for researchers to analyze. In this paper, we compared the differences between the processes of expert-informed manual feature engineering and automated feature engineering using positional data for predicting student group interaction in four middle school and high school mathematics classroom videos. Our results highlighted notable differences, including improved model accuracy for the combined (manual features + automated features) models compared to the only-manual-features models (mean AUC = .778 vs. .706) at the cost of feature interpretability, increased number of features for automated feature engineering (1523 vs. 178), and engineering approach (domain-agnostic in automated vs. domain-knowledge-informed in manual). We carried out feature importance analyses and discuss the implications of the results for potentially augmenting human perspectives about qualitatively coding classroom video data by confirming and expanding views on which body areas and characteristics may be relevant to the target interaction behavior. Lastly, we discuss our study’s limitations and future work.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {630–636},
numpages = {7},
keywords = {classroom video data, expert-informed feature engineering, student positional data},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576089,
author = {Thomas, Danielle and Yang, Xinyu and Gupta, Shivang and Adeniran, Adetunji and Mclaughlin, Elizabeth and Koedinger, Kenneth},
title = {When the Tutor Becomes the Student: Design and Evaluation of Efficient Scenario-based Lessons for Tutors},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576089},
doi = {10.1145/3576050.3576089},
abstract = {Tutoring is among the most impactful educational influences on student achievement, with perhaps the greatest promise of combating student learning loss. Due to its high impact, organizations are rapidly developing tutoring programs and discovering a common problem- a shortage of qualified, experienced tutors. This mixed methods investigation focuses on the impact of short (∼15 min.), online lessons in which tutors participate&nbsp;in situational judgment tests based on everyday tutoring scenarios. We developed three lessons on strategies for supporting student self-efficacy and motivation and tested them with 80 tutors from a national, online tutoring organization. Using a mixed-effects logistic regression model, we found a statistically significant learning effect indicating tutors performed about 20% higher post-instruction than pre-instruction (β = 0.811, p &lt; 0.01). Tutors scored ∼30% better on selected compared to constructed responses at posttest with evidence that tutors are learning from selected-response questions alone. Learning analytics and qualitative feedback suggest future design modifications for larger scale deployment, such as creating more authentically challenging selected-response options, capturing common misconceptions using learnersourced data, and varying modalities of scenario delivery with the aim of maintaining learning gains while reducing time and effort for tutor participants and trainers.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {250–261},
numpages = {12},
keywords = {Design-based research, Learnersourcing, Scenario-based learning, Tutoring},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576088,
author = {Berland, Matthew and Kumar, Vishesh},
title = {Joint Choice Time: A Metric for Better Understanding Collaboration in Interactive Museum Exhibits},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576088},
doi = {10.1145/3576050.3576088},
abstract = {In this paper, we propose a new metric – Joint Choice Time (JCT) – to measure how and when visitors are collaborating around an interactive museum exhibit. This extends dwell time, one of the most commonly used metrics for museum engagement – which tends to be individual, and sacrifices insight into activity and learning details for measurement simplicity. We provide an exemplar of measuring JCT using a common “diversity metric” for collaborative choices and potential outcomes. We provide an implementable description of the metric, results from using the metric with our own data, and potential implications for designing museum exhibits and easily measuring social engagement. Here, we apply JCT to an interactive exhibit game called “Rainbow Agents” where museum visitors can play independently or work together to tend to a virtual garden using computer science concepts. Our data showed that diversity of meaningful choices positively correlated with both dwell time and diversity of positive and creative outcomes. JCT - as a productive as well as easy to access measure of social work - provides an example for learning analytics practitioners and researchers (especially in museums) to consider centering social engagement and work as a rich space for easily assessing effective learning experiences for museum visitors.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {626–629},
numpages = {4},
keywords = {Collaboration, Engagement, Games, Museums, Play, Social participation},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576087,
author = {Dood, Amber and Das, Kapotaksha and Qian, Zhen and Finkenstaedt-Quinn, Solaire and Gere, Anne and Shultz, Ginger},
title = {A Dashboard to Provide Instructors with Automated Feedback on Students’ Peer Review Comments},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576087},
doi = {10.1145/3576050.3576087},
abstract = {Writing-to-Learn (WTL) is an evidence-based instructional practice which can help students construct knowledge across many disciplines. Though it is known to be an effective practice, many instructors do not implement WTL in their courses due to time constraints and inability to provide students with personalized feedback. One way to address this is to include peer review, which allows students to receive feedback on their writing and benefits them as they act as reviewers. To further ease the implementation of peer review and provide instructors with feedback on their students’ work, we labeled students’ peer review comments across courses for type of feedback provided and trained a machine learning model to automatically classify those comments, improving upon models reported in prior work. We then created a dashboard which takes students’ comments, labels the comments using the model, and allows instructors to filter through their students’ comments based on how the model labels the comments. This dashboard can be used by instructors to monitor the peer review collaborations occurring in their courses. The dashboard will allow them to efficiently use information provided by peers to identify common issues in their students’ writing and better evaluate the quality of their students’ peer review.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {619–625},
numpages = {7},
keywords = {Writing-to-Learn, automated feedback, instructor dashboards, machine learning, peer review},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576086,
author = {Russell, Jae-Eun and Smith, Anna Marie and George, Salim and Damman, Bryce},
title = {Instructional Strategies and Student eTextbook Reading},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576086},
doi = {10.1145/3576050.3576086},
abstract = {Students’ reading is an essential part of learning in college courses. However, many instructors are concerned that students do not complete assigned readings, and multiple studies have found evidence to support this concern. A handful of studies suggest adopting strategies to address students’ lack of reading. This research examines various instructional strategies and student eTextbook reading behaviors validated by page view data. Survey responses related to use of instructional strategies were collected. A total of 86 instructors from four public universities participated. Of these participants, 59 submitted the assigned reading pages for their courses. This resulted in reading data from 3,714 students which were examined in this study. The findings indicated that students read about 37% of the assigned pages on any given day during the semester. Also, of the students that read, two-thirds made at least one annotation and students tend to re-read the pages they annotated. Most importantly, student reading in the courses where strategies were used was almost three times higher than in the courses where no strategies were implemented.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {613–618},
numpages = {6},
keywords = {eTextbooks, education, learning analytics, reading},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576085,
author = {Bond, Melissa and Viberg, Olga and Bergdahl, Nina},
title = {The current state of using learning analytics to measure and support K-12 student engagement: A scoping review},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576085},
doi = {10.1145/3576050.3576085},
abstract = {Student engagement has been identified as a critical construct for understanding and predicting educational success. However, research has shown that it can be hard to align data-driven insights of engagement with observed and self-reported levels of engagement. Given the emergence and increasing application of learning analytics (LA) within K-12 education, further research is needed to understand how engagement is being conceptualized and measured within LA research. This scoping review identifies and synthesizes literature published between 2011-2022, focused on LA and student engagement in K-12 contexts, and indexed in five international databases. 27 articles and conference papers from 13 different countries were included for review. We found that most of the research was undertaken in middle school years within STEM subjects. The results show that there is a wide discrepancy in researchers’ understanding and operationalization of engagement and little evidence to suggest that LA improves learning outcomes and support. However, the potential to do so remains strong. Guidance is provided for future LA engagement research to better align with these goals.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {240–249},
numpages = {10},
keywords = {K-12, Learning Analytics, Scoping Review, Student Engagement},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576084,
author = {Caruso, Megan and D'Mello, Sidney},
title = {Do Associations Between Mind Wandering and Learning from Complex Texts Vary by Assessment Depth and Time?},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576084},
doi = {10.1145/3576050.3576084},
abstract = {We examined associations between mind wandering – where attention shifts from the task at hand to task-unrelated thoughts – and learning outcomes. Our data consisted of 177 students who self-reported mind wandering while reading five long, connected texts on scientific research methods and completed learning assessments targeting multiple depths of processing (rote, inference, integration) at different timescales (during and after reading each text, after reading all texts, and after a week-long delay). We found that mind wandering negatively predicted measures of factual, text-based (explicit) information and global integration of information across multiple parts of the text, but not measures requiring a local inference on a single sentence. Further, mind wandering only predicted comprehension measures assessed during the reading session and not after a week-long delay. Our findings provide important nuances to the established negative link between mind wandering and learning outcomes, which has predominantly focused on rote comprehension assessed during the learning session itself. Implications for interventions to address mind wandering during learning are discussed.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {230–239},
numpages = {10},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576083,
author = {Barthakur, Abhinava and Dawson, Shane and Kovanovic, Vitomir},
title = {Advancing leaner profiles with learning analytics: A scoping review of current trends and challenges},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576083},
doi = {10.1145/3576050.3576083},
abstract = {The term Learner Profile has proliferated over the years, and more recently, with the increased advocacy around personalising learning experiences. Learner profiles are at the center of personalised learning, and the characterisation of diversity in classrooms is made possible by profiling learners based on their strengths and weaknesses, backgrounds and other factors influencing learning. In this paper, we discuss three common approaches of profiling learners based on students’ cognitive knowledge, skills and competencies and behavioral patterns, all latter commonly used within Learning Analytics (LA). Although each approach has its strengths and merits, there are also several disadvantages that have impeded adoption at scale. We propose that the broader adoption of learner profiles can benefit from careful combination of the methods and practices of three primary approaches, allowing for scalable implementation of learner profiles across educational systems. In this regard, LA can leverage from other aligned domains to develop valid and rigorous measures of students' learning and propel learner profiles from education research to more mainstream educational practice. LA could provide the scope for monitoring and reporting beyond an individualised context and allow holistic evaluations of progress. There is promise in LA research to leverage the growing momentum surrounding learner profiles and make a substantial impact on the field's core aim - understanding and optimising learning as it occurs.&nbsp;},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {606–612},
numpages = {7},
keywords = {Competency-based profiling, Learner profile, Learning analytics-based profiling, Open learner models, Personalised learning},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576082,
author = {Song, Yukyeong and Xing, Wanli and Tian, Xiaoyi and Li, Chenglu},
title = {Are We on the Same Page? Modeling Linguistic Synchrony and Math Literacy in Mathematical Discussions},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576082},
doi = {10.1145/3576050.3576082},
abstract = {Mathematical discussions have become a popular educational strategy to promote math literacy. While some studies have associated math literacy with linguistic factors such as verbal ability and phonological skills, no studies have examined the relationship between linguistic synchrony and math literacy. In this study, we modeled linguistic synchrony and students’ math literacy from 20,776 online mathematical discussion threads between students and facilitators. We conducted Cross-Recurrence Quantification Analysis (CRQA) to calculate linguistic synchrony within each thread. The statistical testing result comparing CRQA indices between high and low math literacy groups shows that students with high math literacy have a significantly higher Recurrence Rate (RR), Number of Recurrence Lines (NRLINE), and the average Length of lines (L), but lower Determinism (DET) and normalized Entropy (rENTR). This result implies that students with high math literacy are more likely to share common words with facilitators, but they would paraphrase them. On the other hand, students with low math literacy tend to repeat the exact same phrases from the facilitators. The findings provide a better understanding of mathematical discussions and can potentially guide teachers in promoting effective mathematical discussions.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {599–605},
numpages = {7},
keywords = {cross-recurrence quantification analysis, linguistic synchrony, math literacy, mathematical discussion},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576081,
author = {Borchers, Conrad and Pardos, Zachary A.},
title = {Insights into undergraduate pathways using course load analytics},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576081},
doi = {10.1145/3576050.3576081},
abstract = {Course load analytics (CLA) inferred from LMS and enrollment features can offer a more accurate representation of course workload to students than credit hours and potentially aid in their course selection decisions. In this study, we produce and evaluate the first machine-learned predictions of student course load ratings and generalize our model to the full 10,000 course catalog of a large public university. We then retrospectively analyze longitudinal differences in the semester load of student course selections throughout their degree. CLA by semester shows that a student’s first semester at the university is among their highest load semesters, as opposed to a credit hour-based analysis, which would indicate it is among their lowest. Investigating what role predicted course load may play in program retention, we find that students who maintain a semester load that is low as measured by credit hours but high as measured by CLA are more likely to leave their program of study. This discrepancy in course load is particularly pertinent in STEM and associated with high prerequisite courses. Our findings have implications for academic advising, institutional handling of the freshman experience, and student-facing analytics to help students better plan, anticipate, and prepare for their selected courses.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {219–229},
numpages = {11},
keywords = {course load analytics, higher education, on-time graduation, stop-out, workload},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576079,
author = {Fahid, Fahmid Morshed and Lee, Seung and Mott, Bradford and Vandenberg, Jessica and Acosta, Halim and Brush, Thomas and Glazewski, Krista and Hmelo-Silver, Cindy and Lester, James},
title = {Effects of Modalities in Detecting Behavioral Engagement in Collaborative Game-Based Learning},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576079},
doi = {10.1145/3576050.3576079},
abstract = {Collaborative game-based learning environments have significant potential for creating effective and engaging group learning experiences. These environments offer rich interactions between small groups of students by embedding collaborative problem solving within immersive virtual worlds. Students often share information, ask questions, negotiate, and construct explanations between themselves towards solving a common goal. However, students sometimes disengage from the learning activities, and due to the nature of collaboration, their disengagement can propagate and negatively impact others within the group. From a teacher's perspective, it can be challenging to identify disengaged students within different groups in a classroom as they need to spend a significant amount of time orchestrating the classroom. Prior work has explored automated frameworks for identifying behavioral disengagement. However, most prior work relies on a single modality for identifying disengagement. In this work, we investigate the effects of using multiple modalities to detect disengagement behaviors of students in a collaborative game-based learning environment. For that, we utilized facial video recordings and group chat messages of 26 middle school students while they were interacting with Crystal Island: EcoJourneys, a game-based learning environment for ecosystem science. Our study shows that the predictive accuracy of a unimodal model heavily relies on the modality of the ground truth, whereas multimodal models surpass the unimodal models, trading resources for accuracy. Our findings can benefit future researchers in designing behavioral engagement detection frameworks for assisting teachers in using collaborative game-based learning within their classrooms.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {208–218},
numpages = {11},
keywords = {Behavioral engagement, Collaborative game-based learning, K-12 education, Multimodal learning analytics},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576078,
author = {Biedermann, Daniel and Schneider, Jan and Ciordas-Hertel, George-Petru and Eichmann, Beate and Hahnel, Carolin and Goldhammer, Frank and Drachsler, Hendrik},
title = {Detecting the Disengaged Reader - Using Scrolling Data to Predict Disengagement during Reading},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576078},
doi = {10.1145/3576050.3576078},
abstract = {When reading long and complex texts, students may disengage and miss out on relevant content. In order to prevent disengaged behavior or to counteract it by means of an intervention, it is ideally detected an early stage. In this paper, we present a method for early disengagement detection that relies only on the classification of scrolling data. The presented method transforms scrolling data into a time series representation, where each point of the series represents the vertical position of the viewport in the text document. This time series representation is then classified using time series classification algorithms. We evaluated the method on a dataset of 565 university students reading eight different texts. We compared the algorithm performance with different time series lengths, data sampling strategies, the texts that make up the training data, and classification algorithms. The method can classify disengagement early with up to 70% accuracy. However, we also observe differences in the performance depending on which of the texts are included in the training dataset. We discuss our results and propose several possible improvements to enhance the method.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {585–591},
numpages = {7},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576077,
author = {Poquet, Oleksandra and Jovanovic, Jelena and Pardo, Abelardo},
title = {Student Profiles of Change in a University Course: A Complex Dynamical Systems Perspective},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576077},
doi = {10.1145/3576050.3576077},
abstract = {Learning analytics approaches to profiling students based on their study behaviour remain limited in how they integrate temporality and change. To advance this area of work, the current study examines profiles of change in student study behaviour in a blended undergraduate engineering course. The study is conceptualised through complex dynamical systems theory and its applications in psychological and cognitive science research. Students were profiled based on the changes in their behaviour as observed in clickstream data. Measure of entropy in the recurrence of student behaviour was used to indicate the change of a student state, consistent with the evidence from cognitive sciences. Student trajectories of weekly entropy values were clustered to identify distinct profiles. Three patterns were identified: stable weekly study, steep changes in weekly study, and moderate changes in weekly study. The students with steep changes in their weekly study activity had lower exam grades and showed destabilisation of weekly behaviour earlier in the course. The study investigated the relationships between these profiles of change, student performance, and other approaches to learner profiling, such as self-reported measures of self-regulated learning, and profiles based on the sequences of learning actions.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {197–207},
numpages = {11},
keywords = {complex dynamical systems, learning analytics, self-regulated learning},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576076,
author = {Zhao, Linxuan and Swiecki, Zachari and Gasevic, Dragan and Yan, Lixiang and Dix, Samantha and Jaggard, Hollie and Wotherspoon, Rosie and Osborne, Abra and Li, Xinyu and Alfredo, Riordan and Martinez-Maldonado, Roberto},
title = {METS: Multimodal Learning Analytics of Embodied Teamwork Learning},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576076},
doi = {10.1145/3576050.3576076},
abstract = {Embodied team learning is a form of group learning that occurs in co-located settings where students need to interact with others while actively using resources in the physical learning space to achieve a common goal. In such situations, communication dynamics can be complex as team discourse segments can happen in parallel at different locations of the physical space with varied team member configurations. This can make it hard for teachers to assess the effectiveness of teamwork and for students to reflect on their own experiences. To address this problem, we propose METS (Multimodal Embodied Teamwork Signature), a method to model team dialogue content in combination with spatial and temporal data to generate a signature of embodied teamwork. We present a study in the context of a highly dynamic healthcare team simulation space where students can freely move. We illustrate how signatures of embodied teamwork can help to identify key differences between high and low performing teams: i) across the whole learning session; ii) at different phases of learning sessions; and iii) at particular spaces of interest in the learning space.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {186–196},
numpages = {11},
keywords = {Collaborative learning, Communication, Healthcare simulation, Multimodality, Teamwork},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576074,
author = {Dawson, Shane and Pardo, Abelardo and Salehian Kia, Fatemeh and Panadero, Ernesto},
title = {An Integrated Model of Feedback and Assessment: From fine grained to holistic programmatic review},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576074},
doi = {10.1145/3576050.3576074},
abstract = {Abstract: Research in learning analytics (LA) has long held a strong interest in improving student self-regulated learning and measuring the impact of feedback on student outcomes. Despite more than a decade of work in this space very little is known around the contextual factors that influence the topics and diversity of feedback and assessment a student encounters during their full program of study. This paper presents research investigating the institutional adoption of a personalized feedback tool. The reported findings illustrate an association between the topics of feedback, student performance, year level of the course and discipline. The results highlight the need for LA research to capture feedback, assessment and learning outcomes over an entire program of study. Herein we propose a more integrated model drawing on contemporary understandings of feedback with current research findings. The goal is to push LA towards addressing more complex teaching and learning processes from a systems lens. The model posed in this paper begins to illustrate where and how LA can address noted deficits in education practice to better understand how feedback and assessment are enacted by instructors and interpreted by students.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {579–584},
numpages = {6},
keywords = {Assessment, Feedback, Learning Analytics, Personalized Learning},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576073,
author = {Vanacore, Kirk and Gurung, Ashish and Mcreynolds, Andrew and Liu, Allison and Shaw, Stacy and Heffernan, Neil},
title = {Impact of Non-Cognitive Interventions on Student Learning Behaviors and Outcomes: An analysis of seven large-scale experimental inventions},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576073},
doi = {10.1145/3576050.3576073},
abstract = {As evidence grows supporting the importance of non-cognitive factors in learning, computer-assisted learning platforms increasingly incorporate non-academic interventions to influence student learning and learning related-behaviors. Non-cognitive interventions often attempt to influence students’ mindset, motivation, or metacognitive reflection to impact learning behaviors and outcomes. In the current paper, we analyze data from five experiments, involving seven treatment conditions embedded in mastery-based learning activities hosted on a computer-assisted learning platform focused on middle school mathematics. Each treatment condition embodied a specific non-cognitive theoretical perspective. Over seven school years, 20,472 students participated in the experiments. We estimated the effects of each treatment condition on students’ response time, hint usage, likelihood of mastering knowledge components, learning efficiency, and post-tests performance. Our analyses reveal a mix of both positive and negative treatment effects on student learning behaviors and performance. Few interventions impacted learning as assessed by the post-tests. These findings highlight the difficulty in positively influencing student learning behaviors and outcomes using non-cognitive interventions.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {165–174},
numpages = {10},
keywords = {A/B Testing, Causal Inference, Computer Assisted Learning Platform, Non-Cognitive Factors},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576072,
author = {Han, Songhee and Ji, Hyangeun and Jiang, Zilu and West, Michael and Liu, Min},
title = {What do students want to know while taking massive open online courses? Examining massive open online course students’ needs based on online forum discussions from the Universal Design for Learning approach},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576072},
doi = {10.1145/3576050.3576072},
abstract = {We identified the nine most dominant massive open online course (MOOC) students’ needs by topic modeling and qualitative analysis of forum discussion posts (n = 3645) among students, staff, and instructors from 21 courses. We examined the implications of these needs using three main Universal Design for Learning (UDL) principles (representation, action and expression, and engagement). We then offered suggestions for what course providers can do to promote an equitable learning experience for MOOC students. The three suggestions are as follows: (1) providing tools such as a direct messaging application to encourage students’ socializing behaviors, (2) modifying course activities to promote more hands-on projects and sharing them, and (3) implementing a bidirectional channel, such as a natural language processing-based chatbot so that students can access useful information whenever they feel the need. We argue that it is critical to include minority students’ voices when examining needs in courses, and our methodology reflects this purpose. We also discuss how the UDL approach helped us recognize students’ needs, create more accessible MOOC learning experiences, and explore future research directions.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {572–578},
numpages = {7},
keywords = {Massive open online courses, Natural language processing, Universal Design for Learning},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576071,
author = {Haim, Aaron and Shaw, Stacy and Heffernan, Neil},
title = {How to Open Science: A Principle and Reproducibility Review of the Learning Analytics and Knowledge Conference},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576071},
doi = {10.1145/3576050.3576071},
abstract = {Within the field of education technology, learning analytics has increased in popularity over the past decade. Researchers conduct experiments and develop software, building on each other’s work to create more intricate systems. In parallel, open science — which describes a set of practices to make research more open, transparent, and reproducible — has exploded in recent years, resulting in more open data, code, and materials for researchers to use. However, without prior knowledge of open science, many researchers do not make their datasets, code, and materials openly available, and those that are available are often difficult, if not impossible, to reproduce. The purpose of the current study was to take a close look at our field by examining previous papers within the proceedings of the International Conference on Learning Analytics and Knowledge, and document the rate of open science adoption (e.g., preregistration, open data), as well as how well available data and code could be reproduced. Specifically, we examined 133 research papers, allowing ourselves 15 minutes for each paper to identify open science practices and attempt to reproduce the results according to their provided specifications. Our results showed that less than half of the research adopted standard open science principles, with approximately 5% fully meeting some of the defined principles. Further, we were unable to reproduce any of the papers successfully in the given time period. We conclude by providing recommendations on how to improve the reproducibility of our research as a field moving forward. All openly accessible work can be found in an Open Science Foundation project1.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {156–164},
numpages = {9},
keywords = {Open Science, Peer Review, Reproducibility},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576070,
author = {Farrow, Elaine and Moore, Johanna D. and Gasevic, Dragan},
title = {Names, Nicknames, and Spelling Errors: Protecting Participant Identity in Learning Analytics of Online Discussions},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576070},
doi = {10.1145/3576050.3576070},
abstract = {Messages exchanged between participants in online discussion forums often contain personal names and other details that need to be redacted before the data is used for research purposes in learning analytics. However, removing the names entirely makes it harder to track the exchange of ideas between individuals within a message thread and across threads, and thereby reduces the value of this type of conversational data. In contrast, the consistent use of pseudonyms allows contributions from individuals to be tracked across messages, while also hiding the real identities of the contributors. Several factors can make it difficult to identify all instances of personal names that refer to the same individual, including spelling errors and the use of shortened forms. We developed a semi-automated approach for replacing personal names with consistent pseudonyms. We evaluated our approach on a data set of over 1,700 messages exchanged during a distance-learning course, and compared it to a general-purpose pseudonymisation tool that used deep neural networks to identify names to be redacted. We found that our tailored approach out-performed the general-purpose tool in both precision and recall, correctly identifying all but 31 substitutions out of 2,888.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {145–155},
numpages = {11},
keywords = {anonymisation, de-identification, ethical issues, learning analytics, personal name, privacy, pseudonymisation, redaction},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576069,
author = {Zamecnik, Andrew and Joksimovic, Srecko and Kovanovic, Vitomir and Grossmann, Georg and Ladjal, Djazia and Pardo, Abelardo},
title = {Exploring the Feedback Provision of Mentors and Clients for Teams in Work-Integrated Learning Environments},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576069},
doi = {10.1145/3576050.3576069},
abstract = {Industry supervisors play a pivotal role in ongoing learner support and guidance within a work-integrated learning context. Effective provisional feedback from industry supervisors in work-integrated learning environments is essential for increasing a team’s metacognitive awareness and ability to evaluate their performance. However, research that examines the usefulness and type of feedback from industry supervisors for teams remains limited. In this study, we investigate the quality of provisional feedback by comparing the teams’ helpfulness rating of the feedback from two types of industry supervisors (i.e., clients and mentors), based on the feedback type (task, process, regulatory and self-level oriented) using learning analytics. The results show that teams rated the perceived helpfulness scores of clients and mentors as very useful, with mentors providing slightly more helpful feedback. We also found that mentors provide more co-occurrences of feedback classifications than clients. The overall results show that teams perceive mentor feedback as more helpful than clients and that the mentor targets feedback that is more beneficial to the teams learning than the clients. Our findings can aid in developing guidelines that aim to validate and improve existing or new feedback quality frameworks by leveraging backward evaluation data.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {566–571},
numpages = {6},
keywords = {epistemic network analysis, feedback, helpfulness rating, teams, work-integrated learning},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576068,
author = {Hui, Bowen},
title = {Are They Learning or Guessing? Investigating Trial-and-Error Behavior with Limited Test Attempts},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576068},
doi = {10.1145/3576050.3576068},
abstract = {Mastery learning and deliberate practice promote personalized learning, allowing the learner to improve through a repetitive and targeted approach. Unfortunately, this pedagogy is challenging to implement in classrooms where everyone is expected to learn at the same pace. Various studies have successfully demonstrated that certain aspects of mastery learning can be integrated into the curriculum. We adopt a similar pedagogical strategy and explain our implementation approach with online assessments. Since this is a new pedagogical approach to assessing student learning, we collected data to investigate test-taking behavior and evaluated potential learning gains in this new test format. As part of this endeavor, we developed a model to detect trial-and-error sequences in test attempts. Our results point to a small percentage of guessing behavior, which is encouraging evidence supporting this test approach is a viable way to implement mastery learning in our curriculum.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {133–144},
numpages = {12},
keywords = {Assessment, guessing behavior, mastery learning, trial-and-error pattern},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576067,
author = {Nazaretsky, Tanya and Mikeska, Jamie N. and Beigman Klebanov, Beata},
title = {Empowering Teacher Learning with AI: Automated Evaluation of Teacher Attention to Student Ideas during Argumentation-focused Discussion},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576067},
doi = {10.1145/3576050.3576067},
abstract = {Engaging students in argument from evidence is an essential goal of science education. This is a complex skill to develop; recent research in science education proposed the use of simulated classrooms to facilitate the practice of the skill. We use data from one such simulated environment to explore whether automated analysis of the transcripts of the teacher’s interaction with the simulated students using Natural Language Processing techniques could yield an accurate evaluation of the teacher’s performance. We are especially interested in explainable models that could also support formative feedback. The results are encouraging: Not only can the models score the transcript as well as humans can, but they can also provide justifications for the scores comparable to those provided by human raters.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {122–132},
numpages = {11},
keywords = {Automated Feedback, Deep Learning, Practice-based Teacher Education, Simulated Teaching, Teacher Discourse},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576064,
author = {Lin, Jionghao and Dai, Wei and Lim, Lisa-Angelique and Tsai, Yi-Shan and Mello, Rafael Ferreira and Khosravi, Hassan and Gasevic, Dragan and Chen, Guanliang},
title = {Learner-centred Analytics of Feedback Content in Higher Education},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576064},
doi = {10.1145/3576050.3576064},
abstract = {Feedback is an effective way to assist students in achieving learning goals. The conceptualisation of feedback is gradually moving from feedback as information to feedback as a learner-centred process. To demonstrate feedback effectiveness, feedback as a learner-centred process should be designed to provide quality feedback content and promote student learning outcomes on the subsequent task. However, it remains unclear how instructors adopt the learner-centred feedback framework for feedback provision in the teaching practice. Thus, our study made use of a comprehensive learner-centred feedback framework to analyse feedback content and identify the characteristics of feedback content among student groups with different performance changes. Specifically, we collected the instructors’ feedback on two consecutive assignments offered by an introductory to data science course at the postgraduate level. On the basis of the first assignment, we used the status of student grade changes (i.e., students whose performance increased and those whose performance did not increase on the second assignment) as the proxy of the student learning outcomes. Then, we engineered and extracted features from the feedback content on the first assignment using a learner-centred feedback framework and further examined the differences of these features between different groups of student learning outcomes. Lastly, we used the features to predict student learning outcomes by using widely-used machine learning models and provided the interpretation of predicted results by using the SHapley Additive exPlanations (SHAP) framework. We found that 1) most features from the feedback content presented significant differences between the groups of student learning outcomes, 2) the gradient boost tree model could effectively predict student learning outcomes, and 3) SHAP could transparently interpret the feature importance on predictions.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {100–110},
numpages = {11},
keywords = {Content Analysis, Feedback, Interpretability, Learning Analytics},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576063,
author = {Pozdniakov, Stanislav and Martinez-Maldonado, Roberto and Tsai, Yi-Shan and Echeverria, Vanessa and Srivastava, Namrata and Gasevic, Dragan},
title = {How Do Teachers Use Dashboards Enhanced with Data Storytelling Elements According to their Data Visualisation Literacy Skills?},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576063},
doi = {10.1145/3576050.3576063},
abstract = {There is a proliferation of learning analytics (LA) dashboards aimed at supporting teachers. Yet, teachers still find it challenging to make sense of LA dashboards, thereby making informed decisions. Two main strategies to address this are emerging: i) upskilling teachers’ data literacy; ii) improving the explanatory design features of current dashboards (e.g., adding visual cues or text) to minimise the skills required by teachers to effectively use dashboards. While each approach has its own trade-offs, no previous work has explored the interplay between the dashboard design and such "data skills". In this paper, we explore how teachers with varying visualisation literacy (VL) skills use LA dashboards enhanced with (explanatory) data storytelling elements. We conducted a quasi-experimental study with 23 teachers of varied VL inspecting two versions of an authentic multichannel dashboard enhanced with data storytelling elements. We used an eye-tracking device while teachers inspected the students’ data captured from Zoom and Google Docs, followed by interviews. Results suggest that high VL teachers adopted complex exploratory strategies and were more sensitive to subtle inconsistencies in the design; while low VL teachers benefited the most from more explicit data storytelling guidance such as accompanying complex graphs with narrative and semantic colour encoding.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {89–99},
numpages = {11},
keywords = {dashboard, data literacy, data storytelling, human-centred design, learning analytics},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576062,
author = {Ma, Boxuan and Hettiarachchi, Gayan Prasad and Fukui, Sora and Ando, Yuji},
title = {Each Encounter Counts: Modeling Language Learning and Forgetting},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576062},
doi = {10.1145/3576050.3576062},
abstract = {Language learning applications usually estimate the learner’s language knowledge over time to provide personalized practice content for each learner at the optimal timing. However, accurately predicting language knowledge or linguistic skills is much more challenging than math or science knowledge, as many language tasks involve memorization and retrieval. Learners must memorize a large number of words and meanings, which are prone to be forgotten without practice. Although a few studies consider forgetting when modeling learners’ language knowledge, they tend to apply traditional models, consider only partial information about forgetting, and ignore linguistic features that may significantly influence learning and forgetting. This paper focuses on modeling and predicting learners’ knowledge by considering their forgetting behavior and linguistic features in language learning. Specifically, we first explore the existence of forgetting behavior and cross-effects in real-world language learning datasets through empirical studies. Based on these, we propose a model for predicting the probability of recalling a word given a learner’s practice history. The model incorporates key information related to forgetting, question formats, and semantic similarities between words using the attention mechanism. Experiments on two real-world datasets show that the proposed model improves performance compared to baselines. Moreover, the results indicate that combining multiple types of forgetting information and item format improves performance. In addition, we find that incorporating semantic features, such as word embeddings, to model similarities between words in a learner’s practice history and their effects on memory also improves the model.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {79–88},
numpages = {10},
keywords = {Educational data mining, Forgetting behavior, Knowledge tracing, Language learning},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576061,
author = {Herodotou, Christothea and Maguire, Claire and Hlosta, Martin and Mulholland, Paul},
title = {Predictive Learning Analytics and University Teachers: Usage and perceptions three years post implementation},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576061},
doi = {10.1145/3576050.3576061},
abstract = {Predictive learning analytics (PLA) dashboards have been used by teachers to identify students at risk of failing their studies and provide proactive support. Yet, very few of them have been deployed at a large scale or had their use studied at a mature level of implementation. In this study, we surveyed 366 distance learning university teachers across four faculties three years after PLA has been made available across university as business as usual. Informed by the Unified Theory of Acceptance and Use of Technology (UTAUT), we present a context-specific version of UTAUT that reflects teachers’ perceptions of PLA in distance learning higher education. The adoption and use of PLA was shown to be positively influenced by less experience in teaching, performance expectancy, self-efficacy, positive attitudes, and low anxiety, while negatively influenced by a lack of facilitating conditions and low effort expectancy, indicating that the type of technology and context within which it is used are significant factors determining our understanding of technology usage and adoption. This study provides significant insights as to how to design, apply and implement PLA with teachers in higher education.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {68–78},
numpages = {11},
keywords = {Predictive learning analytics, Technology Adoption, UTAUT, University teachers},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576060,
author = {van Haastrecht, Max and Brinkhuis, Matthieu and Peichl, Jessica and Remmele, Bernd and Spruit, Marco},
title = {Embracing Trustworthiness and Authenticity in the Validation of Learning Analytics Systems},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576060},
doi = {10.1145/3576050.3576060},
abstract = {Learning analytics sits in the middle space between learning theory and data analytics. The inherent diversity of learning analytics manifests itself in an epistemology that strikes a balance between positivism and interpretivism, and knowledge that is sourced from theory and practice. In this paper, we argue that validation approaches for learning analytics systems should be cognisant of these diverse foundations. Through a systematic review of learning analytics validation research, we find that there is currently an over-reliance on positivistic validity criteria. Researchers tend to ignore interpretivistic criteria such as trustworthiness and authenticity. In the 38 papers we analysed, researchers covered positivistic validity criteria 221 times, whereas interpretivistic criteria were mentioned 37 times. We motivate that learning analytics can only move forward with holistic validation strategies that incorporate “thick descriptions” of educational experiences. We conclude by outlining a planned validation study using argument-based validation, which we believe will yield meaningful insights by considering a diverse spectrum of validity criteria.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {552–558},
numpages = {7},
keywords = {authenticity, interpretivism, learning analytics, trustworthiness, validation},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576058,
author = {Alfredo, Riordan Dervin and Nie, Lanbing and Kennedy, Paul and Power, Tamara and Hayes, Carolyn and Chen, Hui and McGregor, Carolyn and Swiecki, Zachari and Ga\v{s}evi\'{c}, Dragan and Martinez-Maldonado, Roberto},
title = {"That Student Should be a Lion Tamer!" StressViz: Designing a Stress Analytics Dashboard for Teachers},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576058},
doi = {10.1145/3576050.3576058},
abstract = {In recent years, there has been a growing interest in creating multimodal learning analytics (LA) systems that automatically analyse students’ states that are hard to see with the "naked eye", such as cognitive load and stress levels, but that can considerably shape their learning experience. A rich body of research has focused on detecting such aspects by capturing bodily signals from students using wearables and computer vision. Yet, little work has aimed at designing end-user interfaces that visualise physiological data to support tasks deliberately designed for students to learn from stressful situations. This paper addresses this gap by designing a stress analytics dashboard that encodes students’ physiological data into stress levels during different phases of an authentic team simulation in the context of nursing education. We conducted a qualitative study with teachers to understand (i) how they made sense of the stress analytics dashboard; (ii) the extent to which they trusted the dashboard in relation to students’ cortisol data; and (iii) the potential adoption of this tool to communicate insights and aid teaching practices.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {57–67},
numpages = {11},
keywords = {Affective computing, Healthcare education, LA dashboard, Multimodal dataset, Stress detection, Visualisation},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576055,
author = {Nguyen, Ha},
title = {TikTok as Learning Analytics Data: Framing Climate Change and Data Practices},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576055},
doi = {10.1145/3576050.3576055},
abstract = {Climate change has far-reaching impacts on communities around the world. However, climate change education has more often focused on scientific facts and statistics at a global scale than experiences at personal and local scales. To understand how to frame climate change education, I turn to youth-created videos on TikTok—a video-sharing, social media platform. Semantic network analysis of hashtags related to climate change reveals multifaceted, intertwining discourse around awareness of climate change consequences, call for action to reduce human impacts on natural systems, and environmental activism. I further explore how youth integrate personal, lived experiences data into climate change discussions. A higher usage of second-person perspective ("you"; i.e., addressing the audience), prosocial and agency words, and negative messaging tone are associated with higher odds of a video integrating lived experiences. These findings illustrate the platform’s affordances: In communicating to a broad audience, youth take on agency and pro-social stances and express emotions to relate to viewers and situate their content. Findings suggest the utility of learning analytics to explore youth’s perspectives and provide insights to frame climate change education in ways that elevate lived experiences.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {33–43},
numpages = {11},
keywords = {climate change education, social learning analytics, social media},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576054,
author = {Yan, Lixiang and Martinez-Maldonado, Roberto and Zhao, Linxuan and Li, Xinyu and Gasevic, Dragan},
title = {SeNA: Modelling Socio-spatial Analytics on Homophily by Integrating Social and Epistemic Network Analysis},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576054},
doi = {10.1145/3576050.3576054},
abstract = {Homophily is a fundamental sociological theory that describes the tendency of individuals to interact with others who share similar attributes. This theory has shown evident relevance for studying collaborative learning and classroom orchestration in learning analytics research from a social constructivist perspective. Emerging advancements in multimodal learning analytics have shown promising results in capturing interaction data and generating socio-spatial analytics in physical learning spaces through computer vision and wearable positioning technologies. Yet, there are limited ways for analysing homophily (e.g., social network analysis; SNA), especially for unpacking the temporal connections between different homophilic behaviours. This paper presents a novel analytic approach, Social-epistemic Network Analysis (SeNA), for analysing homophily by combining social network analysis with epistemic network analysis to infuse socio-spatial analytics with temporal insights. The additional insights SeNA may offer over traditional approaches (e.g., SNA) were illustrated through analysing the homophily of 98 students in open learning spaces. The findings showed that SeNA could reveal significant behavioural differences in homophily between comparison groups across different learning designs, which were not accessible to SNA alone. The implications and limitations of SeNA in supporting future learning analytics research regarding homophily in physical learning spaces are also discussed.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {22–32},
numpages = {11},
keywords = {collaborative learning, epistemic network, homophily, learning analytics, social network},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576053,
author = {Watts, Field M. and Dood, Amber J. and Shultz, Ginger V.},
title = {Automated, content-focused feedback for a writing-to-learn assignment in an undergraduate organic chemistry course},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576053},
doi = {10.1145/3576050.3576053},
abstract = {Writing-to-learn (WTL) pedagogy supports the implementation of writing assignments in STEM courses to engage students in conceptual learning. Recent studies in the undergraduate STEM context demonstrate the value of implementing WTL, with findings that WTL can support meaningful learning and elicit students’ reasoning. However, the need for instructors to provide feedback on students’ writing poses a significant barrier to implementing WTL; this barrier is especially notable in the context of introductory organic chemistry courses at large universities, which often have large enrollments. This work describes one approach to overcome this barrier by presenting the development of an automated feedback tool for providing students with formative feedback on their responses to an organic chemistry WTL assignment. This approach leverages machine learning models to identify features of students’ mechanistic reasoning in response to WTL assignments in a second-semester, introductory organic chemistry laboratory course. The automated feedback tool development was guided by a framework for designing automated feedback, theories of self-regulated learning, and the components of effective WTL pedagogy. Herein, we describe the design of the automated feedback tool and report our initial evaluation of the tool through pilot interviews with organic chemistry students.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {531–537},
numpages = {7},
keywords = {automated feedback, self-regulated learning, writing-to-learn},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576052,
author = {Choi, Heeryung and Winne, Philip H. and Brooks, Christopher and Li, Warren and Shedden, Kerby},
title = {Logs or Self-Reports? Misalignment Between Behavioral Trace Data and Surveys When Modeling Learner Achievement Goal Orientation},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576052},
doi = {10.1145/3576050.3576052},
abstract = {While learning analytics researchers have been diligently integrating trace log data into their studies, learners’ achievement goals are still predominantly measured by self-reported surveys. This study investigated the properties of trace data and survey data as representations of achievement goals. Through the lens of goal complex theory, we generated achievement goal clusters using latent variable mixture modeling applied to each kind of data. Findings show significant misalignment between these two data sources. Self-reported goals stated before learning do not translate into goal-relevant behaviors tracked using trace data collected during learning activities. While learners generally articulate an orientation towards mastery learning in self-report surveys, behavioral trace data showed a higher incidence of less engaged learning activities. These findings call into question the utility of survey-based measures when up-to-date achievement goal data are needed. Our results advance methodological and theoretical understandings of achievement goals in the modern age of learning analytics.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {11–21},
numpages = {11},
keywords = {achievement goals, latent variable mixture modeling, survey data, trace data},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@proceedings{10.1145/3576050,
title = {LAK2023: LAK23: 13th International Learning Analytics and Knowledge Conference},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Arlington, TX, USA}
}

@inproceedings{10.1145/3506860.3506978,
author = {Ferreira, M\'{a}verick Andr\'{e} Dion\'{\i}sio and Ferreira Mello, Rafael and Kovanovic, Vitomir and Nascimento, Andr\'{e} and Lins, Rafael and Gasevic, Dragan},
title = {NASC: Network analytics to uncover socio-cognitive discourse of student roles},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506978},
doi = {10.1145/3506860.3506978},
abstract = {Roles that learners assume during online discussions are an important aspect of educational experience. The roles can be assigned to learners and/or can spontaneously emerge through student-student interaction. While existing research proposed several approaches for analytics of emerging roles, there is limited research in analytic methods that can i) automatically detect emerging roles that can be interpreted in terms of higher-order constructs of collaboration; ii) analyse the extent to which students complied to scripted roles and how emerging roles compare to scripted ones; and iii) track progression of roles in social knowledge progression over time. To address these gaps in the literature, this paper propose a network-analytic approach that combines techniques of cluster analysis and epistemic network analysis. The method was validated in an empirical study discovered emerging roles that were found meaningful in terms of social and cognitive dimensions of the well-known model of communities of inquiry. The study also revealed similarities and differences between emerging and script roles played by learners and identified different progression trajectories in social knowledge construction between emerging and scripted roles. The proposed analytic approach and the study results have implications that can inform teaching practice and development techniques for collaboration analytics.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {415–425},
numpages = {11},
keywords = {Epistemic Network Analysis, Emerging Roles, Clustering Analysis, CSCL},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3506860.3506977,
author = {Ferreira Mello, Rafael and Fiorentino, Giuseppe and Oliveira, Hil\'{a}rio and Miranda, P\'{e}ricles and Rakovic, Mladen and Gasevic, Dragan},
title = {Towards automated content analysis of rhetorical structure of written essays using sequential content-independent features in Portuguese},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506977},
doi = {10.1145/3506860.3506977},
abstract = {Brazilian universities have included essay writing assignments in the entrance examination procedure to select prospective students. The essay scorers manually look for the presence of required Rhetorical Structure Theory (RST) categories and evaluate essay coherence. However, identifying RST categories is a time-consuming task. The literature reported several attempts to automate the identification of RST categories in essays with machine learning. Still, previous studies have focused on using machine learning algorithms trained on content-dependent features that can diminish classification performance, leading to over-fitting and hindering model generalisability. Therefore, this paper proposes: (i) the analysis of state-of-the-art classifiers and content-independent features to the task of RST rhetorical moves; (ii) a new approach that considers the sequence of the text to extract features – i.e. sequential content-independent features; (iii) an empirical study about the generalisability of the machine learning models and sequential content-independent features for this context; (iv) the identification of the most predictive features for automated identification of RST categories in essays written in Portuguese. The best performing classifier, XGBoost, based on sequential content-independent features, outperformed the classifiers used in the literature and are based on traditional content-dependent features. The XGBoost classifier based on sequential content-independent features also reached promising accuracy when tested for generalisability.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {404–414},
numpages = {11},
keywords = {rhetoric structure, natural language processing., context analysis, content analytics, Essay analysis},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3506860.3506974,
author = {Slibar, Barbara and Gusic Mundjar, Jelena and Rako, Sabina and Simic, Diana},
title = {Co-occurrence patterns of issues and guidelines related to ethics and privacy of learning analytics in higher education—literature review},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506974},
doi = {10.1145/3506860.3506974},
abstract = {Ethics and privacy issues have been recognized as important factors for acceptance and trustworthy implementation of learning analytics. A large number of different issues has been recognized in the literature. Guidelines related to these issues are continuously being developed and discussed in research literature. The aim of this research was to identify patterns of co-occurrence of issues and guidelines in research papers discussing ethics and privacy issues, to gain better understanding of relationships between different ethics and privacy issues arising during implementation of learning analytics in higher education. A total of 93 papers published between 2010 and 2021 were qualitatively analyzed, and nine categories of issues and respective guidelines related to ethics and privacy in learning analytics were identified. Association rules mining Apriori algorithm was applied, where 93 papers represented transactions, and 18 categories of issues or guidelines (nine each) represented items. Two clusters of issues co-occurring in papers were identified, corresponding to deontology ethics (related to rules and duties), and consequentialism ethics (related to consequences of unethical behavior).},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {577–582},
numpages = {6},
keywords = {Learning analytics, Higher education, Ethics, Apriori algorithm},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3506860.3506972,
author = {Srivastava, Namrata and Fan, Yizhou and Rakovic, Mladen and Singh, Shaveen and Jovanovic, Jelena and van der Graaf, Joep and Lim, Lyn and Surendrannair, Surya and Kilgour, Jonathan and Molenaar, Inge and Bannert, Maria and Moore, Johanna and Gasevic, Dragan},
title = {Effects of Internal and External Conditions on Strategies of Self-regulated Learning: A Learning Analytics Study},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506972},
doi = {10.1145/3506860.3506972},
abstract = {Self-regulated learning (SRL) skills are essential for successful learning in a technology-enhanced learning environment. Learning Analytics techniques have shown a great potential in identifying and exploring SRL strategies from trace data in various learning environments. However, these strategies have been mainly identified through analysis of sequences of learning actions, and thus interpretation of the strategies is heavily task and context dependent. Further, little research has been done on the association of SRL strategies with different influencing factors or conditions. To address these gaps, we propose an analytic method for detecting SRL strategies from theoretically supported SRL processes and applied the method to a dataset collected from a multi-source writing task. The detected SRL strategies were explored in terms of their association with the learning outcome, internal conditions (prior-knowledge, metacognitive knowledge and motivation) and external conditions (scaffolding). The study results showed our analytic method successfully identified three theoretically meaningful SRL strategies. The study results revealed small effect size in the association between the internal conditions and the identified SRL strategies, but revealed a moderate effect size in the association between external conditions and the SRL strategy use.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {392–403},
numpages = {12},
keywords = {Self-regulated learning, Scaffolding, SRL strategies, Learning analytics},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3506860.3506939,
author = {Tsai, Yi-Shan and Singh, Shaveen and Rakovic, Mladen and Lim, Lisa-Angelique and Roychoudhury, Anushka and Gasevic, Dragan},
title = {Charting Design Needs and Strategic Approaches for Academic Analytics Systems through Co-Design},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506939},
doi = {10.1145/3506860.3506939},
abstract = {Academic analytics focuses on collecting, analysing and visualising educational data to generate institutional insights and improve decision-making for academic purposes. However, challenges that arise from navigating a complex organisational structure when introducing analytics systems have called for the need to engage key stakeholders widely to cultivate a shared vision and ensure that implemented systems create desired value. This paper presents a study that takes co-design steps to identify design needs and strategic approaches for the adoption of academic analytics, which serves the purpose of enhancing the measurement of educational quality utilising institutional data. Through semi-structured interviews with 54 educational stakeholders at a large research university, we identified particular interest in measuring student engagement and the performance of courses and programmes. Based on the observed perceptions and concerns regarding data use to measure or evaluate these areas, implications for adoption strategy of academic analytics, such as leadership involvement, communication, and training, are discussed.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {381–391},
numpages = {11},
keywords = {implementation strategy, higher education, educational quality, co-design, academic analytics},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3506860.3506935,
author = {Zhao, Linxuan and Yan, Lixiang and Gasevic, Dragan and Dix, Samantha and Jaggard, Hollie and Wotherspoon, Rosie and Alfredo, Riordan and Li, Xinyu and Martinez-Maldonado, Roberto},
title = {Modelling Co-located Team Communication from Voice Detection and Positioning Data in Healthcare Simulation},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506935},
doi = {10.1145/3506860.3506935},
abstract = {In co-located situations, team members use a combination of verbal and visual signals to communicate effectively, among which positional forms play a key role. The spatial patterns adopted by team members in terms of where in the physical space they are standing, and who their body is oriented to, can be key in analysing and increasing the quality of interaction during such face-to-face situations. In this paper, we model the students’ communication based on spatial (positioning) and audio (voice detection) data captured from 92 students working in teams of four in the context of healthcare simulation. We extract non-verbal events (i.e., total speaking time, overlapped speech,and speech responses to team members and teachers) and investigate to what extent they can serve as meaningful indicators of students’ performance according to teachers’ learning intentions. The contribution of this paper to multimodal learning analytics includes: i) a generic method to semi-automatically model communication in a setting where students can freely move in the learning space; and ii) results from a mixed-methods analysis of non-verbal indicators of team communication with respect to teachers’ learning design.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {370–380},
numpages = {11},
keywords = {Nursing simulation, Multimodal learning analytics, Learning analytics, Communication, Collaborative learning, Audio},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3506860.3506922,
author = {Praharaj, Sambit and Scheffel, Maren and Schmitz, Marcel and Specht, Marcus and Drachsler, Hendrik},
title = {Towards Collaborative Convergence: Quantifying Collaboration Quality with Automated Co-located Collaboration Analytics},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506922},
doi = {10.1145/3506860.3506922},
abstract = {Collaboration is one of the four important 21st-century skills. With the pervasive use of sensors, interest on co-located collaboration (CC) has increased lately. Most related literature used the audio modality to detect indicators of collaboration (such as total speaking time and turn taking). CC takes place in physical spaces where group members share their social (i.e., non-verbal audio indicators like speaking time, gestures) and epistemic space (i.e., verbal audio indicators like the content of the conversation). Past literature has mostly focused on the social space to detect the quality of collaboration. In this study, we focus on both social and epistemic space with an emphasis on the epistemic space to understand different evolving collaboration patterns and collaborative convergence and quantify collaboration quality. We conduct field trials by collecting audio recordings in 14 different sessions in a university setting while the university staff and students collaborate over playing a board game to design a learning activity. This collaboration task consists of different phases with each collaborating member having been assigned a pre-fixed role. We analyze the collected group speech data to do role-based profiling and visualize it with the help of a dashboard.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {358–369},
numpages = {12},
keywords = {multimodal learning analytics, collaboration analytics, collaboration, co-located collaboration},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3506860.3506918,
author = {Brennan, Riordan and Perouli, Debbie},
title = {Generating and Evaluating Collective Concept Maps},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506918},
doi = {10.1145/3506860.3506918},
abstract = {Concept maps are used in education to illustrate ideas and relationships among them. Instructors employ such maps to evaluate a student’s knowledge on a subject. Collective concept maps have been recently proposed as a tool to graphically summarize a group’s rather than an individual’s understanding on a topic. In this paper, we present a methodology that automatically generates collective concept maps, which relies on grouping similar ideas into node-clusters. We present a novel clustering algorithm that is shown to produce more informational maps compared to Markov clustering. We evaluate the collective map framework by applying it to sets of a total of 56 individual maps created by teachers (grades 2-12) and students (grades 6-11) during a week-long cybersecurity camp. Finally, we discuss how collective concept maps can support longitudinal research studies on program and student outcomes by providing a novel format for knowledge exchange. We have made our tool implementation publicly available.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {570–576},
numpages = {7},
keywords = {knowledge exchange formats, education, cybersecurity, concept maps, K-12},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3506860.3506917,
author = {Tavakoli, Mohammadreza and Faraji, Abdolali and Molavi, Mohammadreza and T. Mol, Stefan and Kismih\'{o}k, G\'{a}bor},
title = {Hybrid Human-AI Curriculum Development for Personalised Informal Learning Environments},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506917},
doi = {10.1145/3506860.3506917},
abstract = {Informal learning procedures have been changing extremely fast over the recent decades not only due to the advent of online learning, but also due to changes in what humans need to learn to meet their various life and career goals. Consequently, online, educational platforms are expected to provide personalized, up-to-date curricula to assist learners. Therefore, in this paper, we propose an Artificial Intelligence (AI) and Crowdsourcing based approach to create and update curricula for individual learners. We show the design of this curriculum development system prototype, in which contributors receive AI-based recommendations to be able to define and update high-level learning goals, skills, and learning topics together with associated learning content. This curriculum development system was also integrated into our personalized online learning platform. To evaluate our prototype we compared experts’ opinion with our system’s recommendations, and resulted in 89%, 79%, and 93% F1-scores when recommending skills, learning topics, and educational materials respectively. Also, we interviewed eight senior level experts from educational institutions and career consulting organizations. Interviewees agreed that our curriculum development method has high potential to support authoring activities in dynamic, personalized learning environments.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {563–569},
numpages = {7},
keywords = {Informal Learning, Curriculum Development, Crowdsourcing, Artificial Intelligence},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3506860.3506916,
author = {Aguilar, Stephen J},
title = {Experimental Evidence of Performance Feedback vs. Mastery Feedback on Students’ Academic Motivation},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506916},
doi = {10.1145/3506860.3506916},
abstract = {Work throughout the learning analytics community has examined associations between Learning Analytics Dashboard (LAD) features and a number of important student outcomes, including academic motivation and self-regulated learning strategies. While there are many potential implications of visualized academic information within a LAD on student outcomes, there remains an unanswered question: are there causal differences between showing performance information (e.g., comparing students’ progress to the class average) vs. mastery information (e.g., their individual score) on students’ motivation? Grounded in Achievement Goal Theory, this study answers this question experimentally by analyzing the difference between college students’ (n=445) reported achievement goal orientations as well as their motivated information seeking orientations after being presented with performance or mastery feedback. Results indicate that students in a performance condition which displayed ”above average” achievement on an academic measure reported lower performance-avoidance goals (e.g., not wanting to do worse than everyone else), and performance-avoidance information-seeking goals (e.g., not wanting to seek out information showing that one does worse than peers) when compared to students in the mastery control condition. This study contributes to our understanding of the motivational implications of academic feedback presented to students, and suggests that comparative information has direct effects on student motivation. Results thus uncover a potential tension between what might seem intuitive feedback to give students versus what might be more motivationally appropriate. The implications of this work point to the need to understand LADs not simply as feedback mechanisms, but as embedded features of a learning environment that influence how students engage with course content.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {556–562},
numpages = {7},
keywords = {Visualizations, Non-cognitive factors, Motivation, Higher Education},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3506860.3506915,
author = {Li, Boyi and Minematsu, Tsubasa and Taniguchi, Yuta and Okubo, Fumiya and Shimada, Atsushi},
title = {How Does Analysis of Handwritten Notes Provide Better Insights for Learning Behavior?},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506915},
doi = {10.1145/3506860.3506915},
abstract = {Handwritten notes are one important component of students’ learning process, which is used to record what they have learned in class or tease out knowledge after class for reflection and further strengthen the learning effect. It also helps a lot during review. We hope to divide handwritten notes (Japanese) into different parts, such as text, mathematical expressions, charts, etc., and quantify them to evaluate the condition of the notes and compare them among students. At the same time, data on students’ learning behaviors in the course are collected through the online education platform, such as the use time of textbook and attendance, as well as the scores of the online quiz and course grade. In this paper, the analysis of the relationship between the segmentation results of handwritten notes and learning behavior are reported, as well as the research on automatic page segmentation based on deep learning.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {549–555},
numpages = {7},
keywords = {page segmentation, learning behavior, handwritten note},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3506860.3506914,
author = {Khalil, Mohammad and Wong, Jacqueline and Er, Erkan and Heitmann, Martin and Belokrys, Gleb},
title = {Tweetology of Learning Analytics: What does Twitter tell us about the trends and development of the field?},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506914},
doi = {10.1145/3506860.3506914},
abstract = {Twitter is a very popular microblogging platform that has been actively used by scientific communities to exchange scientific information and to promote scholarly discussions. The present study aimed to leverage the tweet data to provide valuable insights into the development of the learning analytics field since its initial days. Descriptive analysis, geocoding analysis, and topic modeling were performed on over 1.6 million tweets related to learning analytics posted between 2010-2021. The descriptive analysis reveals an increasing popularity of the field on the Twittersphere in terms of number of users, twitter posts, and hashtags emergence. The topic modeling analysis uncovers new insights of the major topics in the field of learning analytics. Emergent themes in the field were identified, and the increasing (e.g., Artificial Intelligence) and decreasing&nbsp;(e.g., Education) trends were shared. Finally, the geocoding analysis indicates an increasing participation in the field from more diverse countries all around the world. Further findings are discussed in the paper.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {347–357},
numpages = {11},
keywords = {topic modeling, learning analytics, geospatial analysis, Twitter analysis, Twitter},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3506860.3506913,
author = {Rotelli, Daniela and Monreale, Anna},
title = {Time-on-Task Estimation by data-driven Outlier Detection based on Learning Activities},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506913},
doi = {10.1145/3506860.3506913},
abstract = {Temporal analysis has been demonstrated to be relevant in Learning Analytics research, and capturing time-on-task, i.e., the amount of time spent by students in quality learning, as a proxy to model learning behaviour, predict performance, and avoid drop-out has been the focus of a number of investigations. Nonetheless, most studies do not provide enough information on how their data were prepared for their findings to be easily replicated, even though data pre-processing decisions have an impact on the analysis’ outcomes and can lead to inaccurate predictions. One of the key aspects in the preparation of learning data for temporal analysis is the detection of anomalous values of temporal duration of students’ activities. Most of the works in the literature address this problem without taking into account the fact that different activities can have very different typical execution times. In this paper, we propose a methodology for estimating time-on-task that starts with a well-defined data consolidation and then applies an outlier detection strategy to the data based on a distinct study of each learning activity and its peculiarities. Our real-world data experiments show that the proposed methodology outperforms the current state of the art, providing more accurate time estimations for students’ learning tasks.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {336–346},
numpages = {11},
keywords = {Time-on-task, Outlier detection., Learning log data, Data pre-processing, Data consolidation},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3506860.3506912,
author = {Shibani, Antonette and Knight, Simon and Buckingham Shum, Simon},
title = {Questioning learning analytics? Cultivating critical engagement as student automated feedback literacy},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506912},
doi = {10.1145/3506860.3506912},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {326–335},
numpages = {10},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3506860.3506911,
author = {Lahza, Hatim and Khosravi, Hassan and Demartini, Gianluca and Gasevic, Dragan},
title = {Effects of Technological Interventions for Self-regulation: A Control Experiment in Learnersourcing},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506911},
doi = {10.1145/3506860.3506911},
abstract = {The benefits of incorporating scaffolds that promote strategies of self-regulated learning (SRL) to help student learning are widely studied and recognised in the literature. However, the best methods for incorporating them in educational technologies and empirical evidence about which scaffolds are most beneficial to students are still emerging. In this paper, we report our findings from conducting an in-the-field controlled experiment with 797 post-secondary students to evaluate the impact of incorporating scaffolds for promoting SRL strategies in the context of assisting students in creating novel content, also known as learnersourcing. The experiment had five conditions, including a control group that had access to none of the scaffolding strategies for creating content, three groups each having access to one of the scaffolding strategies (planning, externally-facilitated monitoring and self-assessing) and a group with access to all of the aforementioned scaffolds. The results revealed that the addition of the scaffolds for SRL strategies increased the complexity and effort required for creating content, were not positively assessed by learners and led to slight improvements in the quality of the generated content. We discuss the implications of our findings for incorporating SRL strategies in educational technologies.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {542–548},
numpages = {7},
keywords = {software-based scaffolding, self-regulation, metacognition, learnersourcing},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3506860.3506910,
author = {Sarmiento, Juan Pablo and Wise, Alyssa Friend},
title = {Participatory and Co-Design of Learning Analytics: An Initial Review of the Literature},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506910},
doi = {10.1145/3506860.3506910},
abstract = {Participatory Design (PD), and Co-design (Co-D), can be effective ways to improve technological innovation and to incorporate users’ needs in the development of learning analytics (LA). However, these methods can be difficult to implement and there has yet to be a synopsis of how its and techniques have been applied to the specific needs of LA. This study reviewed 90 papers that described 52 cases of PD of LA between 2010 and 2020 to address the research question “How is participatory design (PD) being used within LA?”. It focuses on examining which groups of participants are normally included in PD for LA, in what phases of the design process it is used, and what specific tools and techniques have LA designers adapted or developed to co-create with design partners. Findings show that there is a growing number of researchers using these methods in recent years, particularly in higher education and with instructor stakeholders. However, it was also found that often the literature would describe the PD activities only superficially, and that some aspects of PD, such as recruitment, were seldom considered overtly in the descriptions of these processes.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {535–541},
numpages = {7},
keywords = {Participatory Design, Literature Review, Learning Analytics, Co-Design},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3506860.3506908,
author = {Iqbal, Sehrish and Swiecki, Zachari and Joksimovic, Srecko and Ferreira Mello, Rafael and Aljohani, Naif and Ul Hassan, Saeed and Gasevic, Dragan},
title = {Uncovering Associations Between Cognitive Presence and Speech Acts: A Network-Based Approach},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506908},
doi = {10.1145/3506860.3506908},
abstract = {This research aimed to explore the relationship between different indicators of the depth and quality of participation in computer-mediated learning environments. By using network analyses and statistical tests, we discovered significant associations between the cognitive presence phases of the Community of Inquiry framework and speech acts, and examined the impact of two different instructional interventions on these associations. We found that there are strong associations between some speech acts and cognitive presence phases. In addition, the study revealed that the association between speech acts and cognitive presence is moderated by external facilitation, but not affected by user role assignment. The results suggest that speech acts can plausibly be used to provide feedback in relation to cognitive presence and can potentially be used to increase the generalizability of cognitive presence classification.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {315–325},
numpages = {11},
keywords = {speech acts, epistemic network analysis, discussion forums, discourse analysis, community of inquiry, cognitive presence},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3506860.3506907,
author = {Yeckehzaare, Iman and Mulligan, Victoria and Ramstad, Grace and Resnick, Paul},
title = {Semester-level Spacing but Not Procrastination Affected Student Exam Performance},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506907},
doi = {10.1145/3506860.3506907},
abstract = {Spacing and procrastination are often thought of as opposites. It is possible, however, for a student to space their studying by doing something every day throughout the semester and still procrastinate by waiting until late in the semester to increase their amount of studying. To analyze the relationship between spacing and procrastination, we examined 674 students’ interactions with a course eBook over four semesters of an introductory programming course. We measured each student’s semester-level spacing as the number of days they interacted with the eBook, and each student’s semester-level procrastination as the average delay from the start of the semester for all their eBook interactions. Surprisingly, there was a small, yet positive, correlation between the two measures. Which, then, matters for course performance: studying over more days or studying earlier in the semester? When controlling for total amount of studying, as well as a number of academic and demographic characteristics in an SEM analysis, we find a strong positive effect of spacing but no significant effect of procrastination on final exam scores.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {304–314},
numpages = {11},
keywords = {timing behavior, study behavior, spacing, scheduling, procrastination, massing, incentivized spacing, desirable difficulties, cramming, computer science education},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3506860.3506906,
author = {L. Leite, Walter and Roy, Samrat and Chakraborty, Nilanjana and Michailidis, George and Huggins-Manley, A. Corinne and D'Mello, Sidney and Shirani Faradonbeh, Mohamad Kazem and Jensen, Emily and Kuang, Huan and Jing, Zeyuan},
title = {A novel video recommendation system for algebra: An effectiveness evaluation study},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506906},
doi = {10.1145/3506860.3506906},
abstract = {This study presents a novel video recommendation system for an algebra virtual learning environment (VLE) that leverages ideas and methods from engagement measurement, item response theory, and reinforcement learning. Following Vygotsky's Zone of Proximal Development (ZPD) theory, but considering low affect and high affect students separately, we developed a system of five categories of video recommendations: 1) Watch new video; 2) Review current topic video with a new tutor; 3) Review segment of current video with current tutor; 4) Review segment of current video with a new tutor; 5) Watch next video in curriculum sequence. The category of recommendation was determined by student scores on a quiz and a sensor-free engagement detection model. New video recommendations (i.e., category 1) were selected based on a novel reinforcement learning algorithm that takes input from an item response theory model. The recommendation system was evaluated in a large field experiment, both before and after school closures due to the COVID-19 pandemic. The results show evidence of effectiveness of the video recommendation algorithm during the period of normal school operations, but the effect disappears after school closures. Implications for teacher orchestration of technology for normal classroom use and periods of school closure are discussed.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {294–303},
numpages = {10},
keywords = {recommender system, item response theory, engagement detection, effectiveness study, algebra},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3506860.3506905,
author = {Li, Qiujie and Jung, Yeonji and d'Anjou, Bernice and Wise, Alyssa Friend},
title = {Unpacking Instructors’ Analytics Use: Two Distinct Profiles for Informing Teaching},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506905},
doi = {10.1145/3506860.3506905},
abstract = {This study addresses the gap in knowledge about differences in how instructors use analytics to inform teaching by examining the ways that thirteen college instructors engaged with a set of university-provided analytics. Using multiple walk-through interviews with the instructors and qualitative inductive coding, two profiles of instructor analytics use were identified that were distinct from each other in terms of the goals of analytics use, how instructors made sense of and took actions upon the analytics, and the ways that ethical concerns were conceived. Specifically, one group of instructors used analytics to help students get aligned to and engaged in the course, whereas the other group used analytics to align the course to meet students’ needs. Instructors in both profiles saw ethical questions as central to their learning analytics use, with instructors in one profile focusing on transparency and the other on student privacy and agency. These findings suggest the need to view analytics use as an integrated component of instructor teaching practices and envision complementary sets of technical and pedagogical support that can best facilitate the distinct activities aligned with each profile.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {528–534},
numpages = {7},
keywords = {Teacher inquiry, Instructional dashboards, Data-informed teaching},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3506860.3506904,
author = {Lin, Jionghao and Rakovic, Mladen and Lang, David and Gasevic, Dragan and Chen, Guanliang},
title = {Exploring the Politeness of Instructional Strategies from Human-Human Online Tutoring Dialogues},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506904},
doi = {10.1145/3506860.3506904},
abstract = {Existing research indicates that students prefer to work with tutors who express politely in online human-human tutoring, but excessive polite expressions might lower tutoring efficacy. However, there is a shortage of understanding about the use of politeness in online tutoring and the extent to which the politeness of instructional strategies can contribute to students’ achievement. To address these gaps, we conducted a study on a large-scale dataset (5,165 students and 116 qualified tutors in 18,203 online tutoring sessions) of both effective and ineffective human-human online tutorial dialogues. The study made use of a well-known dialogue act coding scheme to identify instructional strategies, relied on the linguistic politeness theory to analyse the politeness levels of the tutors’ instructional strategies, and utilised Gradient Tree Boosting to evaluate the predictive power of these politeness levels in revealing students’ problem-solving performance. The results demonstrated that human tutors used both polite and non-polite expressions in the instructional strategies. Tutors were inclined to express politely in the strategy of providing positive feedback but less politely while providing negative feedback and asking questions to evaluate students’ understanding. Compared to the students with prior progress, tutors provided more polite open questions to the students without prior progress but less polite corrective feedback. Importantly, we showed that, compared to previous research, the accuracy of predicting student problem-solving performance can be improved by incorporating politeness levels of instructional strategies with other documented predictors (e.g., the sentiment of the utterances).},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {282–293},
numpages = {12},
keywords = {Student Performance, Prediction, Politeness, Learning Analytics, Educational Dialogue Analysis},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3506860.3506903,
author = {Benedict, Aileen and Al-Hossami, Erfan and Dorodchi, Mohsen and Benedict, Alexandria and Wiktor, Sandra},
title = {Pilot Recommender System Enabling Students to Indirectly Help Each Other and Foster Belonging Through Reflections},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506903},
doi = {10.1145/3506860.3506903},
abstract = {Without a sense of belonging, students may become disheartened and give up when faced with new challenges. Moreover, with the sudden growth of remote learning due to COVID-19, it may be even more difficult for students to feel connected to the course and peers in isolation. Therefore, we propose a recommendation system to build connections between students while recommending solutions to challenges. This pilot system utilizes students’ reflections from previous semesters, asking about learning challenges and potential solutions. It then generates sentence embeddings and calculates cosine similarities between the challenges of current and prior students. The possible solutions given by previous students are then recommended to present students with similar challenges. Self-reflection encourages students to think deeply about their learning experiences and benefit both learners and instructors. This system has the potential to allow reflections also to help future learners. By demonstrating that previous students encountered and overcame similar challenges, we could help improve students’ sense of belonging. We then perform user studies to evaluate this system’s potential and find that participants rated 70% of the recommended solutions as useful. Our findings suggest an increase in students’ sense of membership and acceptance, and a decrease in the desire to withdraw.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {521–527},
numpages = {7},
keywords = {student success, sense of belonging, semantic similarity, educational recommender systems},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3506860.3506902,
author = {Vasquez Verdugo, Jonathan and Gitiaux, Xavier and Ortega, Cesar and Rangwala, Huzefa},
title = {FairEd: A Systematic Fairness Analysis Approach Applied in a Higher Educational Context},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506902},
doi = {10.1145/3506860.3506902},
abstract = {Higher education institutions increasingly rely on machine learning models. However, a growing body of evidence shows that these algorithms may not serve underprivileged communities well and at times discriminate against them. This is all the more concerning in education as negative outcomes have long-term implications. We propose a systematic process for framing, detecting, documenting, and reporting unfairness risks. The systematic approach’s outcomes are merged into a framework named FairEd, which would help decision-makers to understand unfairness risks along the environmental and analytical fairness dimension. The tool allows to decide (i) whether the dataset contains risks of unfairness; (ii) how the models could perform along many fairness dimensions; (iii) whether potentially unfair outcomes can be mitigated without degrading performance. The systematic approach is applied to a Chilean University case study, where a predicting student dropout model is aimed to build. First, we capture the nuances of the Chilean context where unfairness emerges along income lines and demographic groups. Second, we highlight the benefit of reporting unfairness risks along a diverse set of metrics to shed light on potential discrimination. Third, we find that measuring the cost of fairness is an important quantity to report on when doing the model selection.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {271–281},
numpages = {11},
keywords = {student dropout, educational data mining, algorithmic fairness},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3506860.3506901,
author = {Krumm, Andrew and Coulson, Andrew and Neisler, Julie},
title = {Defining Productive Struggle in ST Math: Implications for Developing Indicators of Learning Behaviors and Strategies in Digital Learning Environments},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506901},
doi = {10.1145/3506860.3506901},
abstract = {This paper describes a process for operationally defining productive struggle in a widely used digital learning environment called ST Math. The process for developing an operational definition involved examining the existing literature for ways in which researchers have previously quantified productive struggle in digital learning environments. Using prior research, we defined productive struggle as a student persisting in a digital learning task while maintaining a likelihood of future success. To develop a machine-executable definition of productive struggle, we identified the typical number of attempts learners needed to complete a level in ST Math and applied a modified Performance Factors Analysis algorithm to estimate learners’ probability of success on a subsequent puzzle attempt within a level. Using definitions that differentially combined re-attempts and predicted probabilities, we examined the proportion of level attempts that could be newly classified as instances of productive struggle. The pragmatic approach described in this paper is intended to serve as an example for other digital learning environments seeking to develop indicators of productive struggle.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {514–520},
numpages = {7},
keywords = {predictive modeling, Productive struggle, Performance Factors Analysis},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3506860.3506900,
author = {Williamson, Kimberly and Kizilcec, Rene},
title = {A Review of Learning Analytics Dashboard Research in Higher Education: Implications for Justice, Equity, Diversity, and Inclusion},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506900},
doi = {10.1145/3506860.3506900},
abstract = {Learning analytics dashboards (LADs) are becoming more prevalent in higher education to help students, faculty, and staff make data-informed decisions. Despite extensive research on the design and implementation of LADs, few studies have investigated their relation to justice, equity, diversity, and inclusion (JEDI). Excluding these issues in LAD research limits the potential benefits of LADs generally and risks reinforcing long-standing inequities in education. We conducted a critical literature review, identifying 45 relevant papers to answer three research questions: how is LAD research improving JEDI, ii. how might it maintain or exacerbate inequitable outcomes, and iii. what opportunities exist in this space to improve JEDI in higher education. Using thematic analysis, we identified four common themes: (1) participant identities and researcher positionality, (2) surveillance concerns, (3) implicit pedagogies, and (4) software development resources. While we found very few studies directly addressing or mentioning JEDI concepts, we used these themes to explore ways researchers could consider JEDI in their studies. Our investigation highlights several opportunities to intentionally incorporate JEDI into LAD research by sharing software resources and conducting cross-border collaborations, better incorporating user needs, and centering considerations of justice in LAD efforts to improve historical inequities.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {260–270},
numpages = {11},
keywords = {Literature Review, Justice, Inclusion, Higher Education, Equity, Diversity, Dashboards},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3506860.3506899,
author = {Hicks, Ben and Kitto, Kirsty and Payne, Leonie and Buckingham Shum, Simon},
title = {Thinking with causal models: A visual formalism for collaboratively crafting assumptions},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506899},
doi = {10.1145/3506860.3506899},
abstract = {Learning Analytics (LA) is a bricolage field that requires a concerted effort to ensure that all stakeholders it affects are able to contribute to its development in a meaningful manner. We need mechanisms that support collaborative sense-making. This paper argues that graphical causal models can help us to span the disciplinary divide, providing a new apparatus to help educators understand, and potentially challenge, the technical models developed by LA practitioners as they form. We briefly introduce causal modelling, highlighting its potential benefits in helping the field to move from associations to causal claims, and illustrate how graphical causal models can help us to reason about complex statistical models. The approach is illustrated by applying it to the well known problem of at-risk modelling.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {250–259},
numpages = {10},
keywords = {transdisciplinary collaboration, directed acyclic graphs, diagrammatic reasoning, causal models},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3506860.3506898,
author = {Boothe, Maurice and Yu, Collin and Lewis, Armanda and Ochoa, Xavier},
title = {Towards a Pragmatic and Theory-Driven Framework for Multimodal Collaboration Feedback},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506898},
doi = {10.1145/3506860.3506898},
abstract = {This paper proposes an overarching framework for automated collaboration feedback that bridges theory and tool as well as technology and pedagogy. This pragmatic and theory-driven framework guides our thinking by outlining the components involved in converting theoretical collaboration constructs into features that can be automatically extracted and then converted into actionable feedback. Focusing on the pedagogical components of the framework, the constructs are validated by mapping them onto a selection of multi-disciplinary collaboration frameworks. The resulting behavioral indicators are then applied to measure collaboration in a sample scenario and those measurements are then used to exemplify how feedback analytics could be calculated. The paper concludes with a discussion on how those analytics could be converted into feedback for students and the next steps needed to advance the technological part of the framework.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {507–513},
numpages = {7},
keywords = {teaching collaboration, learning collaboration, collaboration analytics},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3506860.3506897,
author = {Nicoll, Serena and Douglas, Kerrie and Brinton, Christopher},
title = {Giving Feedback on Feedback: An Assessment of Grader Feedback Construction on Student Performance},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506897},
doi = {10.1145/3506860.3506897},
abstract = {Feedback is a critical element of student-instructor interaction: it provides a direct manner for students to learn from mistakes. However, with student to teacher ratios growing rapidly, challenges arise for instructors to provide quality feedback to individual students. While significant efforts have been directed at automating feedback generation, relatively little attention has been given to underlying feedback characteristics. We develop a methodology for analyzing instructor-provided feedback and determining how it correlates with changes in student grades using data from online higher education engineering classrooms. Specifically, we featurize written feedback on individual assignments using Natural Language Processing (NLP) techniques including sentiment analysis, bigram splitting, and Named Entity Recognition (NER) to quantify post-, sentence-, and word-dependent attributes of grader writing. We demonstrate that student grade improvement can be well approximated by a multivariate linear model with average fits across course sections between 67% and 83%. We determine several statistically significant contributors to and detractors from student success contained in instructor feedback. For example, our results reveal that inclusion of student name is significantly correlated with an improvement in post-feedback grades, as is inclusion of specific assignment-related keywords. Finally, we discuss how this methodology can be incorporated into educational technology systems to make recommendations for feedback content from observed student behavior.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {239–249},
numpages = {11},
keywords = {Student engagement, Learning analytics, Instructor feedback},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3506860.3506896,
author = {Hunkins, Nicholas and Kelly, Sean and D'Mello, Sidney},
title = {“Beautiful work, you're rock stars!”: Teacher Analytics to Uncover Discourse that Supports or Undermines Student Motivation, Identity, and Belonging in Classrooms},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506896},
doi = {10.1145/3506860.3506896},
abstract = {From carefully crafted messages to flippant remarks, warm expressions to unfriendly grunts, teachers’ behaviors set the tone, expectations, and attitudes of the classroom. Thus, it is prudent to identify the ways in which teachers foster motivation, positive identity, and a strong sense of belonging through inclusive messaging and other interactions. We leveraged a new coding of teacher supportive discourse in 156 video clips from 73 6th to 8th grade math teachers from the archival Measures of Effective Teaching (MET) project. We trained Random Forest classifiers using verbal (words used) and paraverbal (acoustic-prosodic cues, e.g., speech rate) features to detect seven features of teacher discourse (e.g., public admonishment, autonomy supportive messages) from transcripts and audio, respectively. While both modalities performed over chance guessing, the specific language content was more predictive than paraverbal cues (mean correlation = .546 vs. .276); combining the two yielded no improvement. We examined the most predictive cues in order to gain a deeper understanding of the underlying messages in teacher talk. We discuss implications of our work for teacher analytics tools that aim to provide educators and researchers with insight into supportive discourse.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {230–238},
numpages = {9},
keywords = {Teacher Analytics, Natural Language Processing, Discourse Analysis, Automated Feedback},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3506860.3506895,
author = {Fernandez Nieto, Gloria Milena and Kitto, Kirsty and Buckingham Shum, Simon and Martinez-Maldonado, Roberto},
title = {Beyond the Learning Analytics Dashboard: Alternative Ways to Communicate Student Data Insights Combining Visualisation, Narrative and Storytelling},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506895},
doi = {10.1145/3506860.3506895},
abstract = {Learning Analytics (LA) dashboards have become a popular medium for communicating to teachers analytical insights obtained from student data. However, recent research indicates that LA dashboards can be complex to interpret, are often not grounded in educational theory, and frequently provide little or no guidance on how to interpret them. Despite these acknowledged problems, few suggestions have been made as to how we might improve the visual design of LA tools to support richer and alternative ways to communicate student data insights. In this paper, we explore three design alternatives to represent student multimodal data insights by combining data visualisation, narratives and storytelling principles. Based on foundations in data storytelling, three visual-narrative interfaces were designed with teachers: i) visual data slices, ii) a tabular visualisation, and iii) a written report. These were validated as a part of an authentic study where teachers explored activity logs and physiological data from co-located collaborative learning classes in the context of healthcare education. Results suggest that alternatives to LA dashboards can be considered as effective tools to support teachers’ reflection, and that LA designers should identify the representation type that best fits teachers’ needs.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {219–229},
numpages = {11},
keywords = {visual learning analytics, qualitative analysis, multimodal data},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3506860.3506894,
author = {Pugh, Samuel L. and Rao, Arjun and Stewart, Angela E.B. and D'Mello, Sidney K.},
title = {Do Speech-Based Collaboration Analytics Generalize Across Task Contexts?},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506894},
doi = {10.1145/3506860.3506894},
abstract = {We investigated the generalizability of language-based analytics models across two collaborative problem solving (CPS) tasks: an educational physics game and a block programming challenge. We analyzed a dataset of 95 triads (N=285) who used videoconferencing to collaborate on both tasks for an hour. We trained supervised natural language processing classifiers on automatic speech recognition transcripts to predict the human-coded CPS facets (skills) of constructing shared knowledge, negotiation / coordination, and maintaining team function. We tested three methods for representing collaborative discourse: (1) deep transfer learning (using BERT), (2) n-grams (counts of words/phrases), and (3) word categories (using the Linguistic Inquiry Word Count [LIWC] dictionary). We found that the BERT and LIWC methods generalized across tasks with only a small degradation in performance (Transfer Ratio of .93 with 1 indicating perfect transfer), while the n-grams had limited generalizability (Transfer Ratio of .86), suggesting overfitting to task-specific language. We discuss the implications of our findings for deploying language-based collaboration analytics in authentic educational environments.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {208–218},
numpages = {11},
keywords = {Natural language processing, Collaborative problem solving, Collaboration analytics},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3506860.3506893,
author = {Chen, Bodong and Zhu, Xinran and Shui, Hong},
title = {Socio-Semantic Network Motifs Framework for Discourse Analysis},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506893},
doi = {10.1145/3506860.3506893},
abstract = {Effective collaborative discourse requires both cognitive and social engagement of students. To investigate complex socio-cognitive dynamics in collaborative discourse, this paper proposes to model collaborative discourse as a socio-semantic network (SSN) and then use network motifs – defined as recurring, significant subgraphs – to characterize the network and hence the discourse. To demonstrate the utility of our SSN motifs framework, we applied it to a sample dataset. While more work needs to be done, the SSN motifs framework shows promise as a novel, theoretically informed approach to discourse analysis.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {500–506},
numpages = {7},
keywords = {two-mode networks, networks, discourse, collaboration},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3506860.3506892,
author = {Dood, Amber and Winograd, Blair and Finkenstaedt-Quinn, Solaire and Gere, Anne and Shultz, Ginger},
title = {PeerBERT: Automated Characterization of Peer Review Comments across Courses},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506892},
doi = {10.1145/3506860.3506892},
abstract = {Writing-to-learn pedagogies are an evidence-based practice known to aid students in constructing knowledge. Barriers exist for the implementation of such assignments; namely, instructors feel they do not have time to provide each student with feedback. To ease implementation of writing-to-learn assignments at scale, we have incorporated automated peer review, which facilitates peer review without input from the instructor. Participating in peer review can positively impact students’ learning and allow students to receive feedback on their writing. Instructors may want to monitor these peer interactions and gain insight into their students’ understanding using the feedback generated by their peers. To facilitate instructors’ use of the content from students’ peer review comments, we pre-trained a transformer model called PeerBERT. PeerBERT was fine-tuned on several downstream tasks to categorize students’ peer review comments as praise, problem/solution, or verification/summary. The model exhibits high accuracy, even across different peer review prompts, assignments, and courses. Additional downstream tasks label problem/solution peer review comments as one or more types: writing/formatting, missing content/needs elaboration, and incorrect content. This approach can help instructors pinpoint common issues in student writing by parsing out which comments are problem/solution and which type of problem/solution students identify.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {492–499},
numpages = {8},
keywords = {writing-to-learn, undergraduate education, peer review},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3506860.3506891,
author = {Matz, Rebecca L and Lee, Albert M and Fowler, Robin R and Hayward, Caitlin},
title = {Teammates Stabilize over Time in How They Evaluate Their Team Experiences},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506891},
doi = {10.1145/3506860.3506891},
abstract = {It is difficult for instructors, and even students themselves, to become aware in real-time of inequitable behaviors occurring on student teams. Here, we explored a potential measure for inequitable teamwork drawing on data from a digital pedagogical tool designed to surface and disrupt such team behaviors. Students in a large, undergraduate business course completed seven surveys about team health (called team checks) at regular intervals throughout the term, providing information about team dynamics, contributions, and processes. The ways in which changes in students’ scores from team check to team check compared to the median changes for their team were used to identify the proportions of teams with outlier student scores. The results show that for every team size and team check item, the proportion of teams with outliers at the end of the term was smaller than at the beginning of the semester, indicating stabilization in how teammates evaluated their team experiences. In all but two cases, outlying students were not disproportionately likely to identify with historically marginalized groups based on gender or race/ethnicity. Thus, we did not broadly identify teamwork inequities in this specific context, but the method provides a basis for future studies about inequitable team behavior.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {485–491},
numpages = {7},
keywords = {undergraduate education, teamwork, race/ethnicity, peer evaluation, learning analytics, higher education, groupwork, gender, equity, Educational technology},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3506860.3506890,
author = {Echeverria, Vanessa and Wong-Villacres, Marisol and Ochoa, Xavier and Chiluiza, Katherine},
title = {An Exploratory Evaluation of a Collaboration Feedback Report},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506890},
doi = {10.1145/3506860.3506890},
abstract = {Providing formative feedback to foster collaboration and improve students’ practice has been an emerging topic in CSCL and LA research communities. However, this pedagogical practice could be unrealistic in authentic classrooms, as observing and annotating improvements for every student and group exceeds the teacher’s capabilities. In the research area of group work and collaborative learning, current learning analytics solutions have reported accurate computational models to understand collaboration processes, yet evaluating formative collaboration feedback, where the final user is the student, is an under-explored research area. This paper reports an exploratory evaluation to understand the effects a collaboration feedback report through an authentic study conducted in regular classes. Fifty students from a Computer Science undergraduate program participated in the study. We followed an user-centered design approach to define six collaboration aspects that are relevant to students. These aspects were part of initial prototypes for the feedback report. From the exploratory intervention, we did not find effects between students who received the feedback (experimental condition) report and those who did not (control condition). Finally, this paper discusses design implications for further feedback report designs and interventions.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {478–484},
numpages = {7},
keywords = {human-centered design, collaboration feedback report, collaboration analytics},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3506860.3506889,
author = {Li, Jiayu and Li, Huiyong and Majumdar, Rwitajit and Yang, Yuanyuan and Ogata, Hiroaki},
title = {Self-directed Extensive Reading Supported with GOAL System: Mining Sequential Patterns of Learning Behavior and Predicting Academic Performance},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506889},
doi = {10.1145/3506860.3506889},
abstract = {Self-directed learning (SDL) is an important skill in the 21st century, while the understanding of its process in behavior has not been well explored. Analysis of the sequential behavior patterns in SDL and the relations with students’ academic performance could help to advance our understanding of SDL in theory and practice. In this study, we mined the behavioral sequences of self-directed extensive reading from students’ learning and self-directed behavioral logs using differential pattern mining technique. Furthermore, we built models to predict students’ academic performance using the conventional behavior frequency features and the behavior sequence features. Experimental results identified 14 sequential patterns of SDL behaviors in the high-performance student group. The prediction model revealed the importance of sequential patterns in SDL behavior, which was built with an acceptable AUC. These findings suggested that several SDL strategies in behavior contribute to students’ academic performance, such as analysis learning status before planning, planning before learning, monitoring after learning.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {472–477},
numpages = {6},
keywords = {Teaching/learning strategies, Sequence mining, Self-directed learning, Machine learning},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3506860.3506888,
author = {Hur, Paul and Bosch, Nigel},
title = {Tracking Individuals in Classroom Videos via Post-processing OpenPose Data},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506888},
doi = {10.1145/3506860.3506888},
abstract = {Analyzing classroom video data provides valuable insights about the interactions between students and teachers, albeit often through time-consuming qualitative coding or the use of bespoke sensors to record individual movement information. We explore measuring classroom posture and movement in secondary classroom video data through computer vision methods (especially OpenPose), and introduce a simple but effective approach to automatically track movement via post-processing of OpenPose output data. Analysis of 67 videos of mathematics classes from middle school and high school levels highlighted the challenges associated with analyzing movement in typical classroom videos: occlusion from low camera angles, difficulty detecting lower body movement due to sitting, and the close proximity of students to one another and their teachers. Despite these challenges, our approach tracked person IDs across classroom videos for 93.0% of detected individuals. The tracking results were manually verified through randomly sampling 240 instances, which revealed notable OpenPose tracking inconsistencies. Finally, we discuss the implications for supporting more scalability of video data classroom movement analysis, and future potential explorations.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {465–471},
numpages = {7},
keywords = {video analysis, posture, movement, classroom video},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3506860.3506887,
author = {Zhang, Tom and Taub, Michelle and Chen, Zhongzhou},
title = {A Multi-Level Trace Clustering Analysis Scheme for Measuring Students’ Self-Regulated Learning Behavior in a Mastery-Based Online Learning Environment},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506887},
doi = {10.1145/3506860.3506887},
abstract = {This study introduces a new analysis scheme to analyze trace data and visualize students’ self-regulated learning strategies in a mastery-based online learning modules platform. The pedagogical design of the platform resulted in fewer event types and less variability in student trace data. The current analysis scheme overcomes those challenges by conducting three levels of clustering analysis. On the event level, mixture-model fitting is employed to distinguish between abnormally short and normal assessment attempts and study events. On the module level, trace level clustering is performed with three different methods for generating distance metrics between traces, with the best performing output used in the next step. On the sequence level, trace level clustering is performed on top of module-level clusters to reveal students’ change of learning strategy over time. We demonstrated that distance metrics generated based on learning theory produced better clustering results than pure data-driven or hybrid methods. The analysis showed that most students started the semester with productive learning strategies, but a significant fraction shifted to a multitude of less productive strategies in response to increasing content difficulty and stress. The observations could prompt instructors to rethink conventional course structure and implement interventions to improve self-regulation at optimal times.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {197–207},
numpages = {11},
keywords = {Self-regulated learning, Online learning environments, Click-stream data},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3506860.3506886,
author = {Pontual Falc\~{a}o, Taciana and Lins Rodrigues, Rodrigo and Cechinel, Cristian and Dermeval, Diego and Harada Teixeira de Oliveira, Elaine and Gasparini, Isabela and D. Ara\'{u}jo, Rafael and Primo, Tiago and Gasevic, Dragan and Ferreira Mello, Rafael},
title = {A Penny for your Thoughts: Students and Instructors’ Expectations about Learning Analytics in Brazil},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506886},
doi = {10.1145/3506860.3506886},
abstract = {Stakeholder engagement is a key aspect for the successful implementation of Learning Analytics (LA) in Higher Education Institutions (HEIs). Studies in Europe and Latin America (LATAM) indicate that, overall, instructors and students have positive views on LA adoption, but there are differences between their ideal expectations and what they consider realistic in the context of their institutions. So far, very little has been found about stakeholders’ views on LA in Brazilian higher education. By replicating the survey conducted in other countries, in seven Brazilian HEIs, we found convergences both with Europe and LATAM, reinforcing the need for local diagnosis and indicating the risk of assuming a ”LATAM identity”. Our findings contribute to building a corpus of knowledge on stakeholders expectations with a contextualised comprehension of the gaps between ideal and predicted scenarios, which can inform institutional policies for LA implementation in Brazil.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {186–196},
numpages = {11},
keywords = {stakeholder perspective, institutional adoption, higher education, Learning analytics},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3506860.3506885,
author = {Pozdniakov, Stanislav and Martinez-Maldonado, Roberto and Tsai, Yi-Shan and Cukurova, Mutlu and Bartindale, Tom and Chen, Peter and Marshall, Harrison and Richardson, Dan and Gasevic, Dragan},
title = {The Question-driven Dashboard: How Can We Design Analytics Interfaces Aligned to Teachers’ Inquiry?},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506885},
doi = {10.1145/3506860.3506885},
abstract = {One of the ultimate goals of several learning analytics (LA) initiatives is to close the loop and support students’ and teachers’ reflective practices. Although there has been a proliferation of end-user interfaces (often in the form of dashboards), various limitations have already been identified in the literature such as key stakeholders not being involved in their design, little or no account for sense-making needs, and unclear effects on teaching and learning. There has been a recent call for human-centred design practices to create LA interfaces in close collaboration with educational stakeholders to consider the learning design, and their authentic needs and pedagogical intentions. This paper addresses the call by proposing a question-driven LA design approach to ensure that end-user LA interfaces explicitly address teachers’ questions. We illustrate the approach in the context of synchronous online activities, orchestrated by pairs of teachers using audio-visual and text-based tools (namely Zoom and Google Docs). This study led to the design and deployment of an open-source monitoring tool to be used in real-time by teachers when students work collaboratively in breakout rooms, and across learning spaces.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {175–185},
numpages = {11},
keywords = {online learning, learning analytics, inquiry-driven practice, human-centred design, dashboard, CSCL},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3506860.3506883,
author = {Xia, Meng and Zhao, Yankun and Erol, Mehmet Hamza and Hong, Jihyeong and Kim, Juho},
title = {Understanding Distributed Tutorship in Online Language Tutoring},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506883},
doi = {10.1145/3506860.3506883},
abstract = {With the rise of the gig economy, online language tutoring platforms are becoming increasingly popular. They provide temporary and flexible jobs for native speakers as tutors and allow language learners to have one-on-one speaking practices on demand. However, the lack of stable relationships hinders tutors and learners from building long-term trust. “Distributed tutorship”—temporally discontinuous learning experience with different tutors—has been underexplored yet has many implications for modern learning platforms. In this paper, we analyzed tutorship sequences of 15,959 learners and found that around 40% of learners change to new tutors every session; 44% learners change to new tutors while reverting to previous tutors sometimes; only 16% learners change to new tutors and then fix on one tutor. We also found suggestive evidence that higher distributedness—higher diversity and lower continuity in tutorship—is correlated to slower improvements in speaking performance scores with a similar number of sessions. We further surveyed 519 and interviewed 40 learners and found that more learners preferred fixed tutorship while some do not have it due to various reasons. Finally, we conducted semi-structured interviews with three tutors and one product manager to discuss the implications for improving the continuity in learning under distributed tutorship.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {164–174},
numpages = {11},
keywords = {online tutoring platforms, lifelong learning, learning analytics, language learning, distributed tutorship},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3506860.3506882,
author = {Takami, Kyosuke and Dai, Yiling and Flanagan, Brendan and Ogata, Hiroaki},
title = {Educational Explainable Recommender Usage and its Effectiveness in High School Summer Vacation Assignment},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506882},
doi = {10.1145/3506860.3506882},
abstract = {Explainable recommendations, which provide explanations about why an item is recommended, help to improve the transparency, persuasiveness, and trustworthiness. However, few research in educational technology utilize explainable recommendations. We developed an explanation generator using the parameters from Bayesian knowledge tracing models. We used this educational explainable recommendation system to investigate the effects of explanation on the summer vacation assignment for high school students. Comparing the click counts of recommended quizzes with and without explanations, we found that the number of clicks was significantly higher for quizzes with explanations. Furthermore, system usage pattern mining revealed that students can be divided to three clusters— none, steady and late users. In the cluster of steady users, recommended quizzes with explanations were continuously used. These results suggest the effectiveness of an explainable recommendation system in the field of education.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {458–464},
numpages = {7},
keywords = {Pattern mining, Long vacation period, Explainable recommendation, Effectiveness of explanation, A/B test},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3506860.3506881,
author = {Tzi-Dong Ng, Jeremy and Hu, Xiao and Que, Ying},
title = {Towards Multi-modal Evaluation of Eye-tracked Virtual Heritage Environment},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506881},
doi = {10.1145/3506860.3506881},
abstract = {In times of pandemic-induced challenges, virtual reality (VR) allows audience to learn about cultural heritage sites without temporal and spatial constraints. The design of VR content is largely determined by professionals, while evaluations of content often rely on learners’ self-report data. Learners’ attentional focus and understanding of VR content might be affected by the presence or absence of different multimedia elements including text and audio-visuals. It remains an open question which design variations are more conducive for learning about heritage sites. Leveraging eye-tracking, a technology often adopted in recent multimodal learning analytics (MmLA) research, we conducted an experiment to collect and analyze 40 learners’ eye movement and self-reported data. Results of statistical tests and heatmap elicitation interviews indicate that 1) text in the VR environment helped learners better understand the presented heritage sites, regardless of having audio narration or not, 2) text diverted learners’ attention away from other visual elements that contextualized the heritage sites, 3) exclusively having audio narration best simulated the experience of a real-world heritage tour, 4) narration accompanying text prompted learners to read the text faster. We make recommendations for improving the design of VR learning materials and discuss the implications for MmLA research.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {451–457},
numpages = {7},
keywords = {Virtual reality, Multimodal learning analytics, Multimedia learning, Eye-tracking, Cultural heritage},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3506860.3506880,
author = {Tzi-Dong Ng, Jeremy and Wang, Zuo and Hu, Xiao},
title = {Needs Analysis and Prototype Evaluation of Student-facing LA Dashboard for Virtual Reality Content Creation},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506880},
doi = {10.1145/3506860.3506880},
abstract = {Being a promising constructionist pedagogy in recent years, maker education empowers students to take agency of their learning process through constructing both knowledge and real-world physical or digital products and fosters peer interactions for collective innovation. Learning Analytics (LA) excels at generating personalized, fine-grained feedback in near real-time and holds much potential in supporting process-oriented and peer-supported learning activities, including maker activities. In the context of virtual reality (VR) content creation for cultural heritage education, this study qualitatively solicited 27 students’ needs on progress monitoring, reflection, and feedback during their making process. Findings have inspired the prototype design of a student-facing LA dashboard (LAVR). Leveraging multimodal learning analytics (MmLA) such as text and audio analytics to fulfill students’ needs, the prototype has various features and functions including automatic task reminders, content quality detection, and real-time feedback on quality of audio-visual elements. A preliminary evaluation of the prototype with 10 students confirms its potential in supporting students’ self-regulated learning during the making process and for improving the quality of VR content. Implications on LA design for supporting maker education are discussed. Future work is planned to include implementation and evaluation of the dashboard in classrooms.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {444–450},
numpages = {7},
keywords = {VR content creation, Prototype evaluation, Needs analysis, Dashboard},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3506860.3506878,
author = {Khalil, Mohammad and Prinsloo, Paul and Slade, Sharon},
title = {A Comparison of Learning Analytics Frameworks: a Systematic Review},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506878},
doi = {10.1145/3506860.3506878},
abstract = {While learning analytics frameworks precede the official launch of learning analytics in 2011, there has been a proliferation of learning analytics frameworks since. This systematic review of learning analytics frameworks between 2011 and 2021 in three databases resulted in an initial corpus of 268 articles and conference proceeding papers based on the occurrence of “learning analytics” and “framework” in titles, keywords and abstracts. The final corpus of 46 frameworks were analysed using a coding scheme derived from purposefully selected learning analytics frameworks. The results found that learning analytics frameworks share a number of elements and characteristics such as source, development and application focus, a form of representation, data sources and types, focus and context. Less than half of the frameworks consider student data privacy and ethics. Finally, while design and process elements of these frameworks may be transferable and scalable to other contexts, users in different contexts will be best-placed to determine their transferability/scalability.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {152–163},
numpages = {12},
keywords = {systematic review, literature review, framework, comparison, Learning analytics},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3506860.3506877,
author = {Vinker, Efrat and Rubinstein, Amir},
title = {Mining Code Submissions to Elucidate Disengagement in a Computer Science MOOC},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506877},
doi = {10.1145/3506860.3506877},
abstract = {Despite the growing prevalence of Massive Open Online Courses (MOOCs) in the last decade, using them effectively is still challenging. Particularly, when MOOCs involve teaching programming, learners often struggle with writing code without sufficient support, which may increase frustration, attrition, and eventually dropout. In this study, we assess the pedagogical design of a fresh introductory computer science MOOC. Keeping in mind MOOC “end-user” instructors, our analyses are based merely on features easily accessible from code submissions, and methods that are relatively simple to apply and interpret. Using visual data mining we discover common patterns of&nbsp;behavior, provide insights on content that may require reevaluation and detect critical points of attrition in the course timeline. Additionally, we extract students’ code submission profiles that reflect various aspects of engagement and performance. Consequently, we predict disengagement towards programming using classic machine learning methods. To the best of our knowledge, our definition for attrition in terms of disengagement towards programming is novel as it suits the unique active hands-on nature of programming. To our perception, the results emphasize that more attention and further research should be aimed at the pedagogical design of hands-on experience, such as programming, in online learning systems.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {142–151},
numpages = {10},
keywords = {massive open online courses (MOOCs), machine learning, learning analytics, educational data mining, code analysis, automated tutoring systems, Introductory computer science education},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3506860.3506876,
author = {Rakovic, Mladen and Fan, Yizhou and van der Graaf, Joep and Singh, Shaveen and Kilgour, Jonathan and Lim, Lyn and Moore, Johanna and Bannert, Maria and Molenaar, Inge and Gasevic, Dragan},
title = {Using Learner Trace Data to Understand Metacognitive Processes in Writing from Multiple Sources},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506876},
doi = {10.1145/3506860.3506876},
abstract = {Writing from multiple sources is a commonly administered learning task across educational levels and disciplines. In this task, learners are instructed to comprehend information from source documents and integrate it into a coherent written composition to fulfil the assignment requirements. Even though educationally potent, multi-source writing tasks are considered challenging to many learners, in particular because many learners underuse monitoring and control, critical metacognitive processes for productive engagement in multi-source writing. To understand these processes, we conducted a laboratory study involving 44 university students. They engaged in multi-source writing task hosted in digital learning environment. Adding to previous research, we unobtrusively measured metacognitive processes using learners’ trace data collected via multiple data channels and in both writing and reading space of the multi-source writing task. We further investigated how these processes affect the quality of a written product, i.e., essay score. In the analysis, we utilised both automatically and human-generated essay score. The rating performance of the essay scoring algorithm was comparable to that of human raters. Our results largely support the theoretical assumptions that engagement in metacognitive monitoring and control benefits the quality of written product. Moreover, our results can inform the development of analytics-based tools that support student writing by making use of trace data and automated essay scoring.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {130–141},
numpages = {12},
keywords = {writing from multiple sources, semantic similarity, reading, monitoring},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3506860.3506875,
author = {Chang, Xinyuan and Wang, Bingxin and Hui, Bowen},
title = {Towards an Automatic Approach for Assessing Program Competencies},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506875},
doi = {10.1145/3506860.3506875},
abstract = {Skills analysis is an interdisciplinary area that studies labor market trends and provides recommendations for developing educational standards and re-skilling efforts. We leverage techniques in this area to develop a scalable approach that identifies and evaluates educational competencies. In this work, we developed a skills extraction algorithm that uses natural language processing and machine learning techniques. We evaluated our algorithm on a labeled dataset and found its performance to be competitive with state-of-the-art methods. Using this algorithm, we analyzed student skills, university course syllabi, and online job postings. Our cross-sector analysis provides an initial landscape of skill needs for specific job titles. Additionally, we conducted a within-sector analysis based on programming jobs, computer science curriculum, and undergraduate students. Our findings suggest that students have a variety of hard skills and soft skills, but they are not necessarily the ones that employers want. The data also suggests these courses teach skills that are somewhat different from industry needs, and there is a lack of emphasis on soft skills. These results provide an initial assessment of the program competencies for a computer science program. Future work includes more data gathering, improving the algorithm, and applying our method to assess additional educational programs.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {119–129},
numpages = {11},
keywords = {text classification, skills extraction, linguistic patterns, gap analysis, competency-based education, Job skills},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3506860.3506874,
author = {Zavaleta Bernuy, Angela and Han, Ziwen and Shaikh, Hammad and Zheng, Qi Yin and Lim, Lisa-Angelique and Rafferty, Anna and Petersen, Andrew and Williams, Joseph Jay},
title = {How can Email Interventions Increase Students’ Completion of Online Homework? A Case Study Using A/B Comparisons},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506874},
doi = {10.1145/3506860.3506874},
abstract = {Email communication between instructors and students is ubiquitous, and it could be valuable to explore ways of testing out how to make email messages more impactful. This paper explores the design space of using emails to get students to plan and reflect on starting weekly homework earlier. We deployed a series of email reminders using randomized A/B comparisons to test alternative factors in the design of these emails, providing examples of an experimental paradigm and metrics for a broader range of interventions. We also surveyed and interviewed instructors and students to compare their predictions about the effectiveness of the reminders with their actual impact. We present our results on which seemingly obvious predictions about effective emails are not borne out, despite there being evidence for further exploring these interventions, as they can sometimes motivate students to attempt their homework more often. We also present qualitative evidence about student opinions and behaviours after receiving the emails, to guide further interventions. These findings provide insight into how to use randomized A/B comparisons in everyday channels such as emails, to provide empirical evidence to test our beliefs about the effectiveness of alternative design choices.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {107–118},
numpages = {12},
keywords = {Reminder, Randomized experiments, Procrastination, Embedded experimentation, A/B comparisons},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3506860.3506873,
author = {Zhang, Qian and Rutherford, Teomara},
title = {Grade 5 Students’ Elective Replay After Experiencing Failures in Learning Fractions in an Educational Game: When Does Replay After Failures Benefit Learning?},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506873},
doi = {10.1145/3506860.3506873},
abstract = {Despite theoretical benefits of replayability in educational games, empirical studies have found mixed evidence about the effects of replaying a previously passed game (i.e., elective replay) on students’ learning. Particularly, we know little about behavioral features of students’ elective replay process after experiencing failures (i.e., interruptive elective replay) and the relationships between these features and learning outcomes. In this study, we analyzed 5th graders’ log data from an educational game, ST Math, when they studied fractions—one of the most important but challenging math topics. We systematically constructed interruptive elective replay features by following students’ sequential behaviors after failing a game and investigated the relationships between these features and students’ post-test performance, after taking into account pretest performance and in-game performance. Descriptive statistics of the features we constructed revealed individual differences in the elective replay process after failures in terms of when to start replaying, what to replay, and how to replay. Moreover, a Bayesian multi-model linear regression showed that interruptive elective replay after failures might be beneficial for students if they chose to replay previously passed games when failing at a higher, more difficult level in the current game and if they passed the replayed games.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {98–106},
numpages = {9},
keywords = {Replay, Learning Analytics, Educational Games},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3506860.3506872,
author = {Yan, Lixiang and Martinez-Maldonado, Roberto and Zhao, Linxuan and Deppeler, Joanne and Corrigan, Deborah and Gasevic, Dragan},
title = {How do Teachers Use Open Learning Spaces? Mapping from Teachers’ Socio-spatial Data to Spatial Pedagogy},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506872},
doi = {10.1145/3506860.3506872},
abstract = {Teacher’s in-class positioning and interaction patterns (termed spatial pedagogy) are an essential part of their classroom management and orchestration strategies that can substantially impact students’ learning. Yet, effective management of teachers’ spatial pedagogy can become increasingly challenging as novel architectural designs, such as open learning spaces, aim to disrupt teaching conventions by promoting flexible pedagogical approaches and maximising student connectedness. Multimodal learning analytics and indoor positioning technologies may hold promises to support teachers in complex learning spaces by making salient aspects of their spatial pedagogy visible for provoking reflection. This paper explores how granular x-y positioning data can be modelled into socio-spatial metrics that can contain insights about teachers’ spatial pedagogy across various learning designs. A total of approximately 172.63 million position data points were collected during 101 classes over eight weeks. The results illustrate how indoor positioning analytics can help generate a deeper understanding of how teachers use their learning spaces, such as their 1) teaching responsibilities; 2) proactive or passive interactions with students; and 3) supervisory, interactional, collaborative, and authoritative teaching approaches. Implications of the current findings to future learning analytics research and educational practices were also discussed.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {87–97},
numpages = {11},
keywords = {spatial pedagogy, proxemics, multimodal learning analytics, learning analytics, indoor positioning},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3506860.3506871,
author = {Pan, Zilong and Liu, Min},
title = {The effects of learning analytics hint system in supporting students problem-solving},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506871},
doi = {10.1145/3506860.3506871},
abstract = {This mixed-method study examined the impacts of a learning-analytics (LA) hints system on middle school students’ problem-solving performance and self-efficacy (SE). Students in condition A received the LA hint system, students in condition B received a static hint system that contains the same set of hints but without the LA mechanism, condition C was a control group that no hints were provided. The statistical results showed that the problem-solving SE for students who engaged with the LA hint system improved significantly. Student interviews revealed that real-time supports and in-time positive feedback played key roles in supporting their SE growth. Moreover, student-generated quantitative and qualitative log data were collected for interpreting the research outcomes. The quantitative logs provided an in-depth examination of problem-solving strategies across the conditions while the qualitative logs provided another perspective to understand students’ problem-solving status. Implications for future implementation of LA-hint system in virtual PBL environments were provided.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {77–86},
numpages = {10},
keywords = {text tagging, neural networks, gaze detection, datasets},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3506860.3506870,
author = {Zhu, Mengxia and Han, Siqi and Yuan, Peisen and Lu, Xuesong},
title = {Enhancing Programming Knowledge Tracing by Interacting Programming Skills and Student Code},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506870},
doi = {10.1145/3506860.3506870},
abstract = {Programming education has received extensive attention in recent years due to the increasing demand for programming ability in almost all industries. Educational institutions have widely employed online judges for programming training, which can help teachers automatically assess programming assignments by executing students’ code with test cases. However, a more important teaching process with online judges should be to evaluate how students master each of the programming skills such as strings or pointers, so that teachers may give personalized feedback and help them proceed to the success more efficiently. Previous studies have adopted deep models of knowledge tracing to evaluate a student’s mastery level of skills during the interaction with programming exercises. However, existing models generally follow the conventional assumption of knowledge tracing that each programming exercise requires only one skill, whereas in practice a programming exercise usually inspects the comprehensive use of multiple skills. Moreover, the feature of student code is often simply concatenated with other input features without the consideration of its relationship with the inspected programming skills. To bridge the gap, we propose a simple attention-based approach to learn from student code the features reflecting the multiple programming skills inspected by each programming exercise. In particular, we first use a program embedding method to obtain the representations of student code. Then we use the skill embeddings of each programming exercise to query the embeddings of student code and form an aggregated hidden state representing how the inspected skills are used in the student code. We combine the learned hidden state with DKT (Deep Knowledge Tracing), an LSTM (Long Short-Term Memory)-based knowledge tracing model, and show the improvements over baseline model. We point out some possible directions to improve the current work.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {438–443},
numpages = {6},
keywords = {programming education, knowledge tracing, intelligent education, code representation, attention mechanism},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3506860.3506869,
author = {Li, Chenglu and Xing, Wanli and Leite, Walter L.},
title = {Do Gender and Race Matter? Supporting Help-Seeking with Fair Peer Recommenders in an Online Algebra Learning Platform},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506869},
doi = {10.1145/3506860.3506869},
abstract = {Discussion forums are important for students’ knowledge inquiry in online contexts, with help-seeking being an essential learning strategy in discussion forums. This study aimed to explore innovative methods to build a peer recommender that can provide fair and accurate intelligence to support help-seeking in online learning. Specifically, we have examined existing network embedding models, Node2Vec and FairWalk, to benchmark with the proposed fair network embedding (Fair-NE). A dataset of 187,450 post-reply pairs by 10,182 Algebra I students from 2015 to 2020 was sampled from Algebra Nation, an online algebra learning platform. The dataset was used to train and evaluate the engines of peer recommenders. We evaluated models with representation fairness, predictive accuracy, and predictive fairness. Our findings suggest that constructing fairness-aware models in learning analytics (LA) is crucial to tackling the potential bias in data and to creating trustworthy LA systems.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {432–437},
numpages = {6},
keywords = {peer recommenders, online learning, help-seeking, fair machine learning, discussion forums},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3506860.3506868,
author = {Sloan-Lynch, Jay and Gay, Nathanael and Watkins, Robert},
title = {Too Fast for Their Own Good: Analyzing a Decade of Student Exercise Responses to Explore the Impact of Math Solving Photo Apps},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506868},
doi = {10.1145/3506860.3506868},
abstract = {The introduction of math solving photo apps in late 2014 presented students with a tempting new way to solve math problems quickly and accurately. Despite widespread acknowledgement that students increasingly use these apps to complete their coursework, as well as growing concerns about cheating as more students learn online, the prevalence and impact of this technology remains largely unexplored. This study uses a large dataset consisting of 700 unique math exercises and over 82 million student submissions to investigate changes in exercise answering speeds during the last decade. Through a series of exploratory analyses, we identify dramatic shifts in exercise submission speed distributions in recent years, with increasing numbers of rapid responses suggesting growing student reliance on math solving photo technology to answer math problems on homework and exams. Our analyses also reveal that decreases in exercise answering speeds have occurred contemporaneously with the introduction and proliferation of math solving photo apps in education and we further substantiate the role of these tools by verifying that exercise susceptibility to math solving photo apps is associated with decreases in submission speeds. We discuss potential applications of our findings to improve math assessment design and support students in adopting better learning strategies.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {67–76},
numpages = {10},
keywords = {response times, math solving photo apps, academic integrity, Photomath},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3506860.3506867,
author = {R\"{u}dian, Sylvio and Dittmeyer, Moritz and Pinkwart, Niels},
title = {Challenges of using auto-correction tools for language learning},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506867},
doi = {10.1145/3506860.3506867},
abstract = {In language learning, getting corrective feedback for writing tasks is an essential didactical concept to improve learners' language skills. Although various tools for automatic correction do exist, open writing texts still need to be corrected manually by teachers to provide helpful feedback to learners. In this paper, we explore the usefulness of an auto-correction tool in the context of language learning. In the first step, we compare the corrections of 100 learner texts suggested by a correction tool with those done by human teachers and examine the differences. In a second step, we do a qualitative analysis, where we investigate the requirements that need to be tackled to make existing proofreading tools useful for language learning. The results reveal that the aim of enhancing texts by proofreading, in general, is quite different from the purpose of providing corrective feedback in language learning. Only one of four relevant errors (recall=.26) marked by human teachers is recorded correctly by the tool, whereas many expressions thought to be faulty by the tool are sometimes no errors at all (precision=.33). We provide and discuss the challenges that need to be addressed to adjust those tools for language learning.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {426–431},
numpages = {6},
keywords = {written corrective feedback, online course, automated feedback, Language learning},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3506860.3506866,
author = {Nazaretsky, Tanya and Cukurova, Mutlu and Alexandron, Giora},
title = {An Instrument for Measuring Teachers’ Trust in AI-Based Educational Technology},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506866},
doi = {10.1145/3506860.3506866},
abstract = {Evidence from various domains underlines the key role that human factors, and especially, trust, play in the adoption of technology by practitioners. In the case of Artificial Intelligence (AI) driven learning analytics tools, the issue is even more complex due to practitioners’ AI-specific misconceptions, myths, and fears (i.e., mass unemployment and ethical concerns). In recent years, artificial intelligence has been introduced increasingly into K-12 education. However, little research has been conducted on the trust and attitudes of K-12 teachers regarding the use and adoption of AI-based Educational Technology (EdTech). The present study introduces a new instrument to measure teachers’ trust in AI-based EdTech, provides evidence of its internal structure validity, and uses it to portray secondary-level school teachers’ attitudes toward AI. First, we explain the instrument items creation process based on our preliminary research and review of existing tools in other domains. Second, using Exploratory Factor Analysis we analyze the results from 132 teachers’ input. The results reveal eight factors influencing teachers’ trust in adopting AI-based EdTech: Perceived Benefits of AI-based EdTech, AI-based EdTech’s Lack of Human Characteristics, AI-based EdTech’s Perceived Lack of Transparency, Anxieties Related to Using AI-based EdTech, Self-efficacy in Using AI-based EdTech, Required Shift in Pedagogy to Adopt AI-based EdTech, Preferred Means to Increase Trust in AI-based EdTech, and AI-based EdTech vs Human Advice/Recommendation. Finally, we use the instrument to discuss 132 high-school Biology teachers’ responses to the survey items and to what extent they align with the findings from the literature in relevant domains. The contribution of this research is twofold. First, it introduces a reliable instrument to investigate the role of teachers’ trust in AI-based EdTech and the factors influencing it. Second, the findings from the teachers’ survey can guide creators of teacher professional development courses and policymakers on improving teachers’ trust in, and in turn their willingness to adopt, AI-based EdTech in K-12 education.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {56–66},
numpages = {11},
keywords = {Trust, Teachers, K-12 Education, Human Factors, Blended Learning, AI},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3506860.3506865,
author = {Ma, Yingbo and Celepkolu, Mehmet and Boyer, Kristy Elizabeth},
title = {Detecting Impasse During Collaborative Problem Solving with Multimodal Learning Analytics},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506865},
doi = {10.1145/3506860.3506865},
abstract = {Collaborative problem solving has numerous benefits for learners, such as improving higher-level reasoning and developing critical thinking. While learners engage in collaborative activities, they often experience impasse, a potentially brief encounter with differing opinions or insufficient ideas to progress. Impasses provide valuable opportunities for learners to critically discuss the problem and re-evaluate their existing knowledge. Yet, despite the increasing research efforts on developing multimodal modeling techniques to analyze collaborative problem solving, there is limited research on detecting impasse in collaboration. This paper investigates multimodal detection of impasse by analyzing 46 middle school learners’ collaborative dialogue—including speech and facial behaviors—during a coding task. We found that the semantics and speaker information in the linguistic modality, the pitch variation in the audio modality, and the facial muscle movements in the video modality are the most significant unimodal indicators of impasse. We also trained several multimodal models and found that combining indicators from these three modalities provided the best impasse detection performance. To the best of our knowledge, this work is the first to explore multimodal modeling of impasse during the collaborative problem solving process. This line of research contributes to the development of real-time adaptive support for collaboration.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {45–55},
numpages = {11},
keywords = {Pair Programming, Multimodal Learning Analytics, Impasse Detection, Collaborative Problem Solving},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3506860.3506864,
author = {Nguyen, Ha and Young, William},
title = {Knowledge Construction and Uncertainty in Real World Argumentation: A Text Analysis Approach},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506864},
doi = {10.1145/3506860.3506864},
abstract = {Collaborative argumentation is key to promoting understanding of scientific issues. However, classroom structures may not always prepare students to engage in argumentation. To address this challenge, education researchers have examined the importance of social knowledge construction and managing uncertainty in group understanding. In this study, we explore these processes using data from /r/ChangeMyView, an online forum on Reddit where users present their opinions, engage others in critiquing ideas, and acknowledge when the discussion has modified their opinions. This unfacilitated environment can illuminate how argumentation evolves naturally towards refined opinions. We employ automated text analyses (LIWC) and discourse analyses to understand the features and discourse sequences of successful arguments. We find that argumentative threads are more likely to be successful if they focus on idea articulation, coherence, and semantic diversity. Findings highlight the role of uncertainty: threads with more certainty words are less likely to be successful. Furthermore, successful arguments are characterized by cycles of raising, managing, and reducing uncertainty, with more occurrences of evidence and idea incorporation. We discuss how learning environments can create norms for idea construction, coherence, and uncertainty, and the potential to provide adaptive prompts to maintain and reduce uncertainty when unproductive argumentative sequences are detected.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {34–44},
numpages = {11},
keywords = {uncertainty, text analysis, online communities, argumentation},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3506860.3506863,
author = {Karumbaiah, Shamya and Baker, Ryan and Tao, Yan and Liu, Ziyang},
title = {How does Students’ Affect in Virtual Learning Relate to Their Outcomes? A Systematic Review Challenging the Positive-Negative Dichotomy},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506863},
doi = {10.1145/3506860.3506863},
abstract = {Several emotional theories that inform the design of Virtual Learning Environments (VLEs) categorize affect as either positive or negative. However, the relationship between affect and learning appears to be more complex than that. Despite several empirical investigations in the last fifteen years, including a few that have attempted to complexify the role of affect in students’ learning in VLE, there has not been an attempt to synthesize the evidence across them. To bridge this gap, we conducted a systematic review of empirical studies that examined the relationship between student outcomes and the affect that arises during their interaction with a VLE. Our synthesis of results across thirty-nine papers suggests that except engagement, all of the commonly studied affective states (confusion, frustration, and boredom) have mixed relationships with outcomes. We further explored the differences in student demographics and study context to explain the variation in the results. Some of our key findings include poorer learning outcomes arising for confusion in classrooms (versus lab studies), differences in brief versus prolonged confusion and resolved versus persistent confusion, more positive (versus null) results for engagement in learning games, and more significant results for rarer affective states like frustration with automated affect detectors (versus student self-reports). We conclude that more careful attention must be paid to contextual differences in affect's role in student learning. We discuss the implication of this review for VLE design and research.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {24–33},
numpages = {10},
keywords = {Virtual learning, Systematic review, Student outcomes, Student affect, Online tutor, Education, Affective computing},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3506860.3506862,
author = {Yan, Lixiang and Zhao, Linxuan and Gasevic, Dragan and Martinez-Maldonado, Roberto},
title = {Scalability, Sustainability, and Ethicality of Multimodal Learning Analytics},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506862},
doi = {10.1145/3506860.3506862},
abstract = {Multimodal Learning Analytics (MMLA) innovations are commonly aimed at supporting learners in physical learning spaces through state-of-the-art sensing technologies and analysis techniques. Although a growing body of MMLA research has demonstrated the potential benefits of sensor-based technologies in education, whether their use can be scalable, sustainable, and ethical remains questionable. Such uncertainty can limit future research and the potential adoption of MMLA by educational stakeholders in authentic learning situations. To address this, we systematically reviewed the methodological, operational, and ethical challenges faced by current MMLA works that can affect the scalability and sustainability of future MMLA innovations. A total of 96 peer-reviewed articles published after 2010 were included. The findings were summarised into three recommendations, including i) improving reporting standards by including sufficient details about sensors, analysis techniques, and the full disclosure of evaluation metrics, ii) fostering interdisciplinary collaborations among experts in learning analytics, software, and hardware engineering to develop affordable sensors and upgrade MMLA innovations that used discontinued technologies, and iii) developing ethical guidelines to address the potential risks of bias, privacy, and equality concerns with using MMLA innovations. Through these future research directions, MMLA can remain relevant and eventually have actual impacts on educational practices.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {13–23},
numpages = {11},
keywords = {sustainability, sensors, scalability, multimodal learning analytics, learning analytics, ethics},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3506860.3506861,
author = {Nazaretsky, Tanya and Bar, Carmel and Walter, Michal and Alexandron, Giora},
title = {Empowering Teachers with AI: Co-Designing a Learning Analytics Tool for Personalized Instruction in the Science Classroom},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506861},
doi = {10.1145/3506860.3506861},
abstract = {AI-powered educational technology that is designed to support teachers in providing personalized instruction can enhance their ability to address the needs of individual students, hopefully leading to better learning gains. This paper presents results from a participatory research aimed at co-designing with science teachers a learning analytics tool that will assist them in implementing a personalized pedagogy in blended learning contexts. The development process included three stages. In the first, we interviewed a group of teachers to identify where and how personalized instruction may be integrated into their teaching practices. This yielded a clustering-based personalization strategy. Next, we designed a mock-up of a learning analytics tool that supports this strategy and worked with another group of teachers to define an ‘explainable learning analytics’ scheme that explains each cluster in a way that is both pedagogically meaningful and can be generated automatically. Third, we developed an AI algorithm that supports this ‘explainable clusters’ pedagogy and conducted a controlled experiment that evaluated its contribution to teachers’ ability to plan personalized learning sequences. The planned sequences were evaluated in a blinded fashion by an expert, and the results demonstrated that the experimental group – teachers who received the clusters with the explanations – designed sequences that addressed the difficulties exhibited by different groups of students better than those designed by teachers who received the clusters without explanations. The main contribution of this study is twofold. First, it presents an effective personalization approach that fits blended learning in the science classroom, which combines a real-time clustering algorithm with an explainable-AI scheme that can automatically build pedagogically meaningful explanations from item-level meta-data (Q Matrix). Second, it demonstrates how such an end-to-end learning analytics solution can be built with teachers through a co-design process and highlights the types of knowledge that teachers add to system-provided analytics in order to apply them to their local context. As a practical contribution, this process informed the design of a new learning analytics tool that was integrated into a free online learning platform that is being used by more than 1000 science teachers.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {1–12},
numpages = {12},
keywords = {Teacher Dashboards, Personalized Instruction, Participatory Design, Learning Analytics, Blended Learning},
location = {Online, USA},
series = {LAK22}
}

@proceedings{10.1145/3506860,
title = {LAK22: LAK22: 12th International Learning Analytics and Knowledge Conference},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Online, USA}
}

@inproceedings{10.1145/3448139.3448212,
author = {Tsai, Yi-Shan and Mello, Rafael Ferreira and Jovanovi\'{c}, Jelena and Ga\v{s}evi\'{c}, Dragan},
title = {Student appreciation of data-driven feedback: A pilot study on OnTask},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448212},
doi = {10.1145/3448139.3448212},
abstract = {Feedback plays a crucial role in student learning. Learning analytics (LA) has demonstrated potential in addressing prominent challenges with feedback practice, such as enabling timely feedback based on insights obtained from large data sets. However, there is insufficient research looking into relations between student expectations of feedback and their experience with LA-based feedback. This paper presents a pilot study that examined students’ experience of LA-based feedback, offered with the OnTask system, taking into consideration the factors of students’self-efficacy and self-regulation skills. Two surveys were carried out at a Brazilian university, and the results highlighted important implications for LA-based feedback practice, including leveraging the ‘partnership’ between the human teacher and the computer, and developing feedback literacy among learners.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {511–517},
numpages = {7},
keywords = {learning theories, learning analytics, feedback literacy, feedback, OnTask},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448211,
author = {Fan, Yizhou and Saint, John and Singh, Shaveen and Jovanovic, Jelena and Ga\v{s}evi\'{c}, Dragan},
title = {A learning analytic approach to unveiling self-regulatory processes in learning tactics},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448211},
doi = {10.1145/3448139.3448211},
abstract = {Investigation of learning tactics and strategies has received increasing attention by the Learning Analytics (LA) community. While previous research efforts have made notable contributions towards identifying and understanding learning tactics from trace data in various blended and online learning settings, there is still a need to deepen our understanding about learning processes that are activated during the enactment of distinct learning tactics. In order to fill this gap, we propose a learning analytic approach to unveiling and comparing self-regulatory processes in learning tactics detected from trace data. Following this approach, we detected four learning tactics (Reading with Quiz Tactic, Assessment and Interaction Tactic, Short Login and Interact Tactic and Focus on Quiz Tactic) as used by 728 learners in an undergrad course. We then theorised and detected five micro-level processes of self-regulated learning (SRL) through an analysis of trace data. We analysed how these micro-level SRL processes were activated during enactment of the four learning tactics in terms of their frequency of occurrence and temporal sequencing. We found significant differences across the four tactics regarding the five micro-level SRL processes based on multivariate analysis of variance and comparison of process models. In summary, the proposed LA approach allows for meaningful interpretation and distinction of learning tactics in terms of the underlying SRL processes. More importantly, this approach shows the potential to overcome the limitations in the interpretation of LA results which stem from the context-specific nature of learning. Specifically, the study has demonstrated how the interpretation of LA results and recommendation of pedagogical interventions can also be provided at the level of learning processes rather than only in terms of a specific course design.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {184–195},
numpages = {12},
keywords = {Self-regulated learning, Process model, Learning tactic, Learning analytics},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448210,
author = {Wise, Alyssa Friend and Sarmiento, Juan Pablo and Boothe Jr., Maurice},
title = {Subversive Learning Analytics},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448210},
doi = {10.1145/3448139.3448210},
abstract = {This paper puts forth the idea of a subversive stance on learning analytics as a theoretically-grounded means of engaging with issues of power and equity in education and the ways in which they interact with the usage of data on learning processes. The concept draws on efforts from fields such as socio-technical systems and critical race studies that have a long history of examining the role of data in issues of race, gender and class. To illustrate the value that such a stance offers the field of learning analytics, we provide examples of how taking a subversive perspective can help us to identify tacit assumptions-in-practice, ask generative questions about our design processes and consider new modes of creation to produce tools that operate differently in the world.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {639–645},
numpages = {7},
keywords = {subversive stance, educational equity, critical frameworks},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448209,
author = {Nguyen, Huy and Lim, Michelle and Moore, Steven and Nyberg, Eric and Sakr, Majd and Stamper, John},
title = {Exploring Metrics for the Analysis of Code Submissions in an Introductory Data Science Course},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448209},
doi = {10.1145/3448139.3448209},
abstract = {While data science education has gained increased recognition in both academic institutions and industry, there has been a lack of research on automated coding assessment for novice students. Our work presents a first step in this direction, by leveraging the coding metrics from traditional software engineering (Halstead Volume and Cyclomatic Complexity) in combination with those that reflect a data science project’s learning objectives (number of library calls and number of common library calls with the solution code). Through these metrics, we examined the code submissions of 97 students across two semesters of an introductory data science course. Our results indicated that the metrics can identify cases where students had overly complicated codes and would benefit from scaffolding feedback. The number of library calls, in particular, was also a significant predictor of changes in submission score and submission runtime, which highlights the distinctive nature of data science programming. We conclude with suggestions for extending our analyses towards more actionable intervention strategies, for example by tracking the fine-grained submission grading outputs throughout a student’s submission history, to better model and support them in their data science learning process.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {632–638},
numpages = {7},
keywords = {Programming Analysis, Linear Mixed Model, Data Science Education, Coding Metrics},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448208,
author = {Tavakoli, Mohammadreza and Elias, Mirette and Kismih\'{o}k, G\'{a}bor and Auer, S\"{o}ren},
title = {Metadata Analysis of Open Educational Resources},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448208},
doi = {10.1145/3448139.3448208},
abstract = {Open Educational Resources (OERs) are openly licensed educational materials that are widely used for learning. Nowadays, many online learning repositories provide millions of OERs. Therefore, it is exceedingly difficult for learners to find the most appropriate OER among these resources. Subsequently, the precise OER metadata is critical for providing high-quality services such as search and recommendation. Moreover, metadata facilitates the process of automatic OER quality control as the continuously increasing number of OERs makes manual quality control extremely difficult. This work uses the metadata of 8,887 OERs to perform an exploratory data analysis on OER metadata. Accordingly, this work proposes metadata-based scoring and prediction models to anticipate the quality of OERs. Based on the results, our analysis demonstrated that OER metadata and OER content qualities are closely related, as we could detect high-quality OERs with an accuracy of 94.6%. Our model was also evaluated on 884 educational videos from Youtube to show its applicability on other educational repositories.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {626–631},
numpages = {6},
keywords = {Prediction Models, Open Educational Resources, OER, Metadata Analysis, Machine Learning, Exploratory Analysis},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448207,
author = {\"{O}ncel, P\"{u}ren and Flynn, Lauren E. and Sonia, Allison N. and Barker, Kennis E. and Lindsay, Grace C. and McClure, Caleb M. and McNamara, Danielle S. and Allen, Laura K.},
title = {Automatic Student Writing Evaluation: Investigating the Impact of Individual Differences on Source-Based Writing},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448207},
doi = {10.1145/3448139.3448207},
abstract = {Automated Writing Evaluation systems have been developed to help students improve their writing skills through the automated delivery of both summative and formative feedback. These systems have demonstrated strong potential in a variety of educational contexts; however, they remain limited in their personalization and scope. The purpose of the current study was to begin to address this gap by examining whether individual differences could be modeled in a source-based writing context. Undergraduate students (n=106) wrote essays in response to multiple sources and then completed an assessment of their vocabulary knowledge. Natural language processing tools were used to characterize the linguistic properties of the source-based essays at four levels: descriptive, lexical, syntax, and cohesion. Finally, machine learning models were used to predict students’ vocabulary scores from these linguistic features. The models accounted for approximately 29% of the variance in vocabulary scores, suggesting that the linguistic features of source-based essays are reflective of individual differences in vocabulary knowledge. Overall, this work suggests that automated text analyses can help to understand the role of individual differences in the writing process, which may ultimately help to improve personalization in computer-based learning environments.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {620–625},
numpages = {6},
keywords = {vocabulary knowledge, source-based writing, machine-learning models, individual differences},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448206,
author = {Li, Fanjie and Wang, Zuo and Tzi Dong Ng, Jeremy and Hu, Xiao},
title = {Studying with Learners’ Own Music: Preliminary Findings on Concentration and Task Load},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448206},
doi = {10.1145/3448139.3448206},
abstract = {Through profiling learners’ music usage in everyday learning settings and depicting their learning experience when studying with a music app powered by a large-scale and real-world music library, this study revealed preliminary observations on how background music impacts learning under varying task load, and manifested intriguing patterns of learners’ music usage and music preferences in various task load conditions. Specifically, we piloted a three-day field experiment in students’ everyday learning environment. During the experiment, participants performed learning tasks with music in the background and completed a set of online surveys before and after each learning session. Our results suggested that learners’ self-selected, real-life background music could enhance their learning effectiveness, while the beneficial effect of background music was more apparent when the learning task was less mentally or temporally demanding. Towards a closer look at the characteristics of preferable music pieces under various task load conditions, our findings showed that music preferred by participants under high versus low temporal demand differs in a number of characteristics, including speechiness, acousticness, danceability, and energy. This study further reveals the effects of background music on learning under varying task load levels and provides implications for context-aware background music selection when designing musically enriched learning environments.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {613–619},
numpages = {7},
keywords = {Task load, Learning experience, Learning environment, Field experiment, Background music},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448205,
author = {Shi, Yang and Shah, Krupal and Wang, Wengran and Marwan, Samiha and Penmetsa, Poorvaja and Price, Thomas},
title = {Toward Semi-Automatic Misconception Discovery Using Code Embeddings},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448205},
doi = {10.1145/3448139.3448205},
abstract = {Understanding students’ misconceptions is important for effective teaching and assessment. However, discovering such misconceptions manually can be time-consuming and laborious. Automated misconception discovery can address these challenges by highlighting patterns in student data, which domain experts can then inspect to identify misconceptions. In this work, we present a novel method for the semi-automated discovery of problem-specific misconceptions from students’ program code in computing courses, using a state-of-the-art code classification model. We trained the model on a block-based programming dataset and used the learned embedding to cluster incorrect student submissions. We found these clusters correspond to specific misconceptions about the problem and would not have been easily discovered with existing approaches. We also discuss potential applications of our approach and how these misconceptions inform domain-specific insights into students’ learning processes.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {606–612},
numpages = {7},
keywords = {Neural Network, Learning Representation, Code Analysis, Automatic Assessment},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448204,
author = {Chockkalingam, Shruthi and Yu, Run and Pardos, Zachary A.},
title = {Which one’s more work? Predicting effective credit hours between courses},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448204},
doi = {10.1145/3448139.3448204},
abstract = {University students select courses for an upcoming term in part based on expected workload. Course credit hours is often the only metric given by the institution relevant to how much work a course will be and does not serve as a precise estimate due to the lack of granularity of the metric which can lead to student under or overestimation. We define a novel task of predicting relative effective course credit hours, or time load; essentially, determining which courses take more time than others. For this task, we draw from institutional data sources including course catalog descriptions, student enrollment histories and ratings from a popular course rating website. To validate this work, we design a personalized survey for university students to collect ground truth labels, presenting them with pairs of courses they had taken and asking which course took more time per week on average. We evaluate which sources of data using which machine representation techniques provide the best prediction of these course time load ratings. We establish a benchmark accuracy of 0.71 on this novel task and find skip-grams applied to enrollment data (i.e., course2vec), not catalog descriptions, to be most useful in predicting the time demands of a course.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {599–605},
numpages = {7},
keywords = {time load, survey study, predictive analytics, institutional data, higher education, enrollment data, course2vec, course load},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448203,
author = {Hilliger, Isabel and Miranda, Constanza and Schuit, Gregory and Duarte, Fernando and Anselmo, Martin and Parra, Denis},
title = {Evaluating a Learning Analytics Dashboard to Visualize Student Self-Reports of Time-on-task: A Case Study in a Latin American University},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448203},
doi = {10.1145/3448139.3448203},
abstract = {In recent years, instructional design has become even more challenging for teaching staff members in higher education institutions. If instructional design causes student overload, it could lead to superficial learning and decreased student well-being. A strategy to avoid overload is reflecting upon the effectiveness of teaching practices in terms of time-on-task. This article presents a Work-In-Progress conducted to provide teachers with a dashboard to visualize student self-reports of time-on-task regarding subject activities. A questionnaire was applied to 15 instructors during a set trial period to evaluate the perceived usability and usefulness of the dashboard. Preliminary findings reveal that the dashboard helped instructors became aware about the number of hours spent outside of class time. Furthermore, data visualizations of time-on-task evidence enabled them to redesign subject activities. Currently, the dashboard has been adopted by 106 engineering instructors. Future work involves the development of a framework to incorporate user-based improvements.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {592–598},
numpages = {7},
keywords = {Time-on-task, Learning Analytics Dashboards, Instructional Design, Higher Education},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448202,
author = {Winograd, Blair A. and Dood, Amber J and Moeller, Robert and Moon, Alena and Gere, Anne and Shultz, Ginger},
title = {Detecting High Orders of Cognitive Complexity in Students’ Reasoning in Argumentative Writing About Ocean Acidification},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448202},
doi = {10.1145/3448139.3448202},
abstract = {Providing students in STEM courses the opportunity to write about scientific content can be beneficial to the learning process. However, it is a logistical challenge to provide feedback to students’ written work in large-enrollment courses. Motivated by these reasons, the study presented herein considers a method to identify the depth of students’ scientific reasoning in their written work. A writing-to-learn (WTL) activity was implemented in a large undergraduate general chemistry class. An analytical framework of cognitive operations that characterizes students’ scientific reasoning evidenced in their writing was applied. Engagement in some of the more complex cognitive operations, such as causal reasoning and argumentation, was a sign that students were properly engaging in meaning making activities. This work considers a method to automate coaching of students in using more complex reasoning in their writing with the desired outcome that it may help students better engage with the science content. We consider a series of new natural language processing models to discern types of reasoning in student essays from the WTL activity.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {586–591},
numpages = {6},
keywords = {writing-to-learn, undergraduate education, peer review, Scientific reasoning},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448201,
author = {Hassan, Javaria and Leong, Jovin and Schneider, Bertrand},
title = {Multimodal Data Collection Made Easy: The EZ-MMLA Toolkit: A data collection website that provides educators and researchers with easy access to multimodal data streams.},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448201},
doi = {10.1145/3448139.3448201},
abstract = {While Multimodal Learning Analytics (MMLA) is becoming a popular methodology in the LAK community, most educational researchers still rely on traditional instruments for capturing learning processes (e.g., click-stream, log data, self-reports, qualitative observations). MMLA has the potential to complement and enrich traditional measures of learning by providing high frequency data on learners’ behavior, cognition and affects. However, there is currently no easy-to-use toolkit for recording multimodal data streams. Existing methodologies rely on the use of physical sensors and custom-written code for accessing sensor data. In this paper, we present the EZ-MMLA toolkit. This toolkit was implemented as a website that provides easy access to the latest machine learning algorithms for collecting a variety of data streams from webcams: attention (eye-tracking), physiological states (heart rate), body posture (skeletal data), hand gestures, emotions (from facial expressions and speech), and lower-level computer vision algorithms (e.g., fiducial / color tracking). This toolkit can run from any browser and does not require special hardware or programming experience. We compare this toolkit with traditional methods and describe a case study where the EZ-MMLA toolkit was used in a classroom context. We conclude by discussing other applications of this toolkit, potential limitations, and future steps.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {579–585},
numpages = {7},
keywords = {Multimodal Analytics, Data Collection Toolkit, Computer Visions},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448200,
author = {Li, Chenglu and Xing, Wanli and Leite, Walter},
title = {Yet Another Predictive Model? Fair Predictions of Students’ Learning Outcomes in an Online Math Learning Platform},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448200},
doi = {10.1145/3448139.3448200},
abstract = {To support online learners at a large scale, extensive studies have adopted machine learning (ML) techniques to analyze students’ artifacts and predict their learning outcomes automatically. However, limited attention has been paid to the fairness of prediction with ML in educational settings. This study intends to fill the gap by introducing a generic algorithm that can orchestrate with existing ML algorithms while yielding fairer results. Specifically, we have implemented logistic regression with the Seldonian algorithm and compared the fairness-aware model with fairness-unaware ML models. The results show that the Seldonian algorithm can achieve comparable predictive performance while producing notably higher fairness.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {572–578},
numpages = {7},
keywords = {predictive analytics, online math learning, fair machine learning},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448199,
author = {Evrard, August and Schulz, Kyle and Hayward, Caitlin},
title = {How Did You Get that A? Selectivity’s Role in Rising Undergraduate Grades at a Large Public University},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448199},
doi = {10.1145/3448139.3448199},
abstract = {For nearly a century, pre-college standardized test scores and undergraduate letter grades have been de facto industry standard measures of achievement in US higher education. We examine a sample of millions of grades and a half million pre-college test scores earned by undergraduates between 2006 and 2019 at a large public research university that became increasingly selective, in terms of test scores of matriculated students, over that time. A persistent, moderate correlation between test score and grades within the period motivates us to employ a simple importance sampling model to address the question, “How much is increased selectivity driving up campus grades?”. Of the overall 0.213 rise in mean undergraduate grade points over the thirteen-year period, we find that nearly half, 0.098 ± 0.004, can be ascribed to increased selectivity. The fraction is higher, nearly 70%, in engineering, business and natural science subjects. Removing selectivity’s influence to surface curricular-related grade inflation within academic domains, we find a factor four range, from a low of ∼ 0.05 in business and engineering to a high of 0.18 in the humanities, over the thirteen year period.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {565–571},
numpages = {7},
keywords = {Undergraduate Education, Student Grades, Standardized Tests, Learning Analytics},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448198,
author = {Hayward, Caitlin and Schulz, Kyle and Fishman, Barry},
title = {Who wins, who learns? Exploring gameful pedagogy as a technique to support student differences},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448198},
doi = {10.1145/3448139.3448198},
abstract = {Gameful pedagogy is a novel approach to teaching that has emerged in the past decade that emphasizes intentionally designing curricula to support student motivation. In this study we investigate how five gameful courses at a large R1 university have impacted students when analyzed with an eye towards equity: are men vs women, underrepresented minorities vs majority students, and first-generation students vs traditional students able to achieve similar amounts of success in gameful courses? Results show that for both men and minority students, there is evidence to suggest they are underachieving in the courses studied as compared to women and majority students, but when we control for prior academic performance these trends disappear. For first-generation students, we see conflicting evidence, with cases of both under and over-achievement present. We again see these discrepancies disappear when we control for prior performance.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {559–564},
numpages = {6},
keywords = {self-determination theory, higher education, curricular design, Gameful},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448197,
author = {Chen, Lujie Karen},
title = {Timing of Support in One-on-one Math Problem Solving Coaching: A Survival Analysis Approach with Multimodal Data},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448197},
doi = {10.1145/3448139.3448197},
abstract = {In this paper, we explore a kind of teaching-oriented temporal analytics on the timing of support in the context of one-on-one math problem-solving coaching. We build the analytical framework upon the human-human multimodal interaction data collected from the naturalist environments. We demonstrated the potential utility of leveraging survival analysis, a class of statistical methods to model time-to-event data, to gain insights into the timing decisions. We shed light on the heterogeneity of coaching decisions as to when to render support in connection to the problem-solving stages, coaching dyads, as well as the pre-intervention event characteristics. This work opens future avenues into a different type of human tutoring study supported by multimodal data, computational models, and statistical frameworks. This model framework may yield useful reflective teaching analytics to tutors, coaches, or teachers when further developed. We also envision that those analyses may ultimately inform the design of AI-supported autonomous agents that could learn the tutorial interaction logic from empirical data.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {553–558},
numpages = {6},
keywords = {multimodal, learning analtyics, human tutoring studies},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448196,
author = {Sun, Zhiru and Chiarandini, Marco},
title = {An Exact Algorithm for Group Formation to Promote Collaborative Learning},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448196},
doi = {10.1145/3448139.3448196},
abstract = {Collaborative learning has been widely used to foster students’ communication and joint knowledge construction. However, the classification of learners into well-structured groups is one of the most challenging tasks in the field. The aim of this study is to propose a novel method to form intra-heterogeneous and inter-homogeneous groups based on relevant student characteristics. Such a method allows for the consideration of multiple student characteristics and can handle both numerical and categorical characteristic types simultaneously. It assumes that the teacher provides an order of importance of the characteristics, then it solves the grouping problem as a lexicographic optimization problem in the given order. We formulate the problem in mixed integer linear programming (MILP) terms and solve it to optimality. A pilot experiment was conducted with 29 college freshmen considering three general characteristics (i.e., 13 specific features) including knowledge level, demographic information, and motivation. Results of such an experiment demonstrate the validity and computational feasibility of the algorithmic approach. Large-scale studies are needed to assess the impact of the proposed grouping method on students’ learning experience and academic achievement.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {546–552},
numpages = {7},
keywords = {student-project assignment, Mixed integer linear programming (MILP), Group formation, Computer-supported collaborative learning (CSCL)},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448195,
author = {Vanacore, Kirk and Dieter, Kevin and Hurwitz, Lisa and Studwell, Jamie},
title = {Longitudinal Clusters of Online Educator Portal Access: Connecting Educator Behavior to Student Outcomes},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448195},
doi = {10.1145/3448139.3448195},
abstract = {The rising prevalence of blended learning programs has provided educators with an abundance of information about students' specific educational needs through educator portals. Full implementation of blended learning models requires educators to utilize these data to inform their teaching practices, yet most research on blended learning programs focuses solely on student engagement with the digital learning environment. In this paper, we utilize a longitudinal clustering method to identify patterns of educator portal usage and examine the associations between these clusters and student program outcomes. The clusters of educators varied in intensity and consistency of educator portal access across a school year and were associated with significant differences in student usage and progress in the program. The analyses allowed us to identify preferable educator usage patterns based upon their associated students’ program outcomes, which provides novel information about the potential impact of educator engagement on overall implementation fidelity of blended learning programs.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {540–545},
numpages = {6},
keywords = {Longitudinal k-means clustering, Implementation fidelity, Educator engagement},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448194,
author = {Guillain, L\'{e}onore V. and Schneider, Bertrand},
title = {Facilitators First: Building a Tool With Facilitators to Foster a More Collaborative Makerspace Community Through Movement Traces},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448194},
doi = {10.1145/3448139.3448194},
abstract = {Research indicates that makerspaces equip students with the practical skills needed to build their own projects and thrive in the twenty-first-century workforce. While the appeal of makerspaces lies in their spirit of tinkering and community-driven ethos, these same attributes make it difficult to monitor and facilitate learning. Makerspaces also attract students from diverse backgrounds and skills, further challenging facilitators to accommodate the needs of each student and their self-directed projects. We propose a dashboard interface that visualizes Kinect sensor data to aid facilitators in monitoring student collaboration. The tool was designed with an iterative and participatory approach. Five facilitators were involved at each phase of the design process, from need-finding to prototyping to implementation and evaluation. Insights derived from interviews were used to inform the design decisions of the final interface. The final evaluation suggests that the use of normalized summary scores and an interactive network graph can successfully support facilitators in tasks related to improving collaboration. Moreover, the use of a red-green color scheme and the inclusion of student photos improved the usability for facilitators, but issues of trustworthiness need to be further examined.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {533–539},
numpages = {7},
keywords = {physical learning analytics, learning analytics dashboards, human-computer interaction},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448193,
author = {Wang, Karen and Nair, Krishnan and Wieman, Carl},
title = {Examining the Links between Log Data and Reflective Problem-solving Practices in An Interactive Task},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448193},
doi = {10.1145/3448139.3448193},
abstract = {Learning how to solve authentic problems is an important goal of education, yet how to assess and teach problem solving are research topics to be further explored. This study examines how interaction log data from a computerized task environment could be used to extract meaningful features in order to automate the assessment of reflective problem-solving practices. We collected survey responses and interaction log data of 40 college students working to solve the mass of a ”mystery object” in an interactive physics simulation. The log data was parsed to reveal both the test trials conducted to solve the problem and the pauses in-between test trials, where potential monitoring and reflection of the problem-solving process took place. The results show that reflective problem-solving practices, as indicated by meaningful pauses, can predict problem-solving performance above and beyond participants’ application of physics knowledge. Our approach to log data processing has implications for how we study problem solving using interactive simulations.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {525–532},
numpages = {8},
keywords = {science simulation, reflection, problem-solving practices, log data processing},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448192,
author = {Jang, JiWoong and Lee, Jaewook and Echeverria, Vanessa and Lawrence, LuEttaMae and Aleven, Vincent},
title = {Explorations of Designing Spatial Classroom Analytics with Virtual Prototyping},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448192},
doi = {10.1145/3448139.3448192},
abstract = {Despite the potential of spatial displays for supporting teachers’ classroom orchestration through real-time classroom analytics, the process to design these displays is a challenging and under-explored topic in the learning analytics (LA) community. This paper proposes a mid-fidelity Virtual Prototyping method (VPM), which involves simulating a classroom environment and candidate designs in virtual space to address these challenges. VPM allows for rapid prototyping of spatial features, requires no specialized hardware, and enables teams to conduct remote evaluation sessions. We report observations and findings from an initial exploration with five potential users through a design process utilizing VPM to validate designs for an AR-based spatial display in the context of middle-school orchestration tools. We found that designs created using virtual prototyping sufficiently conveyed a sense of three-dimensionality to address subtle design issues like occlusion and depth perception. We discuss the opportunities and limitations of applying virtual prototyping, particularly its potential to allow for more robust co-design with stakeholders earlier in the design process.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {518–524},
numpages = {7},
keywords = {virtual prototyping, spatial classroom displays, mixed reality, classroom analytics, augmented reality},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448190,
author = {Xu, Yang and Wilson, Kevin},
title = {Early Alert Systems During a Pandemic: A Simulation Study on the Impact of Concept Drift},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448190},
doi = {10.1145/3448139.3448190},
abstract = {Predictions from early alert systems are increasingly being used by institutions to assist decision-making and support at-risk individuals. Concept drifts caused by the 2020 SARS-CoV-2 pandemic are threatening the performance and usefulness of the machine learning models that power these systems. In this paper, we present an analytical framework that uses imputation-based simulations to perform preliminary evaluation on the extent to which data quality and availability issues impact the performance of machine learning models. Guided by this framework, we studied how these issues would impact the performance of the high school dropout prediction model implemented in the Early Warning System (EWS). Results show that despite the disruptions, this model can still be reasonably useful in assisting decision-making. We discuss the implications of these findings in more general educational contexts and recommend steps in countering the challenges of using predictions from imperfect machine learning models in early alert systems and, more broadly, learning analytic research that uses longitudinal data.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {504–510},
numpages = {7},
keywords = {simulation, pandemic, imputation, early alert system, dropout prediction, concept drift},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448189,
author = {Abdi, Solmaz and Khosravi, Hassan and Sadiq, Shazia},
title = {Modelling Learners in Adaptive Educational Systems: A Multivariate Glicko-based Approach},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448189},
doi = {10.1145/3448139.3448189},
abstract = {The Elo rating system has been recognised as an effective method for modelling students and items within adaptive educational systems. A common characteristic across Elo-based learner models is that they are not sensitive to the lag time between two consecutive interactions of a student within the system. Implicitly, this characteristic assumes that students do not learn or forget between two consecutive interactions. However, this assumption seems insufficient in the context of adaptive learning systems where students could have improved their mastery through practising outside of the system or that their mastery may be declined due to forgetting. In this paper, we extend the existing works on the use of rating systems for modelling learners in adaptive educational systems by proposing a new learner model called MV-Glicko that builds on the Glicko rating system. MV-Glicko is sensitive to the lag time between two consecutive interactions of a student within the system and models it as a parameter that captures the confidence of the system in the current inferred rating. We apply MV-Glicko on three public data sets and three data sets obtained from an adaptive learning system and provide evidence that MV-Glicko outperforms other conventional models in estimating students’ knowledge mastery.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {497–503},
numpages = {7},
keywords = {learner modelling, knowledge tracing, Glicko rating system, Adaptive learning},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448188,
author = {Shin, Dongmin and Shim, Yugeun and Yu, Hangyeol and Lee, Seewoo and Kim, Byungsoo and Choi, Youngduck},
title = {SAINT+: Integrating Temporal Features for EdNet Correctness Prediction},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448188},
doi = {10.1145/3448139.3448188},
abstract = {We propose SAINT+, a successor of SAINT which is a Transformer based knowledge tracing model that separately processes exercise information and student response information. Following the architecture of SAINT, SAINT+ has an encoder-decoder structure where the encoder applies self-attention layers to a stream of exercise embeddings, and the decoder alternately applies self-attention layers and encoder-decoder attention layers to streams of response embeddings and encoder output. Moreover, SAINT+ incorporates two temporal feature embeddings into the response embeddings: elapsed time, the time taken for a student to answer, and lag time, the time interval between adjacent learning activities. We empirically evaluate the effectiveness of SAINT+ on EdNet, the largest publicly available benchmark dataset in the education domain. Experimental results show that SAINT+ achieves state-of-the-art performance in knowledge tracing with an improvement of 1.25% in area under receiver operating characteristic curve compared to SAINT, the current state-of-the-art model in EdNet dataset.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {490–496},
numpages = {7},
keywords = {Transformer, Personalized Learning, Knowledge Tracing, Education, Deep Learning},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448187,
author = {Dourado, Raphael A. and Rodrigues, Rodrigo Lins and Ferreira, Nivan and Mello, Rafael Ferreira and Gomes, Alex Sandro and Verbert, Katrien},
title = {A Teacher-facing Learning Analytics Dashboard for Process-oriented Feedback in Online Learning},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448187},
doi = {10.1145/3448139.3448187},
abstract = {In online learning, teachers need constant feedback about their students’ progress and regulation needs. Learning Analytics Dashboards for process-oriented feedback can be a valuable tool for this purpose. However, few such dashboards have been proposed in literature, and most of them lack empirical validation or grounding in learning theories. We present a teacher-facing dashboard for process-oriented feedback in online learning, co-designed and evaluated through an iterative design process involving teachers and visualization experts. We also reflect on our design process by discussing the challenges, pitfalls, and successful strategies for building this type of dashboard.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {482–489},
numpages = {8},
keywords = {visualization, online learning, learning analytics dashboards, process-oriented feedback},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448186,
author = {Gosch, Nicole and Andrews, David and Barreiros, Carla and Leitner, Philipp and Staudegger, Elisabeth and Ebner, Martin and Lindstaedt, Stefanie},
title = {Learning Analytics as a Service for Empowered Learners: From Data Subjects to Controllers},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448186},
doi = {10.1145/3448139.3448186},
abstract = {As Learning Analytics (LA) in the higher education setting increasingly transitions from a field of research to an implemented matter of fact of the learner's experience, the demand of practical guidelines to support its development is rising. LA Policies bring together different perspectives, like the ethical and legal dimensions, into frameworks to guide the way. Usually the first time learners get in touch with LA is at the act of consenting to the LA tool. Utilising an ethical (TRUESSEC) and a legal framework (GDPR), we question whether sincere consent is possible in the higher education setting. Drawing upon this premise, we then show how it might be possible to recognise the autonomy of the learner by providing LA as a service, rather than an intervention. This could indicate a paradigm shift towards the learner as empowered demander. At last, we show how this might be incorporated within the GDPR by also recognising the demand of the higher education institutions to use the learner's data at the same time. These considerations will in the future influence the development of our own LA policy: a LA criteria catalogue.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {475–481},
numpages = {7},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448185,
author = {Poquet, Oleksandra},
title = {Why Birds of a Feather Flock Together: Factors Triaging Students in Online Forums},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448185},
doi = {10.1145/3448139.3448185},
abstract = {Peer effects, an influence that peers can have on one’s learning and development, have been shown to affect student achievement and attitudes. A large-scale analysis of social influences in digital online interactions showed that students interact in online university forums with peers of similar performance. Mechanisms driving this observed similarity remain unclear. To shed light as to why similar peers interact online, the current study examined the role of organizing factors in the formation of similarity patterns in online university forums, using four-years of forum interaction data of a university cohort. In the study, experiments randomized the timing of student activity, relationship between student activity levels within specific courses, and relationship between student activity and performance. Analysis suggests that similarity between students interacting online is shaped by implications of the course design on individual student behaviour, less so by social processes of selection. Social selection may drive observed similarity in later years of student experience, but its role is relatively small compared to other factors. The results highlight the need to consider what social influences are enacted by the course design and technological scaffolding of learner behaviour in online interactions, towards diversifying student social influences.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {469–474},
numpages = {6},
keywords = {university forums, student activity, learning analytics, digital learning, course design, communication networks},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448184,
author = {Zhang, Yupei and An, Rui and Cui, Jiaqi and Shang, Xuequn},
title = {Undergraduate Grade Prediction in Chinese Higher Education Using Convolutional Neural Networks},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448184},
doi = {10.1145/3448139.3448184},
abstract = {Prediction of undergraduate grades before their course enrollments is beneficial to the student’s learning plan on selective courses and failure warnings to compulsory courses in Chinese higher education. This study proposed to use a deep learning-based model composed of sparse attention layers, convolutional neural layers, and a fully connected layer, called Sparse Attention Convolutional Neural Networks (SACNN), to predict undergraduate grades. Concretely, sparse attention layers response to the fact that courses have different contributions to the grade prediction of the target course; convolutional neural layers aim to capture the one-dimensional temporal feature on these courses organized in terms; the fully connected layer is to complete the final classification based on achieved features. We collected a dataset including grade records, student’s demographics and course descriptions from our institution in the past five years. The dataset contained about 54k grade records from 1307 students and 137 courses, where all mentioned methods were evaluated by the hold-out evaluation. The result shows SACNN achieves 81% prediction precision and 85% accuracy on the failure prediction, which is more effective than those compared methods. Besides, SACNN delivers a potential explanation to the reason of the predicted result, thanks to the sparse attention layer. This study provides a useful technique for personalized learning and course relationship discovery in undergraduate education.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {462–468},
numpages = {7},
keywords = {sparse attention, personalized learning, grade prediction, convolutional neural networks},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448183,
author = {Kyriacou, Harrison and Ramakrishnan, Anand and Whitehill, Jacob},
title = {Learning to Work in a Materials Recovery Facility: Can Humans and Machines Learn from Each Other?},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448183},
doi = {10.1145/3448139.3448183},
abstract = {Workplace learning often requires workers to learn new perceptual and motor skills. The future of work will increasingly feature human users who cooperate with machines, both to learn the tasks and to perform them. In this paper, we examine workplace learning in Materials Recovery Facilities (MRFs), i.e., recycling plants, where workers separate waste items on conveyer belts before they are formed into bales and reprocessed. Using a simulated MRF, we explored the benefit of machine learning assistants (MLAs) that help workers, and help train them, to sort objects efficiently by providing automated perceptual guidance. In a randomized experiment (n = 140), we found: (1) A low-accuracy MLA is worse than no MLA at all, both in terms of task performance and learning. (2) A perfect MLA led to the best task performance, but was no better in helping users to learn than having no MLA at all. (3) Users tend to follow the MLA’s judgments too often, even when they were incorrect. Finally, (4) we devised a novel learning analytics algorithm to assess the worker’s accuracy, with the goal of obtaining additional training labels that can be used for fine-tuning the machine. A simulation study illustrates how even noisy labels can increase the machine’s accuracy.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {456–461},
numpages = {6},
keywords = {perceptual learning, object detection, automated feedback, MRF},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448182,
author = {Matayoshi, Jeffrey and Karumbaiah, Shamya},
title = {Using Marginal Models to Adjust for Statistical Bias in the Analysis of State Transitions},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448182},
doi = {10.1145/3448139.3448182},
abstract = {Many areas of educational research require the analysis of data that have an inherent sequential or temporal ordering. In certain cases, researchers are specifically interested in the transitions between different states—or events—in these sequences, with the goal being to understand the significance of these transitions; one notable example is the study of affect dynamics, which aims to identify important transitions between affective states. Unfortunately, a recent study has revealed a statistical bias with several metrics used to measure and compare these transitions, possibly causing these metrics to return unexpected and inflated values. This issue then causes extra difficulties when interpreting the results of these transition metrics. Building on this previous work, in this study we look in more detail at the specific mechanisms that are responsible for the bias with these metrics. After giving a theoretical explanation for the issue, we present an alternative procedure that attempts to address the problem with the use of marginal models. We then analyze the effectiveness of this procedure, both by running simulations and by applying it to actual student data. The results indicate that the marginal model procedure seemingly compensates for the bias observed in other transition metrics, thus resulting in more accurate estimates of the significance of transitions between states.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {449–455},
numpages = {7},
keywords = {transition metrics, sequential data, marginal models, affect dynamics, L statistic},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448181,
author = {van der Graaf, Joep and Lim, Lyn and Fan, Yizhou and Kilgour, Jonathan and Moore, Johanna and Bannert, Maria and Gasevic, Dragan and Molenaar, Inge},
title = {Do Instrumentation Tools Capture Self-Regulated Learning?},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448181},
doi = {10.1145/3448139.3448181},
abstract = {Researchers have been struggling with the measurement of Self-Regulated Learning (SRL) for decades. Instrumentation tools have been proposed to help capture SRL processes that are difficult to capture. The aim of the present study was to improve measurement of SRL by embedding instrumentation tools in a learning environment and validating the measurement of SRL with these instrumentation tools using think aloud. Synchronizing log data and concurrent think aloud data helped identify which SRL processes were captured by particular instrumentation tools. One tool was associated with a single SRL process: the timer co-occurred with monitoring. Other tools co-occurred with a number of SRL processes, i.e., the highlighter and note taker captured superficial writing down, organizing, and monitoring, whereas the search and planner tools revealed planning and monitoring. When specific learner actions with the tool were analyzed, a clearer picture emerged of the relation between the highlighter and note taker and SRL processes. By aligning log data with think aloud data, we showed that instrumentation tool use indeed reflects SRL processes. The main contribution is that this paper is the first to show that SRL processes that are difficult to measure by trace data can indeed be captured by instrumentation tools such as high cognition and metacognition. Future challenges are to collect and process log data real time with learning analytic techniques to measure ongoing SRL processes and support learners during learning with personalized SRL scaffolds.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {438–448},
numpages = {11},
keywords = {Self-Regulated Learning, Instrumentation tools},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448180,
author = {Lin, Yiwen and Dowell, Nia and Godfrey, Andrew},
title = {Skills Matter: Modeling the relationship between decision making processes and collaborative problem-solving skills during Hidden Profile Tasks},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448180},
doi = {10.1145/3448139.3448180},
abstract = {Collaborative problem-solving (CPS) is one of the most essential 21st century skills for success across educational and professional settings. The hidden-profile paradigm is one of the most prominent avenues of studying group decision making and underlying issues in information sharing. Previous research on the hidden-profile paradigm has primarily focused on static constructs (e.g., group size, group expertise), or on the information itself (whether certain pieces of information is being shared). In the current study, we propose a lens on individual and group’s collaborative problem-solving skills, to explore the relationships between dynamic discourse processes and decision making in a distributed information environment. Specifically, we sought to examine CPS skills in association with decision change and productive decision-making. Our results suggest that while sharing information has significantly positive association with decision change and effective decision-making, other aspects of social processes appear to be negatively correlated with these outcomes. Cognitive CPS skills, however, exhibit a strong positive relationship with making a (productive) change in students final decisions. We also find that these results are more pronounced at the group level, particularly with cognitive CPS skills. Our study shed lights on a more nuanced picture of how social and cognitive CPS interactions are related to effective information sharing and decision making in collaborative problem-solving interactions.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {428–437},
numpages = {10},
keywords = {group processes, decision making, collaborative problem solving, Hidden-profile paradigm},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448179,
author = {Jivet, Ioana and Wong, Jacqueline and Scheffel, Maren and Valle Torre, Manuel and Specht, Marcus and Drachsler, Hendrik},
title = {Quantum of Choice: How learners’ feedback monitoring decisions, goals and self-regulated learning skills are related},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448179},
doi = {10.1145/3448139.3448179},
abstract = {Learning analytics dashboards (LADs) are designed as feedback tools for learners, but until recently, learners rarely have had a say in how LADs are designed and what information they receive through LADs. To overcome this shortcoming, we have developed a customisable LAD for Coursera MOOCs on which learners can set goals and choose indicators to monitor. Following a mixed-methods approach, we analyse 401 learners’ indicator selection behaviour in order to understand the decisions they make on the LAD and whether learner goals and self-regulated learning skills influence these decisions. We found that learners overwhelmingly chose indicators about completed activities. Goals are not associated with indicator selection behaviour, while help-seeking skills predict learners’ choice of monitoring their engagement in discussions and time management skills predict learners’ interest in procrastination indicators. The findings have implications for our understanding of learners’ use of LADs and their design.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {416–427},
numpages = {12},
keywords = {self-regulated learning, learning dashboard, learner goal, feedback, customisable dashboard},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448178,
author = {Park, Kyungjin and Sohn, Hyunwoo and Mott, Bradford and Min, Wookhee and Saleh, Asmalina and Glazewski, Krista and Hmelo-Silver, Cindy and Lester, James},
title = {Detecting Disruptive Talk in Student Chat-Based Discussion within Collaborative Game-Based Learning Environments},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448178},
doi = {10.1145/3448139.3448178},
abstract = {Collaborative game-based learning environments offer significant promise for creating engaging group learning experiences. Online chat plays a pivotal role in these environments by providing students with a means to freely communicate during problem solving. These chat-based discussions and negotiations support the coordination of students’ in-game learning activities. However, this freedom of expression comes with the possibility that some students might engage in undesirable communicative behavior. A key challenge posed by collaborative game-based learning environments is how to reliably detect disruptive talk that purposefully disrupt team dynamics and problem-solving interactions. Detecting disruptive talk during collaborative game-based learning is particularly important because if it is allowed to persist, it can generate frustration and significantly impede the learning process for students. This paper analyzes disruptive talk in a collaborative game-based learning environment for middle school science education to investigate how such behaviors influence students’ learning outcomes and varies across gender and students’ prior knowledge. We present a disruptive talk detection framework that automatically detects disruptive talk in chat-based group conversations. We further investigate both classic machine learning and deep learning models for the framework utilizing a range of dialogue representations as well as supplementary information such as student gender. Findings show that long short-term memory network (LSTM)-based disruptive talk detection models outperform competitive baseline models, indicating that the LSTM-based disruptive talk detection framework offers significant potential for supporting effective collaborative game-based learning through the identification of disruptive talk.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {405–415},
numpages = {11},
keywords = {Text Analytics, Disruptive Talk Detection, Collaborative Game-Based Learning},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448177,
author = {Loh, Hyunbin and Shin, Dongmin and Lee, Seewoo and Baek, Jineon and Hwang, Chanyou and Lee, Youngnam and Cha, Yeongmin and Kwon, Soonwoo and Park, Juneyoung and Choi, Youngduck},
title = {Recommendation for Effective Standardized Exam Preparation},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448177},
doi = {10.1145/3448139.3448177},
abstract = {Finding an optimal learning trajectory is an important question in educational systems. Existing Artificial Intelligence in Education (AiEd) technologies mostly used indirect methods to make the learning process efficient such as recommending contents based on difficulty adjustment, weakness analysis, learning theory, psychometric analysis, or domain specific rules. In this study, we propose a recommender system that optimizes the learning trajectory of a student preparing for a standardized exam by recommending the learning content(question) which directly maximizes the expected score after the consumption of the content. In particular, the proposed RCES model computes the expected score of a user by effectively capturing educational effects. To validate the proposed model in an end-to-end system, we conduct an A/B test on 1713 real students by deploying 4 recommenders to a real mobile application. Result shows that RCES has better educational efficiencies than traditional methods such as expert designed models and item response theory based models.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {397–404},
numpages = {8},
keywords = {Recommender Systems, Personalized learning, Intelligent tutoring system, Education},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448176,
author = {Fernandez-Nieto, Gloria Milena and Martinez-Maldonado, Roberto and Kitto, Kirsty and Buckingham Shum, Simon},
title = {Modelling Spatial Behaviours in Clinical Team Simulations using Epistemic Network Analysis: Methodology and Teacher Evaluation},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448176},
doi = {10.1145/3448139.3448176},
abstract = {In nursing education through team simulations, students must learn to position themselves correctly in coordination with colleagues. However, with multiple student teams in action, it is difficult for teachers to give detailed, timely feedback on these spatial behaviours to each team. Indoor-positioning technologies can now capture student spatial behaviours, but relatively little work has focused on giving meaning to student activity traces, transforming low-level x/y coordinates into language that makes sense to teachers. Even less research has investigated if teachers can make sense of that feedback. This paper therefore makes two contributions. (1) Methodologically, we document the use of Epistemic Network Analysis (ENA) as an approach to model and visualise students’ movements. To our knowledge, this is the first application of ENA to analyse human movement. (2) We evaluated teachers’ responses to ENA diagrams through qualitative analysis of video-recorded sessions. Teachers constructed consistent narratives about ENA diagrams’ meaning, and valued the new insights ENA offered. However, ENA’s abstract visualisation of spatial behaviours was not intuitive, and caused some confusions. We propose, therefore, that the power of ENA modelling can be combined with other spatial representations such as a classroom map, by overlaying annotations to create a more intuitive user experience.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {386–396},
numpages = {11},
keywords = {spatial behaviour, simulation, qualitative analysis, nursing, Epistemic Network Analysis},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448175,
author = {Lall\'{e}, S\'{e}bastien and Yal\c{c}\i{}n, \"{O}zge Nilay and Conati, Cristina},
title = {Combining Data-Driven Models and Expert Knowledge for Personalized Support to Foster Computational Thinking Skills},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448175},
doi = {10.1145/3448139.3448175},
abstract = {Game-Design (GD) environments show promise in fostering Computational Thinking (CT) skills at a young age. However, such environments can be challenging to some students due to their highly open-ended nature. We propose to alleviate this difficulty by learning interpretable student models from data that can drive personalization of a real-world GD learning environment to the student’s needs. We apply our approach on a dataset collected in ecological settings and evaluate the ability of the generated student models at predicting ineffective learning behaviors over the course of the interaction. We then discuss how these behaviors can be used to define personalized support in GD learning activities, by conducting extensive interviews with experienced instructors.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {375–385},
numpages = {11},
keywords = {Student Modeling, Open-Ended Learning Environments, Game Design, Educational Data Mining, Computational Thinking},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448174,
author = {Lim, Lisa-Angelique and Gasevic, Dragan and Matcha, Wannisa and Ahmad Uzir, Nora'Ayu and Dawson, Shane},
title = {Impact of learning analytics feedback on self-regulated learning: Triangulating behavioural logs with students’ recall},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448174},
doi = {10.1145/3448139.3448174},
abstract = {Learning analytics (LA) has been presented as a viable solution for scaling timely and personalised feedback to support students’ self-regulated learning (SRL). Research is emerging that shows some positive associations between personalised feedback with students’ learning tactics and strategies as well as time management strategies, both important aspects of SRL. However, the definitive role of feedback on students’ SRL adaptations is under-researched; this requires an examination of students’ recalled experiences with their personalised feedback. Furthermore, an important consideration in feedback impact is the course context, comprised of the learning design and delivery modality. This mixed-methods study triangulates learner trace data from two different course contexts, with students’ qualitative data collected from focus group discussions, to more fully understand the impact of their personalised feedback and to explicate the role of this feedback on students’ SRL adaptations. The quantitative analysis showed the contextualised impact of the feedback on students’ learning and time management strategies in the different courses, while the qualitative analysis highlighted specific ways in which students used their feedback to adjust these and other SRL processes.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {364–374},
numpages = {11},
keywords = {time management strategies, self-regulated learning, personalised feedback, mixed methods, learning strategies, learning analytics},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448173,
author = {Li, Zhi and Ren, Cheng and Li, Xianyou and Pardos, Zachary A.},
title = {Learning Skill Equivalencies Across Platform Taxonomies},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448173},
doi = {10.1145/3448139.3448173},
abstract = {Assessment and reporting of skills is a central feature of many digital learning platforms. With students often using multiple platforms, cross-platform assessment has emerged as a new challenge. While technologies such as Learning Tools Interoperability (LTI) have enabled communication between platforms, reconciling the different skill taxonomies they employ has not been solved at scale. In this paper, we introduce and evaluate a methodology for finding and linking equivalent skills between platforms by utilizing problem content as well as the platform’s clickstream data. We propose six models to represent skills as continuous real-valued vectors, and leverage machine translation to map between skill spaces. The methods are tested on three digital learning platforms: ASSISTments, Khan Academy, and Cognitive Tutor. Our results demonstrate reasonable accuracy in skill equivalency prediction from a fine-grained taxonomy to a coarse-grained one, achieving an average recall@5 of 0.8 between the three platforms. Our skill translation approach has implications for aiding in the tedious, manual process of taxonomy to taxonomy mapping work, also called crosswalks, within the tutoring as well as standardized testing worlds.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {354–363},
numpages = {10},
keywords = {transfer models, taxonomies, representation learning, machine translation, interoperability, digital learning platforms, crosswalks, app hand-offs., acknowledging prior knowledge, Skill equivalencies},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448172,
author = {Li, Qiujie and Jung, Yeonji and Friend Wise, Alyssa},
title = {Beyond First Encounters with Analytics: Questions, Techniques and Challenges in Instructors’ Sensemaking},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448172},
doi = {10.1145/3448139.3448172},
abstract = {Despite growing implementation of teacher-facing analytics in higher education, relatively little is known about the detailed processes through which instructors make sense of analytics in their teaching practices beyond their initial encounters with tools. This study unpacked the sensemaking process of thirteen instructors with analytic experience, using interviews that included walkthroughs of their analytics use. Qualitative inductive analysis was used to identify themes related to (1) the questions they asked of the analytics, (2) the techniques they used to interpret them, and (3) the challenges they encountered. Findings indicated that instructors went beyond a general curiosity to develop three types of questions of the analytics (goal-oriented, problem-oriented, and instruction modification questions). Instructors also used specific techniques to read and explain data by (a) developing expectations about the answers the analytics would provide, and (b) making comparisons to reveal student diversity, identify effects of instructional revision and diagnose issues. The study found instructors faced an initial learning curve when seeking and making use of relevant information, but also continued to revisit these challenges when they were not able to develop a routine of analytics use. These findings both contribute to a conceptual understanding of instructor analytic sensemaking and have practical implications for its systematic support.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {344–353},
numpages = {10},
keywords = {Instructional dashboards, Human-centered analytics, Data-informed instruction, Analytic sensemaking},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448171,
author = {Saint, John and Fan, Yizhou and Singh, Shaveen and Gasevic, Dragan and Pardo, Abelardo},
title = {Using process mining to analyse self-regulated learning: a systematic analysis of four algorithms},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448171},
doi = {10.1145/3448139.3448171},
abstract = {The conceptualisation of self-regulated learning (SRL) as a process that unfolds over time has influenced the way in which researchers approach analysis. This gave rise to the use of process mining in contemporary SRL research to analyse data about temporal and sequential relations of processes that occur in SRL. However, little attention has been paid to the choice and combinations of process mining algorithms to achieve the nuanced needs of SRL research. We present a study that 1) analysed four process mining algorithms that are most commonly used in the SRL literature – Inductive Miner, Heuristics Miner, Fuzzy Miner, and pMineR; and 2) examined how the metrics produced by the four algorithms complement each. The study looked at micro-level processes that were extracted from trace data collected in an undergraduate course (N=726). The study found that Fuzzy Miner and pMineR offered better insights into SRL than the other two algorithms. The study also found that a combination of metrics produced by several algorithms improved interpretation of temporal and sequential relations between SRL processes. Thus, it is recommended that future studies of SRL combine the use of process mining algorithms and work on new tools and algorithms specifically created for SRL research.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {333–343},
numpages = {11},
keywords = {Self-Regulated Learning, Process Mining, Micro-level Process Analysis, Learning Analytics},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448170,
author = {Ruan, Sherry and Wei, Wei and Landay, James},
title = {Variational Deep Knowledge Tracing for Language Learning},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448170},
doi = {10.1145/3448139.3448170},
abstract = {Deep Knowledge Tracing (DKT), which traces a student’s knowledge change using deep recurrent neural networks, is widely adopted in student cognitive modeling. Current DKT models only predict a student’s performance based on the observed learning history. However, a student’s learning processes often contain latent events not directly observable in the learning history, such as partial understanding, making slips, and guessing answers. Current DKT models fail to model this kind of stochasticity in the learning process. To address this issue, we propose Variational Deep Knowledge Tracing (VDKT), a latent variable DKT model that incorporates stochasticity into DKT through latent variables. We show that VDKT outperforms both a sequence-to-sequence DKT baseline and previous SoTA methods on MAE, F1, and AUC by evaluating our approach on two Duolingo language learning datasets. We also draw various interpretable analyses from VDKT and offer insights into students’ stochastic behaviors in language learning.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {323–332},
numpages = {10},
keywords = {variational inference, student modeling, language learning, knowledge tracing, deep learning},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448169,
author = {Rodriguez, Fernando and Lee, Hye Rin and Rutherford, Teomara and Fischer, Christian and Potma, Eric and Warschauer, Mark},
title = {Using Clickstream Data Mining Techniques to Understand and Support First-Generation College Students in an Online Chemistry Course},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448169},
doi = {10.1145/3448139.3448169},
abstract = {Although online courses can provide students with a high-quality and flexible learning experience, one of the caveats is that they require high levels of self-regulation. This added hurdle may have negative consequences for first-generation college students. In order to better understand and support students’ self-regulated learning, we examined a fully online Chemistry course with high enrollment (N = 312) and a high percentage of first-generation college students (65.70%). Using students’ lecture video clickstream data, we created two indicators of self-regulated learning: lecture video completion and time management. Performing a k-means clustering on these indicators uncovered four distinct self-regulated learning patterns: (1) Early Planning, (2) Planning, (3) Procrastination, and (4) Low Engagement. Early Planning behaviors were especially important for course success—they consistently predicted higher final course grades, even after controlling for important demographic variables. Interestingly, first-generation college students classified as Early Planners achieved at similar levels as their non-first-generation peers, but first-generation students in the Low Engagement group had the lowest average grades among students. Overall, our results show that self-regulation may be an important skill for determining first-generation students’ STEM achievement, and targeting these skills may serve as a useful way to support their specific learning needs.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {313–322},
numpages = {10},
keywords = {Underrepresented Students, Self-Regulation, STEM, Online Learning, College Students, Clickstream Data Mining},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448168,
author = {Jensen, Emily and L. Pugh, Samuel and K. D'Mello, Sidney},
title = {A Deep Transfer Learning Approach to Modeling Teacher Discourse in the Classroom},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448168},
doi = {10.1145/3448139.3448168},
abstract = {Teachers, like everyone else, need objective reliable feedback in order to improve their effectiveness. However, developing a system for automated teacher feedback entails many decisions regarding data collection procedures, automated analysis, and presentation of feedback for reflection. We address the latter two questions by comparing two different machine learning approaches to automatically model seven features of teacher discourse (e.g., use of questions, elaborated evaluations). We compared a traditional open-vocabulary approach using n-grams and Random Forest classifiers with a state-of-the-art deep transfer learning approach for natural language processing (BERT). We found a tradeoff between data quantity and accuracy, where deep models had an advantage on larger datasets, but not for smaller datasets, particularly for variables with low incidence rates. We also compared the models based on the level of feedback granularity: utterance-level (e.g., whether an utterance is a question or a statement), class session-level proportions by averaging across utterances (e.g., question incidence score of 48%), and session-level ordinal feedback based on pre-determined thresholds (e.g., question asking score is medium [vs. low or high]) and found that BERT generally provided more accurate feedback at all levels of granularity. Thus, BERT appears to be the most viable approach to providing automatic feedback on teacher discourse provided there is sufficient data to fine tune the model.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {302–312},
numpages = {11},
keywords = {Teaching Analytics, Teacher Discourse, Natural Language Processing, Deep Learning, Automated Feedback},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448167,
author = {Gurung, Ashish and Botelho, Anthony F. and Heffernan, Neil T.},
title = {Examining Student Effort on Help through Response Time Decomposition},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448167},
doi = {10.1145/3448139.3448167},
abstract = {Many teachers have come to rely on the affordances that computer-based learning platforms offer in regard to aiding in student assessment, supplementing instruction, and providing immediate feedback and help to students as they work through assigned content. Similarly, researchers commonly utilize the large datasets of clickstream logs describing students’ interactions with the platform to study learning. For the teachers that use this information to monitor student progress, as well as for researchers, this data provides limited insights into the learning process; this is particularly the case as it pertains to observing and understanding the effort that students are applying to their work. From the perspective of teachers, it is important for them to know which students are attending to and using computer-provided aid and which are taking advantage of the system to complete work without effectively learning the material. In this paper, we conduct a series of analyses based on response time decomposition (RTD) to explore student help-seeking behavior in the context of on-demand hints within a computer-based learning platform with particular focus on examining which students appear to be exhibiting effort to learn while engaging with the system. Our findings are then leveraged to examine how our measure of student effort correlates with later student performance measures.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {292–301},
numpages = {10},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448166,
author = {Carpenter, Dan and Cloude, Elizabeth and Rowe, Jonathan and Azevedo, Roger and Lester, James},
title = {Investigating Student Reflection during Game-Based Learning in Middle Grades Science},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448166},
doi = {10.1145/3448139.3448166},
abstract = {Reflection plays a critical role in learning by encouraging students to contemplate their knowledge and previous learning experiences to inform their future actions and higher-order thinking, such as reasoning and problem solving. Reflection is particularly important in inquiry-driven learning scenarios where students have the freedom to set goals and regulate their own learning. However, despite the importance of reflection in learning, there are significant theoretical, methodological, and analytical challenges posed by measuring, modeling, and supporting reflection. This paper presents results from a classroom study to investigate middle-school students’ reflection during inquiry-driven learning with Crystal Island, a game-based learning environment for middle-school microbiology. To collect evidence of reflection during game-based learning, we used embedded reflection prompts to elicit written reflections during students’ interactions with Crystal Island. Results from analysis of data from 105 students highlight relationships between features of students’ reflections and learning outcomes related to both science content knowledge and problem solving. We consider implications for building adaptive support in game-based learning environments to foster deep reflection and enhance learning, and we identify key features in students’ problem-solving actions and reflections that are predictive of reflection depth. These findings present a foundation for providing adaptive support for reflection during game-based learning.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {280–291},
numpages = {12},
keywords = {Self-Regulated Learning, Reflection, Game-Based Learning},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448165,
author = {Hakami, Eyad and Hernandez-Leo, Davinia},
title = {Investigating the Well-being Impacts of Educational Technologies Supported by Learning Analytics: An application of the initial phase of IEEE P7010 recommended practice to a set of cases},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448165},
doi = {10.1145/3448139.3448165},
abstract = {The accelerated adoption of digital technologies by people and communities results in a close relation between, on one hand, the state of individual and societal well-being and, on the other hand, the state of the digital technologies that underpin our life experiences. The ethical concerns and questions about the impact of such technologies on human well-being become more crucial when data analytics and intelligent competences are integrated. To investigate how learning technologies could impact human well-being considering the promising and concerning roles of learning analytics, we apply the initial phase of the recently produced IEEE P7010 Well-being Impact Assessment, a methodology and a set of metrics, to allow the digital well-being of a set of educational technologies to be more comprehensively tackled and evaluated. We posit that the use of IEEE P7010 well-being metrics could help identify where educational technologies supported by learning analytics would increase or decrease well-being, providing new routes to future technological innovation in Learning Analytics research.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {269–279},
numpages = {11},
keywords = {Values, Learning analytics, Ethics, Digital well-being},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448164,
author = {Salehian Kia, Fatemeh and Hatala, Marek and Baker, Ryan S. and Teasley, Stephanie D.},
title = {Measuring Students’ Self-Regulatory Phases in LMS with Behavior and Real-Time Self Report},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448164},
doi = {10.1145/3448139.3448164},
abstract = {Research has emphasized that self-regulated learning (SRL) is critically important for learning. However, students have different capabilities of regulating their learning processes and individual needs. To help students improve their SRL capabilities, we need to identify students’ current behaviors. Specifically, we applied instructional design to create visible and meaningful markers of student learning at different points in time in LMS logs. We adopted knowledge engineering to develop a framework of proximal indicators representing SRL phases and evaluated them in a quasi-experiment in two different learning activities. A comparison of two sources of collected students’ SRL data, self-reported and trace data, revealed a relatively high agreement between our classifications (weighted kappa, κ = .74 and κ = .68). However, our indicators did not always discriminate adjacent SRL phases, particularly for enactment and adapting phases, compared with students’ real-time self-reported behaviors. Our behavioral indicators also were comparably successful at classifying SRL phases for different self-regulatory engagement levels. This study demonstrated how the triangulation of various sources of students’ self-regulatory data could help to unravel the complex nature of metacognitive processes.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {259–268},
numpages = {10},
keywords = {self-reported measures, self-regulated learning, pattern recognition, knowledge-engineered trace measures},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448163,
author = {Farrow, Elaine and Moore, Johanna and Gasevic, Dragan},
title = {A network analytic approach to integrating multiple quality measures for asynchronous online discussions},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448163},
doi = {10.1145/3448139.3448163},
abstract = {Asynchronous online discussions within a community of learners can improve learning outcomes through social knowledge construction, but the depth and quality of student contributions often varies widely. Approaches to assessing critical discourse typically use content analysis to identify indicators that correspond to framework constructs, that in turn serve as measures of depth and quality. Often only a single construct is addressed for performing content analysis in the literature, although recent work has used both social presence and cognitive presence constructs from the Community of Inquiry (CoI) framework. Nevertheless, there is no effective, commonly used, analytic approach to combining insights from multiple perspectives about quality and depth of online discussions. This paper addresses the gap by proposing the combined use of cognitive engagement (the ICAP framework) and cognitive presence (CoI); and by proposing a network analytic approach that quantifies the associations between the two frameworks and measures the moderation effects of two instructional interventions on those associations. The present study found that these associations were moderated by one intervention but not the other; and that messages labelled with the most common phase of cognitive presence could be usefully assigned to smaller meaningful subgroups by also considering the mode of cognitive engagement.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {248–258},
numpages = {11},
keywords = {discussion forum, critical thinking, cognitive presence, ICAP, Epistemic Network Analysis, Community of Inquiry},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448162,
author = {Gray, Geraldine and Schalk, Ana and Rooney, Pauline and Lang, Charles},
title = {A Stakeholder Informed Professional Development Framework to Support Engagement with Learning Analytics},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448162},
doi = {10.1145/3448139.3448162},
abstract = {This paper reports on a study aimed at identifying training requirements for both staff and students in higher education to enable more widespread use of learning analytics. Opinions of staff and students were captured through ten focus groups (37 students; 40 staff) and two surveys (1,390 students; 160 staff). Participants were predominantly from two higher education institutions in Ireland. Analysis of the results informed a framework for continuous professional development in learning analytics focusing on aspects of using data, legal and ethical considerations, policy, and workload. The framework presented here differentiates between the training needs of students, academic staff and professional services staff.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {237–247},
numpages = {11},
keywords = {stakeholder perspectives, higher education, continuous professional development, Learning analytics},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448161,
author = {Zhou, Jianing and Bhat, Suma},
title = {Modeling Consistency Using Engagement Patterns in Online Courses},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448161},
doi = {10.1145/3448139.3448161},
abstract = {Consistency of learning behaviors is known to play an important role in learners’ engagement in a course and impact their learning outcomes. Despite significant advances in the area of learning analytics (LA) in measuring various self-regulated learning behaviors, using LA to measure consistency of online course engagement patterns remains largely unexplored. This study focuses on modeling consistency of learners in online courses to address this research gap. Toward this, we propose a novel unsupervised algorithm that combines sequence pattern mining and ideas from information retrieval with a clustering algorithm to first extract engagement patterns of learners, represent learners in a vector space of these patterns and finally group them into groups with similar consistency levels. Using clickstream data recorded in a popular learning management system over two offerings of a STEM course, we validate our proposed approach to detect learners that are inconsistent in their behaviors. We find that our method not only groups learners by consistency levels, but also provides reliable instructor support at an early stage in a course.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {226–236},
numpages = {11},
keywords = {consistency analysis, cluster, behavior modeling},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448160,
author = {Matz, Rebecca and Schulz, Kyle and Hanley, Elizabeth and Derry, Holly and Hayward, Benjamin and Koester, Benjamin and Hayward, Caitlin and McKay, Timothy},
title = {Analyzing the Efficacy of ECoach in Supporting Gateway Course Success Through Tailored Support},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448160},
doi = {10.1145/3448139.3448160},
abstract = {Large courses act as gateways for college students and often have poor outcomes, particularly in STEM fields where the pace of improvement has been glacial. Students encounter barriers to persistence like low grades, competitive cultures, and a lack of motivation and belonging. Tailored technology systems offer one promising path forward. In this observational study, we report on the use of one such system, called ECoach, that provides students resources based on their psychosocial profile, performance metrics, and pattern of ECoach usage. We investigated ECoach efficacy in five courses enrolling 3,599 students using a clustering method to group users by engagement level and subsequent regression analyses. We present results showing significant positive relationships with small effect sizes between ECoach engagement and final course grade as well as grade anomaly, a performance measure that takes into account prior course grades. The courses with the strongest relationship between ECoach engagement and performance offered nominal extra credit incentives yet show improved grades well above this “investment” from instructors. Such small incentives may act as a catalyst that spurs deeper engagement with the platform. The impact of specific ECoach features and areas for future study are discussed.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {216–225},
numpages = {10},
keywords = {undergraduate education, learning analytics, large enrollment courses, higher education, feedback, educational technology, STEM, Academic achievement},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448159,
author = {Williams-Dobosz, Destiny and Azevedo, Renato Ferreira Leit\~{a}o and Jeng, Amos and Thakkar, Vyom and Bhat, Suma and Bosch, Nigel and Perry, Michelle},
title = {A Social Network Analysis of Online Engagement for College Students Traditionally Underrepresented in STEM},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448159},
doi = {10.1145/3448139.3448159},
abstract = {Little is known about the online learning behaviors of students traditionally underrepresented in STEM fields (i.e., UR-STEM students), as well as how those behaviors impact important learning outcomes. The present study examined the relationship between online discussion forum engagement and success for UR-STEM and non-UR-STEM students, using the Community of Inquiry (CoI) model as our theoretical framework. Social network analysis and nested regression models were used to explore how three different measures of forum engagement—1) total number of posts written, 2) number of help-seeking posts written and replied to, and 3) level of connectivity—were related to improvement (i.e., relative performance gains) for 70 undergraduate students enrolled in an online introductory STEM course. We found a significant positive relationship between help-seeking and improvement and nonsignificant effects of general posting and connectivity; these results held for UR-STEM and non-UR-STEM students alike. Our findings suggest that online help-seeking has benefits for course improvement beyond what can be predicted by posting alone and that one need not be well connected in a class network to achieve positive learning outcomes. Finally, UR-STEM students demonstrated greater grade improvement than their non-UR-STEM counterparts, which suggests that the online environment has the potential to combat barriers to success that disproportionately affect underrepresented students.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {207–215},
numpages = {9},
keywords = {Underrepresentation in STEM, Social network connectivity, Online discussion forums, Help-seeking},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448158,
author = {Er, Erkan and Villa-Torrano, Cristina and Dimitriadis, Yannis and Gasevic, Dragan and Bote-Lorenzo, Miguel L. and Asensio-P\'{e}rez, Juan I. and G\'{o}mez-S\'{a}nchez, Eduardo and Mart\'{\i}nez Mon\'{e}s, Alejandra},
title = {Theory-based learning analytics to explore student engagement patterns in a peer review activity},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448158},
doi = {10.1145/3448139.3448158},
abstract = {Peer reviews offer many learning benefits. Understanding students’ engagement in them can help design effective practices. Although learning analytics can be effective in generating such insights, its application in peer reviews is scarce. Theory can provide the necessary foundations to inform the design of learning analytics research and the interpretation of its results. In this paper, we followed a theory-based learning analytics approach to identifying students’ engagement patterns in a peer review activity facilitated via a web-based tool called Synergy. Process mining was applied on temporal learning data, traced by Synergy. The theory about peer review helped determine relevant data points and guided the top-down approach employed for their analysis: moving from the global phases to regulation of learning, and then to micro-level actions. The results suggest that theory and learning analytics should mutually relate with each other. Mainly, theory played a critical role in identifying a priori engagement patterns, which provided an informed perspective when interpreting the results. In return, the results of the learning analytics offered critical insights about student behavior that was not expected by the theory (i.e., low levels of co-regulation). The findings provided important implications for refining the grounding theory and its operationalization in Synergy.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {196–206},
numpages = {11},
keywords = {student engagement, process mining, learning analytics, Peer reviews},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448156,
author = {Diederich, Morgan and Kang, Jina and Kim, Taehyun and Lindgren, Robb},
title = {Developing an In-Application Shared View Metric to Capture Collaborative Learning in a Multi-Platform Astronomy Simulation},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448156},
doi = {10.1145/3448139.3448156},
abstract = {There has been recent interest in the design of collaborative learning activities that are distributed across multiple technology devices for students to engage in scientific inquiry. Emerging research has begun to investigate students’ collaborative behaviors across different device types and students’ shared attention by tracking eye gaze, body posture, and their interactions with the digital environment. Using a 3D astronomy simulation that leverages a VR headset and tablet computers, this paper builds on the ideas described in eye-gaze studies by developing and implementing a metric of shared viewing across multiple devices. Preliminary findings suggest that a higher level of shared view could be related to increased conceptual discussion, as well as point to an early-stage pattern of behavior of decreased SV to prompt facilitator intervention to refocus collaborative efforts. We hope this metric will be a promising first step in further understanding and assessing the quality of collaboration across multiple device platforms in a single shared space. This paper provides an in depth look at a highly exploratory stage of a broader research trajectory to establish a robust, effective way to track screen views, including providing resources to teachers when students engage in similar learning environments, and providing insight from log data to understand how students effectively collaborate.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {173–183},
numpages = {11},
keywords = {Shared view, Science education, Log data, Immersive virtual reality, Astronomy education},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448155,
author = {Huang, Yun and Lobczowski, Nikki G. and Richey, J. Elizabeth and McLaughlin, Elizabeth A. and Asher, Michael W. and Harackiewicz, Judith M. and Aleven, Vincent and Koedinger, Kenneth R.},
title = {A General Multi-method Approach to Data-Driven Redesign of Tutoring Systems},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448155},
doi = {10.1145/3448139.3448155},
abstract = {Analytics of student learning data are increasingly important for continuous redesign and improvement of tutoring systems and courses. There is still a lack of general guidance on converting analytics into better system design, and on combining multiple methods to maximally improve a tutor. We present a multi-method approach to data-driven redesign of tutoring systems and its empirical evaluation. Our approach systematically combines existing and new learning analytics and instructional design methods. In particular, our methods involve identifying difficult skills and creating focused tasks for learning these difficult skills effectively following content redesign strategies derived from analytics. In our past work, we applied this approach to redesigning an algebraic modeling unit and found initial evidence of its effectiveness. In the current work, we extended this approach and applied it to redesigning two other tutor units in addition to a second iteration of redesigning the previously redesigned unit. We conducted a one-month classroom experiment with 129 high school students. Compared to the original tutor, the redesigned tutor led to significantly higher learning outcomes, with time mainly allocated to focused tasks rather than original full tasks. Moreover, it reduced over- and under-practice, yielded a more effective practice experience, and selected skills progressing from easier to harder to a greater degree. Our work provides empirical evidence of the effectiveness and generality of a multi-method approach to data-driven instructional redesign.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {161–172},
numpages = {12},
keywords = {learning engineering, learning design, instructional design, data mining, adaptivity},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448154,
author = {Karumbaiah, Shamya and Lan, Andrew and Nagpal, Sachit and Baker, Ryan S. and Botelho, Anthony and Heffernan, Neil},
title = {Using Past Data to Warm Start Active Machine Learning: Does Context Matter?},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448154},
doi = {10.1145/3448139.3448154},
abstract = {Despite the abundance of data generated from students’ activities in virtual learning environments, the use of supervised machine learning in learning analytics is limited by the availability of labeled data, which can be difficult to collect for complex educational constructs. In a previous study, a subfield of machine learning called Active Learning (AL) was explored to improve the data labeling efficiency. AL trains a model and uses it, in parallel, to choose the next data sample to get labeled from a human expert. Due to the complexity of educational constructs and data, AL has suffered from the cold-start problem where the model does not have access to sufficient data yet to choose the best next sample to learn from. In this paper, we explore the use of past data to warm start the AL training process. We also critically examine the implications of differing contexts (urbanicity) in which the past data was collected. To this end, we use authentic affect labels collected through human observations in middle school mathematics classrooms to simulate the development of AL-based detectors of engaged concentration. We experiment with two AL methods (uncertainty sampling, L-MMSE) and random sampling for data selection. Our results suggest that using past data to warm start AL training could be effective for some methods based on the target population's urbanicity. We provide recommendations on the data selection method and the quantity of past data to use when warm starting AL training in the urban and suburban schools.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {151–160},
numpages = {10},
keywords = {Warm start, Urbanicity, Model generalization, Affect detection},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448153,
author = {Zylich, Brian and Lan, Andrew},
title = {Linguistic Skill Modeling for Second Language Acquisition},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448153},
doi = {10.1145/3448139.3448153},
abstract = {To adapt materials for an individual learner, intelligent tutoring systems must estimate their knowledge or abilities. Depending on the content taught by the tutor, there have historically been different approaches to student modeling. Unlike common skill-based models used by math and science tutors, second language acquisition (SLA) tutors use memory-based models since there are many tasks involving memorization and retrieval, such as learning the meaning of a word in a second language. Based on estimated memory strengths provided by these memory-based models, SLA tutors are able to identify the optimal timing and content of retrieval practices for each learner to improve retention. In this work, we seek to determine whether skill-based models can be combined with memory-based models to improve student modeling and especially retrieval practice performance for SLA. In order to define skills in the context of SLA, we develop methods that can automatically extract multiple types of linguistic features from words. Using these features as skills, we apply skill-based models to a real-world SLA dataset. Our main findings are as follows. First, incorporating lexical features to represent individual words as skills in skill-based models outperforms existing memory-based models in terms of recall probability prediction. Second, incorporating additional morphological and syntactic features of each word via multiple-skill tagging of each word further improves the skill-based models. Third, incorporating semantic features, like word embeddings, to model similarities between words in a learner’s practice history and their effects on memory also improves the models and appears to be a promising direction for future research.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {141–150},
numpages = {10},
keywords = {student modeling, second language acquisition, memory decay},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448152,
author = {Schlotterbeck, Danner and Uribe, Pablo and Araya, Roberto and Jimenez, Abelino and Caballero, Daniela},
title = {What Classroom Audio Tells About Teaching: A Cost-effective Approach for Detection of Teaching Practices Using Spectral Audio Features},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448152},
doi = {10.1145/3448139.3448152},
abstract = {Acoustic features and machine learning models have been recently proposed as promising tools to analyze lessons. Furthermore, acoustic patterns, both in the time and spectral domain, have been found to be related to teacher pedagogical practices. Nonetheless, most of previous work relies on expensive or third party equipment, limiting its scalability, and additionally, it is mainly used for diarization. Instead, in this work we present a cost-effective approach to identify teachers’ practices according to three categories (Presenting, Administration, and Guiding) which are compiled from the Classroom Observation Protocol for Undergraduate STEM. Particularly, we record teachers’ lessons using low-cost microphones connected to their smartphones. We then compute the mean and standard deviation of the amplitude, Mel spectrogram, and Mel Frequency Cepstral coefficients of the recordings to train supervised models for the task of predicting three categories compiled from the Classroom Observation Protocol for Undergraduate STEM. We found that spectral features perform better at the task of predicting teachers’ activities along the lessons and that our models can predict the presence of the two most common teaching practices with over 80% of accuracy and good discriminative power. Finally, with these models, we found that using audio obtained from the teachers’ smartphones it is also possible to automatically discriminate between sessions where students are using or not an online platform. This approach is important for teachers and other stakeholders who could use an automatic and cost-effective tool for analyzing teaching practices.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {132–140},
numpages = {9},
keywords = {Teacher Activity Detection, Spectral Audio Features, Classroom Sound, COPUS},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448151,
author = {Jensen, Emily and Umada, Tetsumichi and Hunkins, Nicholas C. and Hutt, Stephen and Huggins-Manley, A. Corinne and D'Mello, Sidney K.},
title = {What You Do Predicts How You Do: Prospectively Modeling Student Quiz Performance Using Activity Features in an Online Learning Environment},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448151},
doi = {10.1145/3448139.3448151},
abstract = {Students using online learning environments need to effectively self-regulate their learning. However, with an absence of teacher-provided structure, students often resort to less effective, passive learning strategies versus constructive ones. We consider the potential benefits of interventions that promote retrieval practice – retrieving learned content from memory – which is an effective strategy for learning and retention. The goal is to nudge students towards completing short, formative quizzes when they are likely to succeed on those assessments. Towards this goal, we developed a machine-learning model using data from 32,685 students who used an online mathematics platform over an entire school year to prospectively predict scores on three-item assessments (N = 210,020) from interaction patterns up to 9 minutes before the assessment as well as Item Response Theory (IRT) estimates of student ability and quiz difficulty. These models achieved a student-independent correlation of 0.55 between predicted and actual scores on the assessments and outperformed IRT-only predictions (r = 0.34). Model performance was largely independent of the length of the analyzed window preceding a quiz. We discuss potential for future applications of the models to trigger dynamic interventions that aim to encourage students to engage with formative assessments rather than more passive learning strategies.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {121–131},
numpages = {11},
keywords = {Retrieval Practice, Predicting Student Performance, Online Learning, Machine Learning, Item Response Theory, Formative assessment},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448150,
author = {Zhang, Tom and Taub, Michelle and Chen, Zhongzhou},
title = {Measuring the Impact of COVID-19 Induced Campus Closure on Student Self-Regulated Learning in Physics Online Learning Modules},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448150},
doi = {10.1145/3448139.3448150},
abstract = {This paper examines the impact of COVID-19 induced campus closure on university students’ self-regulated learning behavior by analyzing click-stream data collected from student interactions with 70 online learning modules in a university physics course. To do so, we compared the trend of six types of actions related to the three phases of self-regulated learning before and after campus closure and between two semesters. We found that campus closure changed students’ planning and goal setting strategies for completing the assignments, but didn’t have a detectable impact on the outcome or the time of completion, nor did it change students’ self-reflection behavior. The results suggest that most students still manage to complete assignments on time during the pandemic, while the design of online learning modules might have provided the flexibility and support for them to do so.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {110–120},
numpages = {11},
keywords = {Self-regulated learning, Online learning environments, Click-stream data},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448149,
author = {Chan, Wai-Lun and Yeung, Dit-Yan},
title = {Clickstream Knowledge Tracing: Modeling How Students Answer Interactive Online Questions},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448149},
doi = {10.1145/3448139.3448149},
abstract = {Knowledge tracing (KT) is a research topic which seeks to model the knowledge acquisition process of students by analyzing their past performance in answering questions, based on which their performance in answering future questions is predicted. However, existing KT models only consider whether a student answers a question correctly when the answer is submitted but not the in-question activities. We argue that the interaction involved in the in-question activities can at least partially reveal the thinking process of the student, and hopefully even the competence of acquiring or understanding each piece of the knowledge required for the question. Based on real student interaction clickstream data collected from an online learning platform on which students solve mathematics problems, we conduct clustering analysis for each question to show that clickstreams can reflect different student behaviors. We then propose the first clickstream-based KT model, dubbed clickstream knowledge tracing (CKT), which augments a basic KT model by modeling the clickstream activities of students when answering questions. We apply different variants of CKT and compare them with the baseline KT model which does not use clickstream data. Despite the limited number of questions with clickstream data and its noisy nature which may compromise the data quality, we show that incorporating clickstream data leads to performance improvement. Through this pilot study, we hope to open a new direction in KT research to analyze finer-grained interaction data of students on online learning platforms.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {99–109},
numpages = {11},
keywords = {Student Behavior Clustering, Learning Analytics, Learner Modeling, Knowledge Tracing, Knowledge Modeling, Interactive Questions, Clickstream Analytics},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448148,
author = {Srivastava, Namrata and Nawaz, Sadia and Newn, Joshua and Lodge, Jason and Velloso, Eduardo and M. Erfani, Sarah and Gasevic, Dragan and Bailey, James},
title = {Are you with me? Measurement of Learners’ Video-Watching Attention with Eye Tracking},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448148},
doi = {10.1145/3448139.3448148},
abstract = {Video has become an essential medium for learning. However, there are challenges when using traditional methods to measure how learners attend to lecture videos in video learning analytics, such as difficulty in capturing learners’ attention at a fine-grained level. Therefore, in this paper, we propose a gaze-based metric—“with-me-ness direction” that can measure how learners’ gaze-direction changes when they listen to the instructor’s dialogues in a video-lecture. We analyze the gaze data of 45 participants as they watched a video lecture and measured both the sequences of with-me-ness direction and proportion of time a participant spent looking in each direction throughout the lecture at different levels. We found that although the majority of the time participants followed the instructor’s dialogues, their behaviour of looking-ahead, looking-behind or looking-outside differed by their prior knowledge. These findings open the possibility of using eye-tracking to measure learners’ video-watching attention patterns and examine factors that can influence their attention, thereby helping instructors to design effective learning materials.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {88–98},
numpages = {11},
keywords = {video lecture, learning analytics, gaze direction, eye-tracking, co-attention},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448147,
author = {Barbosa, Arthur and Ferreira, M\'{a}verick and Ferreira Mello, Rafael and Dueire Lins, Rafael and Gasevic, Dragan},
title = {The impact of automatic text translation on classification of online discussions for social and cognitive presences},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448147},
doi = {10.1145/3448139.3448147},
abstract = {This paper reports the findings of a study that measured the effectiveness of employing automatic text translation methods in automated classification of online discussion messages according to the categories of social and cognitive presences. Specifically, we examined the classification of 1,500 Portuguese and 1,747 English discussion messages using classifiers trained on the datasets before and after the application of text translation. While the English model generated, with the original and translated texts, achieved results (accuracy and Cohen’s κ) similar to those of the previously reported studies, the translation to Portuguese led to a decrease in the performance. The indicates the general viability of the proposed approach when converting the text to English. Moreover, this study highlighted the importance of different features and resources, and the limitations of the resources for Portuguese as reasons of the results obtained.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {77–87},
numpages = {11},
keywords = {Text Translation, Online Discussion, Content Analytics, Community of Inquiry Model},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448146,
author = {Ahn, June and Nguyen, Ha and Campos, Fabio and Young, William},
title = {Transforming Everyday Information into Practical Analytics with Crowdsourced Assessment Tasks},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448146},
doi = {10.1145/3448139.3448146},
abstract = {Educators use a wide variety of data to inform their practices. Examples of these data include forms of information that are commonplace in schools, such as student work and paper-based artifacts. One limitation in these situations is that there are less efficient ways to process such everyday varieties of information into analytics that are more usable and practical for educators. To explore how to address this constraint, we describe two sets of design experiments that utilize crowdsourced tasks for scoring open-ended assessments. Developing crowdsourced systems and their resulting analytics introduced a variety of challenges, such as attending to the expertise and learning of the crowd. In this paper, we describe the potential efficacy of design decisions such as screening the crowd, providing multimedia instruction, and asking the crowd to explain their answers. We also explore the potential of crowdsourcing as a learning opportunity for those participating in the collective tasks. Our work offers key design implications for leveraging crowdsourcing to process educational data in ways that are relevant to educators, while offering learning experiences for the crowd.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {66–76},
numpages = {11},
keywords = {worker training, education, crowdsourcing, assessment},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448145,
author = {Ahn, June and Campos, Fabio and Nguyen, Ha and Hays, Maria and Morrison, Jan},
title = {Co-Designing for Privacy, Transparency, and Trust in K-12 Learning Analytics},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448145},
doi = {10.1145/3448139.3448145},
abstract = {The process of using Learning Analytics (LA) to improve teaching works from the assumption that data should be readily shared between stakeholders in an educational organization. However, the design of LA tools often does not account for considerations such as data privacy, transparency and trust among stakeholders. Research in human-centered design of LA does attend to these questions, specifically with a focus on including direct input from K-12 educators. In this paper, we present a series of design studies to articulate and refine conjectures about how privacy and transparency might influence better trust-building and data sharing within four school districts in the United States. By presenting the development of four sequential prototypes, our findings illuminate the tensions between designing for existing norms versus potentially challenging these norms by promoting meaningful discussions around the use of data. We conclude with a discussion about practical and methodological implications of our work to the LA community.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {55–65},
numpages = {11},
keywords = {Privacy, K-12 Education, HCI, Ethics, Dashboards, Co-Design},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448144,
author = {Yan, Lixiang and Martinez-Maldonado, Roberto and Cordoba, Beatriz Gallo and Deppeler, Joanne and Corrigan, Deborah and Nieto, Gloria Fernandez and Gasevic, Dragan},
title = {Footprints at School: Modelling In-class Social Dynamics from Students’ Physical Positioning Traces},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448144},
doi = {10.1145/3448139.3448144},
abstract = {Schools are increasingly becoming into complex learning spaces where students interact with various physical and digital resources, educators, and peers. Although the field of learning analytics has advanced in analysing logs captured from digital tools, less progress has been made in understanding the social dynamics that unfold in physical learning spaces. Among the various rapidly emerging sensing technologies, position tracking may hold promises to reveal salient aspects of activities in physical learning spaces such as the formation of interpersonal ties among students. This paper explores how granular x-y physical positioning data can be analysed to model social interactions among students and teachers. We conducted an 8-week longitudinal study in which positioning traces of 98 students and six teachers were automatically captured every day in an open-plan public primary school. Positioning traces were analysed using social network analytics (SNA) to extract a set of metrics to characterise students’ positioning behaviours and social ties at cohort and individual levels. Results illustrate how analysing positioning traces through the lens of SNA can enable the identification of certain pedagogical approaches that may be either promoting or discouraging in-class social interaction, and students who may be socially isolated.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {43–54},
numpages = {12},
keywords = {social ties, social networks analysis, proxemics, indoor positioning, classroom analytics},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448143,
author = {Khosravi, Hassan and Demartini, Gianluca and Sadiq, Shazia and Gasevic, Dragan},
title = {Charting the Design and Analytics Agenda of Learnersourcing Systems},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448143},
doi = {10.1145/3448139.3448143},
abstract = {Learnersourcing is emerging as a viable learner-centred and pedagogically justified approach for harnessing the creativity and evaluation power of learners as experts-in-training. Despite the increasing adoption of learnersourcing in higher education, understanding students’ behaviour while engaged in learnersourcing and best practices for the design and development of learnersourcing systems are still largely under-researched. This paper offers data-driven reflections and lessons learned from the development and deployment of a learnersourcing adaptive educational system called RiPPLE, which to date, has been used in more than 50-course offerings with over 12,000 students. Our reflections are categorised into examples and best practices on (1) assessing the quality of students’ contributions using accurate, explainable and fair approaches to data analysis, (2) incentivising students to develop high-quality contributions and (3) empowering instructors with actionable and explainable insights to guide student learning. We discuss the implications of these findings and how they may contribute to the growing literature on the development of effective learnersourcing systems and more broadly technological educational solutions that support learner-centred learning at scale.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {32–42},
numpages = {11},
keywords = {human-centred computing, explainable AI, crowdsourcing in education, Learnersourcing},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448142,
author = {Shusterman, Einat and Kim, Hyunsoo Gloria and Facciotti, Marc and Igo, Michele and Sripathi, Kamali and Karger, David and Segal, Avi and Gal, Kobi},
title = {Seeding Course Forums using the Teacher-in-the-Loop},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448142},
doi = {10.1145/3448139.3448142},
abstract = {Online forums are an integral part of modern day courses, but motivating students to participate in educationally beneficial discussions can be challenging. Our proposed solution is to initialize (or “seed”) a new course forum with comments from past instances of the same course that are intended to trigger discussion that is beneficial to learning. In this work, we develop methods for selecting high-quality seeds and evaluate their impact over one course instance of a 186-student biology class. We designed a scale for measuring the “seeding suitability” score of a given thread (an opening comment and its ensuing discussion). We then constructed a supervised machine learning (ML) model for predicting the seeding suitability score of a given thread. This model was evaluated in two ways: first, by comparing its performance to the expert opinion of the course instructors on test/holdout data; and second, by embedding it in a live course, where it was actively used to facilitate seeding by the course instructors. For each reading assignment in the course, we presented a ranked list of seeding recommendations to the course instructors, who could review the list and filter out seeds with inconsistent or malformed content. We then ran a randomized controlled study, in which one group of students was shown seeds that were recommended by the ML model, and another group was shown seeds that were recommended by an alternative model that ranked seeds purely by the length of discussion that was generated in previous course instances. We found that the group of students that received posts from either seeding model generated more discussion than a control group in the course that did not get seeded posts. Furthermore, students who received seeds selected by the ML-based model showed higher levels of engagement, as well as greater learning gains, than those who received seeds ranked by length of discussion.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {22–31},
numpages = {10},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448141,
author = {G\"{u}nther, Sebastian A.},
title = {The impact of social norms on students’ online learning behavior: Insights from two randomized controlled trials},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448141},
doi = {10.1145/3448139.3448141},
abstract = {The provision of comparative feedback is a promising approach in digital learning environments to support learners’ self-regulated learning. Yet, empirical evidence suggests that such feedback can sometimes backfire or may only help learners with relatively high self-regulated learning skills, potentially exacerbating educational inequality. In this paper, we try to overcome such drawbacks by re-evaluating a feedback system based on the social norms theory that has previously led to intriguing results: A social comparison component embedded into the learning platform of a blended learning course (elective module, 58 participants) considerably encouraged online learning during the semester. Moreover, there was no heterogeneity in the behavioral response, suggesting that all subgroups responded similarly to the feedback. To further shed light on the generalizability of these results, this paper presents a follow-up study. Specifically, we conducted a second experiment during the COVID-19 pandemic with a different university course (compulsory module, 118 participants) and a non-overlapping sample and find similar results. The feedback shifted students’ online learning from the end towards the middle of the semester. Overall, the findings suggest that our feedback system has a large impact on students’ online learning and that this desirable impact is present in all subgroup analyses.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {12–21},
numpages = {10},
keywords = {social norms, replication study, generalizability, field experiment, feedback, behavioral intervention, Online learning},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448140,
author = {Effenberger, Tomas and Pel\'{a}nek, Radek},
title = {Validity and Reliability of Student Models for Problem-Solving Activities},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448140},
doi = {10.1145/3448139.3448140},
abstract = {Student models are typically evaluated through predicting the correctness of the next answer. This approach is insufficient in the problem-solving context, especially for student models that use performance data beyond binary correctness. We propose more comprehensive methods for validating student models and illustrate them in the context of introductory programming. We demonstrate the insufficiency of the next answer correctness prediction task, as it is neither able to reveal low validity of student models that use just binary correctness, nor does it show increased validity of models that use other performance data. The key message is that the prevalent usage of the next answer correctness for validating student models and binary correctness as the only input to the models is not always warranted and limits the progress in learning analytics.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {1–11},
numpages = {11},
keywords = {validity, student modeling, skills, reliability, problem solving, performance measures, introductory programming, difficulties},
location = {Irvine, CA, USA},
series = {LAK21}
}

@proceedings{10.1145/3448139,
title = {LAK21: LAK21: 11th International Learning Analytics and Knowledge Conference},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Irvine, CA, USA}
}

@inproceedings{10.1145/3375462.3375542,
author = {Agrawal, Sweety and Lalwani, Amar},
title = {Decoding the performance in an out-of-context problem during blocked practice},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375542},
doi = {10.1145/3375462.3375542},
abstract = {To master a skill, students generally practice the content of the skill in a blocking manner. While practicing in a blocked fashion, students know the context of the problems and also which strategy is needed to arrive at a solution. However, in real life standardized tests, where problems from various skills are grouped together, students often find it challenging to identify the correct strategy to solve the problems. This is because, during learning, students often practice the content in isolation. It hinders their ability to discriminate among the contexts of the problem. In this work, using tutor funtoot, we present students working on the topic Addition Word Problems with a subtraction word problem and investigate how they perform in the out-of-context subtraction word problem. We find that students' performance in the topic Addition Word Problems is a strong predictor of their performance in this out-of-context problem. Our results suggest that it is a stronger predictor for higher grades (4th and 5th) compared to the lower (2nd and 3rd) grades.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {118–123},
numpages = {6},
keywords = {mathematics, massed practice, interleaving, blocking, K-12 education, ITS},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375541,
author = {Saarinen, Sam and Cater, Evan and Littman, Michael L.},
title = {Applying prerequisite structure inference to adaptive testing},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375541},
doi = {10.1145/3375462.3375541},
abstract = {Modeling student knowledge is important for assessment design, adaptive testing, curriculum design, and pedagogical intervention. The assessment design community has primarily focused on continuous latent-skill models with strong conditional independence assumptions among knowledge items, while the prerequisite discovery community has developed many models that aim to exploit the interdependence of discrete knowledge items. This paper attempts to bridge the gap by asking, "When does modeling assessment item interdependence improve predictive accuracy?" A novel adaptive testing evaluation framework is introduced that is amenable to techniques from both communities, and an efficient algorithm, Directed Item-Dependence And Confidence Thresholds (DIDACT), is introduced and compared with an Item-Response-Theory based model on several real and synthetic datasets. Experiments suggest that assessments with closely related questions benefit significantly from modeling item interdependence.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {422–427},
numpages = {6},
keywords = {prerequisite inference, knowledge models, assessment design, adaptive testing},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375540,
author = {Wiley, Korah J. and Dimitriadis, Yannis and Bradford, Allison and Linn, Marica C.},
title = {From theory to action: developing and evaluating learning analytics for learning design},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375540},
doi = {10.1145/3375462.3375540},
abstract = {The effectiveness of using learning analytics for learning design primarily depends upon two concepts: grounding and alignment. This is the primary conjecture for the study described in this paper. In our design-based research study, we design, test, and evaluate teacher-facing learning analytics for an online inquiry science unit on global climate change. We design our learning analytics in accordance with a socioconstructivism-based pedagogical framework, called Knowledge Integration, and the principles of learning analytics Implementation Design. Our methodology for the design process draws upon the principle of the Orchestrating for Learning Analytics framework to engage stakeholders (i.e. teachers, researchers, and developers). The resulting learning analytics were aligned to unit activities that engaged students in key aspects of the knowledge integration process. They provided teachers with actionable insight into their students' understanding at critical junctures in the learning process. We demonstrate the efficacy of the learning analytics in supporting the optimization of the unit's learning design. We conclude by synthesizing the principles that guided our design process into a framework for developing and evaluating learning analytics for learning design.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {569–578},
numpages = {10},
keywords = {theory, learning design, learning analytics, design-based research, TEL environment},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375539,
author = {Shabaninejad, Shiva and Khosravi, Hassan and Indulska, Marta and Bakharia, Aneesha and Isaias, Pedro},
title = {Automated insightful drill-down recommendations for learning analytics dashboards},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375539},
doi = {10.1145/3375462.3375539},
abstract = {The big data revolution is an exciting opportunity for universities, which typically have rich and complex digital data on their learners. It has motivated many universities around the world to invest in the development and implementation of learning analytics dashboards (LADs). These dashboards commonly make use of interactive visualisation widgets to assist educators in understanding and making informed decisions about the learning process. A common operation in analytical dashboards is a 'drill-down', which in an educational setting allows users to explore the behaviour of sub-populations of learners by progressively adding filters. Nevertheless, drill-down challenges exist, which hamper the most effective use of the data, especially by users without a formal background in data analysis. Accordingly, in this paper, we address this problem by proposing an approach that recommends insightful drill-downs to LAD users. We present results from an application of our proposed approach using an existing LAD. A set of insightful drill-down criteria from a course with 875 students are explored and discussed.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {41–46},
numpages = {6},
keywords = {learning analytics dashboards, exploratory data analysis, drill-down analysis, decision trees},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375538,
author = {Nguyen, Quan},
title = {Rethinking time-on-task estimation with outlier detection accounting for individual, time, and task differences},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375538},
doi = {10.1145/3375462.3375538},
abstract = {Time-on-task estimation, measured as the duration between two consecutive clicks using student log-files data, has been one of the most frequently used metrics in learning analytics research. However, the process of handling outliers (i.e., excessively long durations) in time-on-task estimation is under-explored and often not explicitly reported in many studies. One common approach to handle outliers in time-to-task estimation is to 'trim' all durations using a cut-off threshold, such as 60 or 30 minutes. This paper challenges this existing approach by demonstrating that the treatment of outliers in an educational context should be individual-specific, time-specific, and task-specific. In other words, what can be considered as outliers in time-on-task depends on the learning pattern of each student, the stages during the learning process, and the nature of the task involved. The analysis showed that predictive models using time-on-task estimation accounting for individual, time, and task differences could explain 3--4% more variances in academic performance than models using an outlier trimming approach. As an implication, this study provides a theoretically grounded and replicable outlier detection approach for future learning analytics research when using time-on-task estimation.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {376–381},
numpages = {6},
keywords = {time-on-task, temporal analysis, outlier detection, measurement, learning analytics},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375537,
author = {Vezzoli, Yvonne and Mavrikis, Manolis and Vasalou, Asimina},
title = {Inspiration cards workshops with primary teachers in the early co-design stages of learning analytics},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375537},
doi = {10.1145/3375462.3375537},
abstract = {Despite the recognition of the need to include practitioners in the design of learning analytics (LA), especially teacher input tends to come later in the design process rather than in the definition of the initial design agenda. This paper presents a case study of a design project tasked with developing LA tools for a reading game for primary school children. Taking a co-design approach, we use the Inspiration Cards Workshop to promote meaningful teacher involvement even for participants with low background in data literacy or experience in using learning analytics. We discuss opportunities and limitations of using the Inspiration Cards Workshops methodology, and particularly Inspiration Cards as a design tool, to inform future LA design efforts.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {73–82},
numpages = {10},
keywords = {learning analytics, inspiration cards, emerging technology, co-design methods},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375536,
author = {Tsai, Yi-Shan and Whitelock-Wainwright, Alexander and Ga\v{s}evi\'{c}, Dragan},
title = {The privacy paradox and its implications for learning analytics},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375536},
doi = {10.1145/3375462.3375536},
abstract = {Learning analytics promises to support adaptive learning in higher education. However, the associated issues around privacy protection, especially their implications for students as data subjects, has been a hurdle to wide-scale adoption. In light of this, we set out to understand student expectations of privacy issues related to learning analytics and to identify gaps between what students desire and what they expect to happen or choose to do in reality when it comes to privacy protection. To this end, an investigation was carried out in a UK higher education institution using a survey (N=674) and six focus groups (26 students). The study highlight a number of key implications for learning analytics research and practice: (1) purpose, access, and anonymity are key benchmarks of ethics and privacy integrity; (2) transparency and communication are key levers for learning analytics adoption; and (3) information asymmetry can impede active participation of students in learning analytics.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {230–239},
numpages = {10},
keywords = {privacy paradox, privacy, learning analytics, higher education, expectations},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375535,
author = {Chen, Bodong and Poquet, Oleksandra},
title = {Socio-temporal dynamics in peer interaction events},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375535},
doi = {10.1145/3375462.3375535},
abstract = {Asynchronous online discussions are broadly used to support peer interaction in online and hybrid courses. In this paper, we argue that the analysis of online peer interactions would benefit from the focus on relational events that are temporal and occur due to a range of factors. To demonstrate the possibility, we applied Relational Event Modeling (REM) to a dataset from online discussions in seven online classes. Informed by a conceptual model of social interaction in online discussions, this modeling included (a) a learner attribute capturing aspects of temporal participation, (b) social dynamics factors such as preferential attachment and reciprocity, and (c) turn-by-turn sequential patterns. Results showed that learner activity and familiarity from recent interactions affected their propensity to form ties. Turn-by-turn sequential patterns, that capture individual posting in bursts, explain how two-star network patterns form. Since two-star network patterns could further facilitate small group formation in the network, we expected the models to also capture communication in triads (i.e. triadic closure). Yet, models, devoid of the content of exchanges, did not capture the social dynamics well, and failed to predict patterns for communication across triads. By bringing in discourse features, future work can investigate the role of knowledge building behaviours in triadic closure of digital networks. This study contributes fresh insights into social interaction in online discussions, calls for attention to micro-level temporal patterns, and motivates future work to scaffold learner participation in similar contexts.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {203–208},
numpages = {6},
keywords = {temporality, relational event modelling, digital peer networks},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375534,
author = {Matcha, Wannisa and Ga\v{s}evi\'{c}, Dragan and Jovanovi\'{c}, Jelena and Uzir, Nora'ayu Ahmad and Oliver, Chris W and Murray, Andrew and Gasevic, Danijela},
title = {Analytics of learning strategies: the association with the personality traits},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375534},
doi = {10.1145/3375462.3375534},
abstract = {Studying online requires well-developed self-regulated learning skills to properly manage one's learning strategies. Learning analytics research has proposed novel methods for extracting theoretically meaningful learning strategies from trace data originating from formal learning settings (online, blended, or flipped classroom). Thus identified strategies proved to be associated with academic achievement. However, automated extraction of theoretically meaningful learning strategies from trace data in the context of massive open online courses (MOOCs) is still under-explored. Moreover, there is a lacuna in research on the relations between automatically detected strategies and the established psychological constructs. The paper reports on a study that (a) applied a state-of-the-art analytic method that combines process and sequence mining techniques to detect learning strategies from the trace data collected in a MOOC (N=1,397), and (b) explored associations of the detected strategies with academic performance and personality traits (Big Five). Four learning strategies detected with the adopted analytics method were shown to be theoretically interpretable as the well-known approaches to learning. The results also revealed that the four detected learning strategies were predicted by conscientiousness, emotional instability, and agreeableness and were associated with academic performance. Implications for theoretical validity and practical application of analytics-detected learning strategies are also provided.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {151–160},
numpages = {10},
keywords = {personality traits, learning strategies, learning analytics, approaches to learning},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375533,
author = {Poquet, Oleksandra and Jovanovic, Jelena},
title = {Intergroup and interpersonal forum positioning in shared-thread and post-reply networks},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375533},
doi = {10.1145/3375462.3375533},
abstract = {Network analysis has become a major approach for analysing social learning, used to capture learner positioning in online forum networks. LA research investigated the association between positioning in forum networks with academic performance and discourse quality, the latter two serving as proxies for learning. However, the research findings have been inconsistent, in part due to the discrepancies in the adopted approaches to network construction. Yet, it is still unclear how online forum networks should be modelled to assure that the learners' network positioning is properly captured. To address this gap, the current study explored if some existing approaches to network construction may complement each other and thus offer richer insights. In particular, we hypothesised that the post-reply learner network could represent interpersonal positioning, whereas the network based on co-participation in discussion threads could encapsulate intergroup positioning. The study used learner social interaction data from a large edX MOOC forum to examine the relationship between these two kinds of network positioning. The results suggest that intergroup and interpersonal positioning may capture different aspects of social learning, potentially related to different learning outcomes. We find that although interpersonal and intergroup positioning indicators covary, these measures are not congruent for some 37% of forum posters. Network coevolution analysis also reveals an interdependent relationship between the intergroup and interpersonal centrality in a forum network. Co-occurrence of learners in a discussion thread prior to direct exchanges is predictive of a direct post-reply interaction at a later stage of the course, and vice-versa, suggesting that intergroup positioning is a precursor of direct communication. The study contributes to the discussion around the definition of learner forum positioning in learning analytics, and validated approaches towards measuring it.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {187–196},
numpages = {10},
keywords = {positioning, learner networks, collective learning, centrality},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375532,
author = {Khosravi, Hassan and Gyamfi, George and Hanna, Barbara E. and Lodge, Jason},
title = {Fostering and supporting empirical research on evaluative judgement via a crowdsourced adaptive learning system},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375532},
doi = {10.1145/3375462.3375532},
abstract = {The value of students developing the capacity to make accurate judgements about the quality of their work and that of others has been widely recognised in higher education literature. However, despite this recognition, little attention has been paid to the development of tools and strategies with the potential both to foster evaluative judgement and to support empirical research into its growth. This paper provides a demonstration of how educational technologies may be used to fill this gap. In particular, we introduce the adaptive learning system RiPPLE and describe how it aims to (1) develop evaluative judgement in large-class settings through suggested strategies from the literature such as the use of rubrics, exemplars and peer review and (2) enable large empirical studies at low cost to determine the effect-size of such strategies. A case study demonstrating how RiPPLE has been used to achieve these goals in a specific context is presented.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {83–88},
numpages = {6},
keywords = {student-authored materials, evaluative judgement, educational technologies, crowd-sourcing},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375531,
author = {Poquet, Oleksandra and Tupikina, Liubov and Santolini, Marc},
title = {Are forum networks social networks? a methodological perspective},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375531},
doi = {10.1145/3375462.3375531},
abstract = {The mission of learning analytics (LA) is to improve learner experiences using the insights from digitally collected learner data. While some areas of LA are maturing, this is not consistent across all LA specialisations. For instance, LA for social learning lack validated approaches to account for the effects of cross-course variability in learner behavior. Although the associations between network structure and learning outcomes have been examined in the context of online forums, it remains unclear whether such associations represent bona fide social effects, or merely reflect heterogeneity in individual posting behavior, leading to seemingly complex but artefactual social network structures. We argue that to start addressing this issue, posting activity should be explicitly included and modelled in forum network representations. To gain insight to what extent learner degree and edge weight are merely derivatives of learner activity, we construct random models that control for the level of posting and post properties, such as popularity and thread hierarchy level. Analysis of forum networks in twenty online courses presented in this paper demonstrates that individual posting behavior is highly predictive of both the breadth (degree) and frequency (strength) in forum communication networks. This implies that, in the context of forum-based modelling, degree and frequency may not reflect the social dynamics. However, results suggest that clustering of the network structure is not a derivative of individual posting behaviour. Hence, weighted local clustering coefficient may be a better proxy for social relationships. The empirical results are relevant to scientists interested in social interactions and learner networks in digital learning, and more generally to researchers interested in deriving informative social network models from online forums.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {366–375},
numpages = {10},
keywords = {social networks, online learning, online forums, null models},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375530,
author = {Akintunde, Ruth Okoilu and Shabrina, Preya and Catete, Veronica and Barnes, Tiffany and Lynch, Collin and Rutherford, Teomara},
title = {Data-informed curriculum sequences for a curriculum-integrated game},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375530},
doi = {10.1145/3375462.3375530},
abstract = {In this paper, we perform a predictive analysis of a curriculum-integrated math game, ST Math, to suggest a partial ordering for the game's curriculum sequence. We analyzed the sequence of ST Math objectives played by elementary school students in 5 U.S. districts and grouped each objective into difficult and easy categories according to how many retries were needed for students to master an objective. We observed that retries on some objectives were high in one district and low in another district where the objectives are played in a different order. Motivated by this observation, we investigated what makes an effective curriculum sequence. To infer a new partially-ordered sequence, we performed an expanded replication study of a novel predictive analysis by a prior study to find predictive relationships between 15 objectives played in different sequences by 3,328 students from 5 districts. Based on the predictive abilities of objectives in these districts, we found 17 suggested objective orderings. After deriving these orderings, we confirmed the validity of the order by evaluating the impact of the suggested sequence on changes in rates of retries and corresponding performance. We observed that when the objectives were played in the suggested sequence, we record a drastic reduction in retries, implying that these objectives are easier for students. This indicates that objectives that come earlier can provide prerequisite knowledge for later objectives. We believe that data-informed sequences, such as the ones we suggest, may improve efficiency of instruction and increase content learning and performance.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {635–644},
numpages = {10},
keywords = {serious game analytics, retries, educational games, curricular sequencing},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375529,
author = {Li, Fanjie and Hu, Xiao and Que, Ying},
title = {Learning with background music: a field experiment},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375529},
doi = {10.1145/3375462.3375529},
abstract = {Empirical evidence of how background music benefits or hinders learning becomes the crux of optimizing music recommendation in educational settings. This study aims to further probe the underlying mechanism through an experiment in naturalistic setting. 30 participants were recruited to join a field experiment which was conducted in their own study places for one week. During the experiment, participants were asked to conduct learning sessions with music in the background and collect music tracks they deemed suitable for learning using a novel mobile-based music discovery application. A set of participant-related, context-related, and music-related data were collected via a pre-experiment questionnaire, surveys popped up in the music app, and the logging system of the music app. Preliminary results reveal correlations between certain music characteristics and learners' task engagement and perceived task performance. This study is expected to provide evidence for understanding cognitive and emotional dimensions of background music during learning, as well as implications for the role of personalization in the selection of background music for facilitating learning.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {224–229},
numpages = {6},
keywords = {naturalistic setting, music information retrieval, learning performance, learning engagement, background music},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375528,
author = {Jung, Yeonji and Wise, Alyssa Friend},
title = {How and how well do students reflect? multi-dimensional automated reflection assessment in health professions education},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375528},
doi = {10.1145/3375462.3375528},
abstract = {Reflection assessment is a critical component of health professions education that can be used for personalized learning support. However, reflection assessment at scale remains a challenge due to the demanding nature of tasks and the common use of simplified criteria of quality. This study addressed this issue by developing a multi-dimensional automated assessment that uses linguistic models to classify reflections by overall quality (depth) and the presence of six constituent elements denoting quality (description, analysis, feeling, perspective, evaluation, and outcome). 1500 reflections from 369 dental students were manually coded to establish ground truth. Classifiers for each of the six elements were trained and tested based on linguistic features extracted using the LIWC tool applying both single-label and multi-label classification approaches. Classifiers for depth were built both directly from linguistic features and based on the presence of the six elements. Results showed that linguistic modeling can be used to reliably detect the presence of reflection elements and the level of depth. However, the depth classifier showed a heavy reliance on cognitive elements (description, analysis, and evaluation) rather than the others. These findings indicate the feasibility of implementing multidimensional automated assessment in health professions education and the need to reconsider how quality of reflection is conceptualized.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {595–604},
numpages = {10},
keywords = {reflection assessment, reflection, natural language processing, health professions education, content analysis, classification},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375527,
author = {Iraj, Hamideh and Fudge, Anthea and Faulkner, Margaret and Pardo, Abelardo and Kovanovi\'{c}, Vitomir},
title = {Understanding students' engagement with personalised feedback messages},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375527},
doi = {10.1145/3375462.3375527},
abstract = {Feedback is a major factor of student success within higher education learning. However, recent changes - such as increased class sizes and socio-economic diversity of the student population - challenged the provision of effective student feedback. Although the use of educational technology for personalised feedback to diverse students has gained traction, the feedback gap still exists: educators wonder which students respond to feedback and which do not. In this study, a set of trackable Call to Action (CTA) links was embedded in two sets of feedback messages focusing on students' time management, with the goal of (1) examining the association between feedback engagement and course success and (2), to predict students' reaction to provided feedback. We also conducted two focus groups to further examine students' perception of provided feedback messages. Our results revealed that early engagement with the feedback was associated with higher chances of succeeding in the course. Likewise, previous engagement with feedback was highly predictive of students' engagement in the future, and also that certain student sub-populations, (e.g., female students), were more likely to engage than others. Such insight enables instructors to ask "why" questions, improve feedback processes and narrow the feedback gap. Practical implications of our findings are further discussed.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {438–447},
numpages = {10},
keywords = {learning analytics, higher education, feedback gap, feedback, data-driven approaches},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375526,
author = {Kitto, Kirsty and Sarathy, Nikhil and Gromov, Aleksandr and Liu, Ming and Musial, Katarzyna and Buckingham Shum, Simon},
title = {Towards skills-based curriculum analytics: can we automate the recognition of prior learning?},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375526},
doi = {10.1145/3375462.3375526},
abstract = {In an era that will increasingly depend upon lifelong learning, the LA community will need to facilitate the movement and sharing of data and information across institutional and geographic boundaries. This will help us to recognise prior learning (RPL) and to personalise the learner experience. Here, we explore the utility of skills-based curriculum analytics and how it might facilitate the process of awarding RPL between two institutions. We explore the potential utility of combining natural language processing and skills taxonomies to map between subject descriptions for these two different institutions, presenting two algorithms we have developed to facilitate RPL and evaluating their performance. We draw attention to some of the issues that arise, listing areas that we consider ripe for future work in a surprisingly underexplored area.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {171–180},
numpages = {10},
keywords = {skills ontologies, semantic spaces, recognition of prior learning, lifelong learning, curriculum analytics},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375525,
author = {Shabrina, Preya and Akintunde, Ruth Okoilu and Maniktala, Mehak and Barnes, Tiffany and Lynch, Collin and Rutherford, Teomara},
title = {Peeking through the classroom window: a detailed data-driven analysis on the usage of a curriculum integrated math game in authentic classrooms},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375525},
doi = {10.1145/3375462.3375525},
abstract = {We present a data-driven analysis that provides generalized insights of how a curriculum integrated educational math game gets used as a routinized classroom activity throughout the year in authentic primary school classrooms. Our study relates observations from a field study on Spatial Temporal Math (ST Math) to our findings mined from ST Math students' sequential game play data. We identified features that vary across game play sessions and modeled their relationship with session performance. We also derived data-informed suggestions that may provide teachers with insights into how to design classroom game play sessions to facilitate more effective learning.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {625–634},
numpages = {10},
keywords = {integration, game analytics, curriculum integrated math games},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375524,
author = {Pardos, Zachary A. and Jiang, Weijie},
title = {Designing for serendipity in a university course recommendation system},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375524},
doi = {10.1145/3375462.3375524},
abstract = {Collaborative filtering based algorithms, including Recurrent Neural Networks (RNN), tend towards predicting a perpetuation of past observed behavior. In a recommendation context, this can lead to an overly narrow set of suggestions lacking in serendipity and inadvertently placing the user in what is known as a "filter bubble." In this paper, we grapple with the issue of the filter bubble in the context of a course recommendation system in production at a public university. Our approach is to present course results that are novel or unexpected to the student but still relevant to their interests. We build one set of models based on course catalog descriptions (BOW) and another set informed by enrollment histories (course2vec). We compare the performance of these models on off-line validation sets and against the system's existing RNN-based recommendation engine in an online user study of undergraduates (N = 70) who rated their course recommendations along six characteristics related to serendipity. Results of the user study show a dramatic lack of novelty in RNN recommendations and depict the characteristic trade-offs that make serendipity difficult to achieve. While the machine learned course2vec models performed best on off-line validation tasks, it was the simple bag-of-words based recommendations that students rated as more serendipitous. We discuss the role of the kind of information presented by the system in a student's decision to accept a recommendation from either algorithm.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {350–359},
numpages = {10},
keywords = {neural networks, higher education, filter bubble, course guidance},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375523,
author = {Erickson, John A. and Botelho, Anthony F. and McAteer, Steven and Varatharaj, Ashvini and Heffernan, Neil T.},
title = {The automated grading of student open responses in mathematics},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375523},
doi = {10.1145/3375462.3375523},
abstract = {The use of computer-based systems in classrooms has provided teachers with new opportunities in delivering content to students, supplementing instruction, and assessing student knowledge and comprehension. Among the largest benefits of these systems is their ability to provide students with feedback on their work and also report student performance and progress to their teacher. While computer-based systems can automatically assess student answers to a range of question types, a limitation faced by many systems is in regard to open-ended problems. Many systems are either unable to provide support for open-ended problems, relying on the teacher to grade them manually, or avoid such question types entirely. Due to recent advancements in natural language processing methods, the automation of essay grading has made notable strides. However, much of this research has pertained to domains outside of mathematics, where the use of open-ended problems can be used by teachers to assess students' understanding of mathematical concepts beyond what is possible on other types of problems. This research explores the viability and challenges of developing automated graders of open-ended student responses in mathematics. We further explore how the scale of available data impacts model performance. Focusing on content delivered through the ASSISTments online learning platform, we present a set of analyses pertaining to the development and evaluation of models to predict teacher-assigned grades for student open responses.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {615–624},
numpages = {10},
keywords = {open responses, natural language processing, automatic grading},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375522,
author = {Kim, Yanghee and Butail, Sachit and Tscholl, Michael and Liu, Lichuan and Wang, Yunlong},
title = {An exploratory approach to measuring collaborative engagement in child robot interaction},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375522},
doi = {10.1145/3375462.3375522},
abstract = {This study explored data analytic approaches to assessing young children's engagement in robot-mediated collaborative interaction. To develop our analytic models, we took a case-study approach and looked closely into four children's behaviors during three conversational sessions. Grounded in engagement theory, three sources of multimodal behavioral data (utterances, kinesics, and vocie) were coded through human annotation and automatic speech recognition and analysis. Then, information-theoretic methods were used to uncover nonlinear dependencies (called mutual information) among the multimodal behaviors of each child. From this, we derived a model to compute a compound variable of engagement. This computation produced engagement trends of each child, the engagement relationship between two children in a pair, and the engagement relationship with the robot over time. The computed trends corresponded well with the data from human observations. This approach has implications for quantifying engagement from rich and natural multimodal behaviors.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {209–217},
numpages = {9},
keywords = {social robotics, mutual information, multimodal data analytics, learning analytics, information theory, human computer interaction, engagement, collaborative problem solving, child robot interaction, automatic speech recognition},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375521,
author = {Wei, Huan and Li, Haotian and Xia, Meng and Wang, Yong and Qu, Huamin},
title = {Predicting student performance in interactive online question pools using mouse interaction features},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375521},
doi = {10.1145/3375462.3375521},
abstract = {Modeling student learning and further predicting the performance is a well-established task in online learning and is crucial to personalized education by recommending different learning resources to different students based on their needs. Interactive online question pools (e.g., educational game platforms), an important component of online education, have become increasingly popular in recent years. However, most existing work on student performance prediction targets at online learning platforms with a well-structured curriculum, predefined question order and accurate knowledge tags provided by domain experts. It remains unclear how to conduct student performance prediction in interactive online question pools without such well-organized question orders or knowledge tags by experts. In this paper, we propose a novel approach to boost student performance prediction in interactive online question pools by further considering student interaction features and the similarity between questions. Specifically, we introduce new features (e.g., think time, first attempt, and first drag-and-drop) based on student mouse movement trajectories to delineate students' problem-solving details. In addition, heterogeneous information network is applied to integrating students' historical problem-solving information on similar questions, enhancing student performance predictions on a new question. We evaluate the proposed approach on the dataset from a real-world interactive question pool using four typical machine learning models. The result shows that our approach can achieve a much higher accuracy for student performance prediction in interactive online question pools than the traditional way of only using the statistical features (e.g., students' historical question scores) in various models. We further discuss the performance consistency of our approach across different prediction models and question classes, as well as the importance of the proposed interaction features in detail.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {645–654},
numpages = {10},
keywords = {student performance prediction, question pool, mouse movement trajectory, heterogeneous information network},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375520,
author = {Abdi, Solmaz and Khosravi, Hassan and Sadiq, Shazia and Gasevic, Dragan},
title = {Complementing educational recommender systems with open learner models},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375520},
doi = {10.1145/3375462.3375520},
abstract = {Educational recommender systems (ERSs) aim to adaptively recommend a broad range of personalised resources and activities to students that will most meet their learning needs. Commonly, ERSs operate as a "black box" and give students no insight into the rationale of their choice. Recent contributions from the learning analytics and educational data mining communities have emphasised the importance of transparent, understandable and open learner models (OLMs) that provide insight and enhance learners' understanding of interactions with learning environments. In this paper, we aim to investigate the impact of complementing ERSs with transparent and understandable OLMs that provide justification for their recommendations. We conduct a randomised control trial experiment using an ERS with two interfaces ("Non-Complemented Interface" and "Complemented Interface") to determine the effect of our approach on student engagement and their perception of the effectiveness of the ERS. Overall, our results suggest that complementing an ERS with an OLM can have a positive effect on student engagement and their perception about the effectiveness of the system despite potentially making the system harder to navigate. In some cases, complementing an ERS with an OLM has the negative consequence of decreasing engagement, understandability and sense of fairness.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {360–365},
numpages = {6},
keywords = {user models, open learner models, educational recommender systems},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375519,
author = {Papamitsiou, Zacharoula and Giannakos, Michail N. and Ochoa, Xavier},
title = {From childhood to maturity: Are we there yet? Mapping the intellectual progress in learning analytics during the past decade},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375519},
doi = {10.1145/3375462.3375519},
abstract = {This study aims to identify the conceptual structure and the thematic progress in Learning Analytics (evolution) and to elaborate on backbone/emerging topics in the field (maturity) from 2011 to September 2019. To address this objective, this paper employs hierarchical clustering, strategic diagrams and network analysis to construct the intellectual map of the Learning Analytics community and to visualize the thematic landscape in this field, using co-word analysis. Overall, a total of 459 papers from the proceedings of the Learning Analytics and Knowledge (LAK) conference and 168 articles published in the Journal of Learning Analytics (JLA), and the respective 3092 author-assigned keywords and 4051 machine-extracted key-phrases, were included in the analyses. The results indicate that the community has significantly focused in areas like Massive Open Online Courses and visualizations; Learning Management Systems, assessment and self-regulated learning are also basic topics, yet topics like natural language processing and orchestration are emerging. The analysis highlights the shift of the research interest throughout the past decade, and the rise of new topics, comprising evidence that the field is expanding. Limitations of the approach and future work plans conclude the paper.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {559–568},
numpages = {10},
keywords = {learning analytics, conceptual evolution, co-word analysis, bibliometrics},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375518,
author = {Zhang, Yingbin and Paquette, Luc and Baker, Ryan S. and Ocumpaugh, Jaclyn and Bosch, Nigel and Munshi, Anabil and Biswas, Gautam},
title = {The relationship between confusion and metacognitive strategies in Betty's Brain},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375518},
doi = {10.1145/3375462.3375518},
abstract = {Confusion has been shown to be prevalent during complex learning and has mixed effects on learning. Whether confusion facilitates or hampers learning may depend on whether it is resolved or not. Confusion resolution, behind which is the resolution of cognitive disequilibrium, requires learners to possess some skills, but it is unclear what these skills are. One possibility may be metacognitive strategies (MS), strategies for regulating cognition. This study examined the relationship between confusion and actions related to MS in Betty's Brain, a computer-based learning environment. The results revealed that MS behavior differed during and outside confusion. However, confusion resolution was not related to MS behavior, and MS did not moderate the effect of confusion on learning.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {276–284},
numpages = {9},
keywords = {metacognitive strategy, learning analytics, confusion resolution, confusion},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375517,
author = {Benedetto, Luca and Cappelli, Andrea and Turrin, Roberto and Cremonesi, Paolo},
title = {R2DE: a NLP approach to estimating IRT parameters of newly generated questions},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375517},
doi = {10.1145/3375462.3375517},
abstract = {The main objective of exams consists in performing an assessment of students' expertise on a specific subject. Such expertise, also referred to as skill or knowledge level, can then be leveraged in different ways (e.g., to assign a grade to the students, to understand whether a student might need some support, etc.). Similarly, the questions appearing in the exams have to be assessed in some way before being used to evaluate students. Standard approaches to questions' assessment are either subjective (e.g., assessment by human experts) or introduce a long delay in the process of question generation (e.g., pretesting with real students). In this work we introduce R2DE (which is a Regressor for Difficulty and Discrimination Estimation), a model capable of assessing newly generated multiple-choice questions by looking at the text of the question and the text of the possible choices. In particular, it can estimate the difficulty and the discrimination of each question, as they are defined in Item Response Theory. We also present the results of extensive experiments we carried out on a real world large scale dataset coming from an e-learning platform, showing that our model can be used to perform an initial assessment of newly created questions and ease some of the problems that arise in question generation.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {412–421},
numpages = {10},
keywords = {natural language processing, learning analytics, latent traits estimation, knowledge tracing, item response theory, educational data mining},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375516,
author = {Quick, Joshua and Motz, Benjamin and Israel, Jamie and Kaetzel, Jason},
title = {What college students say, and what they do: aligning self-regulated learning theory with behavioral logs},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375516},
doi = {10.1145/3375462.3375516},
abstract = {A central concern in learning analytics specifically and educational research more generally is the alignment of robust, coherent measures to well-developed conceptual and theoretical frameworks. Capturing and representing processes of learning remains an ongoing challenge in all areas of educational inquiry and presents substantive considerations on the nature of learning, knowledge, and assessment &amp; measurement that have been continuously refined in various areas of education and pedagogical practice. Learning analytics as a still developing method of inquiry has yet to substantively navigate the alignment of measurement, capture, and representation of learning to theoretical frameworks despite being used to identify various practical concerns such as at risk students. This study seeks to address these concerns by comparing behavioral measurements from learning management systems to established measurements of components of learning as understood through self-regulated learning frameworks. Using several prominent and robustly supported self-reported survey measures designed to identify dimensions of self-regulated learning, as well as typical behavioral features extracted from a learning management system, we conducted descriptive and exploratory analyses on the relational structures of these data. With the exception of learners' self-reported time management strategies and level of motivation, the current results indicate that behavioral measures were not well correlated with survey measurements. Possibilities and recommendations for learning analytics as measurements for self-regulated learning are discussed.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {534–543},
numpages = {10},
keywords = {trace data, self-reports, self-regulated learning, LMS},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375515,
author = {Peri, Sai Santosh Sasank and Chen, Bodong and Dougall, Angela Liegey and Siemens, George},
title = {Towards understanding the lifespan and spread of ideas: epidemiological modeling of participation on Twitter},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375515},
doi = {10.1145/3375462.3375515},
abstract = {How ideas develop and evolve is a topic of interest for educators. By understanding this process, designers and educators are better able to support and guide collaborative learning activities. This paper presents an application of our Lifespan of an Idea framework to measure engagement patterns among individuals in communal socio-technical spaces like Twitter. We correlated engagement with social participation, enabling the process of idea expression, spread, and evolution. Social participation leads to transmission of ideas from one individual to another and can be gauged in the same way as evaluating diseases. The temporal dynamics of the social participation can be modeled through the lens of epidemiological modeling. To test the plausibility of this framework, we investigated social participation on Twitter using the tweet posting patterns of individuals in three academic conferences and one long term chat space. We used a basic SIR epidemiological model, where the rate parameters were estimated through Euler's solutions to SIR model and non-linear least squares optimization technique. We discuss the differences in the social participation among individuals in these spaces based on their transition behavior into different categories of the SIR model. We also made inferences on how the total lifetime of these different twitter spaces affects the engagement among individuals. We conclude by discussing implications of this study and planned future research of refining the Lifespan of an Idea Framework.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {197–202},
numpages = {6},
keywords = {networked learning, knowledge creation, ideas, epidemiology, engagement patterns, connectivism},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375514,
author = {Srivastava, Namrata and Nawaz, Sadia and Lodge, Jason M. and Velloso, Eduardo and Erfani, Sarah and Bailey, James},
title = {Exploring the usage of thermal imaging for understanding video lecture designs and students' experiences},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375514},
doi = {10.1145/3375462.3375514},
abstract = {Video is becoming a dominant medium for the delivery of educational material. Despite the widespread use of video for learning, there is still a lack of understanding about how best to help people learn in this medium. This study demonstrates the use of thermal camera as compared to traditional self-reported methods for assessing learners' cognitive load while watching video lectures of different styles. We evaluated our approach in a study with 78 university students viewing two variants of short video lectures on two different topics. To incorporate subjective measures, the students reported on mental effort, interest, prior knowledge, confidence, and challenge. Moreover, through a physical slider device, the students could continuously report on their perceived level of difficulty. Lastly, we used thermal sensor as an additional indicator of students' level of difficulty and associated cognitive load. This was achieved through, continuous real-time monitoring of students by using a thermal imaging camera. This study aims to address the following: firstly, to analyze if video styles differ in terms of the associated cognitive load. Secondly, to assess the effects of cognitive load on learning outcomes; could an increase in the cognitive load be associated with poorer learning outcomes? Third, to see if there is a match between students' perceived difficulty levels and a biological indicator. The results suggest that thermal imaging could be an effective tool to assess learners' cognitive load, and an increased cognitive load could lead to poorer performance. Moreover, in terms of the lecture styles, the animated video lectures appear to be a better tool than the text-only lectures (in the content areas tested here). The results of this study may guide future works on effective video designs, especially those that consider the cognitive load.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {250–259},
numpages = {10},
keywords = {video lectures, thermal imaging, instructional design, cognitive load},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375513,
author = {Ou, Lu and Andrade, Alejandro and Alberto, Rosa and van Helden, Gitte and Bakker, Arthur},
title = {Using a cluster-based regime-switching dynamic model to understand embodied mathematical learning},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375513},
doi = {10.1145/3375462.3375513},
abstract = {Embodied learning and the design of embodied learning platforms have gained popularity in recent years due to the increasing availability of sensing technologies. In our study, we made use of the Mathematical Imagery Trainer for Proportion (MIT-P) that uses a touchscreen tablet to help students explore the concept of mathematical proportion. The use of sensing technologies provides an unprecedented amount of high-frequency data on students' behaviors. We investigated a statistical model called mixture Regime-Switching Hidden Logistic Transition Process (mixRHLP) and fit it to the students' hand motion data. Simultaneously, the model finds characteristic regimes and assigns students to clusters of regime transitions. To understand the nature of these regimes and clusters, we explore some properties in students' and tutor's verbalization associated with these different phases.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {496–501},
numpages = {6},
keywords = {multimodal learning analytics, mathematical learning, embodied cognition, dynamic models},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375512,
author = {Faucon, Louis and Olsen, Jennifer K. and Dillenbourg, Pierre},
title = {A bayesian model of individual differences and flexibility in inductive reasoning for categorization of examples},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375512},
doi = {10.1145/3375462.3375512},
abstract = {Inductive reasoning is an important educational practice but can be difficult for teachers to support in the classroom due to the high level of preparation and classroom time needed to choose the teaching materials that challenge students' current views. Intelligent tutoring systems can potentially facilitate this work for teachers by supporting the automatic adaptation of examples based on a student model of the induction process. However, current models of inductive reasoning usually lack two main characteristics helpful to adaptive learning environments, individual differences of students and tracing of students' learning as they receive feedback. In this paper, we describe a model to predict and simulate inductive reasoning of students for a categorization task. Our approach uses a Bayesian model for describing the reasoning processes of students. This model allows us to predict students' choices in categorization questions by accounting for their feature biases. Using data gathered from 222 students categorizing three topics, we find that our model has a 75% accuracy, which is 10% greater than a baseline model. Our model is a contribution to learning analytics by enabling us to assign different bias profiles to individual students and tracking these profile changes over time through which we can gain a better understanding of students' learning processes. This model may be relevant for systematically analysing students' differences and evolution in inductive reasoning strategies while supporting the design of adaptive inductive learning environments.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {285–294},
numpages = {10},
keywords = {student modeling, process mining, inductive reasoning, adaptive learning environment},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375511,
author = {Asano, Yuya and Solyst, Jaemarie and Williams, Joseph Jay},
title = {Characterizing and influencing students' tendency to write self-explanations in online homework},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375511},
doi = {10.1145/3375462.3375511},
abstract = {In the context of online programming homework for a university course, we explore the extent to which learners engage with optional prompts to self -explain answers they choose for problems. Such prompts are known to benefit learning in laboratory and classroom settings [4], but there are less data about the extent to which students engage with them when they are optional additions to online homework. We report data from a deployment of self-explanation prompts in online programming homework, providing insight into how the frequency of writing explanations is correlated with different variables, such as how early students start homework, whether they got a problem correct, and how proficient they are in the language of instruction. We also report suggestive results from a randomized experiment comparing several methods for increasing the rate at which people write explanations, such as including more than one kind of prompt. These findings provide insight into promising dimensions to explore in understanding how real students may engage with prompts to explain answers.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {448–453},
numpages = {6},
keywords = {self-explanation, randomized experiment, prompts, online homework, engagement},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375510,
author = {Torre, Manuel Valle and Tan, Esther and Hauff, Claudia},
title = {edX log data analysis made easy: introducing ELAT: An open-source, privacy-aware and browser-based edX log data analysis tool},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375510},
doi = {10.1145/3375462.3375510},
abstract = {Massive Open Online Courses (MOOCs), delivered on platforms such as edX and Coursera, have led to a surge in large-scale learning research. MOOC platforms gather a continuous stream of learner traces, which can amount to several Gigabytes per MOOC, that learning analytics researchers use to conduct exploratory analyses as well as to evaluate deployed interventions. edX has proven to be a popular platform for such experiments, as the data each MOOC generates is easily accessible to the institution running the MOOC. One of the issues researchers face is the preprocessing, cleaning and formatting of those large-scale learner traces. It is a tedious process that requires considerable computational skills. To reduce this burden, a number of tools have been proposed and released with the aim of simplifying this process. Those tools though still have a significant setup cost, are already out-of-date or require already preprocessed data as a starting point. In contrast, in this paper we introduce ELAT, the edX Log file Analysis Tool, which is browser-based (i.e., no setup costs), keeps the data local (i.e., no server is necessary and the privacy-sensitive learner data is not send anywhere) and takes edX data dumps as input. ELAT does not only process the raw data, but also generates semantically meaningful units (learner sessions instead of just click events) that are visualized in various ways (learning paths, forum participation, video watching sequences). We report on two evaluations we conducted: (i) a technological evaluation and a (ii) user study with potential end users of ELAT. ELAT is open-source and available at https://mvallet91.github.io/ELAT/.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {502–511},
numpages = {10},
keywords = {massive open online course, log data analysis, learning analytics, edX log},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375509,
author = {Harrak, Fatima and Bouchet, Fran\c{c}ois and Luengo, Vanda and Gillois, Pierre},
title = {Evaluating teachers' perceptions of students' questions organization},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375509},
doi = {10.1145/3375462.3375509},
abstract = {Students' questions are essential to help teachers in assessing their understanding and adapting their pedagogy. However, in a flipped classroom context where many questions are asked online to be addressed in class, selecting questions can be difficult for teachers. To help them in this task, we present here three alternative ways of organizing questions: one based on pedagogical needs, one based on estimated students' profiles and one mixing both approaches. Results of a survey filled by 37 teachers in a flipped classroom pedagogy show no consensus over a single organization. A cluster analysis based on teachers' flipped classroom experience allowed us to distinguish two profiles, but they were not associated with any particular question organization preference. Qualitative results suggest the need for different organizations may rely more on a pedagogical philosophy and advocates for differentiated dashboards.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {11–16},
numpages = {6},
keywords = {teacher's perception, student's question, student's profile, student's need, question organization, pedagogical interest},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375508,
author = {Eagan, Brendan and Brohinsky, Jais and Wang, Jingyi and Shaffer, David Williamson},
title = {Testing the reliability of inter-rater reliability},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375508},
doi = {10.1145/3375462.3375508},
abstract = {Analyses of learning often rely on coded data. One important aspect of coding is establishing reliability. Previous research has shown that the common approach for establishing coding reliability is seriously flawed in that it produces unacceptably high Type I error rates. This paper focuses on testing whether or not these error rates correspond to specific reliability metrics or a larger methodological problem. Our results show that the method for establishing reliability is not metric specific, and we suggest the adoption of new practices to control Type I error rates associated with establishing coding reliability.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {454–461},
numpages = {8},
keywords = {validity, statistical analysis, reliability, interrater reliability, coding},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375507,
author = {Michos, Konstantinos and Lang, Charles and Hern\'{a}ndez-Leo, Davinia and Price-Dennis, Detra},
title = {Involving teachers in learning analytics design: lessons learned from two case studies},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375507},
doi = {10.1145/3375462.3375507},
abstract = {Involving teachers in the design of technology-enhanced learning environments is a useful method towards bridging the gap between research and practice. This is especially relevant for learning analytics tools, wherein the presentation of educational data to teachers or students requires meaningful sense-making to effectively support data-driven actions. In this paper, we present two case studies carried out in the context of two research projects in the USA and Spain which aimed to involve teachers in the co-design of learning analytics tools through professional development programs. The results of a cross-case analysis highlight lessons learned around challenges and principles regarding the meaningful involvement of teachers in learning analytics tooling design.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {94–99},
numpages = {6},
keywords = {teachers, teacher professional development, learning analytics, co-design, case studies},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375506,
author = {Lang, Charles and Woo, Charlotte and Sinclair, Jeanne},
title = {Quantifying data sensitivity: precise demonstration of care when building student prediction models},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375506},
doi = {10.1145/3375462.3375506},
abstract = {Until recently an assumption within the predictive modelling community has been that collecting more student data is always better. But in reaction to recent high profile data privacy scandals, many educators, scholars, students and administrators have been questioning the ethics of such a strategy. Suggestions are growing that the minimum amount of data should be collected to aid the function for which a prediction is being made. Yet, machine learning algorithms are primarily judged on metrics derived from prediction accuracy or whether they meet probabilistic criteria for significance. They are not routinely judged on whether they utilize the minimum number of the least sensitive features, preserving what we name here as data collection parsimony. We believe the ability to assess data collection parsimony would be a valuable addition to the suite of evaluations for any prediction strategy and to that end, the following paper provides an introduction to data collection parsimony, describes a novel method for quantifying the concept using empirical Bayes estimates and then tests the metric on real world data. Both theoretical and empirical benefits and limitations of this method are discussed. We conclude that for the purpose of model building this metric is superior to others in several ways, but there are some hurdles to effective implementation.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {655–664},
numpages = {10},
keywords = {student models, prediction models, ethics, empirical bayes, data sensitivity, data privacy, data collection parsimony},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375505,
author = {Swiecki, Zachari and Shaffer, David Williamson},
title = {iSENS: an integrated approach to combining epistemic and social network analyses},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375505},
doi = {10.1145/3375462.3375505},
abstract = {Collaborative problem solving is defined as having cognitive and social dimensions. While network analytic techniques such as epistemic network analysis (ENA) and social network analysis (SNA) have been successfully used to investigate the patterns of cognitive and social connections that describe CPS, few attempts have been made to combine the two approaches. Building on prior work that used ENA and SNA metrics as independent predictors of collaborative learning, we propose and test the integrated social-epistemic network signature (iSENS), an approach that affords the simultaneous investigation of cognitive and social connections. We tested iSENS on data collected from military teams participating in training scenarios. Our results suggest that (1) these teams are defined by specific patterns of cognitive and social connections, (2) iSENS networks are able to capture these patterns, and (3) iSENS is a better predictor of team outcomes compared to ENA alone, SNA alone, and a non-integrated SENS approach.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {305–313},
numpages = {9},
keywords = {social network analysis, epistemic network analysis, collaborative problem solving},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375504,
author = {Verbert, Katrien and Ochoa, Xavier and De Croon, Robin and Dourado, Raphael A. and De Laet, Tinne},
title = {Learning analytics dashboards: the past, the present and the future},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375504},
doi = {10.1145/3375462.3375504},
abstract = {Learning analytics dashboards are at the core of the LAK vision to involve the human into the decision-making process. The key focus of these dashboards is to support better human sense-making and decision-making by visualising data about learners to a variety of stakeholders. Early research on learning analytics dashboards focused on the use of visualisation and prediction techniques and demonstrates the rich potential of dashboards in a variety of learning settings. Present research increasingly uses participatory design methods to tailor dashboards to the needs of stakeholders, employs multimodal data acquisition techniques, and starts to research theoretical underpinnings of dashboards. In this paper, we present these past and present research efforts as well as the results of the VISLA19 workshop on "Visual approaches to Learning Analytics" that was held at LAK19 with experts in the domain to identify and articulate common practices and challenges for the domain. Based on an analysis of the results, we present a research agenda to help shape the future of learning analytics dashboards.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {35–40},
numpages = {6},
keywords = {visualisation, learning analytics dashboards, interaction, evaluation},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375503,
author = {Peffer, Melanie and Quigley, David and Brusman, Liza and Avena, Jennifer and Knight, Jennifer},
title = {Trace data from student solutions to genetics problems reveals variance in the processes related to different course outcomes},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375503},
doi = {10.1145/3375462.3375503},
abstract = {Problem solving, particularly in disciplines such as genetics, is an essential but difficult competency for students to master. Prior work indicated that trace data can be leveraged to measure the invisible cognitive processes that undergird learning activities such as problem solving. Building on prior work and given the importance and difficulties associated with genetics problem solving, we used unsupervised statistical methods (k-means clustering and feature selection) to characterize the patterns of processes students use during genetics problem solving and the relationship to proximal and distal outcomes. At the level of the individual problem, we found that conclusion processes, such as making claims and eliminating possible solutions, was an important interim step and associated with getting a particular problem correct. Surprisingly, we noted that a different set of processes was associated with course outcomes. Students who performed multiple metacognitive steps (e.g. monitoring, checking, planning) in a row or who engaged in execution steps (e.g. using information, drawing a picture, restating the process) as part of problem solving during the semester performed better on final assessments. We found a third set of practices, making consecutive conclusion processes, metacognitive processes preceding reasoning and reasoning preceding conclusions to be important for success at both the problem level and on final assessments. This suggests that different problem-solving processes are associated with success on different course benchmarks. This work raises provocative questions regarding best practices for teaching problem solving in genetics classrooms.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {47–52},
numpages = {6},
keywords = {problem solving, metacognition, genetics, clustering},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375502,
author = {Fadljevi\'{c}, Leon and Maitz, Katharina and Kowald, Dominik and Pammer-Schindler, Viktoria and Gasteiger-Klicpera, Barbara},
title = {Slow is good: the effect of diligence on student performance in the case of an adaptive learning system for health literacy},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375502},
doi = {10.1145/3375462.3375502},
abstract = {This paper describes the analysis of temporal behavior of 11--15 year old students in a heavily instructionally designed adaptive e-learning environment. The e-learning system is designed to support student's acquisition of health literacy. The system adapts text difficulty depending on students' reading competence, grouping students into four competence levels. Content for the four levels of reading competence was created by clinical psychologists, pedagogues and medicine students. The e-learning system consists of an initial reading competence assessment, texts about health issues, and learning tasks related to these texts. The research question we investigate in this work is whether temporal behavior is a differentiator between students despite the system's adaptation to students' reading competence, and despite students having comparatively little freedom of action within the system. Further, we also investigated the correlation of temporal behaviour with performance. Unsupervised clustering clearly separates students into slow and fast students with respect to the time they take to complete tasks. Furthermore, topic completion time is linearly correlated with performance in the tasks. This means that we interpret working slowly in this case as diligence, which leads to more correct answers, even though the level of text difficulty matches student's reading competence. This result also points to the design opportunity to integrate advice on overarching learning strategies, such as working diligently instead of rushing through, into the student's overall learning activity. This can be done either by teachers, or via additional adaptive learning guidance within the system.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {112–117},
numpages = {6},
keywords = {reading competence, learning analytics, health literacy, diversity, differentiation, clustering, adaptive e-learning system},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375501,
author = {Saqr, Mohammed and Nouri, Jalal},
title = {High resolution temporal network analysis to understand and improve collaborative learning},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375501},
doi = {10.1145/3375462.3375501},
abstract = {There has been significant efforts in studying collaborative and social learning using aggregate networks. Such efforts have demonstrated the worth of the approach by providing insights about the interactions, student and teacher roles, and predictability of performance. However, using an aggregated network discounts the fine resolution of temporal interactions. By doing so, we might overlook the regularities/irregularities of students' interactions, the process of learning regulation, and how and when different actors influence each other. Thus, compressing a complex temporal process such as learning may be oversimplifying and reductionist. Through a temporal network analysis of 54 students interactions (in total 3134 interactions) in an online medical education course, this study contributes with a methodological approach to building, visualizing and quantitatively analyzing temporal networks, that could help educational practitioners understand important temporal aspects of collaborative learning that might need attention and action. Furthermore, the analysis conducted emphasize the importance of considering the time characteristics of the data that should be used when attempting to, for instance, implement early predictions of performance and early detection of students and groups that need support and attention.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {314–319},
numpages = {6},
keywords = {temporarily, temporal networks, social network analysis, problem-based learning, medical education, learning analytics, collaborative learning},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375500,
author = {Chen, Guanliang and Rolim, Vitor and Mello, Rafael Ferreira and Ga\v{s}evi\'{c}, Dragan},
title = {Let's shine together! a comparative study between learning analytics and educational data mining},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375500},
doi = {10.1145/3375462.3375500},
abstract = {Learning Analytics and Knowledge (LAK) and Educational Data Mining (EDM) are two of the most popular venues for researchers and practitioners to report and disseminate discoveries in data-intensive research on technology-enhanced education. After the development of about a decade, it is time to scrutinize and compare these two venues. By doing this, we expected to inform relevant stakeholders of a better understanding of the past development of LAK and EDM and provide suggestions for their future development. Specifically, we conducted an extensive comparison analysis between LAK and EDM from four perspectives, including (i) the topics investigated; (ii) community development; (iii) community diversity; and (iv) research impact. Furthermore, we applied one of the most widely-used language modeling techniques (Word2Vec) to capture words used frequently by researchers to describe future works that can be pursued by building upon suggestions made in the published papers to shed light on potential directions for future research.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {544–553},
numpages = {10},
keywords = {learning analytics, language modeling, hierarchical topic detection, educational data mining},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375499,
author = {Lecailliez, Louis and Flanagan, Brendan and Chen, Mei-Rong Alice and Ogata, Hiroaki},
title = {Smart dictionary for e-book reading analytics},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375499},
doi = {10.1145/3375462.3375499},
abstract = {Reading, be it intensive or extensive, is one of the key skills required to master English as a foreign language (EFL) learner. Computerized e-book systems provide convenient access to learning materials inside and outside class. Students may regularly check the meaning of a word or expression using a separate tool to progress on their reading, which is not only disruptive but can lead to other learning problems. An example of a particular issue faced in EFL is when a student learns an inappropriate meaning of a polysemous word for the context in which it is presented. This is also a problem for teachers as they often need to investigate the cause. In this paper, we propose a smart dictionary integrated into an e-book reading platform. It allows the learner to search and note word definitions directly with the purpose of reducing context switching and improve vocabulary retention. Finally, we propose that learner interactions with the system can be analyzed to support EFL teachers in identifying possible problems that arise through dictionary use while reading.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {89–93},
numpages = {5},
keywords = {learning analytics, english education, e-book, dictionary},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375498,
author = {Sharma, Kshitij and Papamitsiou, Zacharoula and Olsen, Jennifer K. and Giannakos, Michail},
title = {Predicting learners' effortful behaviour in adaptive assessment using multimodal data},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375498},
doi = {10.1145/3375462.3375498},
abstract = {Many factors influence learners' performance on an activity beyond the knowledge required. Learners' on-task effort has been acknowledged for strongly relating to their educational outcomes, reflecting how actively they are engaged in that activity. However, effort is not directly observable. Multimodal data can provide additional insights into the learning processes and may allow for effort estimation. This paper presents an approach for the classification of effort in an adaptive assessment context. Specifically, the behaviour of 32 students was captured during an adaptive self-assessment activity, using logs and physiological data (i.e., eye-tracking, EEG, wristband and facial expressions). We applied k-means to the multimodal data to cluster students' behavioural patterns. Next, we predicted students' effort to complete the upcoming task, based on the discovered behavioural patterns using a combination of Hidden Markov Models (HMMs) and the Viterbi algorithm. We also compared the results with other state-of-the-art classification algorithms (SVM, Random Forest). Our findings provide evidence that HMMs can encode the relationship between effort and behaviour (captured by the multimodal data) in a more efficient way than the other methods. Foremost, a practical implication of the approach is that the derived HMMs also pinpoint the moments to provide preventive/prescriptive feedback to the learners in real-time, by building-upon the relationship between behavioural patterns and the effort the learners are putting in.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {480–489},
numpages = {10},
keywords = {multimodal learning analytics, hidden Markov models, effort classification, adaptive assessment},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375497,
author = {Duff, Andrea and Zamecnik, Andrew and Pardo, Abelardo and Smith, Elizabeth},
title = {The SEIRA approach: course embedded activities to promote academic integrity and literacies in first year engineering},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375497},
doi = {10.1145/3375462.3375497},
abstract = {Students enrol into STEM programs with varying degrees of confidence with citing and referencing texts in their written work. Students often have an inclination to choose numbers over written language throughout schooling which means less opportunity to practice referencing and citation. This is compounded by large numbers of students for whom English is an additional language or who articulate from different cultural ways-of-doing. The Search, Evaluate, Integrate, Reference and Act Ethically (SEIRA) modules were developed to provide discipline-relevance to a confounding task. Data Analysis looking at the student engagement with the SEIRA site and subsequent student success provides an indication of the value of this approach to developing academic literacy across the STEM disciplines.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {218–223},
numpages = {6},
keywords = {plagiarism, academic literacy, academic integrity},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375496,
author = {Barbosa, Gian and Camelo, Raissa and Cavalcanti, Anderson Pinheiro and Miranda, P\'{e}ricles and Mello, Rafael Ferreira and Kovanovi\'{c}, Vitomir and Ga\v{s}evi\'{c}, Dragan},
title = {Towards automatic cross-language classification of cognitive presence in online discussions},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375496},
doi = {10.1145/3375462.3375496},
abstract = {This paper presents a study that examined automated cross-language classification of online discussion messages for the levels of cognitive presence, a key construct from the widely used Community of Inquiry (CoI) model of online learning. Specifically, we examined the classification of 1,500 Portuguese language discussion messages using a classifier trained on a corpus of the 1,747 English language discussion messages. In the study, a random forest classifier was developed using a small set of 108 validated indicators of psychological processes, linguistic coherence, and online discussion structure. The classifier obtained 67% accuracy and Cohen's κ of 0.32, showing a moderate level of inter-rater agreement above chance and the general viability of the proposed approach. Most importantly, the findings suggest that certain aspects of cognitive presence construct are highly generalizable and transfer across different languages. Finally, the paper also presents a novel method for addressing class imbalance problem using a generic algorithm heuristic technique, which provided substantial improvements over the use of imbalanced dataset. Results and practical implications are further discussed.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {605–614},
numpages = {10},
keywords = {optimization, online discussion, cross-language classification, content analytics, community of inquiry model},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375495,
author = {Ferreira, M\'{a}verick and Rolim, Vitor and Mello, Rafael Ferreira and Lins, Rafael Dueire and Chen, Guanliang and Ga\v{s}evi\'{c}, Dragan},
title = {Towards automatic content analysis of social presence in transcripts of online discussions},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375495},
doi = {10.1145/3375462.3375495},
abstract = {This paper presents an approach to automatic labeling of the content of messages in online discussion according to the categories of social presence. To achieve this goal, the proposed approach is based on a combination of traditional text mining features and word counts extracted with the use of established linguistic frameworks (i.e., LIWC and Coh-metrix). The best performing classifier obtained 0.95 and 0.88 for accuracy and Cohen's kappa, respectively. This paper also provides some theoretical insights into the nature of social presence by looking at the classification features that were most relevant for distinguishing between the different categories. Finally, this study adopted epistemic network analysis to investigate the structural construct validity of the automatic classification approach. Namely, the analysis showed that the epistemic networks produced based on messages manually and automatically coded produced nearly identical results. This finding thus produced evidence of the structural validity of the automatic approach.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {141–150},
numpages = {10},
keywords = {text classification, online discussion, epistemic network analysis, content analytics, community of inquiry model},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375494,
author = {Van Goidsenhoven, Steven and Bogdanova, Daria and Deeva, Galina and Broucke, Seppe vanden and De Weerdt, Jochen and Snoeck, Monique},
title = {Predicting student success in a blended learning environment},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375494},
doi = {10.1145/3375462.3375494},
abstract = {Blended learning is gaining ground in contemporary education. However, studies on predictive learning analytics in the context of blended learning remain relatively scarce compared to Massive Open Online Courses (MOOCs), where such applications have gained a strong foothold. Data sets obtained from blended learning environments suffer from a high dimensionality and typically expose a limited number of instances, which makes predictive analysis a challenging task. In this work, we explore the log data of a master-level blended course to predict the students' grades based entirely on the data obtained from an online module (a small private online course), using and comparing logistic regression and random forest-based predictive models. The results of the analysis show that, despite the limited data, success vs. fail predictions can be made as early as in the middle of the course. This could be used in the future for timely interventions, both for failure prevention as well as for reinforcing positive learning behaviours of students.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {17–25},
numpages = {9},
keywords = {random forest classification, machine learning, logistic regression, learning analytics, grade prediction, feature extraction, e-learning, blended learning},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375493,
author = {Uzir, Nora'ayu Ahmad and Ga\v{s}evi\'{c}, Dragan and Jovanovi\'{c}, Jelena and Matcha, Wannisa and Lim, Lisa-Angelique and Fudge, Anthea},
title = {Analytics of time management and learning strategies for effective online learning in blended environments},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375493},
doi = {10.1145/3375462.3375493},
abstract = {This paper reports on the findings of a study that proposed a novel learning analytics methodology that combines three complimentary techniques - agglomerative hierarchical clustering, epistemic network analysis, and process mining. The methodology allows for identification and interpretation of self-regulated learning in terms of the use of learning strategies. The main advantage of the new technique over the existing ones is that it combines the time management and learning tactic dimensions of learning strategies, which are typically studied in isolation. The new technique allows for novel insights into learning strategies by studying the frequency of, strength of connections between, and ordering and time of execution of time management and learning tactics. The technique was validated in a study that was conducted on the trace data of first-year undergraduate students who were enrolled into two consecutive offerings (N2017 = 250 and N2018 = 232) of a course at an Australian university. The application of the proposed technique identified four strategy groups derived from three distinct time management tactics and five learning tactics. The tactics and strategies identified with the technique were correlated with academic performance and were interpreted according to the established theories and practices of self-regulated learning.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {392–401},
numpages = {10},
keywords = {time management strategies, self-regulated learning, learning strategies, learning analytics, blended learning},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375492,
author = {Ak\c{c}apinar, G\"{o}khan and Chen, Mei-Rong Alice and Majumdar, Rwitajit and Flanagan, Brendan and Ogata, Hiroaki},
title = {Exploring student approaches to learning through sequence analysis of reading logs},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375492},
doi = {10.1145/3375462.3375492},
abstract = {In this paper, we aim to explore students' study approaches (e.g., deep, strategic, surface) from the logs collected by an electronic textbook (eBook) system. Data was collected from 89 students related to their reading activities both in and out of the class in a Freshman English course. Students are given a task to study reading materials through the eBook system, highlight the text that is related to the main or supporting ideas, and answer the questions prepared for measuring their level of comprehension. Students in and out of class reading times and their usage of the marker feature were used as a proxy to understand their study approaches. We used theory-driven and data-driven approaches together to model the study approaches of students. Our results showed that three groups of students who have different study approaches could be identified. Relationships between students' reading behaviors and their academic performance is also investigated by using association rule mining analysis. Obtained results are discussed in terms of monitoring, feedback, predicting learning outcomes, and identifying problems with the content design.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {106–111},
numpages = {6},
keywords = {study approaches, sequence analysis, reading logs, learning analytics, clustering, association rule mining},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375491,
author = {Effenberger, Tom\'{a}\v{s} and Pel\'{a}nek, Radek and \v{C}ech\'{a}k, Jaroslav},
title = {Exploration of the robustness and generalizability of the additive factors model},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375491},
doi = {10.1145/3375462.3375491},
abstract = {Additive Factors Model is a widely used student model, which is primarily used for refining knowledge component models (Q-matrices). We explore the robustness and generalizability of the model. We explicitly formulate simplifying assumptions that the model makes and we discuss methods for visualizing learning curves based on the model. We also report on an application of the model to data from a learning system for introductory programming; these experiments illustrate possibly misleading interpretation of model results due to differences in item difficulty. Overall, our results show that greater care has to be taken in the application of the model and in the interpretation of results obtained with the model.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {472–479},
numpages = {8},
keywords = {student modeling, learning curves, knowledge components, introductory programming},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375490,
author = {Klebanov, Beata Beigman and Loukina, Anastassia and Lockwood, John and Liceralde, Van Rynald T. and Sabatini, John and Madnani, Nitin and Gyawali, Binod and Wang, Zuowei and Lentini, Jennifer},
title = {Detecting learning in noisy data: the case of oral reading fluency},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375490},
doi = {10.1145/3375462.3375490},
abstract = {In a school context, learning is usually detected by repeated measurements of the skill of interest through a sequence of specially designed tests; in particular, this is the case with tracking improvement in oral reading fluency in elementary school children in the U.S. Results presented in this paper suggest that it is possible and feasible to detect improvement in oral reading fluency using data collected during children's independent reading of a book using the Relay Reader™ app. We are thus a step closer to the vision of having a child read for the story, not for a test, yet being able to unobtrusively assess their progress in oral reading fluency.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {490–495},
numpages = {6},
keywords = {reading app, reading analytics, oral reading fluency, fluency, children's reading, book reading},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375489,
author = {Hilliger, Isabel and Aguirre, Camila and Miranda, Constanza and Celis, Sergio and P\'{e}rez-Sanagust\'{\i}n, Mar},
title = {Design of a curriculum analytics tool to support continuous improvement processes in higher education},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375489},
doi = {10.1145/3375462.3375489},
abstract = {Curriculum analytics (CA) emerged as a sub-field of learning analytics, aiming to use evidence to drive curriculum decision-making and program improvement. However, its overall impact on program outcomes remains unknown. In this context, this paper presents work-in-progress of a large research project to understand how CA could support continuous improvement processes at a program-level. We followed an approach based on design-based research to develop a CA tool: The Integrative Learning Design Framework. This paper describes three out of four phases of this framework and its main results, including the evaluation of the local impact of this CA tool. This evaluation consisted of an instrumental case study to evaluate its use to support 124 teaching staff in a 3-year continuous improvement process in a Latin American university. Lessons learned indicate that the tool helped staff to collect information for curriculum discussions, facilitating the availability of evidence regarding student competency attainment. To generalize these lessons, future work will consist of evaluating the tool in different university settings.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {181–186},
numpages = {6},
keywords = {learning analytics, higher education, curriculum analytics},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375488,
author = {Whitelock-Wainwright, Alexander and Tsai, Yi-Shan and Lyons, Kayley and Kaliff, Svetlana and Bryant, Mike and Ryan, Kris and Ga\v{s}evi\'{c}, Dragan},
title = {Disciplinary differences in blended learning design: a network analytic study},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375488},
doi = {10.1145/3375462.3375488},
abstract = {Learning design research has predominately relied upon survey- and interview-based methodologies, both of which are subject to limitations of social desirability and recall. An alternative approach is offered in this manuscript, whereby physical and online learning activity data is analysed using Epistemic Network Analysis. Using a sample of 6,040 course offerings from 10 faculties across a four year period (2016--2019), the utility of networks to understand learning design is illustrated. Specifically, through the adoption of a network analytic approach, the following was found: universities are clearly committed to blended learning, but there are considerable differences both between and within disciplines.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {579–588},
numpages = {10},
keywords = {learning activity types, faculty, epistemic network analysis},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375487,
author = {Saint, John and Ga\v{s}evi\'{c}, Dragan and Matcha, Wannisa and Uzir, Nora'Ayu Ahmad and Pardo, Abelardo},
title = {Combining analytic methods to unlock sequential and temporal patterns of self-regulated learning},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375487},
doi = {10.1145/3375462.3375487},
abstract = {The temporal and sequential nature of learning is receiving increasing focus in Learning Analytics circles. The desire to embed studies in recognised theories of self-regulated learning (SRL) has led researchers to conceptualise learning as a process that unfolds and changes over time. To that end, a body of research knowledge is growing which states that traditional frequency-based correlational studies are limited in narrative impact. To further explore this, we analysed trace data collected from online activities of a sample of 239 computer engineering undergraduate students enrolled on a course that followed a flipped class-room pedagogy. We employed SRL categorisation of micro-level processes based on a recognised model of learning, and then analysed the data using: 1) simple frequency measures; 2) epistemic network analysis; 3) temporal process mining; and 4) stochastic process mining. We found that a combination of analyses provided us with a richer insight into SRL behaviours than any one single method. We found that better performing learners employed more optimal behaviours in their navigation through the course's learning management system.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {402–411},
numpages = {10},
keywords = {self-regulated learning, process mining, micro-level processes, learning analytics, epistemic network analysis},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375486,
author = {Niemeijer, Koen and Feskens, Remco and Krempl, Georg and Koops, Jesse and Brinkhuis, Matthieu J. S.},
title = {Constructing and predicting school advice for academic achievement: a comparison of item response theory and machine learning techniques},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375486},
doi = {10.1145/3375462.3375486},
abstract = {Educational tests can be used to estimate pupils' abilities and thereby give an indication of whether their school type is suitable for them. However, tests in education are usually conducted for each content area separately which makes it difficult to combine these results into one single school advice. To help with school advice, we provide a comparison between both domain-specific and domain-agnostic methods for predicting school types. Both use data from a pupil monitoring system in the Netherlands, a system that keeps track of pupils' educational progress over several years by a series of tests measuring multiple skills.A domain-specific item response theory (IRT) model is calibrated from which an ability score is extracted and is subsequently plugged into a multinomial log-linear regression model. Second, we train domain-agnostic machine learning (ML) models. These are a random forest (RF) and a shallow neural network (NN). Furthermore, we apply case weighting to give extra attention to pupils who switched between school types.When considering the performance of all pupils, RFs provided the most accurate predictions followed by NNs and IRT respectively. When only looking at the performance of pupils who switched school type, IRT performed best followed by NNs and RFs. Case weighting proved to provide a major improvement for this group. Lastly, IRT was found to be much easier to explain in comparison to the other models. Thus, while ML provided more accurate results, this comes at the cost of a lower explainability in comparison to IRT.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {462–471},
numpages = {10},
keywords = {random forests, neural networks, machine learning, item response theory, explainable AI, e-learning},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375485,
author = {Geller, Shay A. and Hoernle, Nicholas and Gal, Kobi and Segal, Avi and Zhang, Amy X. and Karger, David and Facciotti, Marc T. and Igo, Michele},
title = {#Confused and beyond: detecting confusion in course forums using students' hashtags},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375485},
doi = {10.1145/3375462.3375485},
abstract = {Students' confusion is a barrier for learning, contributing to loss of motivation and to disengagement with course materials. However, detecting students' confusion in large-scale courses is both time and resource intensive. This paper provides a new approach for confusion detection in online forums that is based on harnessing the power of students' self-reported affective states (reported using a set of pre-defined hashtags). It presents a rule for labeling confusion, based on students' hashtags in their posts, that is shown to align with teachers' judgement. We use this labeling rule to inform the design of an automated classifier for confusion detection for the case when there are no self-reported hashtags present in the test set. We demonstrate this approach in a large scale Biology course using the Nota Bene annotation platform. This work lays the foundation to empower teachers with better support tools for detecting and alleviating confusion in online courses.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {589–594},
numpages = {6},
keywords = {text classification, self-reported affect, online discussion forum, hashtags, emojis, confusion detection},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375484,
author = {Cukurova, Mutlu and Zhou, Qi and Spikol, Daniel and Landolfi, Lorenzo},
title = {Modelling collaborative problem-solving competence with transparent learning analytics: is video data enough?},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375484},
doi = {10.1145/3375462.3375484},
abstract = {In this study, we describe the results of our research to model collaborative problem-solving (CPS) competence based on analytics generated from video data. We have collected ~500 mins video data from 15 groups of 3 students working to solve design problems collaboratively. Initially, with the help of OpenPose, we automatically generated frequency metrics such as the number of the face-in-the-screen; and distance metrics such as the distance between bodies. Based on these metrics, we built decision trees to predict students' listening, watching, making, and speaking behaviours as well as predicting the students' CPS competence. Our results provide useful decision rules mined from analytics of video data which can be used to inform teacher dashboards. Although, the accuracy and recall values of the models built are inferior to previous machine learning work that utilizes multimodal data, the transparent nature of the decision trees provides opportunities for explainable analytics for teachers and learners. This can lead to more agency of teachers and learners, therefore can lead to easier adoption. We conclude the paper with a discussion on the value and limitations of our approach.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {270–275},
numpages = {6},
keywords = {video analytics, physical learning analytics, multimodal learning analytics, decision trees, collaborative problem-solving},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375483,
author = {Viberg, Olga and Khalil, Mohammad and Baars, Martine},
title = {Self-regulated learning and learning analytics in online learning environments: a review of empirical research},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375483},
doi = {10.1145/3375462.3375483},
abstract = {Self-regulated learning (SRL) can predict academic performance. Yet, it is difficult for learners. The ability to self-regulate learning becomes even more important in emerging online learning settings. To support learners in developing their SRL, learning analytics (LA), which can improve learning practice by transforming the ways we support learning, is critical. This scoping review is based on the analysis of 54 papers on LA empirical research for SRL in online learning contexts published between 2011 and 2019. The research question is: What is the current state of the applications of learning analytics to measure and support students' SRL in online learning environments? The focus is on SRL phases, methods, forms of SRL support, evidence for LA and types of online learning settings. Zimmerman's model (2002) was used to examine SRL phases. The evidence about LA was examined in relation to four propositions: whether LA i) improve learning outcomes, ii) improve learning support and teaching, iii) are deployed widely, and iv) used ethically. Results showed most studies focused on SRL parts from the forethought and performance phase but much less focus on reflection. We found little evidence for LA that showed i) improvements in learning outcomes (20%), ii) improvements in learning support and teaching (22%). LA was also found iii) not used widely and iv) few studies (15%) approached research ethically. Overall, the findings show LA research was conducted mainly to measure rather than to support SRL. Thus, there is a critical need to exploit the LA support mechanisms further in order to ultimately use them to foster student SRL in online learning environments.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {524–533},
numpages = {10},
keywords = {self-regulated learning, literature review, learning analytics},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375482,
author = {Ruip\'{e}rez-Valiente, Jos\'{e} A. and Jenner, Matt and Staubitz, Thomas and Li, Xitong and Rohloff, Tobias and Halawa, Sherif and Turro, Carlos and Cheng, Yuan and Zhang, Jiayin and Despujol, Ignacio and Reich, Justin},
title = {Macro MOOC learning analytics: exploring trends across global and regional providers},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375482},
doi = {10.1145/3375462.3375482},
abstract = {Massive Open Online Courses (MOOCs) have opened new educational possibilities for learners around the world. Most of the research and spotlight has been concentrated on a handful of global, English-language providers, but there are a growing number of regional providers of MOOCS in languages other than English. In this work, we have partnered with thirteen MOOC providers from around the world. We apply a multi-platform approach generating a joint and comparable analysis with data from millions of learners. This allows us to examine learning analytics trends at a macro level across various MOOC providers, with a goal of understanding which MOOC trends are globally universal and which of them are context-dependent. The analysis reports preliminary results on the differences and similarities of trends based on the country of origin, level of education, gender and age of their learners across global and regional MOOC providers. This study exemplifies the potential of macro learning analytics in MOOCs to understand the ecosystem and inform the whole community, while calling for more large scale studies in learning analytics through partnerships among researchers and institutions.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {518–523},
numpages = {6},
keywords = {multi-plaform analytics collaboration, learning analytics, large-scale analytics, cultural factors, MOOCs},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375481,
author = {Farrow, Elaine and Moore, Johanna and Ga\v{s}evi\'{c}, Dragan},
title = {Dialogue attributes that inform depth and quality of participation in course discussion forums},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375481},
doi = {10.1145/3375462.3375481},
abstract = {This paper describes work in progress to answer the question of how we can identify and model the depth and quality of student participation in class discussion forums using the content of the discussion forum messages. We look at two widely-studied frameworks for assessing critical discourse and cognitive engagement: the ICAP and Community of Inquiry (CoI) frameworks. Our goal is to discover where they agree and where they offer complementary perspectives on learning.In this study, we train predictive classifiers for both frameworks on the same data set in order to discover which attributes are most predictive and how those correlate with the framework labels. We find that greater depth and quality of participation is associated with longer and more complex messages in both frameworks, and that the threaded reply structure matters more than temporal order. We find some important differences as well, particularly in the treatment of messages of affirmation.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {129–134},
numpages = {6},
keywords = {text analysis, participation, engagement, discussion forum, community of inquiry, cognitive presence, ICAP},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375480,
author = {Seitlinger, Paul and Bibi, Abida and Uus, \~{O}nne and Ley, Tobias},
title = {How working memory capacity limits success in self-directed learning: a cognitive model of search and concept formation},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375480},
doi = {10.1145/3375462.3375480},
abstract = {With this work we intend to develop cognitive modules for learning analytics solutions used in inquiry learning environments that can monitor and assess mental abilities involved in self-directed learning activities. We realize this idea by drawing on models from mathematical psychology, which specify assumptions about the human mind algorithmically and thereby automate a theory-driven data analysis.We report a study to exemplify this approach in which N=105 15-year-old high school students perform a self-determined navigation in a taxonomy of dinosaur concepts. We analyze their search and learning traces through the lens of a connectionist network model of working memory (WM). The results are encouraging in three ways. First, the model predicts students' average progress (as well as difficulties) in forming new concepts at high accuracy. Second, a simple (1-parameter) extension, which we derive from a meta-cognitive learning framework, is sufficient to also predict aggregated search patterns. Third, our initial attempt to fit the model to individual data offers some promising results: estimates of a free parameter correlate significantly with a measure of WM capacity.Together, we believe that these results help demonstrate a novel and promising way towards extending learner models by cognitive variables. We also discuss current limitations in the light of our future work on cognitive-computational scaffolding techniques in inquiry learning scenarios.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {53–62},
numpages = {10},
keywords = {working memory capacity, self-directed learning, concept formation, cognitive-computational modeling},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375479,
author = {Tuti, Timothy and Paton, Chris and Winters, Niall},
title = {Learning to represent healthcare providers knowledge of neonatal emergency care: findings from a smartphone-based learning intervention targeting clinicians from LMICs},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375479},
doi = {10.1145/3375462.3375479},
abstract = {Modelling healthcare providers' knowledge while they are gaining new concepts is an important step towards supporting self-regulated personalised learning at scale. This is especially important if we are to address health workforce skills development and enhance the subsequent quality of care patients receive in the Global South, where a huge skills gap exists. Rich data about healthcare providers' learning can be captured by their responses to close-ended problems within conjunctive solution space -such as clinical training scenarios for emergency care delivery- on smartphone-based learning interventions which are being proposed as a solution for reducing the healthcare skills gap in this context. Together with sequential data detailing a learner's progress while they are solving a learning task, this provides useful insights into their learning behaviour. Predicting learning or forgetting curves from representations of healthcare providers knowledge is a difficult task, but recent promising machine learning advances have produced techniques capable of learning knowledge representations and overcoming this challenge. In this study, we train a Long Short-Term Memory neural network for predicting learners' future performance and forgetting curves by feeding it sequence embeddings of learning task attempts from healthcare providers from Global South. From this training, the model captures nuanced representations of a healthcare provider's clinical knowledge and their patterns of learning behaviours, predicting their future performance with high accuracy. More significantly, by differentiating reduced performance based on spaced learning, the model can help provide timely warning that helps support healthcare providers to reinforce their self-regulated learning while providing a basis for personalised instructional support to aid improved clinical outcomes from their professional practices.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {320–329},
numpages = {10},
keywords = {smartphones, neonatal care, global health, forgetting curves, emergency care, deep knowledge tracing, clinical training},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375478,
author = {Falc\~{a}o, Taciana Pontual and Mello, Rafael Ferreira and Rodrigues, Rodrigo Lins and Diniz, Juliana Regueira Basto and Tsai, Yi-Shan and Ga\v{s}evi\'{c}, Dragan},
title = {Perceptions and expectations about learning analytics from a brazilian higher education institution},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375478},
doi = {10.1145/3375462.3375478},
abstract = {Several tools to support learning processes based on educational data have emerged from research on Learning Analytics (LA) in the last few years. These tools aim to support students and instructors in daily activities, and academic managers in making institutional decisions. Although the adoption of LA tools is spreading, the field still needs to deepen the understanding of the contexts where learning takes place, and of the views of the stakeholders involved in implementing and using these tools. In this sense, the SHEILA framework proposes a set of instruments to perform a detailed analysis of the expectations and needs of different stakeholders in higher education institutions, regarding the adoption of LA. Moreover, there is a lacuna in research on stakeholders' expectations from LA outside the Global North. Therefore, this paper reports on the findings of the application of interviews and focus groups, based on the SHEILA framework, with students and teaching staff from a Brazilian public university, to investigate their perceptions of the potential benefits and risks of using LA in higher education in the country. Findings indicate that there is a high interest in using LA for improving the learning experience, in particular, being able to provide personalized feedback, to adapt teaching practices to students' needs, and to make evidence-based pedagogical decisions. From the analysis of these perspectives, we point to opportunities for using LA in Brazilian higher education.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {240–249},
numpages = {10},
keywords = {qualitative research, learning analytics, human factors, higher education institutions},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375477,
author = {Cavalcanti, Anderson Pinheiro and Diego, Arthur and Mello, Rafael Ferreira and Mangaroska, Katerina and Nascimento, Andr\'{e} and Freitas, Fred and Ga\v{s}evi\'{c}, Dragan},
title = {How good is my feedback? a content analysis of written feedback},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375477},
doi = {10.1145/3375462.3375477},
abstract = {Feedback is a crucial element in helping students identify gaps and assess their learning progress. In online courses, feedback becomes even more critical as it is one of the resources where the teacher interacts directly with the student. However, with the growing number of students enrolled in online learning, it becomes a challenge for instructors to provide good quality feedback that helps the student self-regulate. In this context, this paper proposed a content analysis of feedback text provided by instructors based on different indicators of good feedback. A random forest classifier was trained and evaluated at different feedback levels. The results achieved outcomes up to 87% and 0.39 of accuracy and Cohen's κ, respectively. The paper also provides insights into the most influential textual features of feedback that predict feedback quality.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {428–437},
numpages = {10},
keywords = {online learning, learning analytics, feedback, content analysis},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375476,
author = {Alvarez, Carlos Prieto and Martinez-Maldonado, Roberto and Buckingham Shum, Simon},
title = {LA-DECK: a card-based learning analytics co-design tool},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375476},
doi = {10.1145/3375462.3375476},
abstract = {Human-centred software design gives all stakeholders an active voice in the design of the systems that they are expected to use. However, this is not yet commonplace in Learning Analytics (LA). Co-design techniques from other domains therefore have much to offer to LA, in principle, but there are few detailed accounts of exactly how such sessions unfold. This paper presents the rationale driving a card-based co-design tool specifically tuned for LA, called LA-DECK. In the context of a pilot study with students, educators, LA researchers and developers, we provide qualitative and quantitative accounts of how participants used the cards. Using three different forms of analysis (transcript-centric design vignettes, card-graphs and time-on-topic), we characterise in what ways the sessions were "participatory" in nature, and argue that the cards succeeded in playing very similar roles to those documented in the literature on successful card-based design tools.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {63–72},
numpages = {10},
keywords = {participatory design, learning analytics, co-design, cards},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375475,
author = {Prestigiacomo, Rita and Hadgraft, Roger and Hunter, Jane and Locker, Lori and Knight, Simon and van den Hoven, Elise and Martinez-Maldonado, Roberto},
title = {Learning-centred translucence: an approach to understand how teachers talk about classroom data},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375475},
doi = {10.1145/3375462.3375475},
abstract = {Teachers are increasingly being encouraged to embrace evidence-based practices. Learning analytics (LA) offer great promise in supporting these by providing evidence for teachers and learners to make informed decisions and transform the educational experience. However, LA limitations and their uptake by educators are coming under critical scrutiny. This is in part due to the lack of involvement of teachers and learners in the design of LA tools. In this paper, we propose a human-centred approach to generate understanding of teachers' data needs through the lens of three key principles of translucence: visibility, awareness and accountability. We illustrate our approach through a participatory design sprint to identify how teachers talk about classroom data. We describe teachers' perspectives on the evidence they need for making better-informed decisions and discuss the implications of our approach for the design of human-centred LA in the next years.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {100–105},
numpages = {6},
keywords = {human-centred design, evidence-based decision-making},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375474,
author = {Jovanovi\'{c}, Jelena and Dawson, Shane and Joksimovi\'{c}, Sre\'{c}ko and Siemens, George},
title = {Supporting actionable intelligence: reframing the analysis of observed study strategies},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375474},
doi = {10.1145/3375462.3375474},
abstract = {Models and processes developed in learning analytics research are increasing in sophistication and predictive power. However, the ability to translate analytic findings to practice remains problematic. This study aims to address this issue by establishing a model of learner behaviour that is both predictive of student course performance, and easily interpreted by instructors. To achieve this aim, we analysed fine grained trace data (from 3 offerings of an undergraduate online course, N=1068) to establish a comprehensive set of behaviour indicators aligned with the course design. The identified behaviour patterns, which we refer to as observed study strategies, proved to be associated with the student course performance. By examining the observed strategies of high and low performers throughout the course, we identified prototypical pathways associated with course success and failure. The proposed model and approach offers valuable insights for the provision of process-oriented feedback early in the course, and thus can aid learners in developing their capacity to succeed online.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {161–170},
numpages = {10},
keywords = {trace data, learning tactics and strategies, learning analytics, learner behaviour, explanatory models},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375473,
author = {Hu, Yuanyuan and Donald, Claire and Giacaman, Nasser and Zhu, Zexuan},
title = {Towards automated analysis of cognitive presence in MOOC discussions: a manual classification study},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375473},
doi = {10.1145/3375462.3375473},
abstract = {This paper reports on early stages of a machine learning research project, where phases of cognitive presence in MOOC discussions were manually coded in preparation for training automated cognitive classifiers. We present a manual-classification rubric combining Garrison, Anderson and Archer's [11] coding scheme with Park's [25] revised version for a target MOOC. The inter-rater reliability between two raters achieved 95.4% agreement with a Cohen's weighted kappa of 0.96, demonstrating our classification rubric is plausible for the target MOOC dataset. The classification rubric, originally intended for for-credit, undergraduate courses, can be applied to a MOOC context. We found that the main disagreements between two raters lay on adjacent cognitive phases, implying that additional categories may exist between cognitive phases in such MOOC discussion messages. Overall, our results suggest a reliable rubric for classifying cognitive phases in discussion messages of the target MOOC by two raters. This indicates we are in a position to apply machine learning algorithms which can also cater for data with inter-rater disagreements in future automated classification studies.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {135–140},
numpages = {6},
keywords = {text classification, online discussions, cognitive presence, MOOC},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375472,
author = {Kia, Fatemeh Salehian and Teasley, Stephanie D. and Hatala, Marek and Karabenick, Stuart A. and Kay, Matthew},
title = {How patterns of students dashboard use are related to their achievement and self-regulatory engagement},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375472},
doi = {10.1145/3375462.3375472},
abstract = {The aim of student-facing dashboards is to support learning by providing students with actionable information and promoting self-regulated learning. We created a new dashboard design aligned with SRL theory, called MyLA, to better understand how students use a learning analytics tool. We conducted sequence analysis on students' interactions with three different visualizations in the dashboard, implemented in a LMS, for a large number of students (860) in ten courses representing different disciplines. To evaluate different students' experiences with the dashboard, we computed chi-squared tests of independence on dashboard users (52%) to find frequent patterns that discriminate students by their differences in academic achievement and self-regulated learning behaviors. The results revealed discriminating patterns in dashboard use among different levels of academic achievement and self-regulated learning, particularly for low achieving students and high self-regulated learners. Our findings highlight the importance of differences in students' experience with a student-facing dashboard, and emphasize that one size does not fit all in the design of learning analytics tools.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {340–349},
numpages = {10},
keywords = {student-facing dashboard, sequential pattern mining, self-regulated learning, academic achievement},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375471,
author = {van Leeuwen, Anouschka and Rummel, Nikol},
title = {Comparing teachers' use of mirroring and advising dashboards},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375471},
doi = {10.1145/3375462.3375471},
abstract = {Teachers play an essential role during collaborative learning. To provide effective support, teachers have to be constantly aware of students' activities and make fast decisions about which group to offer support, without disrupting students' collaborative process. Teacher dashboards are visual displays that provide analytics about learners to help teachers increase their awareness of the situation. However, if teachers are not able to efficiently and effectively distill information from the dashboard, the dashboard can become an obstacle instead of an aid. In the present study, we compared dashboards that provide information (mirroring) to dashboards that provide information and alert the teacher to groups that are in need of support (advising). Teachers were shown standardized, fictitious collaborative situations on one of the types of dashboards and were asked to detect the group that was in need of support. The results showed that teachers in the advising condition more often detected the problematic group, needed less effort to do so, and were more confident of their decisions. The teacher-dashboard interaction patterns showed that teachers in the advising condition generally started by checking the given alert, but also that they tried to look at as much information about other groups as they could. In the mirroring condition, teachers generally started by examining information from class overviews, but did not always have time to check information for individual groups. These findings are discussed in light of the role of a teacher dashboard in teachers' decision making in the context of student collaboration.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {26–34},
numpages = {9},
keywords = {teaching/learning strategies, improving classroom teaching, human-computer interface, elementary education, cooperative/collaborative learning},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375470,
author = {Sher, Varshita and Hatala, Marek and Ga\v{s}evi\'{c}, Dragan},
title = {Analyzing the consistency in within-activity learning patterns in blended learning},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375470},
doi = {10.1145/3375462.3375470},
abstract = {Performance and consistency play a large role in learning. This study analyzes the relation between consistency in students' online work habits and academic performance in a blended course. We utilize the data from logs recorded by a learning management system (LMS) in two information technology courses. The two courses required the completion of monthly asynchronous online discussion tasks and weekly assignments, respectively. We measure consistency by using Data Time Warping (DTW) distance for two successive tasks (assignments or discussions), as an appropriate measure to assess similarity of time series, over 11-day timeline starting 10 days before and up to the submission deadline. We found meaningful clusters of students exhibiting similar behavior and we use these to identify three distinct consistency patterns: highly consistent, incrementally consistent, and inconsistent users. We also found evidence of significant associations between these patterns and learner's academic performance.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {1–10},
numpages = {10},
keywords = {work habits, time-series analysis, time management, student persistence, regularity, learner performance and consistency},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375469,
author = {Kuzilek, Jakub and Zdrahal, Zdenek and Vaclavek, Jonas and Fuglik, Viktor and Skocilas, Jan},
title = {Exploring exam strategies of successful first year engineering students},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375469},
doi = {10.1145/3375462.3375469},
abstract = {At present, universities collect study-related data about their students. This information can be used to support students at risk of failing their studies. At the Faculty of Mechanical Engineering (FME), Czech Technical University in Prague (CTU), the group of the first-year students is the most vulnerable. The most critical part of the first year is the winter exam period when students usually divide into those who will pass and fail. One of the most important abilities, students need to learn, is exam planning, and our research aims at the exploration of the exam strategies of successful students. These strategies can be used for improving first-year students retention. The outgoing research on the analysis of exam strategies of the first-year students in the academic year 2017/2018 is reported. From a total of 361 first-year students, successful students have been selected. The successful student is the one who finished all three mandatory exams before the end of the first exam period. From the exam sequences of 153 selected students, a "layered" Markov chain probabilistic model has been constructed. It uncovered the most common exam strategies taken by those students.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {124–128},
numpages = {5},
keywords = {modelling, exam strategies, Markov chains},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375468,
author = {Malekian, Donia and Bailey, James and Kennedy, Gregor},
title = {Prediction of students' assessment readiness in online learning environments: the sequence matters},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375468},
doi = {10.1145/3375462.3375468},
abstract = {Online learning environments are now pervasive in higher education. While not exclusively the case, in these environments, there is often modest teacher presence, and students are provided with access to a range of learning, assessment, and support materials. This places pressure on their study skills, including self-regulation. In this context, students may access assessment material without being fully prepared. This may result in limited success and, in turn, raise a significant risk of disengagement. Therefore, if the prediction of students' assessment readiness was possible, it could be used to assist educators or online learning environments to postpone assessment tasks until students were deemed "ready". In this study, we employed a range of machine learning techniques with aggregated and sequential representations of students' behaviour in a Massive Open Online Course (MOOC), to predict their readiness for assessment tasks. Based on our results, it was possible to successfully predict students' readiness for assessment tasks, particularly if the sequential aspects of behaviour were represented in the model. Additionally, we used sequential pattern mining to investigate which sequences of behaviour differed between high or low level of performance in assessments. We found that a high level of performance had the most sequences related to viewing and reviewing the lecture materials, whereas a low level of performance had the most sequences related to successive failed submissions for an assessment. Based on the findings, implications for supporting specific behaviours to improve learning in online environments are discussed.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {382–391},
numpages = {10},
keywords = {sequential pattern mining, learning analytics, assessment readiness prediction, MOOCs, LSTM},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375467,
author = {Vrzakova, Hana and Amon, Mary Jean and Stewart, Angela and Duran, Nicholas D. and D'Mello, Sidney K.},
title = {Focused or stuck together: multimodal patterns reveal triads' performance in collaborative problem solving},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375467},
doi = {10.1145/3375462.3375467},
abstract = {Collaborative problem solving (CPS) in virtual environments is an increasingly important context of 21st century learning. However, our understanding of this complex and dynamic phenomenon is still limited. Here, we examine unimodal primitives (activity on the screen, speech, and body movements), and their multimodal combinations during remote CPS. We analyze two datasets where 116 triads collaboratively engaged in a challenging visual programming task using video conferencing software. We investigate how UI-interactions, behavioral primitives, and multimodal patterns were associated with teams' subjective and objective performance outcomes. We found that idling with limited speech (i.e., silence or backchannel feedback only) and without movement was negatively correlated with task performance and with participants' subjective perceptions of the collaboration. However, being silent and focused during solution execution was positively correlated with task performance. Results illustrate that in some cases, multimodal patterns improved the predictions and improved explanatory power over the unimodal primitives. We discuss how the findings can inform the design of real-time interventions for remote CPS.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {295–304},
numpages = {10},
keywords = {multimodal learning analytics, interpretability, CSCW, CSCL},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375466,
author = {Lang, David and Chen, Guanling and Mirzaei, Kathy and Paepcke, Andreas},
title = {Is faster better? a study of video playback speed},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375466},
doi = {10.1145/3375462.3375466},
abstract = {We explore the relationship between video playback speed and student learning outcomes. Using an experimental design, we present the results of a pre-registered study that assigns users to watch videos at either 1.0x or 1.25x speed. We find that students who consume sped content are more likely to get better grades in a course, attempt more content, and obtain more certificates. We also find that when videos are sped up, students spend less time consuming videos and are marginally more likely to complete more video content. These findings suggest that future study of playback speed as a tool for optimizing video content for MOOCs is warranted. Applications for reinforcement learning and adaptive content are discussed.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {260–269},
numpages = {10},
keywords = {video analytics, randomized controlled trials, playback speed, clickstreams, MOOCs},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375465,
author = {Molenaar, Inge and Horvers, Anne and Dijkstra, Rick and Baker, Ryan S.},
title = {Personalized visualizations to promote young learners' SRL: the learning path app},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375465},
doi = {10.1145/3375462.3375465},
abstract = {This paper describes the design and evaluation of personalized visualizations to support young learners' Self-Regulated Learning (SRL) in Adaptive Learning Technologies (ALTs). Our learning path app combines three Personalized Visualizations (PV) that are designed as an external reference to support learners' internal regulation process. The personalized visualizations are based on three pillars: grounding in SRL theory, the usage of trace data and the provision of clear actionable recommendations for learners to improve regulation. This quasi-experimental pre-posttest study finds that learners in the personalized visualization condition improved the regulation of their practice behavior, as indicated by higher accuracy and less complex moment-by-moment learning curves compared to learners in the control group. Learners in the PV condition showed better transfer on learning. Finally, students in the personalized visualizations condition were more likely to under-estimate instead of over-estimate their performance. Overall, these findings indicates that the personalized visualizations improved regulation of practice behavior, transfer of learning and changed the bias in relative monitoring accuracy.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {330–339},
numpages = {10},
keywords = {self-regulated learning, learner-faced dashboards, hybrid human-system intelligence, adaptive learning technologies},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375464,
author = {Alexandron, Giora and Wiltrout, Mary Ellen and Berg, Aviram and Ruip\'{e}rez-Valiente, Jos\'{e} A.},
title = {Assessment that matters: balancing reliability and learner-centered pedagogy in MOOC assessment},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375464},
doi = {10.1145/3375462.3375464},
abstract = {Learner-centered pedagogy highlights active learning and formative feedback. Instructors often incentivize learners to engage in such formative assessment activities by crediting their completion and score in the final grade, a pedagogical practice that is very relevant to MOOCs as well. However, previous studies have shown that too many MOOC learners exploit the anonymity to abuse the formative feedback, which is critical in the learning process, to earn points without effort. Unfortunately, limiting feedback and access to decrease cheating is counter-pedagogic and reduces the openness of MOOCs. We aimed to identify and analyze a MOOC assessment strategy that balances this tension between learner-centered pedagogy, incentive design, and reliability of the assessment. In this study, we evaluated an assessment model that MITx Biology introduced in a MOOC to reduce cheating with respect to its effect on two aspects of learner behavior - the amount of cheating and learners' engagement in formative course activities. The contribution of the paper is twofold. First, this work provides MOOC designers with an 'analytically-verified' MOOC assessment model to reduce cheating without compromising learner engagement in formative assessments. Second, this study provides a learning analytics methodology to approximate the effect of such an intervention.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {512–517},
numpages = {6},
keywords = {learning analytics, assessment, MOOCs},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375463,
author = {Pel\'{a}nek, Radek},
title = {Learning analytics challenges: trade-offs, methodology, scalability},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375463},
doi = {10.1145/3375462.3375463},
abstract = {Ryan Baker presented in a LAK 2019 keynote a list of six grand challenges for learning analytics research. The challenges are specified as problems with clearly defined success criteria. Education is, however, a domain full of ill-defined problems. I argue that learning analytics research should reflect this nature of the education domain and focus on less clearly defined, but practically essential issues. As an illustration, I discuss three important challenges of this type: addressing inherent trade-offs in learning environments, the clarification of methodological issues, and the scalability of system development.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {554–558},
numpages = {5},
keywords = {trade-offs, scalability, methodology},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@proceedings{10.1145/3375462,
title = {LAK '20: Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {We are pleased to welcome you to the Tenth International Conference on Learning Analytics and Knowledge (LAK20), organized by the Society for Learning Analytics Research (SoLAR). This year's conference is hosted by the Goethe University in the beautiful city of Frankfurt, Germany on March 23--27, 2020, a place of tremendous importance and rich history of science, art, and philosophy. While this is the first time that LAK is organized in Germany, we want to acknowledge that it is a tremendous collaborative effort by the international community of learning analytics researchers and practitioners. Being the tenth anniversary of the conference, the theme for LAK20 is "Celebrating 10 years of LAK: Shaping the future of the field" and focuses on celebrating the achievements of the learning analytics community in the first ten years as well as on charting a pathway for the next ten years. The LAK20 conference is intended for both researchers and practitioners, and we invite them to come and join a proactive dialogue around the future of learning analytics and its practical adoption. We further extend our invite to educators, leaders, administrators, and government and industry professionals interested in the field of learning analytics and related disciplines.},
location = {Frankfurt, Germany}
}

@inproceedings{10.1145/3303772.3303841,
author = {Rodriguez, Fernando and Yu, Renzhe and Park, Jihyun and Rivas, Mariela Janet and Warschauer, Mark and Sato, Brian K.},
title = {Utilizing Learning Analytics to Map Students' Self-Reported Study Strategies to Click Behaviors in STEM Courses},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303841},
doi = {10.1145/3303772.3303841},
abstract = {Informed by cognitive theories of learning, this work examined how students' self-reported study patterns (spacing vs. cramming) corresponded to their engagement with the Learning Management System (LMS) across two years in a large biology course. We specifically focused on how students accessed non-mandatory resources (lecture videos, lecture slides) and considered whether this pattern differed by underrepresented minority (URM) status. Overall, students who self-reported utilizing spacing strategies throughout the course had higher grades than students who reported cramming throughout the course. When examining LMS engagement, only a small percentage of students accessed the lecture videos and lecture slides. Applying a negative binomial regression model to daily counts of click activities, we also found that students who utilized spacing strategies accessed LMS resources more often but not earlier before major deadlines. Moreover, this finding was not different for underrepresented students. Our results provide some initial evidence showing how spacing behaviors correspond to accessing learning resources. However, given the lack of general engagement with LMS resources, our results underscore the value of encouraging students to utilize these resources when studying course material.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {456–460},
numpages = {5},
keywords = {Underrepresented Students, Study Skills, Spacing Effect, STEM Education, Learning Analytics, Higher Education},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3303772.3303840,
author = {Wise, Alyssa Friend and Cui, Yi},
title = {Top Concept Networks of Professional Education Reflections},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303840},
doi = {10.1145/3303772.3303840},
abstract = {This study explores the application of computational techniques to extract information about dental students' developing conceptions of their profession from digital reflective journal entries. Top concept networks were created for two cohorts of students at the beginning and end of their four-year program. A shift from a collection of general notions about becoming a professional to a more integrated, patient-centered conceptualization was found for both cohorts. The two groups initially differed in their perception of dental school (a mechanism for being able to work as a dentist versus a place to learn the skills to serve patients well) and subsequently in the extent of attention they paid to the feelings of their patients and themselves, as well as the continual growth of skill after graduation. Several useful linguistic markers were identified for examining these same issues in other cohorts. The results suggest that top concept networks can offer a useful window into students' developing conceptions of their profession. This kind of information can support student success on a macro level by offering feedback on existing curricula / informing learning designs to cultivate desired conceptions, and on a micro level through identifying particular ways individuals align with and diverge from the common trajectories.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {260–264},
numpages = {5},
keywords = {professional education, concept network, Reflection},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3303772.3303839,
author = {Hu, Xiao and Li, Fanjie and Kong, Runzhi},
title = {Can Background Music Facilitate Learning? Preliminary Results on Reading Comprehension},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303839},
doi = {10.1145/3303772.3303839},
abstract = {It is a common phenomenon for students to listen to background music while studying. However, there are mixed and inconclusive Kindings in the literature, leaving it unclear whether and in which circumstances background music can facilitate or hinder learning. This paper reports a study investigating the effects of Kive different types of background audio (four types of music and one environmental sound) on reading comprehension. An experiment was conducted with 33 graduate students, where a series of cognitive, metacognitive, affective variables and physiological signals were collected and analyzed. Preliminary results show that there were differences on these variables across different music types. This study contributes to the understanding and optimizing of background music for facilitating learning.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {101–105},
numpages = {5},
keywords = {reading comprehension, physiological signals, meta cognition, learning performance, affect, Background music},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3303772.3303838,
author = {Doroudi, Shayan and Brunskill, Emma},
title = {Fairer but Not Fair Enough On the Equitability of Knowledge Tracing},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303838},
doi = {10.1145/3303772.3303838},
abstract = {Adaptive educational technologies have the capacity to meet the needs of individual students in theory, but in some cases, the degree of personalization might be less than desired, which could lead to inequitable outcomes for students. In this paper, we use simulations to demonstrate that while knowledge tracing algorithms are substantially more equitable than giving all students the same amount of practice, such algorithms can still be inequitable when they rely on inaccurate models. This can arise as a result of two factors: (1) using student models that are fit to aggregate populations of students, and (2) using student models that make incorrect assumptions about student learning. In particular, we demonstrate that both the Bayesian knowledge tracing algorithm and the N-Consecutive Correct Responses heuristic are susceptible to these concerns, but that knowledge tracing with the additive factor model may be more equitable. The broader message of this paper is that when designing learning analytics algorithms, we need to explicitly consider whether the algorithms act fairly with respect to different populations of students, and if not, how we can make our algorithms more equitable.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {335–339},
numpages = {5},
keywords = {model misspecification, knowledge tracing, fairness, equity},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3303772.3303837,
author = {Lin, Yiwen and Dowell, Nia and Godfrey, Andrew and Choi, Heeryung and Brooks, Christopher},
title = {Modeling gender dynamics in intra and interpersonal interactions during online collaborative learning},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303837},
doi = {10.1145/3303772.3303837},
abstract = {There has been long-standing stereotypes on men and women's communication styles, such as men using more assertive or aggressive language and women showing more agreeableness and emotions in interactions. In the context of collaborative learning, male learners often believed to be more active participants while female learners are less engaged. To further explore gender differences in learners communication behavior and whether it has changed in the context of online synchronous collaboration, we examined students interactions at a sociocognitive level with a methodology called Group Communication Analysis (GCA). We found that there were no significant differences between men and women in the degree of participation. However, women exhibited significantly higher average social impact, responsivity and internal cohesion compared to men. We also compared the proportion of learners interaction profiles, and results suggest that women are more likely to be effective and cohesive communicators. We discussed implications of these findings for pedagogical practices to promote inclusivity and equity in collaborative learning online.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {431–435},
numpages = {5},
keywords = {learner characteristics, interaction profile, gender differences, collaborative learning, Communication behavior},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3303772.3303836,
author = {Yan, Wenfei and Dowell, Nia and Holman, Caitlin and Welsh, Stephen S. and Choi, Heeryung and Brooks, Christopher},
title = {Exploring Learner Engagement Patterns in Teach-Outs Using Topic, Sentiment and On-topicness to Reflect on Pedagogy},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303836},
doi = {10.1145/3303772.3303836},
abstract = {MOOCs have developed into multiple learning design models with a wide range of objectives. Teach-Outs are one such example, aiming to drive meaningful discussions around topics of pressing social urgency without the use of formal assessments. Given this approach, it is crucial to evaluate learners' engagement in the discussion forum to understand their experiences. This paper presents a pilot study that applied unsupervised natural language processing techniques to understand what and how students engage in dialogue in a Teach-Out. We used topic modeling to discover the emerging topics in the discussion forums and evaluated the on-topicness of the discussions (i.e. the degree to which discussions were relevant to the Teach-Out content). We also applied content analysis to investigate the sentiments associated with the discussions. We have taken a step toward extracting structure from students' discussions to understand learning behaviors happen in the discussion forum. This is the first study to analyze discussion forums in a Teach-Out.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {180–184},
numpages = {5},
keywords = {topic modeling, sentiment analysis, semantic similarity, online discussion forum, learning experience, Teach-Out, MOOC},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3303772.3303835,
author = {Allen, Laura K. and Mills, Caitlin and Perret, Cecile and McNamara, Danielle S.},
title = {Are You Talking to Me? Multi-Dimensional Language Analysis of Explanations during Reading},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303835},
doi = {10.1145/3303772.3303835},
abstract = {This study examines the extent to which instructions to self-explain vs. other-explain a text lead readers to produce different forms of explanations. Natural language processing was used to examine the content and characteristics of the explanations produced as a function of instruction condition. Undergraduate students (n = 146) typed either self-explanations or other-explanations while reading a science text. The linguistic properties of these explanations were calculated using three automated text analysis tools. Machine learning classifiers in combination with the features were used to predict instruction condition (i.e., self- or other-explanation). The best machine learning model performed at rates above chance (kappa = .247; accuracy = 63%). Follow-up analyses indicated that students in the self-explanation condition generated explanations that were more cohesive and that contained words that were more related to social order (e.g., ethics). Overall, the results suggest that natural language processing techniques can be used to detect subtle differences in students' processing of complex texts.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {116–120},
numpages = {5},
keywords = {reading, corpus linguistics, comprehension, Natural Language Processing, Intelligent Tutoring Systems},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3303772.3303834,
author = {Fiallos, Angel and Ochoa, Xavier},
title = {Semi-Automatic Generation of Intelligent Curricula to Facilitate Learning Analytics},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303834},
doi = {10.1145/3303772.3303834},
abstract = {Several Learning Analytics applications are limited by the cost of generating a computer understandable description of the course domain, what is called an Intelligent Curriculum. The following work contributes a novel approach to (semi-)automatically generate Intelligent Curriculum through ontologies extracted from existing learning materials such as digital books or web content. Through a series of natural language processing steps, the semi-structured information present in existing content is transformed into a concept-graph. This work also evaluates the proposed methodology by applying it to learning content for two different courses and measuring the quality of the extracted ontologies against manually generated ones. The results obtained suggest that the technique can be readily used to provide domain information to other Learning Analytics tools.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {46–50},
numpages = {5},
keywords = {ontologies, intelligent curriculum, NLP},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3303772.3303833,
author = {Klebanov, Beata Beigman and Loukina, Anastassia and Madnani, Nitin and Sabatini, John and Lentini, Jennifer},
title = {Would you? Could you? On a tablet? Analytics of Children's eBook Reading},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303833},
doi = {10.1145/3303772.3303833},
abstract = {It is difficult to overstate the importance of literacy for adequate functioning in society, from educational attainment and employment opportunities to health outcomes. We created a reading app with the goal of helping readers improve their reading skill while reading for meaning and pleasure, and used it to collect unique data on children's extended reading. Analysis of the data reveals the importance of a behavioral factor in understanding observed reading performance.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {106–110},
numpages = {5},
keywords = {reading app, reading analytics, motivation, fluency, comprehension, children's reading, book reading},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3303772.3303832,
author = {Reilly, Joseph M. and Dede, Chris},
title = {Differences in Student Trajectories via Filtered Time Series Analysis in an Immersive Virtual World},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303832},
doi = {10.1145/3303772.3303832},
abstract = {To scaffold students' investigations of an inquiry-based immersive virtual world for science education without undercutting the affordances an open-ended activity provides, this study explores ways time-stamped log files of groups' actions may enable the automatic generation of formative supports. Groups' logged actions in the virtual world are filtered via principal component analysis to provide a time series trajectory showing the rate of their investigative activities over time. This technique functions well in open-ended environments and examines the entire course of their experience in the virtual world instead of specific subsequences. Groups' trajectories are grouped via k-means clustering to identify different typical pathways taken through the immersive virtual world. These different approaches are then correlated with learning gains across several survey constructs (affective dimensions, ecosystem science content, understanding of causality, and experimental methods) to see how various trends are associated with different outcomes. Differences by teacher and school are explored to see how best to support inclusion and success of a diverse array of learners.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {130–134},
numpages = {5},
keywords = {time-series analysis, scientific inquiry, log file analysis, learning analytics, immersive virtual world},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3303772.3303831,
author = {Peffer, Melanie and Quigley, David and Mostowfi, Mehrgan},
title = {Clustering Analysis Reveals Authentic Science Inquiry Trajectories Among Undergraduates},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303831},
doi = {10.1145/3303772.3303831},
abstract = {Science education reforms in the United States call for an emphasis on teaching of scientific practices, such as inquiry. Previous work examined expert versus novice practices in authentic science inquiry and found although experts have fairly consistent inquiry strategies, novices exist on a continuum. In this paper, we extend our previous qualitative work to quantitatively analyze differences in inquiry practices among novices. Using clustering analysis, we found that non-science majors who performed simple investigations tended to cluster together and biology majors who performed complex investigations also tended to cluster together. We observed two additional clusters that contain both non-science majors and biology majors, but who performed distinct inquiry strategies. This raises some critical questions about how to pedagogically target students within each cluster.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {96–100},
numpages = {5},
keywords = {science practices, science classroom inquiry simulations, clustering, Authentic science inquiry},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3303772.3303830,
author = {Mongkhonvanit, Kritphong and Kanopka, Klint and Lang, David},
title = {Deep Knowledge Tracing and Engagement with MOOCs},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303830},
doi = {10.1145/3303772.3303830},
abstract = {MOOCs and online courses have notoriously high attrition [1]. One challenge is that it can be difficult to tell if students fail to complete because of disinterest or because of course difficulty. Utilizing a Deep Knowledge Tracing framework, we account for student engagement by including course interaction covariates. With these, we find that we can predict a student's next item response with over 88% accuracy. Using these predictions, targeted interventions can be offered to students and targeted improvements can be made to courses. In particular, this approach would allow for gating of content until a student has reasonable likelihood of succeeding.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {340–342},
numpages = {3},
keywords = {video interactions, neural networks, item response, MOOCS},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3303772.3303829,
author = {Arcuria, Philip and Morgan, William and Fikes, Thomas G.},
title = {Validating the Use of LMS-Derived Rubric Structural Features to Facilitate Automated Measurement of Rubric Quality},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303829},
doi = {10.1145/3303772.3303829},
abstract = {Rubrics are widely used throughout postsecondary education as means for aiding in instruction and evaluation. However, despite their broad global adoption, very little is known about the quality of rubrics in use. We develop two measures to assess the quality of rubrics: (1) a checklist identifying criteria of high-quality rubrics based on analytic rubric design best practices and (2) a set of LMS-derived features that are hypothesized to represent structural components that are, in general, necessary but not sufficient for high quality rubrics. The validity of using the feature-generated scores as proxies for identifying rubric quality is evaluated through several means. First, the feature-generated scores are calculated for a set of external exemplary rubrics of known high quality. Second, the feature-scores for a subset of internal rubrics are compared to average human rater scores of rubric quality based on the checklist. We discuss the results, practical applications, and a larger research program surrounding the feature-generated scores.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {270–274},
numpages = {5},
keywords = {student success, string distance metrics, Rubrics},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3303772.3303828,
author = {Carnell, Stephanie and Lok, Benjamin and James, Melva T. and Su, Jonathan K.},
title = {Predicting Student Success in Communication Skills Learning Scenarios with Virtual Humans},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303828},
doi = {10.1145/3303772.3303828},
abstract = {Virtual humans are frequently used to help medical students practice communication skills. Here, we show that communication skills features drawn from the literature on best practices for doctor-patient communication can be used to predict student interviewers' success in a given domain skill. We also demonstrate the viability of Bayesian Rule Lists, an interpretable machine learning model, for this use case. Bayesian Rule Lists' predictive performance is comparable to that of other other commonly used algorithms, including decision trees. This suggests that Bayesian Rule Lists, which produce simple, human-readable trained binary classifiers, may be suitable for providing feedback for educational purposes.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {436–440},
numpages = {5},
keywords = {virtual humans, interpretable machine learning, doctor-patient communication, communications skills learning},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3303772.3303827,
author = {Chounta, Irene-Angelica and Carvalho, Paulo F.},
title = {Square it up! How to model step duration when predicting student performance},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303827},
doi = {10.1145/3303772.3303827},
abstract = {In this paper, we explore how we can model students' response times to predict student performance in Intelligent Tutoring Systems. Related research suggests that response time can provide information with respect to correctness. However, time is not consistently used when modeling students' performance. Here, we build on previous work that indicated that the relationship between response time and student performance is non-linear. Based on this concept, we compare three models: a standard Additive Factors Analysis Model (AFM), an AFM model enhanced with a linear step duration parameter and an AFM model enhanced with a quadratic, step duration parameter. The results of this comparison show that the AFM model that is enhanced with the quadratic step duration parameter outperforms the other models over four different datasets and for most of the metrics we used to evaluate the models in cross validation and prediction.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {330–334},
numpages = {5},
keywords = {step duration, intelligent tutoring systems, Student Modeling},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3303772.3303826,
author = {Hecking, Tobias and Doberstein, Dorian and Hoppe, H. Ulrich},
title = {Predicting the Well-functioning of Learning Groups under Privacy Restrictions},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303826},
doi = {10.1145/3303772.3303826},
abstract = {Establishing small learning groups in online courses is a possible way to foster collaborative knowledge building in an engaging and effective learning community. To enable group activities it is not enough to design collaborative tasks and to provide collaboration tools for online scenarios. Collaboration in such learning groups is prone to fail or even not to be initiated without explicit guidance. In the target situations, interventions and guiding mechanisms have to scale with a growing number of course participants. To achieve this under privacy constraints, we aim at identifying target indicators for well-functioning group work that do not rely on any kind of information about individual learners.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {245–249},
numpages = {5},
keywords = {Predictive Models, Online courses, Group support},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3303772.3303825,
author = {Mangaroska, Katerina and Vesin, Boban and Giannakos, Michail},
title = {Cross-Platform Analytics: A step towards Personalization and Adaptation in Education},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303825},
doi = {10.1145/3303772.3303825},
abstract = {Learning analytics are used to track learners' progress and empower educators and learners to make well-informed data-driven decisions. However, due to the distributed nature of the learning process, analytics need to be combined to offer broader insights into learner's behavior and experiences. Consequently, this paper presents an architecture of a learning ecosystem, that integrates and utilizes cross-platform analytics. The proposed cross-platform architecture has been put into practice via a Java programming course. After a series of studies, a proof of concept was derived that shows how cross-platform analytics amplify the relevant analytics for the learning process. Such analytics could improve educators' and learners' understanding of their own actions and the environments in which learning occurs.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {71–75},
numpages = {5},
keywords = {multimodal systems, learning analytics, architecture},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3303772.3303824,
author = {Varao-Sousa, Trish L. and Mills, Caitlin and Kingstone, Alan},
title = {Where You Are, Not What You See: The Impact of Learning Environment on Mind Wandering and Material Retention},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303824},
doi = {10.1145/3303772.3303824},
abstract = {Online lectures are an increasingly popular tool for learning, yet research on instructor visibility during an online lecture, and students' environmental settings, has not been well-explored. The current study addresses this gap in the literature by experimentally manipulating online display format and social learning settings to understand their influence on student learning and mind-wandering experiences. Results suggest that instructor visibility within an online lecture does not impact students' MW or retention performance. However, we found some evidence that students' social setting during viewing has an impact on MW (p = .05). Specifically, students who watched the lecture in a classroom with others reported significantly more MW than students who watched the lecture alone. Finally, social setting also moderated the negative relationship between MW and material retention. Our results demonstrate that learning experiences during online lectures can vary based on where, and with whom, the lectures are watched.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {421–425},
numpages = {5},
keywords = {mind-wandering, memory, learning setting, Online lectures},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3303772.3303823,
author = {Lu, Yihan and Hsiao, I-Han},
title = {Exploring Programming Semantic Analytics with Deep Learning Models},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303823},
doi = {10.1145/3303772.3303823},
abstract = {There are numerous studies have reported the effectiveness of example-based programming learning. However, less is explored recommending code examples with advanced Machine Learning-based models. In this work, we propose a new method to explore the semantic analytics between programming codes and the annotations. We hypothesize that these semantics analytics will capture mass amount of valuable information that can be used as features to build predictive models. We evaluated the proposed semantic analytics extraction method with multiple deep learning algorithms. Results showed that deep learning models outperformed other models and baseline in most cases. Further analysis indicated that in special cases, the proposed method outperformed deep learning models by restricting false-positive classifications.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {155–159},
numpages = {5},
keywords = {deep learning, Text based classification, Semantic modeling, Programming semantics, Coding concept detection},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3303772.3303822,
author = {Qiao, Chen and Hu, Xiao},
title = {Measuring Knowledge Gaps in Student Responses by Mining Networked Representations of Texts},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303822},
doi = {10.1145/3303772.3303822},
abstract = {Gaps between knowledge sources are interesting to various stakeholders: they might indicate potential misconceptions awaiting correction, complex or novel knowledge that requires careful delivery or studying. Motivated by these underlying values, this study explores the knowledge gap phenomenon in the context of student textual responses. In the method proposed in this study, discourses are first mapped into structured knowledge spaces where gaps between correct/incorrect responses and assessed knowledge are measured by network-based metrics. Empirical results demonstrate the effectiveness of the proposed method in measuring gaps in student responses. The networked representation of texts proposed in this study is novel in quantitatively framing gaps of knowledge. It also offers a set of validated metrics for analyzing student responses in research and practice.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {275–279},
numpages = {5},
keywords = {text mining, student responses, network analysis, knowledge gap measurement, educational data mining},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3303772.3303821,
author = {Mitra, Ritayan and Chavan, Pankaj},
title = {DEBE feedback for large lecture classroom analytics},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303821},
doi = {10.1145/3303772.3303821},
abstract = {Learning Analytics (LA) research has demonstrated the potential of LA in detecting and monitoring cognitive-affective parameters and improving student success. But most of it has been applied to online and computerized learning environments whereas physical classrooms have largely remained outside the scope of such research. This paper attempts to bridge that gap by proposing a student feedback model in which they report on the difficult/easy and engaging/boring aspects of their lecture. We outline the pedagogical affordances of an aggregated time-series of such data and discuss it within the context of LA research.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {426–430},
numpages = {5},
keywords = {quantified self, mobile application, live feedback, learning analytics, Large lectures},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3303772.3303820,
author = {Babik, Dmytro and Stevens, Scott and Waters, Andrew E.},
title = {Comparison of Ranking and Rating Scales in Online Peer Assessment: Simulation Approach},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303820},
doi = {10.1145/3303772.3303820},
abstract = {This study examines fidelity of ranking and rating scales in the context of online peer review and assessment. Using the Monte-Carlo simulation technique, we demonstrated that rating scales outperform ranking scales in revealing the relative "true" latent quality of the peer-assessed artifacts via the observed aggregate peer assessment scores. Our analysis focused on a simple, single-round peer assessment process and took into account peer assessment network topology, network size, the number of assessments per artifact, and the correlation statistics used. This methodology allows to separate the effects of structural components of peer assessment from cognitive effects.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {205–209},
numpages = {5},
keywords = {validity, scales, reliability, rating, ranking, peer review, peer evaluation, Peer assessment},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3303772.3303819,
author = {Macarini, Luiz Antonio and Cechinel, Cristian and Santos, Henrique Lemos dos and Ochoa, Xavier and Rod\'{e}s, Virg\'{\i}nia and Alonso, Guillermo Ettlin and Casas, Al\'{e}n P\'{e}rez and D\'{\i}az, Patricia},
title = {Challenges on implementing Learning Analytics over countrywide K-12 data},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303819},
doi = {10.1145/3303772.3303819},
abstract = {The present work describes the challenges faced during the development of a countrywide Learning Analytics tool focused on tracking the trajectories of Uruguayan students during their first three years of secondary education. Due to the large-scale of the project, which covers an entire national educational system, several challenges and constraints (both technical and legal) were faced during its conception and development. This paper presents the design decisions and solutions found to address or mitigate the problems found, with the current state of the project. Early results point out the feasibility of finding meaningful patterns in the available data (using data mining techniques) which can be embedded into a prototype for tracking the students scholar trajectory.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {441–445},
numpages = {5},
keywords = {Primary and Secondary Education, Learning Analytics, Educational Data Mining, Early Warning System, Academic Trajectory},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3303772.3303818,
author = {Martinez-Maldonado, Roberto},
title = {"I Spent More Time with that Team": Making Spatial Pedagogy Visible Using Positioning Sensors},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303818},
doi = {10.1145/3303772.3303818},
abstract = {Teachers are often encouraged to adopt different positioning strategies at various stages of a classroom lesson as each can influence learners in different ways. However, little work has been done to make evidence of the use of classrooms visible to teachers and students. As sensors drop in price, it is becoming more viable to capture traces of the use of the physical classroom space automatically. In this paper, we build on the notion of spatial pedagogy to propose an approach to visualise digital traces of teacher positioning in the classroom. We illustrate our approach through an authentic case study of a teacher enacting three distinctive learning designs. We document the teacher's and students' reactions to visual representations of positioning data to explore their potential as proxies of spatial pedagogy.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {21–25},
numpages = {5},
keywords = {wearables, mobility tracking, learning spaces, classroom, IoT},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3303772.3303817,
author = {Thaker, Khushboo and Carvalho, Paulo and Koedinger, Kenneth},
title = {Comprehension Factor Analysis: Modeling student's reading behaviour: Accounting for reading practice in predicting students' learning in MOOCs},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303817},
doi = {10.1145/3303772.3303817},
abstract = {Massive Open Online Courses (MOOCs) often incorporate lecture-based learning along with lecture notes, textbooks, and videos to students. Moreover, MOOCs also incorporate practice activities and quizzes. Student learning in MOOCs can be tracked and improved using state-of-the-art student modeling. Currently, this means employing conventional student models that are constructed around Intelligent Tutoring Systems (ITS). Traditional ITS systems only utilize students performance interactions (quiz, problem-solving or practice activities). Therefore, text interactions are entirely ignored while modeling students performance in MOOCs using these cognitive models. In this work, we propose a Comprehension Factor Analysis model (CFM) for online courses, which integrates student reading interactions in student models to track and predict learning outcomes. Our model evaluation shows that CFM outperforms state-of-the-art models in predicting students' performance in a MOOC. These models can help better student-wise adaptation in the context of MOOCs.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {111–115},
numpages = {5},
keywords = {Student modeling, Reading Behaviour, MOOCs, Education Data Mining},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3303772.3303816,
author = {Vytasek, Jovita M. and Patzak, Alexandra and Winne, Philip H.},
title = {Topic Development to Support Revision Feedback},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303816},
doi = {10.1145/3303772.3303816},
abstract = {Revision is important but challenging for novice writers, particularly in post-secondary education where opportunities for personalized feedback are limited. Inexperienced writers typically overlook revision; when they do revise, they focus on surface errors rather than global revisions that enhance meaning and coherence. Writing analytics can automate personalized prompts to guide revision. We use topic modelling LDA as grounds for an analytic to scaffold holistic revision at paragraph and essay levels. The analytic visualizes topic distribution and generates three types of prompts: Introduction, Paragraph and Conclusion. Feedback encourages revisions focusing on sequencing topics, expanding underdeveloped ideas, and making holistic revisions to improve clarity and coherence of paragraphs. Model feedback was evaluated using undergraduate student essays on various topics scored by human evaluators. Model accuracy was strong for all types of feedback. This opens new branches of research to explore generating personalized feedback at paragraph and essay levels.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {220–224},
numpages = {5},
keywords = {Writing revision, Writing Analytics, Topic modeling, Student support},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3303772.3303815,
author = {Gal, Tomer and Hershkovitz, Arnon},
title = {Different Types of Response-Based Feedback in Mathematics: The case of textual and symbolic messages},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303815},
doi = {10.1145/3303772.3303815},
abstract = {The current study compares textual and symbolic elaborated, response-based feedback in mathematics. We use a randomized experiment in Khan Academy to measure feedback effect in four different topics. Overall, we point out to the superiority of symbolic feedback.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {265–269},
numpages = {5},
keywords = {mathematics education, log analysis, computer-based learning, Response-based feedback},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3303772.3303814,
author = {Jiang, Weijie and Pardos, Zachary A. and Wei, Qiang},
title = {Goal-based Course Recommendation},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303814},
doi = {10.1145/3303772.3303814},
abstract = {With cross-disciplinary academic interests increasing and academic advising resources over capacity, the importance of exploring data-assisted methods to support student decision making has never been higher. We build on the findings and methodologies of a quickly developing literature around prediction and recommendation in higher education and develop a novel recurrent neural network-based recommendation system for suggesting courses to help students prepare for target courses of interest, personalized to their estimated prior knowledge background and zone of proximal development. We validate the model using tests of grade prediction and the ability to recover prerequisite relationships articulated by the university. In the third validation, we run the fully personalized recommendation for students the semester before taking a historically difficult course and observe differential overlap with our would-be suggestions. While not proof of causal effectiveness, these three evaluation perspectives on the performance of the goal-based model build confidence and bring us one step closer to deployment of this personalized course preparation affordance in the wild.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {36–45},
numpages = {10},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3303772.3303813,
author = {Azcona, David and Arora, Piyush and Hsiao, I-Han and Smeaton, Alan},
title = {user2code2vec: Embeddings for Profiling Students Based on Distributional Representations of Source Code},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303813},
doi = {10.1145/3303772.3303813},
abstract = {In this work, we propose a new methodology to profile individual students of computer science based on their programming design using a technique called embeddings. We investigate different approaches to analyze user source code submissions in the Python language. We compare the performances of different source code vectorization techniques to predict the correctness of a code submission. In addition, we propose a new mechanism to represent students based on their code submissions for a given set of laboratory tasks on a particular course. This way, we can make deeper recommendations for programming solutions and pathways to support student learning and progression in computer programming modules effectively at a Higher Education Institution. Recent work using Deep Learning tends to work better when more and more data is provided. However, in Learning Analytics, the number of students in a course is an unavoidable limit. Thus we cannot simply generate more data as is done in other domains such as FinTech or Social Network Analysis. Our findings indicate there is a need to learn and develop better mechanisms to extract and learn effective data features from students so as to analyze the students' progression and performance effectively.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {86–95},
numpages = {10},
keywords = {user2code2vec, code2vec, Representation Learning for Source Code, Machine Learning, Distributed Representations, Computer Science Education, Code Embeddings},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3303772.3303812,
author = {NeCamp, Timothy and Gardner, Josh and Brooks, Christopher},
title = {Beyond A/B Testing: Sequential Randomization for Developing Interventions in Scaled Digital Learning Environments},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303812},
doi = {10.1145/3303772.3303812},
abstract = {Randomized experiments ensure robust causal inference that is critical to effective learning analytics research and practice. However, traditional randomized experiments, like A/B tests, are limiting in large scale digital learning environments. While traditional experiments can accurately compare two treatment options, they are less able to inform how to adapt interventions to continually meet learners' diverse needs. In this work, we introduce a trial design for developing adaptive interventions in scaled digital learning environments -- the sequential randomized trial (SRT). With the goal of improving learner experience and developing interventions that benefit all learners at all times, SRTs inform how to sequence, time, and personalize interventions. In this paper, we provide an overview of SRTs, and we illustrate the advantages they hold compared to traditional experiments. We describe a novel SRT run in a large scale data science MOOC. The trial results contextualize how learner engagement can be addressed through culturally-targeted reminder emails. We also provide practical advice for researchers who aim to run their own SRTs to develop adaptive interventions in scaled digital learning environments.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {539–548},
numpages = {10},
keywords = {sequential randomization, MOOCs, Experimental design},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3303772.3303811,
author = {Chua, Yi Han Victoria and Dauwels, Justin and Tan, Seng Chee},
title = {Technologies for automated analysis of co-located, real-life, physical learning spaces: Where are we now?},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303811},
doi = {10.1145/3303772.3303811},
abstract = {The motivation for this paper is derived from the fact that there has been increasing interest among researchers and practitioners in developing technologies that capture, model and analyze learning and teaching experiences that take place beyond computer-based learning environments. In this paper, we review case studies of tools and technologies developed to collect and analyze data in educational settings, quantify learning and teaching processes and support assessment of learning and teaching in an automated fashion. We focus on pipelines that leverage information and data harnessed from physical spaces and/or integrates collected data across physical and digital spaces. Our review reveals a promising field of physical classroom analysis. We describe some trends and suggest potential future directions. Specifically, more research should be geared towards a) deployable and sustainable data collection set-ups in physical learning environments, b) teacher assessment, c) developing feedback and visualization systems and d) promoting inclusivity and generalizability of models across populations.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {11–20},
numpages = {10},
keywords = {physical learning analytics, educational technologies, educational data mining, co-located learning, Face-to-face classroom analysis},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3303772.3303810,
author = {Zeng, Ziheng and Chaturvedi, Snigdha and Bhat, Suma and Roth, Dan},
title = {DiAd: Domain Adaptation for Learning at Scale},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303810},
doi = {10.1145/3303772.3303810},
abstract = {Massive online courses occupy an important place in the educational landscape of today. We study an approach to scale predictive analytic models derived from online course discussion fora--specifically that of confusion detection--onto other courses. The primary challenge here is the lack of labeled examples in a new course and this calls for unsupervised domain adaptation (DA). As a first step in exploring DA in the education domain, we propose a simple algorithm, DiAd, which adapts a classifier trained on a course with labeled data by selectively choosing instances from a new course (with no labeled data) that are most dissimilar to the course with labeled data and on which the classifier is very confident of classification. Our algorithm is empirically validated on the confusion detection task across multiple online courses. We find that DiAd outperforms other methods on the target domain, while showing a comparable performance to a popular method that uses labeled data from the target domain.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {185–194},
numpages = {10},
keywords = {Learning at Scale, Domain Adaptation, Confusion Detection},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3303772.3303809,
author = {Li, Warren and Brooks, Christopher and Schaub, Florian},
title = {The Impact of Student Opt-Out on Educational Predictive Models},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303809},
doi = {10.1145/3303772.3303809},
abstract = {Privacy concerns may lead people to opt-in or opt-out of having their educational data collected. These decisions may impact the performance of educational predictive models. To understand this, we conducted a survey to determine the propensity of students to withhold or grant access to their data for the purposes of training predictive models. We simulated the effects of opt-out on the accuracy of educational predictive models by dropping a random sample of data over a range of increments, and then contextualize our findings using the survey results. We find that grade predictive models are fairly robust and that kappa scores do not decrease unless there is signiicant opt-out, but when there is, the deteriorating performance disproportionately affects certain subpopulations.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {411–420},
numpages = {10},
keywords = {student agency, opt-in/opt-out, machine learning bias, ensembling, data privacy, Predictive models},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3303772.3303808,
author = {Fiacco, James and Cotos, Elena and Ros\'{e}, Carolyn},
title = {Towards Enabling Feedback on Rhetorical Structure with Neural Sequence Models},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303808},
doi = {10.1145/3303772.3303808},
abstract = {Analysis of student writing, both for assessment and for enabling feedback have been of interest to the field of learning analytics. While much progress can be made through detection of local cues in writing, structured prediction approaches offer capabilities that are particularly well tailored to the needs of models aiming to offer substantive feedback on rhetorical structure. We thus cast the analysis of rhetorical structure in academic writing as a structured prediction task in which we employ models that leverage both local and global cues in writing. In particular, this paper presents a hierarchical neural architecture that performs this task. The evaluation demonstrates that the architecture achieves near-human performance while significantly surpassing state-of-the-art baselines. A multifaceted approach to model interpretation offers insights into the inner workings of the model.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {310–319},
numpages = {10},
keywords = {writing feedback, neural sequence model, neural network interpretation, hierarchical, conditional random field, bidirectional LSTM, automatic essay evaluation, Rhetorical structure},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3303772.3303807,
author = {Andres, Juliana Ma. Alexandra L. and Ocumpaugh, Jaclyn and Baker, Ryan S. and Slater, Stefan and Paquette, Luc and Jiang, Yang and Karumbaiah, Shamya and Bosch, Nigel and Munshi, Anabil and Moore, Allison and Biswas, Gautam},
title = {Affect Sequences and Learning in Betty's Brain},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303807},
doi = {10.1145/3303772.3303807},
abstract = {Education research has explored the role of students' affective states in learning, but some evidence suggests that existing models may not fully capture the meaning or frequency of how students transition between different states. In this study we examine the patterns of educationally-relevant affective states within the context of Betty's Brain, an open-ended, computer-based learning system used to teach complex scientific processes. We examine three types of affective transitions based on similarity with the theorized D'Mello and Graesser model, transition between two affective states, and the sustained instances of certain states. We correlate of the frequency of these patterns with learning outcomes and our findings suggest that boredom is a powerful indicator of students' knowledge, but not necessarily indicative of learning. We discuss our findings within the context of both research and theory on affect dynamics and the implications for pedagogical and system design.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {383–390},
numpages = {8},
keywords = {learning analytics, affect, Affect dynamics},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3303772.3303806,
author = {Botelho, Anthony F. and Varatharaj, Ashvini and Inwegen, Eric G. Van and Heffernan, Neil T.},
title = {Refusing to Try: Characterizing Early Stopout on Student Assignments},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303806},
doi = {10.1145/3303772.3303806},
abstract = {A prominent issue faced by the education research community is that of student attrition. While large research efforts have been devoted to studying course-level attrition, widely referred to as dropout, less research has been focused on finer-grained assignment-level attrition commonly observed in K-12 classrooms. This later instantiation of attrition, referred to in this paper as "stopout," is characterized by students failing to complete their assigned work, but the cause of such behavior are not often known. This becomes a large problem for educators and developers of learning platforms as students who give up on assignments early are missing opportunities to learn and practice the material which may affect future performance on related topics; similarly, it is difficult for researchers to develop, and subsequently difficult for computer-based systems to deploy interventions aimed at promoting productive persistence once a student has ceased interaction with the software. This difficulty highlights the importance to understand and identify early signs of stopout behavior in order to provide aid to students preemptively to promote productive persistence in their learning. While many cases of student stopout may be attributable to gaps in student knowledge and indicative of struggle, student attributes such as grit and persistence may be further affected by other factors. This work focuses on identifying different forms of stopout behavior in the context of middle school math by observing student behaviors at the sub-problem level. We find that students exhibit disproportionate stopout on the first problem of their assignments in comparison to stopout on subsequent problems, identifying a behavior that we call "refusal," and use the emerging patterns of student activity to better understand the potential causes underlying stopout behavior early in an assignment.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {391–400},
numpages = {10},
keywords = {Student Attrition, Stopout, Refusal, Persistence, Dropout},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3303772.3303805,
author = {Aguilar, Stephen J. and Baek, Clare},
title = {Motivated Information Seeking and Graph Comprehension Among College Students},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303805},
doi = {10.1145/3303772.3303805},
abstract = {Learning Analytics Dashboards (LADs) are predicated on the notion that access to more academic information can help students regulate their academic behaviors, but what is the association between information seeking preferences and help-seeking practices among college students? If given access to more information, what might college students do with it?We investigated these questions in a series of two studies. Study 1 validates a measure of information-seeking preferences---the Motivated Information-Seeking Questionnaire (MISQ)----using a college student sample drawn from across the country (n = 551). In a second study, we used the MISQ to measure college students' (n=210) performance-avoid (i.e., avoiding seeming incompetent in relation to one's peers) and performance-approach (i.e., wishing to outperform one's peers) information seeking preferences, their help-seeking behaviors, and their ability to comprehend line graphs and bar graphs---two common graphs types for LADs.Results point to a negative relationship between graph comprehension and help-seeking strategies, such as attending office hours, emailing one's professor for help, or visiting a study center---even after controlling for academic performance and demographic characteristics. This suggests that students more capable of readings graphs might not seek help when needed. Further results suggest a positive relationship between performance-approach information-seeking preferences, and how often students compare themselves to their peers.This study contributes to our understanding of the motivational implications of academic data visualizations in academic settings, and increases our knowledge of the way students interpret visualizations. It uncovers tensions between what students want to see, versus what it might be more motivationally appropriate for them to see. Importantly, the MISQ and graph comprehension measure can be used in future studies to better understand the role of students' information seeking tendencies with regard to their interpretation of various kinds of feedback present in LADs.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {280–289},
numpages = {10},
keywords = {Visualizations, Non-cognitive factors, Motivation, Instrument Validation, Higher Education},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3303772.3303804,
author = {Lim, Lisa and Dawson, Shane and Joksimovic, Srecko and Ga\v{s}evi\'{c}, Dragan},
title = {Exploring students' sensemaking of learning analytics dashboards: Does frame of reference make a difference?},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303804},
doi = {10.1145/3303772.3303804},
abstract = {Learning Analytics Dashboards (LAD) are becoming an increasingly popular way to provide students with personalised feedback. Despite the number of LADs being developed, significant research gaps exist around the student perspective, especially how students make sense of graphics provided in LADs, and how they intend to act on the feedback provided therein. This study employed a randomized-controlled trial to examine students' sense-making of LADs showing four different frames of reference, and to what extent the impact of LADs was mediated by baseline self-regulation. Using a mix of quantitative and qualitative data analysis, the results revealed rather distinct patterns in students' sense-making across the four LADs. These patterns involved the intersection of visual salience and planned learning actions. However, collectively, across all four LADs a consistent theme emerged around students planned learning actions. This theme was classified as time and study environment management. A key finding of the study is that the use of LADs as a primary feedback process should be personalized and include training and support to aid student sensemaking.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {250–259},
numpages = {10},
keywords = {social comparison, sensemaking, epistemic network analysis, Learning dashboards},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3303772.3303803,
author = {Syed, Munira and Anggara, Trunojoyo and Lanski, Alison and Duan, Xiaojing and Ambrose, G. Alex and Chawla, Nitesh V.},
title = {Integrated Closed-loop Learning Analytics Scheme in a First Year Experience Course},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303803},
doi = {10.1145/3303772.3303803},
abstract = {Identifying non-thriving students and intervening to boost them are two processes that recent literature suggests should be more tightly integrated. We perform this integration over six semesters in a First Year Experience (FYE) course with the aim of boosting student success, by using an integrated closed-loop learning analytics scheme that consists of multiple steps broken into three main phases, as follows: Architecting for Collection (steps: design, build, capture), Analyzing for Action (steps: identify, notify, boost), and Assessing for Improvement (steps: evaluate, report). We close the loop by allowing later steps to inform earlier ones in real-time during a semester and iteratively year to year, thereby improving the course from data-driven insights. This process depends on the purposeful design of an integrated learning environment that facilitates data collection, storage, and analysis. Methods for evaluating the effectiveness of our analytics-based student interventions show that our criterion for identifying non-thriving students was satisfactory and that non-thriving students demonstrated more substantial changes from mid-term to final course grades than already-thriving students. Lastly, we make a case for using early performance in the FYE as an indicator of overall performance and retention of first-year students.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {521–530},
numpages = {10},
keywords = {learning analytics, intervention, first year seminars, first year experience, at-risk students, advising},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3303772.3303802,
author = {Hu, Qian and Rangwala, Huzefa},
title = {Reliable Deep Grade Prediction with Uncertainty Estimation},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303802},
doi = {10.1145/3303772.3303802},
abstract = {Currently, college-going students are taking longer to graduate than their parental generations. Further, in the United States, the six-year graduation rate has been 59% for decades. Improving the educational quality by training better-prepared students who can successfully graduate in a timely manner is critical. Accurately predicting students' grades in future courses has attracted much attention as it can help identify at-risk students early so that personalized feedback can be provided to them on time by advisors. Prior research on students' grade prediction include shallow linear models; however, students' learning is a highly complex process that involves the accumulation of knowledge across a sequence of courses that can not be sufficiently modeled by these linear models. In addition to that, prior approaches focus on prediction accuracy without considering prediction uncertainty, which is essential for advising and decision making. In this work, we present two types of Bayesian deep learning models for grade prediction under a course-specific framework: i)Multilayer Perceptron (MLP) and ii) Recurrent Neural Network (RNN). These course-specific models are based on the assumption that prior courses can provide students with knowledge for future courses so that grades of prior courses can be used to predict grades in a future course. The MLP ignores the temporal dynamics of students' knowledge evolution. Hence, we propose RNN for students' performance prediction. To evaluate the performance of the proposed models, we performed extensive experiments on data collected from a large public university. The experimental results show that the proposed models achieve better performance than prior state-of-the-art approaches. Besides more accurate results, Bayesian deep learning models estimate uncertainty associated with the predictions. We explore how uncertainty estimation can be applied towards developing a reliable educational early warning system. In addition to uncertainty, we also develop an approach to explain the prediction results, which is useful for advisors to provide personalized feedback to students.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {76–85},
numpages = {10},
keywords = {Uncertainty, Sequential Models, Grade Prediction, Educational Data Mining, Bayesian Deep Learning},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3303772.3303801,
author = {Stone, Cathlyn and Quirk, Abigail and Gardener, Margo and Hutt, Stephen and Duckworth, Angela L. and D'Mello, Sidney K.},
title = {Language as Thought: Using Natural Language Processing to Model Noncognitive Traits that Predict College Success},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303801},
doi = {10.1145/3303772.3303801},
abstract = {It is widely acknowledged that the language we use reflects numerous psychological constructs, including our thoughts, feelings, and desires. Can the so called "noncognitive" traits with known links to success, such as growth mindset, leadership ability, and intrinsic motivation, be similarly revealed through language? We investigated this question by analyzing students' 150-word open-ended descriptions of their own extracurricular activities or work experiences included in their college applications. We used the Common Application-National Student Clearinghouse data set, a six-year longitudinal dataset that includes college application data and graduation outcomes for 278,201 U.S. high-school students. We first developed a coding scheme from a stratified sample of 4,000 essays and used it to code seven traits: growth mindset, perseverance, goal orientation, leadership, psychological connection (intrinsic motivation), self-transcendent (prosocial) purpose, and team orientation, along with earned accolades. Then, we used standard classifiers with bag-of-n-grams as features and deep learning techniques (recurrent neural networks) with word embeddings to automate the coding. The models demonstrated convergent validity with the human coding with AUCs ranging from .770 to .925 and correlations ranging from .418 to .734. There was also evidence of discriminant validity in the pattern of inter-correlations (rs between -.206 to .306) for both human- and model-coded traits. Finally, the models demonstrated incremental predictive validity in predicting six-year graduation outcomes net of sociodemographics, intelligence, academic achievement, and institutional graduation rates. We conclude that language provides a lens into noncognitive traits important for college success, which can be captured with automated methods.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {320–329},
numpages = {10},
keywords = {n-grams, Noncognitive traits, Neural Networks, Natural Language Processing, Deep learning, Common App, College Success},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3303772.3303800,
author = {Manrique, Rub\'{e}n and Nunes, Bernardo Pereira and Marino, Olga and Casanova, Marco Antonio and Nurmikko-Fuller, Terhi},
title = {An Analysis of Student Representation, Representative Features and Classification Algorithms to Predict Degree Dropout},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303800},
doi = {10.1145/3303772.3303800},
abstract = {Identifying and monitoring students who are likely to dropout is a vital issue for universities. Early detection allows institutions to intervene, addressing problems and retaining students. Prior research into the early detection of at-risk students has opted for the use of predictive models, but a comprehensive assessment of the suitability of different algorithms and approaches is complicated by the large number of variable features that constitute a student's educational experience. Predictive models vary in terms of their amplitude, temporality and the learning algorithms employed. While amplitude refers to the ability of the model to operate on multiple degrees, temporality is often considered due to the natural temporal aspect of the data. In the absence of a comparative framework of learning algorithms, the aim of this paper has been to provide such an analysis, based on a proposed classification of strategies for predicting dropouts in Higher Education Institutions. Three different student representations are implemented (namely Global Feature-Based, Local Feature-Based, and Time Series) in conjunction with the appropriate learning algorithms for each of them. A description of each approach, as well as its implementation process, are presented in this paper as technical contributions. An experiment based on a dataset of student information from two degrees, namely Business Administration and Architecture, acquired through an automated management system from a university in Brazil is used. Our findings can be summarized as: (i) of the three proposed student representations, the Local Feature-Based was the most suitable approach for predicting dropout. In addition to providing high quality results, the Local Feature-Based representations are simple to build, and the construction of the model is less expensive when compared to more complex ones; (ii) as a conclusion of the results obtained via Local Feature-Based, dropout can be said to be accurately predicted using grades of a few core courses, so there is no need for a complex features extraction process; (iii) considering temporal aspects of the data does not seem to contribute to the prediction performance although it increases computational costs as the model complexity increases.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {401–410},
numpages = {10},
keywords = {Temporal Analysis, Student Representation, Features Extraction, Dropout Prediction, Degree Dropout Analysis},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3303772.3303799,
author = {Elbadrawy, Asmaa and Karypis, George},
title = {UPM: Discovering Course Enrollment Sequences Associated with Success},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303799},
doi = {10.1145/3303772.3303799},
abstract = {Identifying enrollment patterns associated with course success can help educators design better degree plans, and students make informed decisions about future enrollments. While discriminating pattern mining techniques can be used to address this problem, course enrollment patterns include sequence and quantity (grades) information. None of the existing methods were designed to account for both factors. In this work we present UPM, a Universal discriminating Pattern Mining framework that simultaneously mines various types of enrollment patterns while accounting for sequence and quantity using an expansion-specific approach. Unlike the existing methods, UPM expands a given pattern with an item by finding a minimum-entropy split over the item's quantities. We then use UPM to extract discriminating enrollment patterns from the high and the low performing student groups. These patterns can be utilized by educators for degree planning. To evaluate the quality of the extracted patterns, we adopt a supervised classification approach where we apply various classification techniques to label students according tho their performance based on the extracted patterns. Our evaluation shows that the classification accuracies obtained using the UPM extracted patterns are higher than the accuracies obtained using patterns extracted by other techniques. Accuracy improves significantly for students with larger numbers of patterns. Moreover, expansion-specific quantitative mining leads to more accurate classifications than the methods that do not account for quantities (grades).},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {373–382},
numpages = {10},
keywords = {Student Enrollment Sequences, Discriminative Sequence Analysis},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3303772.3303798,
author = {Chen, Bodong and Zhu, Haiyi},
title = {Towards Value-Sensitive Learning Analytics Design},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303798},
doi = {10.1145/3303772.3303798},
abstract = {To support ethical considerations and system integrity in learning analytics, this paper introduces two cases of applying the Value Sensitive Design methodology to learning analytics design. The first study applied two methods of Value Sensitive Design, namely stakeholder analysis and value analysis, to a conceptual investigation of an existing learning analytics tool. This investigation uncovered a number of values and value tensions, leading to design trade-offs to be considered in future tool refinements. The second study holistically applied Value Sensitive Design to the design of a recommendation system for the Wikipedia WikiProjects. To proactively consider values among stakeholders, we derived a multi-stage design process that included literature analysis, empirical investigations, prototype development, community engagement, iterative testing and refinement, and continuous evaluation. By reporting on these two cases, this paper responds to a need of practical means to support ethical considerations and human values in learning analytics systems. These two cases demonstrate that Value Sensitive Design could be a viable approach for balancing a wide range of human values, which tend to encompass and surpass ethical issues, in learning analytics design.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {343–352},
numpages = {10},
keywords = {values, value sensitive design, social media, learning analytics, data ethics},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3303772.3303797,
author = {Harpstead, Erik and Richey, J. Elizabeth and Nguyen, Huy and McLaren, Bruce M.},
title = {Exploring the Subtleties of Agency and Indirect Control in Digital Learning Games},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303797},
doi = {10.1145/3303772.3303797},
abstract = {How do the features of a learning environment's user interface impact learners' agency and, further, their learning? We explored this question in the context of Decimal Point, a digital learning game designed to support middle school students in learning decimals. Previous studies of the game showed that giving students the ability to choose the order and number of mini-games to play did not significantly impact their learning outcomes compared to a condition without choice. In this paper we explore whether some elements of the game's interface may have inadvertently exerted indirect control over students' choice, leading to the previous effects. We conducted a classroom study using a new version of the game that varied whether students saw a visual path connecting mini-games on the game map to modulate the level of indirect control students would experience with an implied ordering. Ultimately, we found that students in the no-line condition exercised significantly more agency but did not learn any less than the line condition. These results suggest that indirect control can be a subtle but powerful way to direct student attention in digital learning games.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {121–129},
numpages = {9},
keywords = {self-regulated learning, indirect control, agency, Digital learning games},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3303772.3303796,
author = {Slade, Sharon and Prinsloo, Paul and Khalil, Mohammad},
title = {Learning analytics at the intersections of student trust, disclosure and benefit},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303796},
doi = {10.1145/3303772.3303796},
abstract = {Evidence suggests that individuals are often willing to exchange personal data for (real or perceived) benefits. Such an exchange may be impacted by their trust in a particular context and their (real or perceived) control over their data.Students remain concerned about the scope and detail of surveillance of their learning behavior, their privacy, their control over what data are collected, the purpose of the collection, and the implications of any analysis. Questions arise as to the extent to which students are aware of the benefits and risks inherent in the exchange of their data, and whether they are willing to exchange personal data for more effective and supported learning experiences.This study reports on the views of entry level students at the Open University (OU) in 2018. The primary aim is to explore differences between stated attitudes to privacy and their online behaviors, and whether these same attitudes extend to their university's uses of their (personal) data. The analysis indicates, inter alia, that there is no obvious relationship between how often students are online or their awareness of/concerns about privacy issues in online contexts and what they actually do to protect themselves. Significantly though, the findings indicate that students overwhelmingly have an inherent trust in their university to use their data appropriately and ethically.Based on the findings, we outline a number of issues for consideration by higher education institutions, such as the need for transparency (of purpose and scope), the provision of some element of student control, and an acknowledgment of the exchange value of information in the nexus of the privacy calculus.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {235–244},
numpages = {10},
keywords = {surveillance, privacy, informed consent, boundary management, Learning analytics},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3303772.3303795,
author = {Ding, Mucong and Yang, Kai and Yeung, Dit-Yan and Pong, Ting-Chuen},
title = {Effective Feature Learning with Unsupervised Learning for Improving the Predictive Models in Massive Open Online Courses},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303795},
doi = {10.1145/3303772.3303795},
abstract = {The effectiveness of learning in massive open online courses (MOOCs) can be significantly enhanced by introducing personalized intervention schemes which rely on building predictive models of student learning behaviors such as some engagement or performance indicators. A major challenge that has to be addressed when building such models is to design handcrafted features that are effective for the prediction task at hand. In this paper, we make the first attempt to solve the feature learning problem by taking the unsupervised learning approach to learn a compact representation of the raw features with a large degree of redundancy. Specifically, in order to capture the underlying learning patterns in the content domain and the temporal nature of the clickstream data, we train a modified auto-encoder (AE) combined with the long short-term memory (LSTM) network to obtain a fixed-length embedding for each input sequence. When compared with the original features, the new features that correspond to the embedding obtained by the modified LSTM-AE are not only more parsimonious but also more discriminative for our prediction task. Using simple supervised learning models, the learned features can improve the prediction accuracy by up to 17% compared with the supervised neural networks and reduce overfitting to the dominant low-performing group of students, specifically in the task of predicting students' performance. Our approach is generic in the sense that it is not restricted to a specific supervised learning model nor a specific prediction task for MOOC learning analytics.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {135–144},
numpages = {10},
keywords = {Unsupervised Learning, Long Short-Term Memory, Learning Behavior, Feature Learning, Dimensionality Reduction, Autoencoder},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3303772.3303794,
author = {Ding, Mucong and Wang, Yanbang and Hemberg, Erik and O'Reilly, Una-May},
title = {Transfer Learning using Representation Learning in Massive Open Online Courses},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303794},
doi = {10.1145/3303772.3303794},
abstract = {In a Massive Open Online Course (MOOC), predictive models of student behavior can support multiple aspects of learning, including instructor feedback and timely intervention. Ongoing courses, when the student outcomes are yet unknown, must rely on models trained from the historical data of previously offered courses. It is possible to transfer models, but they often have poor prediction performance. One reason is features that inadequately represent predictive attributes common to both courses. We present an automated transductive transfer learning approach that addresses this issue. It relies on problem-agnostic, temporal organization of the MOOC clickstream data, where, for each student, for multiple courses, a set of specific MOOC event types is expressed for each time unit. It consists of two alternative transfer methods based on representation learning with auto-encoders: a passive approach using transductive principal component analysis and an active approach that uses a correlation alignment loss term. With these methods, we investigate the transferability of dropout prediction across similar and dissimilar MOOCs and compare with known methods. Results show improved model transferability and suggest that the methods are capable of automatically learning a feature representation that expresses common predictive characteristics of MOOCs.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {145–154},
numpages = {10},
keywords = {Transfer Learning, Representation Learning, MOOC, Dropout Prediction, Dimensionality Reduction, Autoencoder},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3303772.3303793,
author = {de Quincey, Ed and Briggs, Chris and Kyriacou, Theocharis and Waller, Richard},
title = {Student Centred Design of a Learning Analytics System},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303793},
doi = {10.1145/3303772.3303793},
abstract = {Current Learning Analytics (LA) systems are primarily designed with University staff members as the target audience; very few are aimed at students, with almost none being developed with direct student involvement and undertaking a comprehensive evaluation. This paper describes a HEFCE funded project that has employed a variety of methods to engage students in the design, development and evaluation of a student facing LA dashboard. LA was integrated into the delivery of 4 undergraduate modules with 169 student sign-ups. The design of the dashboard uses a novel approach of trying to understand the reasons why students want to study at university and maps their engagement and predicted outcomes to these motivations, with weekly personalised notifications and feedback. Students are also given the choice of how to visualise the data either via a chart-based view or to be represented as themselves. A mixed-methods evaluation has shown that students' feelings of dependability and trust of the underlying analytics and data is variable. However, students were mostly positive about the usability and interface design of the system and almost all students once signed-up did interact with their LA. The majority of students could see how the LA system could support their learning and said that it would influence their behaviour. In some cases, this has had a direct impact on their levels of engagement. The main contribution of this paper is the transparent documentation of a User Centred Design approach that has produced forms of LA representation, recommendation and interaction design that go beyond those used in current similar systems and have been shown to motivate students and impact their learning behaviour.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {353–362},
numpages = {10},
keywords = {Visualisation, User Experience, User Centred Design, Usability, Learning Analytics, Laddering},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3303772.3303792,
author = {Heo, Joon and Lim, Hyoungjoon and Yun, Sung Bum and Ju, Sungha and Park, Sangyoon and Lee, Rebekah},
title = {Descriptive and Predictive Modeling of Student Achievement, Satisfaction, and Mental Health for Data-Driven Smart Connected Campus Life Service},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303792},
doi = {10.1145/3303772.3303792},
abstract = {Yonsei University in Korea launched an educational innovation project entitled "Data-Driven Smart-Connected Campus Life Service", for which student-related data have been accumulated at university level since spring of 2015, and descriptive, predictive and prescriptive modeling have been conducted to offer innovative education service to students. The dataset covers not only conventional student information, student questionnaire survey, and university administrative data, but also unconventional data sets such as student location data and learning management system (LMS) log data. Based on the datasets, with respect to 4,000+ freshman students at residential college, we conducted preliminary implementation of descriptive and predictive modeling for student achievement, satisfaction, and mental health. The results were overall promising. First, descriptive and predictive modeling of GPA for student achievement presented a list of significant predictive variables from student locations and LMS activities. Second, descriptive modeling of student satisfaction revealed influential variables such as "improvement of creativity" and "ability of cooperation". Third, similar descriptive modeling was applied to students' mental health changes by semesters, and the study uncovered influential factors such as "difficulty with relationship" and "time spent with friends increased' as key determinants of student mental health. Although the educational innovation project is still in its early stages, we have three strategies of the future modelling efforts: They are: (1) step-by-step improvement from descriptive, predictive, to prescriptive modelling; (2) full use of recurring data acquisition; (3) different level of segmentation.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {531–538},
numpages = {8},
keywords = {Satisfaction, Predictive Modeling, Mental Health, Higher Education, Descriptive Modeling, Academic Achievement},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3303772.3303791,
author = {Gardner, Josh and Brooks, Christopher and Baker, Ryan},
title = {Evaluating the Fairness of Predictive Student Models Through Slicing Analysis},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303791},
doi = {10.1145/3303772.3303791},
abstract = {Predictive modeling has been a core area of learning analytics research over the past decade, with such models currently deployed in a variety of educational contexts from MOOCs to K-12. However, analyses of the differential effectiveness of these models across demographic, identity, or other groups has been scarce. In this paper, we present a method for evaluating unfairness in predictive student models. We define this in terms of differential accuracy between subgroups, and measure it using a new metric we term the Absolute Between-ROC Area (ABROCA). We demonstrate the proposed method through a gender-based "slicing analysis" using five different models replicated from other works and a dataset of 44 unique MOOCs and over four million learners. Our results demonstrate (1) significant differences in model fairness according to (a) statistical algorithm and (b) feature set used; (2) that the gender imbalance ratio, curricular area, and specific course used for a model all display significant association with the value of the ABROCA statistic; and (3) that there is not evidence of a strict tradeoff between performance and fairness. This work provides a framework for quantifying and understanding how predictive models might inadvertently privilege, or disparately impact, different student subgroups. Furthermore, our results suggest that learning analytics researchers and practitioners can use slicing analysis to improve model fairness without necessarily sacrificing performance.1},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {225–234},
numpages = {10},
keywords = {machine learning, MOOCs, Fairness},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3303772.3303790,
author = {Sher, Varshita and Hatala, Marek and Ga\v{s}evi\'{c}, Dragan},
title = {On multi-device use: Using technological modality profiles to explain differences in students' learning},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303790},
doi = {10.1145/3303772.3303790},
abstract = {With increasing abundance and ubiquity of mobile phones, desktop PCs, and tablets in the last decade, we are seeing students intermixing these modalities to learn and regulate their learning. However, the role of these modalities in educational settings is still largely under-researched. Similarly, little attention has been paid to the research on the extension of learning analytics to analyze the learning processes of students adopting various modalities during a learning activity. Traditionally, research on how modalities affect the way in which activities are completed has mainly relied upon self-reported data or mere counts of access from each modality. We explore the use of technological modalities in regulating learning via learning management systems (LMS) in the context of blended courses. We used data mining techniques to analyze patterns in sequences of actions performed by learners (n = 120) across different modalities in order to identify technological modality profiles of sequences. These profiles were used to detect the technological modality strategies adopted by students. We found a moderate effect size (∈2 = 0.12) of students' adopted strategies on the final course grade. Furthermore, when looking specifically at online discussion engagement and performance, students' adopted technological modality strategies explained a large amount of variance (η2 = 0.68) in their engagement and quality of contributions. The result implications and further research are discussed.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {1–10},
numpages = {10},
keywords = {Trace Analysis, Online discussions, Multi-device use, Mobile Learning, Learning analytics, Blended learning},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3303772.3303789,
author = {Motz, Benjamin and Quick, Joshua and Schroeder, Noah and Zook, Jordon and Gunkel, Matthew},
title = {The validity and utility of activity logs as a measure of student engagement},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303789},
doi = {10.1145/3303772.3303789},
abstract = {Learning management system (LMS) web logs provide granular, near-real-time records of student behavior as learners interact with online course materials in digital learning environments. However, it remains unclear whether LMS activity indeed reflects behavioral properties of student engagement, and it also remains unclear how to deal with variability in LMS usage across a diversity of courses. In this study, we evaluate whether instructors' subjective ratings of their students' engagement are related to features of LMS activity for 9,021 students enrolled in 473 for-credit courses. We find that estimators derived from LMS web logs are closely related to instructor ratings of engagement, however, we also observe that there is not a single generic relationship between activity and engagement, and what constitutes the behavioral components of "engagement" will be contingent on course structure. However, for many of these courses, modeled engagement scores are comparable to instructors' ratings in their sensitivity for predicting academic performance. As long as they are tuned to the differences between courses, activity indices from LMS web logs can provide a valid and useful proxy measure of student engagement.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {300–309},
numpages = {10},
keywords = {web logs, trace data, student engagement, LMS},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3303772.3303788,
author = {Niaki, Sahba Akhavan and George, Clint P. and Michailidis, George and Beal, Carole R.},
title = {Investigating the Usage Patterns of Algebra Nation Tutoring Platform},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303788},
doi = {10.1145/3303772.3303788},
abstract = {We study the usage of a self-guided online tutoring platform called Algebra Nation, which is widely by middle school and high school students who take the End-of-Course Algebra I exam at the end of the school year. This article aims to study how the platform contributes to increasing students' exam scores by examining users' logs over a three year period. The platform under consideration was used by more than 36,000 students in the first year, to nearly 67,000 by the third year, thus enabling us to examine how usage patterns evolved and influenced students' performance at scale. We first identify which Algebra Nation usage factors in conjunction with math overall preparation and socioeconomic factors contribute to the students' exam performance. Subsequently, we investigate the effect of increased teacher familiarity level with the Algebra Nation on students' scores across different grades through mediation analysis. The results show that the indirect effect of teacher's familiarity with the platform through increasing student's usage dosage is more significant in higher grades.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {481–490},
numpages = {10},
keywords = {Online tutoring platform, Mediation analysis, Math education, Hierarchical linear models},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3303772.3303787,
author = {Matcha, Wannisa and Ga\v{s}evi\'{c}, Dragan and Uzir, Nora'Ayu Ahmad and Jovanovi\'{c}, Jelena and Pardo, Abelardo},
title = {Analytics of Learning Strategies: Associations with Academic Performance and Feedback},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303787},
doi = {10.1145/3303772.3303787},
abstract = {Learning analytics has the potential to detect and explain characteristics of learning strategies through analysis of trace data and communicate the findings via feedback. However, the role of learning analytics-based feedback in selection and regulation of learning strategies is still insufficiently explored and understood. This research aims to examine the sequential and temporal characteristics of learning strategies and investigate their association with feedback. Three years of trace data were collected from online pre-class activities of a flipped classroom, where different types of feedback were employed in each year. Clustering, sequence mining, and process mining were used to detect and interpret learning tactics and strategies. Inferential statistics were used to examine the association of feedback with the learning performance and the detected learning strategies. The results suggest a positive association between the personalised feedback and the effective strategies.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {461–470},
numpages = {10},
keywords = {Self-regulated Learning, Learning Tactics, Learning Strategies, Learning Analytics, Feedback, Data Mining},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3303772.3303786,
author = {Lee, Jinseok and Yeung, Dit-Yan},
title = {Knowledge Query Network for Knowledge Tracing: How Knowledge Interacts with Skills},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303786},
doi = {10.1145/3303772.3303786},
abstract = {Knowledge Tracing (KT) is to trace the knowledge of students as they solve a sequence of problems represented by their related skills. This involves abstract concepts of students' states of knowledge and the interactions between those states and skills. Therefore, a KT model is designed to predict whether students will give correct answers and to describe such abstract concepts. However, existing methods either give relatively low prediction accuracy or fail to explain those concepts intuitively. In this paper, we propose a new model called Knowledge Query Network (KQN) to solve these problems. KQN uses neural networks to encode student learning activities into knowledge state and skill vectors, and models the interactions between the two types of vectors with the dot product. Through this, we introduce a novel concept called probabilistic skill similarity that relates the pairwise cosine and Euclidean distances between skill vectors to the odds ratios of the corresponding skills, which makes KQN interpretable and intuitive.On four public datasets, we have carried out experiments to show the following: 1. KQN outperforms all the existing KT models based on prediction accuracy. 2. The interaction between the knowledge state and skills can be visualized for interpretation. 3. Based on probabilistic skill similarity, a skill domain can be analyzed with clustering using the distances between the skill vectors of KQN. 4. For different values of the vector space dimensionality, KQN consistently exhibits high prediction accuracy and a strong positive correlation between the distance matrices of the skill vectors.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {491–500},
numpages = {10},
keywords = {Massive Open Online Courses, Learning Analytics, Learner Modeling, Knowledge Tracing, Knowledge Modeling, Intelligent Tutoring Systems, Educational Data Mining, Domain Modeling, Deep Learning},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3303772.3303785,
author = {Shibani, Antonette and Knight, Simon and Buckingham Shum, Simon},
title = {Contextualizable Learning Analytics Design: A Generic Model and Writing Analytics Evaluations},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303785},
doi = {10.1145/3303772.3303785},
abstract = {A major promise of learning analytics is that through the collection of large amounts of data we can derive insights from authentic learning environments, and impact many learners at scale. However, the context in which the learning occurs is important for educational innovations to impact student learning. In particular, for student-facing learning analytics systems like feedback tools to work effectively, they have to be integrated with pedagogical approaches and the learning design. This paper proposes a conceptual model to strike a balance between the concepts of generalizable scalable support and contextualized specific support by clarifying key elements that help to contextualize student-facing learning analytics tools. We demonstrate an implementation of the model using a writing analytics example, where the features, feedback and learning activities around the automated writing feedback tool are tuned for the pedagogical context and the assessment regime in hand, by co-designing them with the subject experts. The model can be employed for learning analytics to move from generalized support to meaningful contextualized support for enhancing learning.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {210–219},
numpages = {10},
keywords = {writing analytics, learning design, contextualizable learning analytics, conceptual model, CLAD},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3303772.3303784,
author = {Dawson, Shane and Joksimovic, Srecko and Poquet, Oleksandra and Siemens, George},
title = {Increasing the Impact of Learning Analytics},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303784},
doi = {10.1145/3303772.3303784},
abstract = {Learning Analytics (LA) studies the learning process in order to optimize learning opportunities for students. Although LA has quickly risen to prominence, there remain questions regarding the impact LA has made to date. To evaluate the extent that LA has impacted our understanding of learning and produced insights that have been translated to mainstream practice or contributed to theory, we reviewed the research published in 2011-2018 LAK conferences and Journal of Learning Analytics. The reviewed studies were coded according to five dimensions: study focus, data types, purpose, institutional setting, and scale of research and implementation. The coding and subsequent epistemic network analysis indicates that while LA research has developed in the areas of focus and sophistication of analyses, the impact on practice, theory and frameworks have been limited. We hypothesize that this finding is due to a continuing predominance of small-scale techno-centric exploratory studies that to date have not fully accounted for the multi-disciplinarity that comprises education. For the field to reach its potential in understanding and optimizing learning and learning environments, there must be a purposeful shift to move from exploratory models to more holistic and integrative systems-level research. This necessitates greater effort applied to understanding the research cycles that emerge when multiple knowledge domains coalesce into new fields of research.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {446–455},
numpages = {10},
keywords = {epistemic network analysis, adoption, Learning Analytics},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3303772.3303783,
author = {Morsy, Sara and Karypis, George},
title = {A Study on Curriculum Planning and Its Relationship with Graduation GPA and Time To Degree},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303783},
doi = {10.1145/3303772.3303783},
abstract = {In recent years, several data-driven methods have been developed to help undergraduate students during course selection and sequencing. These methods tend to utilize the whole set of past course registration data, regardless of the past students' graduation GPA and time to degree (TTD). Though some previous work has shown through the results of their developed models that students of different GPA tend to take courses in different sequence, the actual analysis of the degree plans and how/if they relate to the students' graduation GPA and time-to-degree has not received much attention. This study analyzes how the student's academic level when they take different courses, as well as the pairwise degree similarity between pairs of students relate to the students' graduation GPA and TTD. Our study uses a large-scale dataset that contains 25 majors from different colleges at the University of Minnesota and spans 16 years. The analysis shows that TTD is highly correlated with both the timing and ordering of courses that students follow in their degree plans, while the correlation between graduation GPA and the course timing and ordering is not as high. We also perform a case study that uses course timing and ordering features to predict whether the student at each semester will graduate on-time or overtime. The results show that careful curriculum planning is needed to improve graduation rates in universities.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {26–35},
numpages = {10},
keywords = {undergraduate education, time to degree prediction, time to degree, degree similarity, degree planning, curriculum planning, course timing, course sequencing, academic performance, GPA},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3303772.3303782,
author = {Jovanovi\'{c}, Jelena and Ga\v{s}evi\'{c}, Dragan and Pardo, Abelardo and Dawson, Shane and Whitelock-Wainwright, Alexander},
title = {Introducing meaning to clicks: Towards traced-measures of self-efficacy and cognitive load},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303782},
doi = {10.1145/3303772.3303782},
abstract = {The use of learning trace data together with various analytical methods has proven successful in detecting patterns in learning behaviour, identifying student profiles, and clustering learning resources. However, interpretation of the findings is often difficult and uncertain due to a lack of contextual data (e.g., data on student motivation, emotion or curriculum design). In this study we explored the integration of student self-reports about cognitive load and self-efficacy into the learning process and collection of relevant students' perceptions as learning traces. Our objective was to examine the association of traced measures of relevant learning constructs (cognitive load and self-efficacy) with i) indicators of the students' learning behaviour derived from trace data, and ii) the students' academic performance. The results indicated the presence of association between some indicators of students' engagement with learning activities and traced measures of cognitive load and self-efficacy. Correlational analysis demonstrated significant positive correlation between the students' course performance and traced measures of cognitive load and self-efficacy.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {511–520},
numpages = {10},
keywords = {trace data, self-reports, self-efficacy, perceived difficulty, learning analytics, cognitive load},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3303772.3303781,
author = {Wang, Yanbang and Law, Nancy and Hemberg, Erik and O'Reilly, Una-May},
title = {Using Detailed Access Trajectories for Learning Behavior Analysis},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303781},
doi = {10.1145/3303772.3303781},
abstract = {Student learning activity in MOOCs can be viewed from multiple perspectives. We present a new organization of MOOC learner activity data at a resolution that is in between the fine granularity of the clickstream and coarse organizations that count activities, aggregate students or use long duration time units. A detailed access trajectory (DAT) consists of binary values and is two dimensional with one axis that is a time series, and the other that is a chronologically ordered list of a MOOC component type's instances, videos in instructional order, for example. Most popular MOOC platforms generate data that can be organized as detailed access trajectories (DATs). We explore the value of DATs by conducting four empirical mini-studies. Our studies suggest DATs contain rich information about students' learning behaviors and facilitate MOOC learning analyses.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {290–299},
numpages = {10},
keywords = {representation learning, marginalized learner, learning pattern, Massive Open Online Course, Detailed Access Trajectory},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3303772.3303780,
author = {Molenaar, Inge and Horvers, Anne and Baker, Ryan S.},
title = {Towards Hybrid Human-System Regulation: Understanding Children' SRL Support Needs in Blended Classrooms},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303780},
doi = {10.1145/3303772.3303780},
abstract = {This paper proposes a new approach to translate learner data into self-regulated learning support. Learning phases in blended classrooms place unique requirements on students' self-regulated learning (SRL). Learning path graphs merge moment-by-moment learning curves and learning phase data to understand student' SRL support needs. Results indicate 4 groups with different SRL support needs. Students in the self-regulated learning group are capable of learning without external regulation. In the teacher regulation group students need initial teacher regulation but rely on SRL thereafter. Students in the system regulation group require teacher and system regulation to learn. Finally, the advanced system support group is in need of support beyond the current level of system regulation. Based on these insights, the application of personalized dashboards and hybrid human-system regulation is further specified.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {471–480},
numpages = {10},
keywords = {Self-Regulated Learning, Hybrid Human-System Intelligence, Blended Classrooms, Adaptive Learning Technologies},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3303772.3303779,
author = {Farrow, Elaine and Moore, Johanna and Ga\v{s}evi\'{c}, Dragan},
title = {Analysing discussion forum data: a replication study avoiding data contamination},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303779},
doi = {10.1145/3303772.3303779},
abstract = {The widespread use of online discussion forums in educational settings provides a rich source of data for researchers interested in how collaboration and interaction can foster effective learning. Such online behaviour can be understood through the Community of Inquiry framework, and the cognitive presence construct in particular can be used to characterise the depth of a student's critical engagement with course material. Automated methods have been developed to support this task, but many studies used small data sets, and there have been few replication studies.In this work, we present findings related to the robustness and generalisability of automated classification methods for detecting cognitive presence in discussion forum transcripts. We closely examined one published state-of-the-art model, comparing different approaches to managing unbalanced classes in the data. By demonstrating how commonly-used data preprocessing practices can lead to over-optimistic results, we contribute to the development of the field so that the results of automated content analysis can be used with confidence.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {170–179},
numpages = {10},
keywords = {replication, data contamination, cognitive presence, Community of Inquiry},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3303772.3303778,
author = {Larmuseau, Charlotte and Vanneste, Pieter and Desmet, Piet and Depaepe, Fien},
title = {Multichannel data for understanding cognitive affordances during complex problem solving},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303778},
doi = {10.1145/3303772.3303778},
abstract = {This exploratory study challenges the current practices in cognitive load measurement by using multichannel data to investigate cognitive load affordances during online complex problem solving. Moreover, it is an attempt to investigate how cognitive load is related to strategy use. Accordingly, in the current study a well- and an ill-structured problem were developed in a virtual learning environment. Online support was provided. Participants were 15 students from the teacher training program. This study incorporated subjective measurements of students' cognitive load (i.e., intrinsic, extraneous, germane load and their mental effort) combined with physiological data containing galvanic skin response (GSR) and skin temperature (ST). A first aim was to investigate whether there was a significant difference for the subjective measurements, physiological data and consultation of support between the well-and ill-structured problem. Secondly this study investigated how individual differences of subjective measurements are related to individual differences of physiological data and consultation of support. Results reveal significant differences for intrinsic load, mental effort between a well- and ill-structured problem. Moreover, when investigating individual differences, findings reveal that GSR might be related to mental effort. Additionally, results indicate that cognitive load influences strategy use. Future research with larger sample sizes should verify these findings in order to have more insight into how we can measure cognitive load and how its related to self-directed learning. These insights should allow us to provide adaptive support in virtual learning environments.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {61–70},
numpages = {10},
keywords = {virtual learning environments, strategy use, physiological data, online measurements, complex problem solving, Cognitive load},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3303772.3303777,
author = {Niaki, Sahba Akhavan and George, Clint P. and Michailidis, George and Beal, Carole R.},
title = {The Impact of an Online Tutoring Program for Algebra Readiness on Mathematics Achievements; Results of a Randomized Experiment},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303777},
doi = {10.1145/3303772.3303777},
abstract = {We study the impact of an online tutoring program, AnimalWatch, for algebra readiness on mathematics achievements of grade 6 students. We use the data from a randomized experimental design conducted on 69 teachers and 2025 students in California in the academic years 2011-2012. After a brief description of the experimental design and the system implementation, we analyze the treatment effect of employing AnimalWatch using the popular hierarchical linear models and find a small positive effect. We further use the logged system usage data such as time spent in the system, modules completed, correct/incorrect/no-answers records of students in each login to analyze how system implementation and usage helped different students. Our results provide insights into the limitations in implementing such a study in a real world setting and suggests recommendations for future research.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {363–372},
numpages = {10},
keywords = {Randomized control trial(RCT), Online tutoring platform, Matheducation, Hierarchical linear models},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3303772.3303776,
author = {Di Mitri, Daniele and Schneider, Jan and Klemke, Roland and Specht, Marcus and Drachsler, Hendrik},
title = {Read Between the Lines: An Annotation Tool for Multimodal Data for Learning},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303776},
doi = {10.1145/3303772.3303776},
abstract = {This paper introduces the Visual Inspection Tool (VIT) which supports researchers in the annotation of multimodal data as well as the processing and exploitation for learning purposes. While most of the existing Multimodal Learning Analytics (MMLA) solutions are tailor-made for specific learning tasks and sensors, the VIT addresses the data annotation for different types of learning tasks that can be captured with a customisable set of sensors in a flexible way. The VIT supports MMLA researchers in 1) triangulating multimodal data with video recordings; 2) segmenting the multimodal data into time-intervals and adding annotations to the time-intervals; 3) downloading the annotated dataset and using it for multimodal data analysis. The VIT is a crucial component that was so far missing in the available tools for MMLA research. By filling this gap we also identified an integrated workflow that characterises current MMLA research. We call this workflow the Multimodal Learning Analytics Pipeline, a toolkit for orchestration, the use and application of various MMLA tools.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {51–60},
numpages = {10},
keywords = {sensors, Multimodal data, Learning Analytics, Internet of Things},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3303772.3303775,
author = {Fincham, Ed and Whitelock-Wainwright, Alexander and Kovanovi\'{c}, Vitomir and Joksimovi\'{c}, Sre\'{c}ko and van Staalduinen, Jan-Paul and Ga\v{s}evi\'{c}, Dragan},
title = {Counting Clicks is Not Enough: Validating a Theorized Model of Engagement in Learning Analytics},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303775},
doi = {10.1145/3303772.3303775},
abstract = {Student engagement is often considered an overarching construct in educational research and practice. Though frequently employed in the learning analytics literature, engagement has been subjected to a variety of interpretations and there is little consensus regarding the very definition of the construct. This raises grave concerns with regards to construct validity: namely, do these varied metrics measure the same thing? To address such concerns, this paper proposes, quantifies, and validates a model of engagement which is both grounded in the theoretical literature and described by common metrics drawn from the field of learning analytics. To identify a latent variable structure in our data we used exploratory factor analysis and validated the derived model on a separate sub-sample of our data using confirmatory factor analysis. To analyze the associations between our latent variables and student outcomes, a structural equation model was fitted, and the validity of this model across different course settings was assessed using MIMIC modeling. Across different domains, the broad consistency of our model with the theoretical literature suggest a mechanism that may be used to inform both interventions and course design.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {501–510},
numpages = {10},
keywords = {Structural Equation Modeling, Measurement Invariance, MOOCs, Factor Analysis, Engagement},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3303772.3303774,
author = {Krauss, Christopher and Merceron, Agathe and Arbanowski, Stefan},
title = {The Timeliness Deviation: A novel Approach to Evaluate Educational Recommender Systems for Closed-Courses},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303774},
doi = {10.1145/3303772.3303774},
abstract = {The decision on what item to learn next in a course can be supported by a recommender system (RS), which aims at making the learning process more efficient and effective. However, learners and learning activities frequently change over time. The question is: how are timely appropriate recommendations of learning resources actually evaluated and how can they be compared?Researchers have found that, in addition to a standardized dataset definition, there is also a lack of standardized definitions of evaluation procedures for RS in the area of Technology Enhanced Learning. This paper argues that, in a closed-course setting, a time-dependent split into the training set and test set is more appropriate than the usual cross-validation to evaluate the Top-N recommended learning resources at various points in time. Moreover, a new measure is introduced to determine the timeliness deviation between the point in time of an item recommendation and the point in time of the actual access by the user. Different recommender algorithms, including two novel ones, are evaluated with the time-dependent evaluation framework and the results, as well as the appropriateness of the framework, are discussed.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {195–204},
numpages = {10},
keywords = {Timeliness Deviation, Time-Dependent EvaluationFramework, EducationalRecommender Systems},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3303772.3303773,
author = {Choi, Heeryung and Dowell, Nia and Brooks, Christopher and Teasley, Stephanie},
title = {Social Comparison in MOOCs: Perceived SES, Opinion, and Message Formality},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303773},
doi = {10.1145/3303772.3303773},
abstract = {There has been limited research on how perceptions of socioeconomic status (SES) and opinion difference could influence peer feedback in Massive Open Online Courses (MOOCs). Using social comparison theory [12], we investigated the influence of ability and opinion-related factors on peer feedback text in a data science MOOC. Perceived SES of peers and the formality of written responses were used as the ability-related factor, while agreement between learners represented the opinion-related factor. We focused on understanding the behaviors of those learners who are most prevalent in MOOCs; those from high socioeconomic countries. Through two studies, we found a strong and repeated influence of agreement on affect and formality in feedback to peers. While a mediation effect of perceived SES was found, a significant effect of formality was not. This work contributes to an understanding of how social comparison theory can be operationalized in online peer writing environments.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {160–169},
numpages = {10},
keywords = {peer feedback, Social comparison, MOOC},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@proceedings{10.1145/3303772,
title = {LAK19: Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Tempe, AZ, USA}
}

@inproceedings{10.1145/3170358.3170422,
author = {Liaqat, Amna and Munteanu, Cosmin},
title = {Towards a writing analytics framework for adult english language learners},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170422},
doi = {10.1145/3170358.3170422},
abstract = {Improving the written literacy of newcomers to English-speaking countries can lead to better education, employment, or social integration opportunities. However, this remains a challenge in traditional classrooms where providing frequent, timely, and personalized feedback is not always possible. Analytics can scaffold the writing development of English Language Learners (ELLs) by providing such feedback. To design these analytics, we conducted a field study analyzing essay samples from immigrant adult ELLs (a group often overlooked in writing analytics research) and identifying their epistemic beliefs and learning motivations. We identified common themes across individual learner differences and patterns of errors in the writing samples. The study revealed strong associations between epistemic writing beliefs and learning strategies. The results are used to develop guidelines for designing writing analytics for adult ELLs, and to propose ideas for analytics that scaffold writing development for this group.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {121–125},
numpages = {5},
keywords = {writing, learning analytics, immigrant, adult learners},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3170358.3170421,
author = {Jivet, Ioana and Scheffel, Maren and Specht, Marcus and Drachsler, Hendrik},
title = {License to evaluate: preparing learning analytics dashboards for educational practice},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170421},
doi = {10.1145/3170358.3170421},
abstract = {Learning analytics can bridge the gap between learning sciences and data analytics, leveraging the expertise of both fields in exploring the vast amount of data generated in online learning environments. A typical learning analytics intervention is the learning dashboard, a visualisation tool built with the purpose of empowering teachers and learners to make informed decisions about the learning process. Related work has investigated learning dashboards, yet none have explored the theoretical foundation that should inform the design and evaluation of such interventions. In this systematic literature review, we analyse the extent to which theories and models from learning sciences have been integrated into the development of learning dashboards aimed at learners. Our analysis revealed that very few dashboard evaluations take into account the educational concepts that were used as a theoretical foundation for their design. Furthermore, we report findings suggesting that comparison with peers, a common reference frame for contextualising information on learning analytics dashboards, was not perceived positively by all learners. We summarise the insights gathered through our literature review in a set of recommendations for the design and evaluation of learning analytics dashboards for learners.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {31–40},
numpages = {10},
keywords = {systematic review, social comparison, learning theory, learning science, learning dashboards, learning analytics, evaluation, competition},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3170358.3170420,
author = {Worsley, Marcelo},
title = {(Dis)engagement matters: identifying efficacious learning practices with multimodal learning analytics},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170420},
doi = {10.1145/3170358.3170420},
abstract = {Video analysis is a staple of the education research community. For many contemporary education researchers, participation in the video coding process serves as a rite of passage. However, recent developments in multimodal learning analytics may help to accelerate and enhance this process by providing researchers with a more nuanced glimpse into a set of learning experiences. As an example of how to use multimodal learning analytics towards these ends, this paper includes a preliminary analysis from 54 college students, who completed two engineering design tasks in pairs. Gesture, speech and electro-dermal activation data were collected as students completed these tasks. The gesture data was used to learn a set of canonical clusters (N=4). A decision tree was trained based on individual students' cluster frequencies, and pre-post learning gains. The nodes in the decision tree were then used to identify a subset of video segments that were human coded based on prior work in learning analytics and engineering design. The combination of machine learning and human inference helps elucidate the practices that seem to correlate with student learning. In particular, both engagement and disengagement seem to correlate with student learning, albeit in a somewhat nuanced fashion.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {365–369},
numpages = {5},
keywords = {qualitative analysis, gesture, engineering design, collaboration},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3170358.3170419,
author = {Broos, Tom and Verbert, Katrien and Langie, Greet and Van Soom, Carolien and De Laet, Tinne},
title = {Multi-institutional positioning test feedback dashboard for aspiring students: lessons learnt from a case study in flanders},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170419},
doi = {10.1145/3170358.3170419},
abstract = {Our work focuses on a multi-institutional implementation and evaluation of a Learning Analytics Dashboards (LAD) at scale, providing feedback to N=337 aspiring STEM (science, technology, engineering and mathematics) students participating in a region-wide positioning test before entering the study program. Study advisors were closely involved in the design and evaluation of the dashboard. The multi-institutional context of our case study requires careful consideration of external stakeholders and data ownership and portability issues, which gives shape to the technical design of the LAD. Our approach confirms students as active agents with data ownership, using an anonymous feedback code to access the LAD and to enable students to share their data with institutions at their discretion. Other distinguishing features of the LAD are the support for active content contribution by study advisors and LATEX type-setting of question item feedback to enhance visual recognizability. We present our lessons learnt from a first iteration in production.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {51–55},
numpages = {5},
keywords = {student dashboard, positioning test, learning analytics, higher education, feedback, case study},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3170358.3170418,
author = {Cooper, Kendra and Khosravi, Hassan},
title = {Graph-based visual topic dependency models: supporting assessment design and delivery at scale},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170418},
doi = {10.1145/3170358.3170418},
abstract = {Educational environments continue to rapidly evolve to address the needs of diverse, growing student populations, while embracing advances in pedagogy and technology. In this changing landscape ensuring the consistency among the assessments for different offerings of a course (within or across terms), providing meaningful feedback about students' achievements, and tracking students' progression over time are all challenging tasks, particularly at scale. Here, a collection of visual Topic Dependency Models (TDMs) is proposed to help address these challenges. It visualises the required topics and their dependencies at a course level (e.g., CS 100) and assessment achievement data at the classroom level (e.g., students in CS 100 Term 1 2016 Section 001) both at one point in time (static) and over time (dynamic). The collection of TDMs share a common, two-weighted graph foundation. An algorithm is presented to create a TDM (static achievement for a cohort). An open-source, proof of concept implementation of the TDMs is under development; the current version is described briefly in terms of its support for visualising existing (historical, test) and synthetic data generated on demand.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {11–15},
numpages = {5},
keywords = {visual analytics, student assessment, graph algorithms},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3170358.3170417,
author = {Millecamp, Martijn and Guti\'{e}rrez, Francisco and Charleer, Sven and Verbert, Katrien and De Laet, Tinne},
title = {A qualitative evaluation of a learning dashboard to support advisor-student dialogues},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170417},
doi = {10.1145/3170358.3170417},
abstract = {This paper presents an evaluation of a learning dashboard that supports the dialogue between a student and a study advisor. The dashboard was designed, developed, and evaluated in collaboration with study advisers. To ensure scalability to other contexts, the dashboard uses data that is commonly available at any higher education institute. It visualizes the grades of the student, an overview of the progress through the year, his/her position in comparison with peers, sliders to plan the next years and a prediction of the length of the bachelor program for this student in years based on historic data. The dashboard was deployed at KU Leuven, Belgium and used in September 2017 to support 224 sessions between students and study advisers. We observed twenty of these conversations. We also collected feedback from 101 students with questionnaires. Results of our observations indicate that the dashboard primarily triggers insights at the beginning of a conversation. The number of insights and the level of these insights (factual, interpretative and reflective) depends on the context of the conversation. Most insights were triggered in conversations with students doubting to continue the program, indicating that our dashboard is useful to support difficult decision-making processes.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {56–60},
numpages = {5},
keywords = {learning technologies, learning analytics dashboards, insights, information visualization},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3170358.3170416,
author = {Poquet, Oleksandra and Dowell, Nia and Brooks, Christopher and Dawson, Shane},
title = {Are MOOC forums changing?},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170416},
doi = {10.1145/3170358.3170416},
abstract = {There has been a growing trend in higher education towards increased use and adoption of Massive Open Online Courses (MOOCs). Despite this interest in learning at scale, limited work has compared MOOC activity across subsequent course offerings. In this study, we explore forum activity in ten iterations of the same MOOC. Our results suggest that participation in MOOC forums has changed over the past four years of delivery. First, overall participation in MOOC forums have decreased. Second, in later iterations cohorts of more committed forum users start to resemble formal online courses in size (67&gt;n&gt;36). However, despite the smaller groups of learners that should find it easier to form connections with one another, our analysis did not reveal the expected increase in the quality of social activity. Instead, MOOC forums evolved into smaller on-task question and answer (Q&amp;A) spaces, not capitalizing on the opportunities for social learning. We discuss practical and research implications of such changes.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {340–349},
numpages = {10},
keywords = {trends, social learning, forum use, MOOCs},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3170358.3170415,
author = {Taylor, Sarah and Munguia, Pablo},
title = {Towards a data archiving solution for learning analytics},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170415},
doi = {10.1145/3170358.3170415},
abstract = {Data solutions in the teaching and learning space are in need of pro-active innovations in data management, to ensure that systems for learning analytics can scale up to match the size of datasets now available. Here, we illustrate the scale at which a Learning Management System (LMS) accumulates data, and discuss the barriers to using this data for in-depth analyses. We illustrate the exponential growth of our LMS data to represent a single example dataset, and highlight the broader need for taking a pro-active approach to dimensional modelling in learning analytics, anticipating that common learning analytics questions will be computationally expensive, and that the most useful data structures for learning analytics will not necessarily follow those of the source dataset.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {260–264},
numpages = {5},
keywords = {learning management systems, learning analytics, dimensional modelling, data retention, big data, barriers to adoption},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3170358.3170414,
author = {Fougt, Simon Skov and Siebert-Evenstone, Amanda and Eagan, Brendan and Tabatabai, Sara and Misfeldt, Morten},
title = {Epistemic network analysis of students' longer written assignments as formative/summative evaluation},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170414},
doi = {10.1145/3170358.3170414},
abstract = {This paper reports on an exploratory trial of developing pedagogical visualizations of 16 students' written assignments on literary analysis using two sets of keywords and Epistemic Network Analysis (ENA). The visualizations are aimed at summative evaluation as a tool for the professor to support assessment and understanding of subject learning. Results show that ENA can visually distinguish low, middle and high performing students, but not statistically significantly. Thus, our trial provides a tool for the professor that supports understanding of subject learning and formative assessment.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {126–130},
numpages = {5},
keywords = {student assignment, pedagogical learning analytics, formative and summative assessment, epistemic network analysis},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3170358.3170413,
author = {Kitto, Kirsty and Buckingham Shum, Simon and Gibson, Andrew},
title = {Embracing imperfection in learning analytics},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170413},
doi = {10.1145/3170358.3170413},
abstract = {Learning Analytics (LA) sits at the confluence of many contributing disciplines, which brings the risk of hidden assumptions inherited from those fields. Here, we consider a hidden assumption derived from computer science, namely, that improving computational accuracy in classification is always a worthy goal. We demonstrate that this assumption is unlikely to hold in some important educational contexts, and argue that embracing computational "imperfection" can improve outcomes for those scenarios. Specifically, we show that learner-facing approaches aimed at "learning how to learn" require more holistic validation strategies. We consider what information must be provided in order to reasonably evaluate algorithmic tools in LA, to facilitate transparency and realistic performance comparisons.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {451–460},
numpages = {10},
keywords = {validation, performance, pedagogy, hidden assumptions, accuracy},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3170358.3170412,
author = {Shimada, Atsushi and Taniguchi, Yuta and Okubo, Fumiya and Konomi, Shin'ichi and Ogata, Hiroaki},
title = {Online change detection for monitoring individual student behavior via clickstream data on E-book system},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170412},
doi = {10.1145/3170358.3170412},
abstract = {We propose a new change detection method using clickstream data collected through an e-Book system. Most of the prior work has focused on the batch processing of clickstream data. In contrast, the proposed method is designed for online processing, with the model parameters for change detection updated sequentially based on observations of new click events. More specifically, our method generates a model for an individual student and performs minute-by-minute change detection based on click events during a classroom lecture. We collected clickstream data from four face-to-face lectures, and conducted experiments to demonstrate how the proposed method discovered change points and how such change points correlated with the students' performances.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {446–450},
numpages = {5},
keywords = {online processing, learning analytics, clickstream, change detection},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3170358.3170411,
author = {Long, Yanjin and Holstein, Kenneth and Aleven, Vincent},
title = {What exactly do students learn when they practice equation solving? refining knowledge components with the additive factors model},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170411},
doi = {10.1145/3170358.3170411},
abstract = {Accurately modeling individual students' knowledge growth is important in many applications of learning analytics. A key step is to decompose the knowledge targeted in the instruction into detailed knowledge components (KCs). We search for an accurate KC model for basic equation solving skills, using data from an intelligent tutoring system (ITS), Lynnette. Key criteria are data fit and predictive accuracy based on a standard logistic model called the Additive Factors Model (AFM). We focus on three difficulty factors for equation solving: understanding of variables, the negative sign, and the complexity of the equation. Fine-grained KC models were found to have greater fit and predictive accuracy than an "ideal," more abstract model, indicating that there is substantial under-generalization in students' equation-solving skill related to all three difficulty factors. The work enhances scientific understanding of the challenges students face in learning equation solving. It illustrates how learning analytics could inform the improvement of technology-enhanced learning environments.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {399–408},
numpages = {10},
keywords = {student modeling, knowledge components, intelligent tutoring systems, equation solving, educational data mining, K-12},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3170358.3170410,
author = {Chen, Yujing and Johri, Aditya and Rangwala, Huzefa},
title = {Running out of STEM: a comparative study across STEM majors of college students at-risk of dropping out early},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170410},
doi = {10.1145/3170358.3170410},
abstract = {Higher education institutions in the United States and across the Western world face a critical problem of attrition of college students and this problem is particularly acute within the Science, Technology, Engineering, and Mathematics (STEM) fields. Students are especially vulnerable in the initial years of their academic programs; more than 60% of the dropouts occur in the first two years. Therefore, early identification of at-risk students is crucial for a focused intervention if institutions are to support students towards completion. In this paper we developed and evaluated a survival analysis framework for the early identification of students at the risk of dropping out. We compared the performance of survival analysis approaches to other machine learning approaches including logistic regression, decision trees and boosting. The proposed methods show good performance for early prediction of at-risk students and are also able to predict when a student will dropout with high accuracy. We performed a comparative analysis of nine different majors with varying levels of academic rigor, challenge and student body. This study enables advisors and university administrators to intervene in advance to improve student retention.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {270–279},
numpages = {10},
keywords = {survival analysis, student retention, regression, classification},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3170358.3170409,
author = {Bodily, Robert and Kay, Judy and Aleven, Vincent and Jivet, Ioana and Davis, Dan and Xhakaj, Franceska and Verbert, Katrien},
title = {Open learner models and learning analytics dashboards: a systematic review},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170409},
doi = {10.1145/3170358.3170409},
abstract = {This paper aims to link student facing Learning Analytics Dashboards (LADs) to the corpus of research on Open Learner Models (OLMs), as both have similar goals. We conducted a systematic review of literature on OLMs and compared the results with a previously conducted review of LADs for learners in terms of (i) data use and modelling, (ii) key publication venues, (iii) authors and articles, (iv) key themes, and (v) system evaluation. We highlight the similarities and differences between the research on LADs and OLMs. Our key contribution is a bridge between these two areas as a foundation for building upon the strengths of each. We report the following key results from the review: in reports of new OLMs, almost 60% are based on a single type of data; 33% use behavioral metrics; 39% support input from the user; 37% have complex models; and just 6% involve multiple applications. Key associated themes include intelligent tutoring systems, learning analytics, and self-regulated learning. Notably, compared with LADs, OLM research is more likely to be interactive (81% of papers compared with 31% for LADs), report evaluations (76% versus 59%), use assessment data (100% versus 37%), provide a comparison standard for students (52% versus 38%), but less likely to use behavioral metrics, or resource use data (33% against 75% for LADs). In OLM work, there was a heightened focus on learner control and access to their own data.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {41–50},
numpages = {10},
keywords = {open student models, open learner models, literature review, learning analytics dashboards},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3170358.3170408,
author = {Lebis, Alexis and Lefevre, Marie and Luengo, Vanda and Guin, Nathalie},
title = {Capitalisation of analysis processes: enabling reproducibility, openness and adaptability thanks to narration},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170408},
doi = {10.1145/3170358.3170408},
abstract = {Analysis processes of learning traces, used to gain important pedagogical insights, are yet to be easily shared and reused. They face what is commonly called a reproducibility crisis. From our observations, we identify two important factors that may be the cause of this crisis: technical constraints due to runnable necessities, and context dependencies. Moreover, the meaning of the reproducibility itself is ambiguous and a source of misunderstanding. In this paper, we present an ontological framework dedicated to taking full advantage of already implemented educational analyses. This framework shifts the actual paradigm of analysis processes by representing them from a narrative point of view, instead of a technical one. This enables a formal description of analysis processes with high-level concepts. We show how this description is performed, and how it can help analysts. The goal is to empower both expert and non-expert analysis stakeholders with the possibility to be involved in the elaboration of analysis processes and their reuse in different contexts, by improving both human and machine understanding of these analyses. This possibility is known as the capitalisation of analysis processes of learning traces.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {245–254},
numpages = {10},
keywords = {reuse, reproducibility, openness, ontology, learning analytics, context, capitalization, analysis processes of learning traces, adaptability},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3170358.3170407,
author = {Likens, Aaron D. and McCarthy, Kathryn S. and Allen, Laura K. and McNamara, Danielle S.},
title = {Recurrence quantification analysis as a method for studying text comprehension dynamics},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170407},
doi = {10.1145/3170358.3170407},
abstract = {Self-explanations are commonly used to assess on-line reading comprehension processes. However, traditional methods of analysis ignore important temporal variations in these explanations. This study investigated how dynamical systems theory could be used to reveal linguistic patterns that are predictive of self-explanation quality. High school students (n = 232) generated self-explanations while they read a science text. Recurrence Plots were generated to show qualitative differences in students' linguistic sequences that were later quantified by indices derived by Recurrence Quantification Analysis (RQA). To predict self-explanation quality, RQA indices, along with summative measures (i.e., number of words, mean word length, and type-token ration) and general reading ability, served as predictors in a series of regression models. Regression analyses indicated that recurrence in students' self-explanations significantly predicted human rated self-explanation quality, even after controlling for summative measures of self-explanations, individual differences, and the text that was read (R2 = 0.68). These results demonstrate the utility of RQA in exposing and quantifying temporal structure in student's self-explanations. Further, they imply that dynamical systems methodology can be used to uncover important processes that occur during comprehension.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {111–120},
numpages = {10},
keywords = {text comprehension, self-explanation, recurrence quantification analysis, reading, dynamical systems theory},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3170358.3170406,
author = {Ochoa, Xavier and Dom\'{\i}nguez, Federico and Guam\'{a}n, Bruno and Maya, Ricardo and Falcones, Gabriel and Castells, Jaime},
title = {The RAP system: automatic feedback of oral presentation skills using multimodal analysis and low-cost sensors},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170406},
doi = {10.1145/3170358.3170406},
abstract = {Developing communication skills in higher education students could be a challenge to professors due to the time needed to provide formative feedback. This work presents RAP, a scalable system to provide automatic feedback to entry-level students to develop basic oral presentation skills. The system improves the state-of-the-art by analyzing posture, gaze, volume, filled pauses and the slides of the presenters through data captured by very low-cost sensors. The system also provides an off-line feedback report with multimodal recordings of their performance. An initial evaluation of the system indicates that the system's feedback highly agrees with human feedback and that students considered that feedback useful to develop their oral presentation skills.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {360–364},
numpages = {5},
keywords = {posture, multimodal learning analytics, gaze, filled-pauses},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3170358.3170404,
author = {Allen, Laura K. and Likens, Aaron D. and McNamara, Danielle S.},
title = {A multi-dimensional analysis of writing flexibility in an automated writing evaluation system},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170404},
doi = {10.1145/3170358.3170404},
abstract = {The assessment of writing proficiency generally includes analyses of the specific linguistic and rhetorical features contained in the singular essays produced by students. However, researchers have recently proposed that an individual's ability to flexibly adapt the linguistic properties of their writing might more closely capture writing skill. However, the features of the task, learner, and educational context that influence this flexibility remain largely unknown. The current study extends this research by examining relations between linguistic flexibility, reading comprehension ability, and feedback in the context of an automated writing evaluation system. Students (n = 131) wrote and revised six essays in an automated writing evaluation system and were provided both summative and formative feedback on their writing. Additionally, half of the students had access to a spelling and grammar checker that provided lower-level feedback during the writing period. The results provide evidence for the fact that developing writers demonstrate linguistic flexibility across the essays that they produce. However, analyses also indicate that lower-level feedback (i.e., spelling and grammar feedback) have little to no impact on the properties of students' essays nor on their variability across prompts or drafts. Overall, the current study provides important insights into the role of flexibility in writing skill and develops a strong foundation on which to conduct future research and educational interventions.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {380–388},
numpages = {9},
keywords = {writing, revision, natural language processing, flexibility, feedback},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3170358.3170403,
author = {Wise, Alyssa Friend and Cui, Yi},
title = {Unpacking the relationship between discussion forum participation and learning in MOOCs: content is key},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170403},
doi = {10.1145/3170358.3170403},
abstract = {This study examined the relationship between discussion forum contributions and course assessment results in a statistics MOOC. An important feature of the study is that it distinguished between discussions that were related to the learning of course material ("content-related") and those which were not ("non-content"). Another contribution is that the study evaluated the additional usefulness of social centrality measures in predicting course grade after the quantity of forum contributions has been accounted for. Results showed that, overall, 15% of course learners contributed to the forums and these learners had a significantly higher rate of successfully passing the course than non-contributors (64% vs 32% passing). Learners who made posts to both content-related and non-content threads had a higher passing rate than those who only contributed to one type or the other. Among learners who successfully passed the course, there were no differences in course grade when comparing discussion contributors and non-contributors overall; however those who contributed to content-related threads performed slightly better than those who did not (course grade of 87% vs 85%). A predictive model based on the number of posts made to content-related threads explained a small proportion of variance in course grades; addition of social centrality measures did not significantly improve the variance explained by the model.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {330–339},
numpages = {10},
keywords = {social network analysis, massive open online courses, learning outcomes, discussion forum},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3170358.3170402,
author = {Durand, Guillaume and Goutte, Cyril and Belacel, Nabil and Bouslimani, Yassine and L\'{e}ger, Serge},
title = {A diagnostic tool for competency-based program engineering},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170402},
doi = {10.1145/3170358.3170402},
abstract = {Competency based education (CBE) is seen by many as a way to optimize learning on cost, efficiency and flexibility. However, defining the required competencies, assigning them to specific courses and building the assessments evaluating student's proficiency can be tedious. More precisely, making sure that the assessments evaluate what they are supposed to evaluate requires a fair amount of psychometrics knowledge and time that can be difficult for teachers to acquire, maintain and use. Addressing assessment validity and more specifically competency frameworks mapping adequacy, we propose a rule-based tool to ease the building and the refinement of CBE courses and curricula. After introducing the context and briefly the related work, we present our set of rules before illustrating the capacity of the proposed diagnostic tool on an engineering curriculum. Experiments show that this tool can improve mapping adequacy in term of predictive accuracy and would require more efforts towards competency parameters reliability measurement.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {315–319},
numpages = {5},
keywords = {diagnostic tool, competency framework reliability, competency based education, cognitive diagnostic model},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3170358.3170401,
author = {Brennan, Amelia and Peace, Christina and Munguia, Pablo},
title = {Classroom size, activity and attendance: scaling up drivers of learning space occupation},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170401},
doi = {10.1145/3170358.3170401},
abstract = {Teaching face-to-face is still a major education mode in many universities, yet institutions are increasingly tasked with improving efficient use of teaching spaces. This need to understand space use can be coupled with learning and teaching data to better inform student attendance and subsequently participation. Here, we analyse thermal sensor data used to monitor traffic into classrooms; these data are associated with the timetable to provide knowledge of the course and the teaching mode (such as lecture, tutorial or workshop). Further, we integrate these traffic data with student feedback data to investigate the drivers of student attendance patterns, and aim to also include online activity and behaviour to develop broad models of both room occupancy and student attendance. Combining space utilisation data with information on teaching modality and in-class and out-of-class participation can inform on how to both improve learning and design effective and efficient teaching spaces.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {255–259},
numpages = {5},
keywords = {class, attendance},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3170358.3170400,
author = {Potts, Boyd A. and Khosravi, Hassan and Reidsema, Carl and Bakharia, Aneesha and Belonogoff, Mark and Fleming, Melanie},
title = {Reciprocal peer recommendation for learning purposes},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170400},
doi = {10.1145/3170358.3170400},
abstract = {Larger student intakes by universities and the rise of education through Massive Open Online Courses has led to less direct contact time with teaching staff for each student. One potential way of addressing this contact deficit is to invite learners to engage in peer learning and peer support; however, without technological support they may be unable to discover suitable peer connections that can enhance their learning experience. Two different research subfields with ties to recommender systems provide partial solutions to this problem. Reciprocal recommender systems provide sophisticated filtering techniques that enable users to connect with one another. To date, however, the main focus of reciprocal recommender systems has been on providing recommendation in online dating sites. Recommender systems for technology enhanced learning have employed and tailored exemplary recommenders towards use in education, with a focus on recommending learning content rather than other users. In this paper, we first discuss the importance of supporting peer learning and the role recommending reciprocal peers can play in educational settings. We then introduce our open-source course-level recommendation platform called RiPPLE that has the capacity to provide reciprocal peer recommendation. The proposed reciprocal peer recommender algorithm is evaluated against key criteria such as scalability, reciprocality, coverage, and quality and shows improvement over a baseline recommender. Primary results indicate that the system can help learners connect with peers based on their knowledge gaps and reciprocal preferences, with designed flexibility to address key limitations of existing algorithms identified in the literature.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {226–235},
numpages = {10},
keywords = {recsysTEL, reciprocal recommender, peer support, peer learning},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3170358.3170399,
author = {Diana, Nicholas and Eagle, Michael and Stamper, John and Grover, Shuchi and Bienkowski, Marie and Basu, Satabdi},
title = {Data-driven generation of rubric criteria from an educational programming environment},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170399},
doi = {10.1145/3170358.3170399},
abstract = {We demonstrate that, by using a small set of hand-graded student work, we can automatically generate rubric criteria with a high degree of validity, and that a predictive model incorporating these rubric criteria is more accurate than a previously reported model. We present this method as one approach to addressing the often challenging problem of grading assignments in programming environments. A classic solution is creating unit-tests that the student-generated program must pass, but the rigid, structured nature of unit-tests is suboptimal for assessing the more open-ended assignments students encounter in introductory programming environments like Alice. Furthermore, the creation of unit-tests requires predicting the various ways a student might correctly solve a problem - a challenging and time-intensive process. The current study proposes an alternative, semi-automated method for generating rubric criteria using low-level data from the Alice programming environment.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {16–20},
numpages = {5},
keywords = {programming education, data-driven, computer science education, assessment},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3170358.3170398,
author = {Nguyen, Quan and Huptych, Michal and Rienties, Bart},
title = {Linking students' timing of engagement to learning design and academic performance},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170398},
doi = {10.1145/3170358.3170398},
abstract = {In recent years, the connection between Learning Design (LD) and Learning Analytics (LA) has been emphasized by many scholars as it could enhance our interpretation of LA findings and translate them to meaningful interventions. Together with numerous conceptual studies, a gradual accumulation of empirical evidence has indicated a strong connection between how instructors design for learning and student behaviour. Nonetheless, students' timing of engagement and its relation to LD and academic performance have received limited attention. Therefore, this study investigates to what extent students' timing of engagement aligned with instructor learning design, and how engagement varied across different levels of performance. The analysis was conducted over 28 weeks using trace data, on 387 students, and replicated over two semesters in 2015 and 2016. Our findings revealed a mismatch between how instructors designed for learning and how students studied in reality. In most weeks, students spent less time studying the assigned materials on the VLE compared to the number of hours recommended by instructors. The timing of engagement also varied, from in advance to catching up patterns. High-performing students spent more time studying in advance, while low-performing students spent a higher proportion of their time on catching-up activities. This study reinforced the importance of pedagogical context to transform analytics into actionable insights.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {141–150},
numpages = {10},
keywords = {virtual learning environment, temporal, learning design, learning analytics, higher education, engagement},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3170358.3170397,
author = {Papamitsiou, Zacharoula and Economides, Anastasios A. and Pappas, Ilias O. and Giannakos, Michail N.},
title = {Explaining learning performance using response-time, self-regulation and satisfaction from content: an fsQCA approach},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170397},
doi = {10.1145/3170358.3170397},
abstract = {This study focuses on compiling students' response-time allocated to answer correctly or wrongly, their self-regulation, as well as their satisfaction from content, in order to explain high or medium/low learning performance. To this end, it proposes a conceptual model in conjunction with research propositions. For the evaluation of the approach, an empirical study with 452 students was conducted. The fuzzy set qualitative comparative analysis (fsQCA) revealed five configurations driven by the admitted factors that explain students' high performance, as well as five additional patterns, interpreting students' medium/low performance. These findings advance our understanding of the relations between actual usage and latent behavioral factors, as well as their combined effect on students' test score. Limitations and potential implications of these findings are also discussed.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {181–190},
numpages = {10},
keywords = {self-regulation, satisfaction from content, response-time, fuzzy set qualitative comparative analysis, configurations},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3170358.3170396,
author = {Lang, Charles and Macfadyen, Leah P. and Slade, Sharon and Prinsloo, Paul and Sclater, Niall},
title = {The complexities of developing a personal code of ethics for learning analytics practitioners: implications for institutions and the field},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170396},
doi = {10.1145/3170358.3170396},
abstract = {In this paper we explore the potential role, value and utility of a personal code of ethics (COE) for learning analytics practitioners, and in particular we consider whether such a COE might usefully mediate individual actions and choices in relation to a more abstract institutional COE. While several institutional COEs now exist, little attention has been paid to detailing the ethical responsibilities of individual practitioners. To investigate the problems associated with developing and implementing a personal COE, we drafted an LA Practitioner COE based on other professional codes, and invited feedback from a range of learning analytics stakeholders and practitioners: ethicists, students, researchers and technology executives. Three main themes emerged from their reflections: 1. A need to balance real world demands with abstract principles, 2. The limits to individual accountability within the learning analytics space, and 3. The continuing value of debate around an aspirational code of ethics within the field of learning analytics.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {436–440},
numpages = {5},
keywords = {professionalization, professional responsibility, code of ethics},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3170358.3170395,
author = {Hutt, Stephen and Gardener, Margo and Kamentz, Donald and Duckworth, Angela L. and D'Mello, Sidney K.},
title = {Prospectively predicting 4-year college graduation from student applications},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170395},
doi = {10.1145/3170358.3170395},
abstract = {We leverage a unique national dataset of 41,359 college applications to prospectively predict 4-year bachelor's graduation in a generalizable manner. Our features include sociodemographics, institutional graduation rates, academic achievement, standardized test scores, engagement in extracurricular activities, work experiences, and ratings by teachers and high-school guidance counselors. A random forest classifier successfully predicted 4-year graduation for 71.4% of the students (base rate = 44%) using all 166 of the aforementioned features and a split-half validation method. A stochastic hill-climbing feature selection procedure effectively maintained the same classification accuracy, but with a minimal set of 37 features, consisting of an approximately equal representation of sociodemographics, cognitive, and noncognitive factors. We advocate against using these results for admissions decisions, instead contemplating how they might be used to provide parents and educators with actionable information to guide students towards college success.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {280–289},
numpages = {10},
keywords = {noncognitive factors, national student clearinghouse, common app, college success, college applications},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3170358.3170394,
author = {Herder, Tiffany and Swiecki, Zachari and Fougt, Simon Skov and Tamborg, Andreas Lindenskov and Allsopp, Benjamin Brink and Shaffer, David Williamson and Misfeldt, Morten},
title = {Supporting teachers' intervention in students' virtual collaboration using a network based model},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170394},
doi = {10.1145/3170358.3170394},
abstract = {This paper reports a Design-Based Research project developing a tool (the Process Tab) that supports teachers' interventions with students in virtual internships. The tool uses a networked approach and allows insights into the discourse of groups and individuals based on contributions in chat fora and assignments.In the paper, we present the tool and reports from interviews with three teachers who used the tool. The interviews provide insights about the teachers' hopes, actual use, and difficulties with the tool. The main insight is that even though the teachers genuinely liked the idea and specific representations of the Process Tab, their lack of ability to teach and look at the tool at the same time hindered their use. In the final part of the paper, we discuss how to address this issue.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {21–25},
numpages = {5},
keywords = {virtual collaboration, teacher practices, teacher intervention, learning analytics, immersive learning environments, ACM proceedings},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3170358.3170393,
author = {Nacu, Denise and Baltes, Jennifer and Hamid, Taha and Gemmell, Jonathan and Pinkard, Nichole},
title = {A framework for developing metrics of youth engagement in informal learning environments},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170393},
doi = {10.1145/3170358.3170393},
abstract = {This paper proposes a framework which aims to leverage data from informal learning environments to provide insights about youth engagement for various stakeholders. To explore the framework, we created metrics to examine the engagement of 98 middle school-aged girls during a 20-week STEM program using attendance records and log data from an online learning platform coded to reflect 21st century learning activities. We present preliminary analyses using the metrics, focusing on how they can help stakeholders understand engagement and equity of participation.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {310–314},
numpages = {5},
keywords = {log analysis, learning analytics, informal learning, engagement},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3170358.3170392,
author = {Fancsali, Stephen E. and Zheng, Guoguo and Tan, Yanyan and Ritter, Steven and Berman, Susan R. and Galyardt, April},
title = {Using embedded formative assessment to predict state summative test scores},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170392},
doi = {10.1145/3170358.3170392},
abstract = {If we wish to embed assessment for accountability within instruction, we need to better understand the relative contribution of different types of learner data to statistical models that predict scores on assessments used for accountability purposes. The present work scales up and extends predictive models of math test scores from existing literature and specifies six categories of models that incorporate information about student prior knowledge, socio-demographics, and performance within the MATHia intelligent tutoring system. Linear regression and random forest models are learned within each category and generalized over a sample of 23,000+ learners in Grades 6, 7, and 8 over three academic years in Miami-Dade County Public Schools. After briefly exploring hierarchical models of this data, we discuss a variety of technical and practical applications, limitations, and open questions related to this work, especially concerning to the potential use of instructional platforms like MATHia as a replacement for time-consuming standardized tests.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {161–170},
numpages = {10},
keywords = {predictive modeling, mathematics education, intelligent tutoring systems, formative assessment, assessment, accountability},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3170358.3170391,
author = {Milligan, Sandra K.},
title = {Methodological foundations for the measurement of learning in learning analytics},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170391},
doi = {10.1145/3170358.3170391},
abstract = {Learning analysts often claim to measure learning, but their work has attracted growing concern about whether or not the measures are sufficiently accurate, fair, reliable, and valid, with utility for educators and interpretable by them. This paper considers these issues in the light of practices of scholars in more established fields, educational measurement particularly. The focus is on what really matters about methodologies for measuring learning, including foundational assumptions about the nature of learning, what is understood by the term `measured', the criteria applied when assessing quality of data, the standards of proof required to establish validity, reliability, generalizability, utility and interpretability of findings, and assumptions about learners and learning underlying data modeling techniques used to abstract meaning from the data. This paper argues that, for learning analytics to take its place as a fully-fledged member of the learning sciences, it needs seriously to consider how to measure learning. Methodology crafted at the interface of measurement science and learning analytics may be of sufficient interest to create a new subfield of scholarship - dubbed here `metrilytics' - to make a distinctive contribution to the science of learning.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {466–470},
numpages = {5},
keywords = {validation, measurement of learning, learning analytics, IRT},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3170358.3170390,
author = {Alhadad, Sakinah S. J. and Thompson, Kate and Knight, Simon and Lewis, Melinda and Lodge, Jason M.},
title = {Analytics-enabled teaching as design: reconceptualisation and call for research},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170390},
doi = {10.1145/3170358.3170390},
abstract = {As a human-centred educational practice and field of research, learning analytics must account for key stakeholders in teaching and learning. The focus of this paper is on the role of institutions to support teachers to incorporate learning analytics into their practice by understanding the confluence of internal and external factors that influence what they do. In this paper, we reconceptualise `teaching as design' for `analytics-enabled teaching as design' to shape this discussion to allow for the consideration of external factors, such as professional learning or ethical considerations of student data, as well as personal considerations, such as data literacy and teacher beliefs and identities. In order to address the real-world challenges of progressing teachers' efficacy and capacity toward analytics-enabled teaching as design, we have placed the teacher - as a cognitive, social, and emotional being - at the center. In so doing, we discuss potential directions towards research for practice in elucidating underpinning factors of teacher inquiry in the process of authentic design.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {427–435},
numpages = {9},
keywords = {team teaching, teaching as design, teacher identity, professional learning, institutional culture, ethics, design cognition, analytics},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3170358.3170389,
author = {Harrak, Fatima and Bouchet, Fran\c{c}ois and Luengo, Vanda and Gillois, Pierre},
title = {Profiling students from their questions in a blended learning environment},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170389},
doi = {10.1145/3170358.3170389},
abstract = {Automatic analysis of learners' questions can be used to improve their level and help teachers in addressing them. We investigated questions (N=6457) asked before the class by 1st year medicine/pharmacy students on an online platform, used by professors to prepare their on-site Q&amp;A session. Our long-term objectives are to help professors in categorizing those questions, and to provide students with feedback on the quality of their questions. To do so, first we manually categorized students' questions, which led to a taxonomy then used for an automatic annotation of the whole corpus. We identified students' characteristics from the typology of questions they asked using K-Means algorithm over four courses. The students were clustered by the proportion of each question asked in each dimension of the taxonomy. Then, we characterized the clusters by attributes not used for clustering such as the students' grade, the attendance, the number and popularity of questions asked. Two similar clusters always appeared: a cluster (A), made of students with grades lower than average, attending less to classes, asking a low number of questions but which are popular; and a cluster (D), made of students with higher grades, high attendance, asking more questions which are less popular. This work demonstrates the validity and the usefulness of our taxonomy, and shows the relevance of this classification to identify different students' profiles.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {102–110},
numpages = {9},
keywords = {student's behavior, question taxonomy, clustering, blended learning},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3170358.3170388,
author = {Boroujeni, Mina Shirvani and Dillenbourg, Pierre},
title = {Discovery and temporal analysis of latent study patterns in MOOC interaction sequences},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170388},
doi = {10.1145/3170358.3170388},
abstract = {Capturing students' behavioral patterns through analysis of sequential interaction logs is an important task in educational data mining and could enable more effective and personalized support during the learning processes. This study aims at discovery and temporal analysis of learners' study patterns in MOOC assessment periods. We propose two different methods to achieve this goal. First, following a hypothesis-driven approach, we identify learners' study patterns based on their interaction with lectures and assignments. Through clustering of study pattern sequences, we capture different longitudinal activity profiles among learners and describe their properties. Second, we propose a temporal clustering pipeline for unsupervised discovery of latent patterns in learners' interaction data. We model and cluster activity sequences at each time step and perform cluster matching to enable tracking learning behaviours over time. Our proposed pipeline is general and applicable in different learning environments such as MOOC and ITS. Moreover, it allows for modeling and temporal analysis of interaction data at different levels of actions granularity and time resolution. We demonstrate the application of this method for detecting latent study patterns in a MOOC course.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {206–215},
numpages = {10},
keywords = {temporal analysis, study pattern, sequence mining, markov model, learning analytics, clustering, MOOCs, LA, EDM},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3170358.3170387,
author = {Fiorini, Stefano and Sewell, Adrienne and Bumbalough, Mathew and Chauhan, Pallavi and Shepard, Linda and Rehrey, George and Groth, Dennis},
title = {An application of participatory action research in advising-focused learning analytics},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170387},
doi = {10.1145/3170358.3170387},
abstract = {Advisors assist students in developing successful course pathways through the curriculum. The purpose of this project is to augment advisor institutional and tacit knowledge with knowledge from predictive algorithms (i.e., Matrix Factorization and Classifiers) specifically developed to identify risk. We use a participatory action research approach that directly involves key members from both advising and research communities in the assessment and provisioning of information from the predictive analytics. The knowledge gained from predictive algorithms is evaluated using a mixed method approach. We first compare the predictive evaluations with advisors evaluations of student performance in courses and actual outcomes in those courses We next expose and classify advisor knowledge of student risk and identify ways to enhance the value of the prediction model. The results highlight the contribution that this collaborative approach can give to the constructive integration of Learning Analytics in higher education settings.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {89–96},
numpages = {8},
keywords = {participatory approaches to learning analytics, intervention evaluation, advising},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3170358.3170386,
author = {Mangaroska, Katerina and Sharma, Kshitij and Giannakos, Michail and Tr\ae{}tteberg, Hallvard and Dillenbourg, Pierre},
title = {Gaze insights into debugging behavior using learner-centred analysis},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170386},
doi = {10.1145/3170358.3170386},
abstract = {The presented study tries to tackle an intriguing question of how user-generated data from current technologies can be used to reinforce learners' reflections, improve teaching practices, and close the learning analytics loop. In particular, the aim of the study is to utilize users' gaze to examine the role of a mirroring tool (i.e. Exercise View in Eclipse) in orchestrating basic behavioral regulation of participants engaged in a debugging task. The results demonstrated that students who processed the information presented in the Exercise View and acted upon it, improved their performance and achieved higher level of success than those who failed to do it. The findings shed a light how to capture what constitute relevant data within a particular context using gaze patterns, that could guide collection of essential learner-centred analytics for the purpose of designing usable and modular learning environments based on data-driven approaches.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {350–359},
numpages = {10},
keywords = {mirroring tools, learner-centred analysis, eye-tracking, debugging, behaviour regulation},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3170358.3170385,
author = {Tempelaar, Dirk and Rienties, Bart and Nguyen, Quan},
title = {Investigating learning strategies in a dispositional learning analytics context: the case of worked examples},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170385},
doi = {10.1145/3170358.3170385},
abstract = {This study aims to contribute to recent developments in empirical studies on students' learning strategies, whereby the use of trace data is combined with self-report data to distinguish profiles of learning strategy use [3--5]. We do so in the context of an application of dispositional learning analytics in a large introductory course mathematics and statistics, based on blended learning. Building on our previous work which showed marked differences in how students used worked examples as a learning strategy [7, 11], this study compares different profiles of learning strategies with learning approaches, learning outcomes, and learning dispositions. One of our key findings is that deep learners were less dependent on worked examples as a resource for learning, and that students who only sporadically used worked examples achieved higher test scores.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {201–205},
numpages = {5},
keywords = {technology enhanced learning, prediction models, learning strategies, dispositional learning analytics, blended learning},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3170358.3170384,
author = {Gibson, Andrew and Lang, Charles},
title = {The pragmatic maxim as learning analytics research method},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170384},
doi = {10.1145/3170358.3170384},
abstract = {It is arguable that the chief aim of Learning Analytics is to use analytics for meaningful purposes in learning and teaching contexts, and that research in the field should advance this cause. However the field does not present a single clear understanding of what constitutes quality in Learning Analytics research.In this paper we present the Pragmatic Inquiry for Learning Analytics Research (PILAR) method as one approach to conducting Learning Analytics research. Rather than creating a new method, we reintroduce an old method to a new field, drawing on the Pragmatic Maxim, proposed by Charles Sanders Peirce as a principle for making ideas clear. Our instantiation of the Pragmatic Maxim requires the researcher to situate Learning Analytics research within a clearly defined learning context and to consider the analytics in terms of the practical effects on learning. We propose three essential elements and a five step process for addressing them in research.After presenting PILAR we address two potential limitations of the approach, and conclude with some implications for its future use in Learning Analytics research.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {461–465},
numpages = {5},
keywords = {research methods, pragmatism, learning analytics, inquiry},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3170358.3170383,
author = {Davis, Dan and Kizilcec, Ren\'{e} F. and Hauff, Claudia and Houben, Geert-Jan},
title = {The half-life of MOOC knowledge: a randomized trial evaluating knowledge retention and retrieval practice in MOOCs},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170383},
doi = {10.1145/3170358.3170383},
abstract = {Retrieval practice has been established in the learning sciences as one of the most effective strategies to facilitate robust learning in traditional classroom contexts. The cognitive theory underpinning the "testing effect" states that actively recalling information is more effective than passively revisiting materials for storing information in long-term memory. We document the design, deployment, and evaluation of an Adaptive Retrieval Practice System (ARPS) in a MOOC. This push-based system leverages the testing effect to promote learner engagement and achievement by intelligently delivering quiz questions from prior course units to learners throughout the course. We conducted an experiment in which learners were randomized to receive ARPS in a MOOC to track their performance and behavior compared to a control group. In contrast to prior literature, we find no significant effect of retrieval practice in this MOOC environment. In the treatment condition, passing learners engaged more with ARPS but exhibited similar levels of knowledge retention as non-passing learners.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {1–10},
numpages = {10},
keywords = {testing effect, retrieval practice, knowledge retention, experiment},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3170358.3170382,
author = {van der Zee, Tim and Davis, Dan and Saab, Nadira and Giesbers, Bas and Ginn, Jasper and van der Sluis, Frans and Paas, Fred and Admiraal, Wilfried},
title = {Evaluating retrieval practice in a MOOC: how writing and reading summaries of videos affects student learning},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170382},
doi = {10.1145/3170358.3170382},
abstract = {Videos are often the core content in open online education, such as in Massive Open Online Courses (MOOCs). Students spend most of their time in a MOOC on watching educational videos. However, merely watching a video is a relatively passive learning activity. To increase the educational benefits of online videos, students could benefit from more actively interacting with the to-be-learned material. In this paper two studies (n = 13k) are presented which examined the educational benefits of two more active learning strategies: 1) Retrieval Practice tasks which asked students to shortly summarize the content of videos, and 2) Given Summary tasks in which the students were asked to read pre-written summaries of videos. Writing, as well as reading summaries of videos were positively related to quiz grades. Both interventions seemed to help students to perform better, but there was no apparent difference between the efficacy of these interventions. These studies show how the quality of online education can be improved by adapting course design to established approaches from the learning sciences.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {216–225},
numpages = {10},
keywords = {writing, videos, summary, retrieval practice, online education, learning, experiment, MOOC},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3170358.3170381,
author = {Cicchinelli, Anal\'{\i}a and Veas, Eduardo and Pardo, Abelardo and Pammer-Schindler, Viktoria and Fessl, Angela and Barreiros, Carla and Lindst\"{a}dt, Stefanie},
title = {Finding traces of self-regulated learning in activity streams},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170381},
doi = {10.1145/3170358.3170381},
abstract = {This paper aims to identify self-regulation strategies from students' interactions with the learning management system (LMS). We used learning analytics techniques to identify metacognitive and cognitive strategies in the data. We define three research questions that guide our studies analyzing i) self-assessments of motivation and self regulation strategies using standard methods to draw a baseline, ii) interactions with the LMS to find traces of self regulation in observable indicators, and iii) self regulation behaviours over the course duration. The results show that the observable indicators can better explain self-regulatory behaviour and its influence in performance than preliminary subjective assessments.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {191–200},
numpages = {10},
keywords = {self regulation, learning strategies, learning analytics, clickstream activity, blended-learning},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3170358.3170380,
author = {Echeverria, Vanessa and Martinez-Maldonado, Roberto and Granda, Roger and Chiluiza, Katherine and Conati, Cristina and Buckingham Shum, Simon},
title = {Driving data storytelling from learning design},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170380},
doi = {10.1145/3170358.3170380},
abstract = {Data science is now impacting the education sector, with a growing number of commercial products and research prototypes providing learning dashboards. From a human-centred computing perspective, the end-user's interpretation of these visualisations is a critical challenge to design for, with empirical evidence already showing that `usable' visualisations are not necessarily effective from a learning perspective. Since an educator's interpretation of visualised data is essentially the construction of a narrative about student progress, we draw on the growing body of work on Data Storytelling (DS) as the inspiration for a set of enhancements that could be applied to data visualisations to improve their communicative power. We present a pilot study that explores the effectiveness of these DS elements based on educators' responses to paper prototypes. The dual purpose is understanding the contribution of each visual element for data storytelling, and the effectiveness of the enhancements when combined.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {131–140},
numpages = {10},
keywords = {visualisations, visual design, data storytelling, dashboards},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3170358.3170379,
author = {Martinez-Maldonado, Roberto and Echeverria, Vanessa and Santos, Olga C. and Santos, Augusto Dias Pereira Dos and Yacef, Kalina},
title = {Physical learning analytics: a multimodal perspective},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170379},
doi = {10.1145/3170358.3170379},
abstract = {The increasing progress in ubiquitous technology makes it easier and cheaper to track students' physical actions unobtrusively, making it possible to consider such data for supporting research, educator interventions, and provision of feedback to students. In this paper, we reflect on the underexplored, yet important area of learning analytics applied to physical/motor learning tasks and to the physicality aspects of `traditional' intellectual tasks that often occur in physical learning spaces. Based on Distributed Cognition theory, the concept of Internet of Things and multimodal learning analytics, this paper introduces a theoretical perspective for bringing learning analytics into physical spaces. We present three prototypes that serve to illustrate the potential of physical analytics for teaching and learning. These studies illustrate advances in proximity, motion and location analytics in collaborative learning, dance education and healthcare training.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {375–379},
numpages = {5},
keywords = {wearables, physical spaces, motor learning, mobility tracking, internet of things, indoor positioning, classroom},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3170358.3170378,
author = {Kostyuk, Victor and Almeda, Ma. Victoria and Baker, Ryan S.},
title = {Correlating affect and behavior in reasoning mind with state test achievement},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170378},
doi = {10.1145/3170358.3170378},
abstract = {Previous studies have investigated the relationship between affect, behavior, and learning in blended learning systems. These articles have found that affect and behavior are closely linked with learning outcomes. In this paper, we attempt to replicate prior work on how affective states and behaviors relate to mathematics achievement, investigating these issues within the context of 5th-grade students in South Texas using a mathematics blended learning system, Reasoning Mind. We use automatic detectors of student behavior and affect, and correlate inferred rates of each behavior and affective state with the students' end-of-year standardized assessment score. A positive correlation between engaged concentration and test scores replicates previous studies, as does a negative correlation between boredom and test scores. However, our findings differ from previous findings relating to confusion, frustration, and off-task behavior, suggesting the importance of contextual factors for the relationship between behavior, affect, and learning. Our study represents a step in understanding how broadly findings on the relationships between affect/behavior and learning generalize across different learning platforms.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {26–30},
numpages = {5},
keywords = {prediction, intelligent tutoring system, high-stakes tests, affection detection},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3170358.3170377,
author = {Holstein, Kenneth and Hong, Gena and Tegene, Mera and McLaren, Bruce M. and Aleven, Vincent},
title = {The classroom as a dashboard: co-designing wearable cognitive augmentation for K-12 teachers},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170377},
doi = {10.1145/3170358.3170377},
abstract = {When used in classrooms, personalized learning software allows students to work at their own pace, while freeing up the teacher to spend more time working one-on-one with students. Yet such personalized classrooms also pose unique challenges for teachers, who are tasked with monitoring classes working on divergent activities, and prioritizing help-giving in the face of limited time. This paper reports on the co-design, implementation, and evaluation of a wearable classroom orchestration tool for K-12 teachers: mixed-reality smart glasses that augment teachers' realtime perceptions of their students' learning, metacognition, and behavior, while students work with personalized learning software. The main contributions are: (1) the first exploration of the use of smart glasses to support orchestration of personalized classrooms, yielding design findings that may inform future work on real-time orchestration tools; (2) Replay Enactments: a new prototyping method for real-time orchestration tools; and (3) an in-lab evaluation and classroom pilot using a prototype of teacher smart glasses (Lumilo), with early findings suggesting that Lumilo can direct teachers' time to students who may need it most.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {79–88},
numpages = {10},
keywords = {real-time analytics, prototyping, personalized classrooms, orchestration, mixed-reality, co-design, awareness, K-12},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3170358.3170376,
author = {Poquet, Oleksandra and Lim, Lisa and Mirriahi, Negin and Dawson, Shane},
title = {Video and learning: a systematic review (2007--2017)},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170376},
doi = {10.1145/3170358.3170376},
abstract = {Video materials have become an integral part of university learning and teaching practice. While empirical research concerning the use of videos for educational purposes has increased, the literature lacks an overview of the specific effects of videos on diverse learning outcomes. To address such a gap, this paper presents preliminary results of a large-scale systematic review of peer-reviewed empirical studies published from 2007-2017. The study synthesizes the trends observed through the analysis of 178 papers selected from the screening of 2531 abstracts. The findings summarize the effects of manipulating video presentation, content and tasks on learning outcomes, such as recall, transfer, academic achievement, among others. The study points out the gap between large-scale analysis of fine-grained data on video interaction and experimental findings reliant on established psychological instruments. Narrowing this gap is suggested as the future direction for the research on video-based learning.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {151–160},
numpages = {10},
keywords = {video-based learning, systematic review},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3170358.3170375,
author = {Dawson, Shane and Poquet, Oleksandra and Colvin, Cassandra and Rogers, Tim and Pardo, Abelardo and Gasevic, Dragan},
title = {Rethinking learning analytics adoption through complexity leadership theory},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170375},
doi = {10.1145/3170358.3170375},
abstract = {Despite strong interest in learning analytics (LA), adoption at a large-scale organizational level continues to be problematic. This may in part be due to the lack of acknowledgement of existing conceptual LA models to operationalize how key adoption dimensions interact to inform the realities of the implementation process. This paper proposes the framing of LA adoption in complexity leadership theory (CLT) to study the overarching system dynamics. The framing is empirically validated in a study analysing interviews with senior staff in Australian universities (n=32). The results were coded for several adoption dimensions including leadership, governance, staff development, and culture. The coded data were then analysed with latent class analysis. The results identified two classes of universities that either i) followed an instrumental approach to adoption - typically top-down leadership, large scale project with high technology focus yet demonstrating limited staff uptake; or ii) were characterized as emergent innovators - bottom up, strong consultation process, but with subsequent challenges in communicating and scaling up innovations. The results suggest there is a need to broaden the focus of research in LA adoption models to move on from small-scale course/program levels to a more holistic and complex organizational level.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {236–244},
numpages = {9},
keywords = {learning analytics adoption, leadership, complexity},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3170358.3170374,
author = {Kovanovi\'{c}, Vitomir and Joksimovi\'{c}, Sre\'{c}ko and Mirriahi, Negin and Blaine, Ellen and Ga\v{s}evi\'{c}, Dragan and Siemens, George and Dawson, Shane},
title = {Understand students' self-reflections through learning analytics},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170374},
doi = {10.1145/3170358.3170374},
abstract = {Reflective writing has been widely recognized as one of the most effective activities for fostering students' reflective and critical thinking. The analysis of students' reflective writings has been the focus of many research studies. However, to date this has been typically a very labor-intensive manual process involving content analysis of student writings. With recent advancements in the field of learning analytics, there have been several attempts to use text analytics to examine student reflective writings. This paper presents the results of a study examining the use of theoretically-sound linguistic indicators of different psychological processes for the development of an analytics system for assessment of reflective writing. More precisely, we developed a random-forest classification system using linguistic indicators provided by the LIWC and Coh-Metrix tools. We also examined what particular indicators are representative of the different types of student reflective writings.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {389–398},
numpages = {10},
keywords = {text mining, self-reflections, online learning, learning analytics},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3170358.3170373,
author = {Gardner, Josh and Brooks, Christopher},
title = {Coenrollment networks and their relationship to grades in undergraduate education},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170373},
doi = {10.1145/3170358.3170373},
abstract = {In this paper, we evaluate the complete undergraduate coenrollment network over a decade of education at a large American public university. We provide descriptive properties of the network, demonstrating that the coenrollment networks evaluated follow power-law degree distributions similar to many other large-scale networks; that they reveal strong performance-based assortativity; and that network-based features can significantly improve GPA-based student performance predictors. We then implement a network-based, multi-view classification model to predict students' final course grades. In particular, we adapt a structural modeling approach from [19, 34], whereby we model the university-wide undergraduate coenrollment network as an undirected graph. We compare the performance of our predictor to traditional methods used for grade prediction in undergraduate university courses, and demonstrate that a multi-view ensembling approach outperforms both prior "flat" and network-based models for grade prediction across several classification metrics. These findings demonstrate the usefulness of combining diverse approaches in models of student success, and demonstrate specific network-based modeling strategies which are likely to be most effective for grade prediction.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {295–304},
numpages = {10},
keywords = {learning analytics, grade prediction},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3170358.3170372,
author = {Dollinger, Mollie and Lodge, Jason M.},
title = {Co-creation strategies for learning analytics},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170372},
doi = {10.1145/3170358.3170372},
abstract = {In order to further the field of learning analytics (LA), researchers and experts may need to look beyond themselves and their own perspectives and expertise to innovate LA platforms and interventions. We suggest that by co-creating with the users of LA, such as educators and students, researchers and experts can improve the usability, usefulness, and draw greater understanding from LA interventions. Within this article we discuss the current LA issues and barriers and how co-creation strategies can help address many of these challenges. We further outline the considerations, both pre- and during interventions, which support and foster a co-created strategy for learning analytics interventions.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {97–101},
numpages = {5},
keywords = {participatory design, co-design, co-creation},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3170358.3170371,
author = {Papamitsiou, Zacharoula and Economides, Anastasios A.},
title = {Can't get more satisfaction? game-theoretic group-recommendation of educational resources},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170371},
doi = {10.1145/3170358.3170371},
abstract = {Students' satisfaction from educational resources is a subjective perception of how well these resources meet students' expectations for learning. Recommending educational resources to groups of students, targeting at optimizing all students' satisfaction, is a complicated task due to the lack of joint group profiles. Instead of merging individual profiles or fusing individual recommendations, this paper follows a game-theoretic perspective for solving conflict of interest among students and recommending resources to groups in online collaborative learning contexts: the group members are the players, the resources comprise the set of possible actions, and maximizing each individual member's satisfaction from the selected resources is a problem of finding the Nash Equilibrium. In case the Nash Equilibrium is Pareto efficient, none of the players can get more payoff (satisfaction) without decreasing the payoff of any other player, indicating an optimal benefit for the group as a whole. The comparative evaluation of the suggested approach to other state-of-the-art methods provided statistically significant results regarding the error in predicted group satisfaction from the recommendation and the goodness of the ranked list of recommendations.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {409–416},
numpages = {8},
keywords = {satisfaction, non-cooperative games, group recommendation, collaborative learning},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3170358.3170370,
author = {Feild, Jacqueline and Lewkow, Nicholas and Burns, Sean and Gebhardt, Karen},
title = {A generalized classifier to identify online learning tool disengagement at scale},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170370},
doi = {10.1145/3170358.3170370},
abstract = {Student success, a major focus in higher education, in part, requires students to remain actively engaged in the required coursework. Identifying student disengagement, when a student stops completing coursework, at scale has been a continuing challenge for higher education due to the heterogeneity of traditional college courses. This research uses data from Connect by McGraw-Hill Education, a widely used online learning tool, to build a classifier to identify learning tool disengagement at scale. This classifier was trained and tested on four years of historical data, representing 4.5 million students in 175,000 courses, across 256 disciplines. Results show that the classifier is effective in identifying disengagement within the online learning tool against baselines, across time, and within and across disciplines. The classifier was also effective in identifying students at risk of disengaging from Connect and then earning unsuccessful grades in a pilot course for which the assignments in Connect were worth a relatively small portion of the overall course grade. Because Connect is widely used, this classifier is positioned to be a good tool for instructors and institutions to identify students at risk for disengagement from coursework. Instructors and institutions can use this information to design and implement interventions to improve engagement and improve student success at the institution in key courses.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {61–70},
numpages = {10},
keywords = {student success, logistic regression, learning tool disengagement, higher education, economic education, apache spark},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3170358.3170369,
author = {Andres, Juan Miguel L. and Baker, Ryan S. and Ga\v{s}evi\'{c}, Dragan and Siemens, George and Crossley, Scott A. and Joksimovi\'{c}, Sre\'{c}ko},
title = {Studying MOOC completion at scale using the MOOC replication framework},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170369},
doi = {10.1145/3170358.3170369},
abstract = {Research on learner behaviors and course completion within Massive Open Online Courses (MOOCs) has been mostly confined to single courses, making the findings difficult to generalize across different data sets and to assess which contexts and types of courses these findings apply to. This paper reports on the development of the MOOC Replication Framework (MORF), a framework that facilitates the replication of previously published findings across multiple data sets and the seamless integration of new findings as new research is conducted or new hypotheses are generated. In the proof of concept presented here, we use MORF to attempt to replicate 15 previously published findings across 29 iterations of 17 MOOCs. The findings indicate that 12 of the 15 findings replicated significantly across the data sets, and that two findings replicated significantly in the opposite direction. MORF enables larger-scale analysis of MOOC research questions than previously feasible, and enables researchers around the world to conduct analyses on huge multi-MOOC data sets without having to negotiate access to data.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {71–78},
numpages = {8},
keywords = {replication, multi-MOOC analysis, meta-analysis, completion, MORF, MOOCs, MOOC replication framework},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3170358.3170368,
author = {Kim, Bugeun and Rhim, Jungwook and Rho, Jihyun and Hwang, Taehyun and Lee, Gunho and Gweon, Gahgene},
title = {"I'll do it!": examining the relationship between locus of control and math game retention for preschoolers},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170368},
doi = {10.1145/3170358.3170368},
abstract = {Acquiring simple arithmetic skills at the preschool level requires repetitive practices. One method for encouraging students to spend longer time practicing is by presenting the skills in an engaging game. As student retention on the game increases, the student will be more likely to acquire the practiced skill since she will have spent more time practicing. In this paper, we examine the relationship between internal locus of control and retention in game-based learning applications for young children using Todo Math, a mobile-based math learning application for children from Pre-K to 2nd grade. We examine 345,783 users' log data to show that when children prefer "free" mode, which has high internal locus of control, their retention on Todo Math is higher than children who prefer "daily" mode, which has high external locus of control. We present three analyses that support our findings using survival analysis, post-hoc analysis, and t-test.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {290–294},
numpages = {5},
keywords = {retention, preschooler, online math game, locus of control},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3170358.3170367,
author = {Tsai, Yi-Shan and Moreno-Marcos, Pedro Manuel and Tammets, Kairit and Kollom, Kaire and Ga\v{s}evi\'{c}, Dragan},
title = {SHEILA policy framework: informing institutional strategies and policy processes of learning analytics},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170367},
doi = {10.1145/3170358.3170367},
abstract = {This paper introduces a learning analytics policy development framework developed by a cross-European research project team - SHEILA (Supporting Higher Education to Integrate Learning Analytics), based on interviews with 78 senior managers from 51 European higher education institutions across 16 countries. The framework was developed using the RAPID Outcome Mapping Approach (ROMA), which is designed to develop effective strategies and evidence-based policy in complex environments. This paper presents three case studies to illustrate the development process of the SHEILA policy framework, which can be used to inform strategic planning and policy processes in real world environments, particularly for large-scale implementation in higher education contexts.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {320–329},
numpages = {10},
keywords = {strategy, policy, learning analytics, higher education, ROMA model},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3170358.3170366,
author = {Brown, Michael Geoffrey and DeMonbrun, R. Matthew and Teasley, Stephanie D.},
title = {Conceptualizing co-enrollment: accounting for student experiences across the curriculum},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170366},
doi = {10.1145/3170358.3170366},
abstract = {In this study, we develop and test three measures for conceptualizing the potential impact of co-enrollment in different courses on students' changing risk for academic difficulty in a focal course. Two of these measures, concurrent enrollment in at least one difficult course and academic difficulty in the prior week in courses other than the focal course, significantly increase students' odds of academic difficulty in the focal course in our models. Our results have implications for the designs of Early Warning Systems and suggest that academic planners consider the relationship between course co-enrollment and students' academic success.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {305–309},
numpages = {5},
keywords = {undergraduate education, survival analysis, educational technology, early warning systems, curriculum analytics},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3170358.3170365,
author = {Ocheja, Patrick and Flanagan, Brendan and Ogata, Hiroaki},
title = {Connecting decentralized learning records: a blockchain based learning analytics platform},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170365},
doi = {10.1145/3170358.3170365},
abstract = {As Learners move from one learning environment to another, there is a key necessity of taking with them a proof of previous learning achievements or experiences. In most cases, this is either expressed in terms of receipt of scores or a certificate of completion. While this may be sufficient for enrollment and other administrative decisions, it poses some limitations to the depth of learning analytics and consequently a slow onboarding process. Also, with different institutions having their learning data isolated from each other, it becomes more difficult to easily access a learner's learning history for all learning activities on other systems. In this paper, we propose a blockchain based approach for connecting learning data across different Learning Management Systems (LMS), Learning Record Stores (LRS), institutions and organizations. Leveraging on unique properties of blockchain technology, we also propose solutions to ensuring learning data consistency, availability, immutability, security, privacy and access control.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {265–269},
numpages = {5},
keywords = {smart contracts, privacy, learning record store, learning management systems, learning data, learning analytics, blockchain},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3170358.3170364,
author = {Rodr\'{\i}guez-Triana, Mar\'{\i}a Jes\"{u}s and Prieto, Luis P. and Mart\'{\i}nez-Mon\'{e}s, Alejandra and Asensio-P\'{e}rez, Juan I. and Dimitriadis, Yannis},
title = {The teacher in the loop: customizing multimodal learning analytics for blended learning},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170364},
doi = {10.1145/3170358.3170364},
abstract = {In blended learning scenarios, evidence needs to be gathered from digital and physical spaces to obtain a more complete view of the teaching and learning processes. However, these scenarios are highly heterogeneous, and the varying data sources available in each particular context can condition the accuracy, relevance, interpretability and actionability of the Learning Analytics (LA) solutions, affecting also the user's sense of agency and trust in such solutions. To aid stakeholders in making use of learning analytics, we propose a process to involve teachers in customizing multimodal LA (MMLA) solutions, adapting them to their particular blended learning situation (e.g., identifying relevant data sources and metrics). Since measuring the added value of adopting an LA solution is not straightforward, we also propose a concrete method for doing so. The results obtained from two case studies in authentic, blended computer-supported collaborative learning settings show an improvement in the sensitivity and F1 scores of the customized MMLA solution. Aside from these quantitative improvements, participant teachers reported both an increment in the effort involved, but also increased relevance, understanding and actionability of the results.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {417–426},
numpages = {10},
keywords = {personalization, multimodal learning analytics, customization, blended learning},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3170358.3170363,
author = {Larmuseau, Charlotte and Elen, Jan and Depaepe, Fien},
title = {The influence of students' cognitive and motivational characteristics on students' use of a 4C/ID-based online learning environment and their learning gain},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170363},
doi = {10.1145/3170358.3170363},
abstract = {Research has revealed that the design of online learning environments can influence students' use and performance. In this study, an online learning environment for learning French as a foreign language was developed in line with the four component instructional design (4C/ID) model. While the 4C/ID-model is a well-established instructional design model, little is known about (1) factors impacting students' use of the four components, namely, learning tasks, part-task practice, supportive and procedural information during their learning process as well as about (2) the way in which students' differences in use of the 4C/ID-based online learning environment impacts course performance. The aim of this study is, therefore, twofold. Firstly, it investigates the influence of students' prior knowledge, task value and self-efficacy on students' use of the four different components of the 4C/ID-model. Secondly, it examines the influence of students' use of the components on their learning gain, taking into account their characteristics. The sample consisted of 161 students in higher education. Results, based on structural equation modelling (SEM), indicate that prior knowledge has a negative influence on students' use of learning tasks and part-task practice. Task value has a positive influence on use of learning tasks and supportive information. Additionally, results indicate that use of use of learning tasks, procedural information, controlled for students' prior knowledge significantly contribute to students' learning gain. Results suggest that students' use of the four components is based on their cognitive and motivational characteristics. Furthermore, results reveal the impact of students' use of learning tasks and procedural information on students' learning gain.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {171–180},
numpages = {10},
keywords = {online learning, instructional design, adaptive learning},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3170358.3170360,
author = {Watanabe, Eiji and Ozeki, Takashi and Kohama, Takeshi},
title = {Analysis of interactions between lecturers and students},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170360},
doi = {10.1145/3170358.3170360},
abstract = {In this paper, we discuss the interactions between lecturers and students. First, we detect their behaviors by extracting the face regions of lecturers and students from moving images. Thereafter, we model the interaction between the lecturers' writing and explanation behaviors and the students' note taking and listening behaviors by using multilayered neural networks. We discuss the above interactions based on the internal representations of multilayered neural networks.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {370–374},
numpages = {5},
keywords = {student, modeling, lecturer, interaction, behavior},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@proceedings{10.1145/3170358,
title = {LAK '18: Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The aim of the conference is to provide a forum for presentation, exchange and discussion of research and practices related to the transdisciplinary field of learning analytics.},
location = {Sydney, New South Wales, Australia}
}

@inproceedings{10.1145/3027385.3029489,
author = {Hu, Xiao and Yang, Chengrui and Qiao, Chen and Lu, Xiaoyu and Chu, Sam K. W.},
title = {New features in Wikiglass, a learning analytic tool for visualizing collaborative work on wikis},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029489},
doi = {10.1145/3027385.3029489},
abstract = {Wikiglass is a learning analytic tool for visualizing collaborative work on Wikis built by groups of secondary or primary school students. This poster presents new features of Wikiglass developed recently based on requests from teachers, including flexible selection of date range, revision network, and thinking order detection. Currently the new features are used and evaluated in two secondary schools in Hong Kong.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {616–617},
numpages = {2},
keywords = {wiki, visualization, thinking orders, revision network},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3029488,
author = {Molenaar, Inge and Knoop-van Campen, Carolien A. N. and Hasselman, Fred},
title = {The effects of a learning analytics empowered technology on students' arithmetic skill development},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029488},
doi = {10.1145/3027385.3029488},
abstract = {Learning analytics empowered educational technologies (LA-ET) in primary classrooms allow for blended learning scenarios with teacher-lead instructions, class-paced and individually-paced practice. This quasi-experimental study investigates the effects of a LA-ET on the development of students' arithmetic skills over one schoolyear. Children learning in a traditional paper &amp; pencil condition were compared to learners using a LA-ET on tablet computers in grade 4. The educational technology combined teacher dashboards (extracted analytics) and class and individually paced assignments (embedded analytics). The results indicated that children in the LA-ET condition made significantly more progress on arithmetic skills in one schoolyear compared to children in the paper &amp; pencil condition.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {614–615},
numpages = {2},
keywords = {primary education, educational technologies, arithmetic's, ability levels},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3029487,
author = {Alabi, Halimat and Hatala, Marek},
title = {Best intentions: learner feedback on learning analytics visualization design},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029487},
doi = {10.1145/3027385.3029487},
abstract = {A mixed methods approach was undertaken in this exploratory study to better understand how learners perceive and utilize learning analytics visualizations during online discussions activities. Internal conditions such as goal orientation and numeracy were measured alongside the external conditions created by the discussion structure and learning analytics. Our results emphasize key factors that should be considered when designing learning analytics tools.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {612–613},
numpages = {2},
keywords = {learning analytics, judgments of learning, information visualization, evaluation},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3029486,
author = {Vytasek, Jovita M. and Wise, Alyssa F. and Woloshen, Sonya},
title = {Topic models to support instructors in MOOC forums},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029486},
doi = {10.1145/3027385.3029486},
abstract = {This paper explores the potential of using na\"{\i}ve topic modeling to support instructors in navigating MOOC discussion forums. Categorizing discussion threads into topics can provide an overview of the discussion, improve navigation of the forum, and support replying to a representative sample of content related posts. We investigate four different approaches to using topic models to organize and present discussion posts, highlighting the strength and weaknesses of each approach to support instructors.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {610–611},
numpages = {2},
keywords = {topic modeling, discussion forums, MOOC},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3029485,
author = {Wong, Tak-Lam and Xie, Haoran and Wang, Fu Lee and Poon, Chung Keung and Zou, Di},
title = {An automatic approach for discovering skill relationship from learning data},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029485},
doi = {10.1145/3027385.3029485},
abstract = {We have developed a method called skill2vec, which applies big data techniques to automatically analyze the learning data to discover skill relationship, leading to a more objective and data-informed decision making. Skill2vec is a neural network architecture which can transform a skill to a new vector space called embedding. The embedding can facilitate the comparison and visualization of different skills and their relationship. We conducted a pilot experiment using benchmark dataset to demonstrate the effectiveness of our method.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {608–609},
numpages = {2},
keywords = {skill relationship, embedding, deep learning, data analytics},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3029484,
author = {Elouazizi, Noureddine and Birol, G\"{u}lnur and Jandciu, Eric and \"{O}berg, Gunilla and Welsh, Ashley and Han, Andrea and Campbell, Alice},
title = {Automated analysis of aspects of written argumentation},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029484},
doi = {10.1145/3027385.3029484},
abstract = {In this paper, we report on a model that uses a mathematically and cognitively augmented Latent Semantic Analysis method to automatically assess aspects of written argumentation, produced by students in a science communication course.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {606–607},
numpages = {2},
keywords = {latent semantics analysis, automated assessment},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3029483,
author = {Hu, Xiao and Hou, Xiangyu and Lei, Chi-Un and Yang, Chengrui and Ng, Jeremy},
title = {An outcome-based dashboard for moodle and Open edX},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029483},
doi = {10.1145/3027385.3029483},
abstract = {This poster presents a cross-platform learning analytics dashboard on Moodle and Open edX for monitoring outcome-based learning progress. The dashboard visualizes students' interactions with the platforms in near real-time, aiming to help teachers and students monitor students' learning progress. The dashboard has been used in four large-size general education courses in a comprehensive university in Hong Kong, undergoing evaluation and improvement.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {604–605},
numpages = {2},
keywords = {outcome-based learning, moodle, dashboard, Open edX},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3029482,
author = {Quigley, David and McNamara, Conor and Sumner, Tamara},
title = {Using learning analytics in iterative design of a digital modeling tool},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029482},
doi = {10.1145/3027385.3029482},
abstract = {Iterative design is a powerful method for developing digital classroom tools and curricula. We explore how infusing learning analytics into this process has influenced our development of EcoSurvey, a digital modeling tool for mapping the organisms and interactions in the local ecosystem. We have found that analytic techniques can help us discover areas in which students struggle to engage with scientific modeling, and we can iteratively use learning analytics to demonstrate the impact of design changes.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {602–603},
numpages = {2},
keywords = {scientific modeling, iterative design, collaborative modeling},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3029480,
author = {Brooks, Christopher and Teasley, Stephanie and Siemens, George},
title = {Challenges and opportunities facing educational discourse researchers},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029480},
doi = {10.1145/3027385.3029480},
abstract = {The scholarly investigation of discourse in teaching and learning is multi-disciplinary, theoretically rich, and highly technical. Researchers with backgrounds in education, cognitive psychology, computer science, and the social sciences apply a diverse set of techniques to understand how student discussions affect learning. Aided by big data coming from learning content management and massive open online course systems, these researchers have an unparalleled opportunity for insight into the teaching and learning process. In this paper we summarize some of the challenges and opportunities arising out of three workshops on Educational Discourse. These workshops convened both expert and emerging scholars to discuss the (i) ethical, (ii) technical, and (iii) infrastructure barriers to building a research community focused on computer-mediated educational discourse. Of particular note is that while computational infrastructure exists for storing and manipulating educational discourse, there is a need for a sociotechnical infrastructure upon which community members can come together to engage in joint work.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {600–601},
numpages = {2},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3029479,
author = {Okubo, F. and Yamashita, T. and Shimada, A. and Ogata, H.},
title = {A neural network approach for students' performance prediction},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029479},
doi = {10.1145/3027385.3029479},
abstract = {In this paper, we propose a method for predicting final grades of students by a Recurrent Neural Network (RNN) from the log data stored in the educational systems. We applied this method to the log data from 108 students and examined the accuracy of prediction. From the experimental results, comparing with multiple regression analysis, it is confirmed that an RNN is effective to early prediction of final grades.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {598–599},
numpages = {2},
keywords = {recurrent neural network, predication of student's performance, learning log},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3029478,
author = {Jeremic, Zoran and Kumar, Vive and Graf, Sabine},
title = {MORPH: supporting the integration of learning analytics at institutional level},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029478},
doi = {10.1145/3027385.3029478},
abstract = {While there is high potential in using learning analytics to provide educational institutions as well as teachers and learners with actionable information and improve learning experiences, currently only very few learning analytics tools are actually used in educational institutions. In this paper, we introduce MORPH, a platform that facilitates the integration of learning analytics modules and tools into institutional learning systems. MORPH provides a robust distributed architecture which combines batch, stream and real-time data processing using a parallel processing model to enable and support efficient processing of large amounts of data. Furthermore, it provides common management and administration features that enable the seamless integration of learning analytics research modules and tools into existing institutional learning systems.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {596–597},
numpages = {2},
keywords = {real-time processing, learning analytics, institutional learning environments, data streaming, dashboards, batch processing},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3029477,
author = {Choi, Heeryung and Brooks, Christopher and Collins-Thompson, Kevyn},
title = {What does student writing tell us about their thinking on social justice?},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029477},
doi = {10.1145/3027385.3029477},
abstract = {In this work we investigate the use of deep learning for text analysis to measure elements of student thinking related to issues of privilege, oppression, diversity and social justice. We leverage historical expert annotations as well as a large lexical model to create a more generalizable vocabulary for identifying these characteristics in short student writing. We demonstrate the feasibility of this approach, and identify further areas for research.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {594–595},
numpages = {2},
keywords = {writing analytics, social work, social justice, natural language processing},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3029476,
author = {Fortenbacher, Albrecht and Pinkwart, Niels and Yun, Haeseon},
title = {[LISA] learning analytics for sensor-based adaptive learning},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029476},
doi = {10.1145/3027385.3029476},
abstract = {This paper reports on research conducted in a project named LISA which aims at supporting learners through learner-centered learning analytics using physiological sensor data as well as environmental sensors. We present the concept and a prototypical realization of a mobile sensor device used in LISA.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {592–593},
numpages = {2},
keywords = {sensor based learning, self-regulated learning, learner-centric learning analytics, learner awareness, adaptive learning},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3029475,
author = {Burns, Sean and Corwin, Kimberley},
title = {Automating student survey reports in online education for faculty and instructional designers},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029475},
doi = {10.1145/3027385.3029475},
abstract = {In this paper, we discuss Colorado State University Online's progress toward designing automated survey reports for student feedback data collected through our newly designed LTI survey tool. Using multiple R packages, including 'rmarkdown' and 'likert', the reporting tool imports student survey response data and generates reports for faculty and instructional designers. These reports focus on student perceptions of communication, course design, academic challenge, general satisfaction, and more. These reports display visual representations of the Likert-type response frequencies, basic descriptive statistics, and free-response comments. Surveys are administered just before half-way through the semester to provide formative feedback and just before the end of the semester to provide summative feedback. In this way, faculty and instructional designers can obtain a quick and easily digestible report to make changes and improvements to their classes with minimal effort in the back end production.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {590–591},
numpages = {2},
keywords = {xtable, survey, student feedback, rmarkdown, likert, course quality, course design, automated reporting, RODBCext, R},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3029474,
author = {Healion, Donal and Russell, Sam and Cukurova, Mutlu and Spikol, Daniel},
title = {Tracing physical movement during practice-based learning through multimodal learning analytics},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029474},
doi = {10.1145/3027385.3029474},
abstract = {In this paper, we pose the question, can the tracking and analysis of the physical movements of students and teachers within a Practice-Based Learning (PBL) environment reveal information about the learning process that is relevant and informative to Learning Analytics (LA) implementations? Using the example of trials conducted in the design of a LA system, we aim to show how the analysis of physical movement from a macro level can help to enrich our understanding of what is happening in the classroom. The results suggest that Multimodal Learning Analytics (MMLA) could be used to generate valuable information about the human factors of the collaborative learning process and we propose how this information could assist in the provision of relevant supports for small group work. More research is needed to confirm the initial findings with larger sample sizes and refine the data capture and analysis methodology to allow automation.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {588–589},
numpages = {2},
keywords = {practice-based learning, movement, learning analytics, collaborative problem solving, collaborative learning environment},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3029473,
author = {Gardner, Josh and Onuoha, Ogechi and Brooks, Christopher},
title = {Integrating syllabus data into student success models},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029473},
doi = {10.1145/3027385.3029473},
abstract = {In this work, we present (1) a methodology for collecting, evaluating, and utilizing human-annotated data about course syllabi in predictive models of student success, and (2) an empirical analysis of the predictiveness of such features as they relate to others in modeling end-of-course grades in traditional higher education courses. We present a two-stage approach to (1) that addresses several challenges unique to the annotation task, and address (2) using variable importance metrics from a series of exploratory models. We demonstrate that the process of supplementing traditional course data with human-annotated data can potentially improve predictive models with information not contained in university records, and highlight specific features that demonstrate these potential information gains.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {586–587},
numpages = {2},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3029472,
author = {Olivares, Daniel M. and Hundhausen, Christopher D.},
title = {Supporting learning analytics in computing education},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029472},
doi = {10.1145/3027385.3029472},
abstract = {As is the case for many undergraduate STEM degree programs, computing degree programs are plagued by high attrition rates. This is especially true in early computing courses, in which failure and drop-out rates in the 35 to 50 percent range are common. By collecting learning process data as students engage in computer programming assignments, computing educators can place themselves in a position not only to better understand students' struggles, but also to better tailor instructional interventions to students' needs. We have developed OSBLE+, a learning management and analytics environment that interfaces with a computer programming environment to support the automatic collection of learners' programming process and social data as they work on programming assignments, while also providing an interactive environment for the analysis and visualization of those data. In ongoing work, we are using OSBLE+ to explore two possibilities: (a) leveraging learning and social data to strategically deliver automated learning interventions, and (b) presenting learners with visual representations of their learning data in order to prompt them to reflect on and discuss their learning processes.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {584–585},
numpages = {2},
keywords = {visualizations, social learning, learning management system, learning analytics, data collection, computing education research, OSBLE},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3029471,
author = {Kovanovi\'{c}, Vitomir and Joksimovi\'{c}, Sre\'{c}ko and Poquet, Oleksandra and Hennis, Thieme and Dawson, Shane and Ga\v{s}evi\'{c}, Dragan and de Vries, Pieter and Hatala, Marek and Siemens, George},
title = {Understanding the relationship between technology use and cognitive presence in MOOCs},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029471},
doi = {10.1145/3027385.3029471},
abstract = {In this poster, we present the results of the study which examined the relationship between student differences in their use of the available technology and their perceived levels of cognitive presence within the MOOC context. The cognitive presence is a construct used to measure the level of practical inquiry in the Communities of Inquiry model. Our results revealed the existence of three clusters based on student technology use. The clusters significantly differed in terms of their levels of cognitive presence, most notably they differed on the levels of problem resolution.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {582–583},
numpages = {2},
keywords = {student clustering, community of inquiry model, MOOCs},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3029470,
author = {Atapattu, Thushari and Falkner, Katrina},
title = {Discourse analysis to improve the effective engagement of MOOC videos},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029470},
doi = {10.1145/3027385.3029470},
abstract = {Lecture videos are amongst the most commonly used instructional methods in present Massive Open Online Courses (MOOCs). As the main form of instruction, students' engagement behaviour with MOOC videos directly impacts the students' success or failure. This research focuses on an in-depth analysis of 1.5 million video interactions (e.g. pause, seek video) of a Programming MOOC. Our video-by-video analysis explores the rationale behind the time-wise variation of video interactions. We aim to analyse discourse features (e.g. syntactic simplicity of text, and speaking rate) and their correlation with the video interaction patterns. This paper presents preliminary results and educational video design implications.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {580–581},
numpages = {2},
keywords = {video analytics, linguistic, discourse, coh-metrix, MOOCs},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3029469,
author = {Chen, Guanliang and Davis, Dan and Krause, Markus and Hauff, Claudia and Houben, Geert-Jan},
title = {Buying time: enabling learners to become earners with a real-world paid task recommender system},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029469},
doi = {10.1145/3027385.3029469},
abstract = {Massive Open Online Courses (MOOCs) aim to educate the world, especially learners from developing countries. While MOOCs are certainly available to the masses, they are not yet fully accessible. Although all course content is just clicks away, deeply engaging with a MOOC requires a substantial time commitment, which frequently becomes a barrier to success. To mitigate the time required to learn from a MOOC, we here introduce a design that enables learners to earn money by applying what they learn in the course to real-world marketplace tasks. We present a Paid Task Recommender System (Rec-$ys), which automatically recommends course-relevant tasks to learners as drawn from online freelance platforms. Rec-$ys has been deployed into a data analysis MOOC and is currently under evaluation.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {578–579},
numpages = {2},
keywords = {learning design, learning analytics, MOOCs},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3029467,
author = {Wham, Drew},
title = {Forecasting student outcomes at university-wide scale using machine learning},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029467},
doi = {10.1145/3027385.3029467},
abstract = {Elements of applied statistics and computer science are quickly integrating and being applied to a diverse set of problems in academia and industry. Here I explore the potential value of this multi-disciplinary approach to applications in higher education by applying it to forecasting course level outcomes for individual students at all of Penn State's campuses. Utilizing hundreds of data sources on individual students, ranging from past performance to current course engagement, I demonstrate the potential accuracy of forecasting techniques at identifying high risk students early in the course term. Our preliminary results suggest that %50 of students that earned a D or F in 2015 could have been identified prior to the start of the course.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {576–577},
numpages = {2},
keywords = {student success, scalability, modeling, machine learning, feature finding},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3029466,
author = {Schweighart, M.},
title = {Using item response theory to generate an item pool for an e-learning-system},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029466},
doi = {10.1145/3027385.3029466},
abstract = {This paper1 demonstrates how the application of item response theory yields useful item characteristics, which further can be the foundation of item pools and therefore adaptive educational software to come.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {574–575},
numpages = {2},
keywords = {learning analytics, item response theory, item pool, evaluation, big data analysis, adaptive learning},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3029465,
author = {Jaakonm\"{a}ki, Roope and Drachsler, Hendrik and Kickmeier-Rust, Michael and Dietze, Stefan and Fortenbacher, Albrecht and Marenzi, Ivana},
title = {Cooking with learning analytics recipes},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029465},
doi = {10.1145/3027385.3029465},
abstract = {Learning Analytics is a melting pot for a multitude of research fields and origin of many developments about learning and its environment. There is a serious hype over the concepts of learning analytics, however, concrete solutions and applications are comparably scarce. Of course, data rich environments, such as MOOCs, come with statistical analytics dashboards, although the educational value is often limited. Practical solutions for scenarios in data-lean environments or for small-scale organizations are rarely adopted. The LA4S project is dedicated to gather practical solutions, provide a tool box for practitioners, and publish a cook book with concrete learning analytics recipes for everyone.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {572–573},
numpages = {2},
keywords = {solutions, recipes, learning analytics, cookbook, applications},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3029464,
author = {Stoeffler, Kristin and Rosen, Yigal and von Davier, Alina},
title = {Exploring the measurement of collaborative problem solving using a human-agent educational game},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029464},
doi = {10.1145/3027385.3029464},
abstract = {Collaborative problem solving (CPS) is a process that relies on both cognitive and social skills contributions by those involved in the joint activity. If a student is matched with a problematic group of peers, then there will be no valid measurement of the CPS skills. In the human-agent settings, CPS skills are measured by pairing each individual student with a computer agent or agents that can be programmed to act as team members with varying characteristics relevant to different CPS skills and contexts. This paper describes current research on measuring CPS skills through human-agent interactions in a prototype of a collaborative educational game.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {570–571},
numpages = {2},
keywords = {teamwork, performance assessment, computer agent, collaborative problem solving},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3029463,
author = {Bannert, Maria and Molenaar, Inge and Azevedo, Roger and J\"{a}rvel\"{a}, Sanna and Ga\v{s}evi\'{c}, Dragan},
title = {Relevance of learning analytics to measure and support students' learning in adaptive educational technologies},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029463},
doi = {10.1145/3027385.3029463},
abstract = {In this poster, we describe the aim and current activities of the EARLI-Centre for Innovative Research (E-CIR) "Measuring and Supporting Student's Self-Regulated Learning in Adaptive Educational Technologies" which is funded by the European Association for Research on Learning and Instruction (EARLI) from 2015 to 2019. The aim is to develop our understanding of multimodal data that unobtrusively capture cognitive, meta-cognitive, affective and motivational states of learners over time. This demands for a concerted interdisciplinary dialogue combining findings from psychology and educational sciences with advances in computer sciences and artificial intelligence. The participants in this E-CIR are leading international researchers who have articulated different emerging perspectives and methodologies to measure cognition, metacognition, motivation, and emotions during learning. The participants recognize the need for intensive collaboration to accelerate progress with new interdisciplinary methods including learning analytics to develop more powerful adaptive educational technologies.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {568–569},
numpages = {2},
keywords = {self-regulated learning, multimodal data, learning analytics, educational data mining, adaptive educational technologies},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3029462,
author = {Zimmerman, Neil L. and Baker, Ryan S.},
title = {Mining knowledge components from many untagged questions},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029462},
doi = {10.1145/3027385.3029462},
abstract = {An ongoing study is being run to ensure that the McGraw-Hill Education LearnSmart platform teaches students as efficiently as possible. The first step in doing so is to identify what Knowledge Components (KCs) exist in the content; while the content is tagged by experts, these tags need to be re-calibrated periodically.LearnSmart courses are organized into chapters corresponding to those found in a textbook; each chapter can have anywhere from about a hundred to a few thousand questions. The KC extraction algorithms proposed by Barnes [1] and Desmarais et al [3] are applied on a chapter-by-chapter basis. To assess the ability of each mined q matrix to describe the observed learning, the PFA model of Pavlik et al [4] is fitted to it and a cross-validated AUC is calculated. The models are assessed based on whether PFA's predictions of student correctness are accurate.Early results show that both algorithms do a reasonable job of describing student progress, but q matrices with very different numbers of KCs fit observed data similarly well. Consequently, further consideration is required before automated extraction is practical in this context.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {566–567},
numpages = {2},
keywords = {knowledge tracing, knowledge components, data mining},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3029461,
author = {Hansen, Cecilie Johanne Slokvik and Wasson, Barbara and Skretting, Hans and Netteland, Grete and Hirnstein, Marina},
title = {When learning is high stake},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029461},
doi = {10.1145/3027385.3029461},
abstract = {Firefighter learning is high stake. They need to maintain certain competence levels related to physical, mental, and firefighting and rescue skills in order to provide the public with a high level of emergency service. Fire and Rescue Services need to maintain an overview of the current competences of their personnel and to react when there is a competence gap. This poster presents our approach to using competence modelling, learner models, learning analytics, and visualisations in order provide insight into competence status and development on the individual, team, and organisation level, and to provide early-alerts and automated messages to instructors responsible for planning training activities, as well as to team leaders responsible for making decisions about teams in high stakes situations.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {564–565},
numpages = {2},
keywords = {visualization, open learner model, learning analytics, competence development},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3029458,
author = {Sluijter, J. and Otten, M.},
title = {Business intelligence (BI) for personalized student dashboards},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029458},
doi = {10.1145/3027385.3029458},
abstract = {At Stenden University students from all over the world study together; all these different nationalities and cultures result in different ideas concerning academic success. The basis of this project was to develop a personalized dashboard for students via Microsoft Office 365 Power BI in which students can set their own personal KPI's. The raw data from the Student Information System (SIS) was transformed into clear visualizations that will help students gain better insight into their academic performance. This information can be used either independently or in consultation with their student advisor.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {562–563},
numpages = {2},
keywords = {power query, power BI, personalized dashboards, grade goals, excel, business intelligence},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3029457,
author = {Zheng, Longwei and Gong, Wei and Gu, Xiaoqing},
title = {Predicting e-textbook adoption based on event segmentation of teachers' usage},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029457},
doi = {10.1145/3027385.3029457},
abstract = {Customized content of e-textbook require teachers to spend greater efforts using authoring tools and planning activities before class, and teachers with various contexts have different demands on e-textbook. However, some teachers who lack ICT skills and dissatisfy with the features tend to give up using e-textbook. Thus we need to know the status of teachers' usage earlier before we decide to give them some technical supports. This paper describes an analysis method for predicting e-textbook adoption from usage records in early days, and an event segmentation method of teachers' usage is used in effort to provide features of predictive model.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {560–561},
numpages = {2},
keywords = {technology adoption, predictive analytics, feature finding, event segmentation},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3029456,
author = {Kumar, Vishesh and Tissenbaum, Mike and Berland, Matthew},
title = {What are visitors up to? helping museum facilitators know what visitors are doing},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029456},
doi = {10.1145/3027385.3029456},
abstract = {In this paper, we describe a tablet application designed around an interactive game-based science museum exhibit. It is aimed to help provide museum docents useful information about the visitors' actions, in a way that is actionable, and enables docents to provide assistance and prompts to visitors that are more meaningful, compared to what they are typically able to do without this interface augmentation.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {558–559},
numpages = {2},
keywords = {tinkering, tablet, scaffolding, real-time analysis, museum, markov models, engineering},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3029455,
author = {Edwards, Rebecca L. and Davis, Sarah K. and Hadwin, Allyson F. and Milford, Todd M.},
title = {Using predictive analytics in a self-regulated learning university course to promote student success},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029455},
doi = {10.1145/3027385.3029455},
abstract = {Prior research offers evidence that differing levels of student engagement are associated with different outcomes in terms of performance. In this study, we investigating the efficacy of a model of behavioural and agentic engagement to predict student performance (low, middle, high) at four timepoints in a semester. The model was significant at all four timepoints. Measures of behavioural and agentic engagement predicted membership across the three groups differently. With a few exceptions, these differences were consistent across timepoints. Looking at variations in student engagement across time can be used to target interventions to support student success at the undergraduate level.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {556–557},
numpages = {2},
keywords = {student engagement, self-regulated learning, predictive modeling, learning analytics, higher education},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3029454,
author = {Lei, Chi-Un and Gonda, Donn and Hou, Xiangyu and Oh, Elizabeth and Qi, Xinyu and Kwok, Tyrone T. O. and Yeung, Yip-Chun Au and Lau, Ray},
title = {Data-assisted instructional video revision via course-level exploratory video retention analysis},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029454},
doi = {10.1145/3027385.3029454},
abstract = {Since teachers are not physically present in an online class, instructional video is the major carrier of course contents in an online learning environment. This paper aims to investigate how course-level exploratory video retention analysis can be used for identifying moments with abnormal watching behaviors and revising videos for a higher video retention. We have empirically evaluated the effectiveness of video analysis and revisions, based on evaluating retentions of revised videos.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {554–555},
numpages = {2},
keywords = {video retention, exploratory analysis, data-informed revision},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3029453,
author = {Xiong, Ye and Wu, Yi-Fang Brook},
title = {Write-and-learn: promoting meaningful learning through concept map-based formative feedback on writing assignments},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029453},
doi = {10.1145/3027385.3029453},
abstract = {A primary goal of higher education is to promote meaningful learning: the delivery of core academic content to students in innovative ways that allow them to learn and then apply what they have learned. As a pedagogical strategy, Writing-to-Learn (WTL) intends to use writing to improve students' understanding of course content and concepts. To improve students' meaningful learning of conceptual knowledge in WTL activities, the project proposes to develop the Write-and-Learn system to generate automated formative feedback by taking advantage of the concept maps constructed from instructors' lecture notes and individual students' writing assignments. The proposed research aims to provide insights into how to apply concept maps into WTL activities to generate effective formative feedback on the acquisition and development of conceptual knowledge, and explore how and to what extent concept map-based formative feedback can be utilized to scaffold and promote meaningful learning in WTL activities.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {552–553},
numpages = {2},
keywords = {writing-to-learn, natural language processing, concept mapping, automated formative feedback},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3029452,
author = {Aslan, Sinem and Okur, Eda and Alyuz, Nese and Mete, Sinem Emine and Oktay, Ece and Genc, Utku and Esme, Asli Arslan},
title = {Students' emotional self-labels for personalized models},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029452},
doi = {10.1145/3027385.3029452},
abstract = {There are some implementations towards understanding students' emotional states through automated systems with machine learning models. However, generic AI models of emotions lack enough accuracy to autonomously and meaningfully trigger any interventions. Collecting self-labels from students as they assess their internal states can be a way to collect labeled subject specific data necessary to obtain personalized emotional engagement models. In this paper, we outline preliminary analysis on emotional self-labels collected from students while using a learning platform.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {550–551},
numpages = {2},
keywords = {self-report, personalized learning, personalized emotional engagement, intelligent tutoring systems (ITS), affective computing},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3029451,
author = {Manai, Ouajdi and Yamada, Hiroyuki},
title = {How can we accelerate dissemination of knowledge and learning? developing an online knowledge management platform for networked improvement communities},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029451},
doi = {10.1145/3027385.3029451},
abstract = {The Networked Improvement Learning and Support (NILS) platform is an online tool designed to accelerate the initiation and development of Networked Improvement Communities in a disciplined manner. Its main goal is to promote social, organizational learning through curation and synthesis and tacit to explicit knowledge conversion to facilitate knowledge construction and ownership by the communities regarding improvement practice in education. In this proposal we will discuss the NILS platform, a few use cases, and a plan of analytics development that advances knowledge dissemination and monitors the health status of networks.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {548–549},
numpages = {2},
keywords = {statistical analysis, social pressure, social network analysis, recommender engine, networked improvement community, machine learning, knowledge management systems, improvement science, improvement, dissemination, curation, behavior modeling},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3029450,
author = {Marbouti, Farshid and Diefes-Dux, Heidi and Madhavan, Krishna},
title = {Utilizing visualization and feature selection methods to identify important learning objectives in a course},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029450},
doi = {10.1145/3027385.3029450},
abstract = {There have been numerous efforts to increase students' academic success. One data-driven approach is to highlight the important learning objectives in a course. In this paper, we used visualization and three feature selection methods to highlight the important learning objectives in a course. Identifying important learning objectives as well as the relation among the learning objectives have multiple educational advantages. First, it informs the instructors and students of the important topics in the course; without learning them properly students will not be successful. Second, it highlights any inconsistencies in defining the learning objective, how they are being assessed, and design of the course. Thus, this approach can be used as a course design diagnostic tool.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {546–547},
numpages = {2},
keywords = {learning objectives, information visualization, first-year engineering, feature selection, engineering education},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3029449,
author = {Chen, Lujie and Dubrawski, Artur},
title = {Learning from learning curves: discovering interpretable learning trajectories},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029449},
doi = {10.1145/3027385.3029449},
abstract = {We propose a data driven method for decomposing population level learning curve models into mutually exclusive distinctive groups each consisting of similar learning trajectories. We validate this method on six knowledge components from the log data from an online tutoring system ASSIST-ment. Preliminary analysis reveals interpretable patterns of "skill growth" that correlate with students' performance in the subsequently administered state standardized tests.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {544–545},
numpages = {2},
keywords = {student models, learning curves, clustering},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3029448,
author = {Chen, Bodong and Fan, Yizhou and Zhang, Guogang and Wang, Qiong},
title = {Examining motivations and self-regulated learning strategies of returning MOOCs learners},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029448},
doi = {10.1145/3027385.3029448},
abstract = {The present study examines behavioral patterns, motivations, and self-regulated learning strategies of returning learners---a special learner subpopulation in massive open online courses (MOOCs). To this end, data were collected from a teacher professional development MOOC that has been offered for seven iterations during 2014--2016. Data analysis identified more than 15% of all registrants as returning learners. Findings from click log analysis identified possible motivations of re-enrollment including improving grades, refreshing theoretical understanding, and solving practical problems. Further analysis uncovered evidence of self-regulated learning strategies among returning learners. Taken together, this study contributes to ongoing inquiry into MOOCs learning pathways, informs future MOOC design, and sheds light on the exploration of MOOCs as a viable option for teacher professional development.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {542–543},
numpages = {2},
keywords = {teacher professional development, clustering, MOOCs},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3029447,
author = {Clow, Doug and Ferguson, Rebecca and Kitto, Kirsty and Cho, Yong-Sang and Sharkey, Mike and Aguerrebere, Cecilia},
title = {Beyond failure: the 2nd LAK Failathon poster},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029447},
doi = {10.1145/3027385.3029447},
abstract = {This poster will be a chance for a wider LAK audience to engage with the 2nd LAK Failathon workshop. Both of these will build on the successful Failathon event in 2016 and extend beyond discussing individual experiences of failure to exploring how the field can improve, particularly regarding the creation and use of evidence.Failure in research is an increasingly hot topic, with high-profile crises of confidence in the published research literature in medicine and psychology. Among the major factors in this research crisis are the many incentives to report and publish only positive findings. These incentives prevent the field in general from learning from negative findings, and almost entirely preclude the publication of mistakes and errors. Thus providing an alternative forum for practitioners and researchers to learn from each other's failures can be very productive. The first LAK Failathon, held in 2016, provided just such an opportunity for researchers and practitioners to share their failures and negative findings in a lower-stakes environment, to help participants learn from each other's mistakes. It was very successful, and there was strong support for running it as an annual event. The 2nd LAK Failathon workshop will build on that success, with twin objectives to provide an environment for individuals to learn from each other's failures, and also to co-develop plans for how we as a field can better build and deploy our evidence base.This poster is an opportunity for wider feedback on the plans developed in the workshop, with interactive use of sticky notes to add new ideas and coloured dots to illustrate prioritisation. This broadens the participant base in this important work, which should improve the quality of the plans and the commitment of the community to delivering them.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {540–541},
numpages = {2},
keywords = {learning from failure, learning analytics, evidence, analytics},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3029446,
author = {Schulte, Jurgen and Fernandez de Mendonca, Pedro and Martinez-Maldonado, Roberto and Buckingham Shum, Simon},
title = {Large scale predictive process mining and analytics of university degree course data},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029446},
doi = {10.1145/3027385.3029446},
abstract = {For students, in particular freshmen, the degree pathway from semester to semester is not that transparent, although students have a reasonable idea what courses are expected to be taken each semester. An often-pondered question by students is: "what can I expect in the next semester?" More precisely, given the commitment and engagement I presented in this particular course and the respective performance I achieved, can I expect a similar outcome in the next semester in the particular course I selected? Are the demands and expectations in this course much higher so that I need to adjust my commitment and engagement and overall workload if I expect a similar outcome? Is it better to drop a course to manage expectations rather than to (predictably) fail, and perhaps have to leave the degree altogether? Degree and course advisors and student support units find it challenging to provide evidence based advise to students. This paper presents research into educational process mining and student data analytics in a whole university scale approach with the aim of providing insight into the degree pathway questions raised above. The beta-version of our course level degree pathway tool has been used to shed light for university staff and students alike into our university's 1,300 degrees and associated 6 million course enrolments over the past 20 years.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {538–539},
numpages = {2},
keywords = {process mining, predictive modeling, learning analytics, educational process visualization, educational data mining},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3029445,
author = {Oi, Misato and Yamada, Masanori and Okubo, Fumiya and Shimada, Atsushi and Ogata, Hiroaki},
title = {Reproducibility of findings from educational big data: a preliminary study},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029445},
doi = {10.1145/3027385.3029445},
abstract = {In this paper, we examined whether previous findings on educational big data consisting of e-book logs from a given academic course can be reproduced with different data from other academic courses. The previous findings showed that (1) students who attained consistently good achievement more frequently browsed different e-books and their pages than low achievers and that (2) this difference was found only for logs of preparation for course sessions (preview), not for reviewing material (review). Preliminarily, we analyzed e-book logs from four courses. The results were reproduced in only one course and only partially, that is, (1) high achievers more frequently changed e-books than low achievers (2) for preview. This finding suggests that to allow effective usage of learning and teaching analyses, we need to carefully construct an educational environment to ensure reproducibility.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {536–537},
numpages = {2},
keywords = {reproducibility, educational big data, e-book},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3029444,
author = {Yaginuma, Yoshitomo and Furukawa, Masako and Yamada, Tsuneo},
title = {Video annotation tool for learning job interview},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029444},
doi = {10.1145/3027385.3029444},
abstract = {In this paper, video annotation tool for learning job interview is proposed. To visualize the difference of obtained descriptions, the proposed tool uses correspondence analysis. The results of correspondence analysis are used to give feedback to learners. By the results, the learner can understand the characteristics of his/her descriptions among the others.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {534–535},
numpages = {2},
keywords = {visualization, video annotation tool, job interview},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3029443,
author = {Knight, Simon and Anderson, Theresa and Tall, Kelly},
title = {Dear learner: participatory visualisation of learning data for sensemaking},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029443},
doi = {10.1145/3027385.3029443},
abstract = {We discuss the application of a hand-drawn self-visualization approach to learner-data, to draw attention to the space of representational possibilities, the power of representation interactions, and the performativity of information representation.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {532–533},
numpages = {2},
keywords = {visualization, sensemaking, participatory, learning analytics},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3029442,
author = {Koedinger, Ken and Liu, Ran and Stamper, John and Thille, Candace and Pavlik, Phil},
title = {Community based educational data repositories and analysis tools},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029442},
doi = {10.1145/3027385.3029442},
abstract = {This workshop will explore community based repositories for educational data and analytic tools that are used to connect researchers and reduce the barriers to data sharing. Leading innovators in the field, as well as attendees, will identify and report on bottlenecks that remain toward our goal of a unified repository. We will discuss these as well as possible solutions. We will present LearnSphere, an NSF funded system that supports collaborating on and sharing a wide variety of educational data, learning analytics methods, and visualizations while maintaining confidentiality. We will then have hands-on sessions in which attendees have the opportunity to apply existing learning analytics workflows to their choice of educational datasets in the repository (using a simple drag-and-drop interface), add their own learning analytics workflows (requires very basic coding experience), or both. Leaders and attendees will then jointly discuss the unique benefits as well as the limitations of these solutions. Our goal is to create building blocks to allow researchers to integrate their data and analysis methods with others, in order to advance the future of learning science.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {524–525},
numpages = {2},
keywords = {scalability, modeling, learning metrics, data-informed learning theories, data-informed efforts, data storage and sharing},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3029441,
author = {Pardo, Abelardo and Mart\'{\i}nez-Maldonado, Roberto and Buckingham Shum, Simon and Schulte, Jurgen and McIntyre, Simon and Ga\v{s}evi\'{c}, Dragan and Gao, Jing and Siemens, George},
title = {Connecting data with student support actions in a course: a hands-on tutorial},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029441},
doi = {10.1145/3027385.3029441},
abstract = {The amount of data extracted from learning experiences has grown at an astonishing pace both in depth due to the increasing variety of data sources, and in breath with courses now being offered to massive student cohorts. However, in this emerging scenario instructors are now facing the challenge of connecting the knowledge emerging from data analysis with the provision of meaningful support actions to students within the context of an instructional design.The objective of this tutorial is to give attendees a set of hypothetical scenarios in which the knowledge extracted from a learning experience needs to be used to provide frequent personalized feedback to students.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {522–523},
numpages = {2},
keywords = {student support, predictive analytics, learning analytics, instructional design, feedback},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3029440,
author = {Grover, Shuchi and Bienkowski, Marie and Basu, Satabdi and Eagle, Michael and Diana, Nicholas and Stamper, John},
title = {A framework for hypothesis-driven approaches to support data-driven learning analytics in measuring computational thinking in block-based programming},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029440},
doi = {10.1145/3027385.3029440},
abstract = {K-12 classrooms use block-based programming environments (BBPEs) for teaching computer science and computational thinking (CT). To support assessment of student learning in BBPEs, we propose a learning analytics framework that combines hypothesis- and data-driven approaches to discern students' programming strategies from BBPE log data. We use a principled approach to design assessment tasks to elicit evidence of specific CT skills. Piloting these tasks in high school classrooms enabled us to analyze student programs and video recordings of students as they built their programs. We discuss a priori patterns derived from this analysis to support data-driven analysis of log data in order to better assess understanding and use of CT in BBPEs.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {530–531},
numpages = {2},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3029439,
author = {Lang, Charles and Teasley, Stephanie and Stamper, John},
title = {Building the learning analytics curriculum: workshop},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029439},
doi = {10.1145/3027385.3029439},
abstract = {Learning Analytics courses and degree programs both on-and offline have begun to proliferate over the last three years. As a result of this growth in interest from students, university administrators, researchers and instructors we believe it is a good time to review how these educational efforts are impacting the field, how synergy between instructors might be developed to greater serve the field and what kinds of best practices could be developed.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {520–521},
numpages = {2},
keywords = {teaching, learning analytics instruction, curriculum development},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3029438,
author = {Hu, Xiao and Cheong, Christy W. L. and Ding, Wenwen and Woo, Michelle},
title = {A systematic review of studies on predicting student learning outcomes using learning analytics},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029438},
doi = {10.1145/3027385.3029438},
abstract = {Predicting student learning outcomes is one of the prominent themes in Learning Analytics research. These studies varied to a significant extent in terms of the techniques being used, the contexts in which they were situated, and the consequent effectiveness of the prediction. This paper presented the preliminary results of a systematic review of studies in predictive learning analytics. With the goal to find out what methodologies work for what circumstances, this study will be able to facilitate future research in this area, contributing to relevant system developments that are of pedagogic values.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {528–529},
numpages = {2},
keywords = {systematic review, prediction, performances, methods, learning outcomes, learning context},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3029437,
author = {Spikol, Daniel and Prieto, Luis P. and Rodr\'{\i}guez-Triana, M. J. and Worsley, Marcelo and Ochoa, Xavier and Cukurova, Mutlu and Vogel, Bahtijar and Ruffaldi, Emanuele and Ringtved, Ulla Lunde},
title = {Current and future multimodal learning analytics data challenges},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029437},
doi = {10.1145/3027385.3029437},
abstract = {Multimodal Learning Analytics (MMLA) captures, integrates and analyzes learning traces from different sources in order to obtain a more holistic understanding of the learning process, wherever it happens. MMLA leverages the increasingly widespread availability of diverse sensors, high-frequency data collection technologies and sophisticated machine learning and artificial intelligence techniques. The aim of this workshop is twofold: first, to expose participants to, and develop, different multimodal datasets that reflect how MMLA can bring new insights and opportunities to investigate complex learning processes and environments; second, to collaboratively identify a set of grand challenges for further MMLA research, built upon the foundations of previous workshops on the topic.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {518–519},
numpages = {2},
keywords = {multimodal learning analytics, datasets, challenges},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3029436,
author = {Bowe, Megan and Chen, Weiqin and Griffiths, Dai and Hoel, Tore and Lee, Jaeho and Ogata, Hiroaki and Richards, Griff and Yuan, Li and Zhang, Jingjing},
title = {Learning analytics and policy (LAP): international aspirations, achievements and constraints},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029436},
doi = {10.1145/3027385.3029436},
abstract = {The Learning Analytics and Policy (LAP) workshop explores and documents the ways in which policies at national and regional level are shaping the development of learning analytics. It brings together representatives from around the world who report on the circumstances in their own country. The workshop is preceded by an information gathering phase, and followed by the authoring of a report. The aspirations, achievements and constraints in the different countries are contrasted and documented, providing a valuable resource for the future development of learning analytics.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {516–517},
numpages = {2},
keywords = {privacy, policy, open data, learning analytics, data protection},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3029435,
author = {Cooper, Adam and Berg, Alan and Sclater, Niall and Dorey-Elias, Tanya and Kitto, Kirsty},
title = {LAK17 hackathon: getting the right information to the right people so they can take the right action},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029435},
doi = {10.1145/3027385.3029435},
abstract = {The hackathon is intended to be a practical hands-on workshop involving participants from academia and commercial organizations with both technical and practitioner expertise. It will consider the outstanding challenge of visualizations which are effective for the intended audience: informing action, not likely to be misinterpreted, and embodying contextual appropriacy, etc. It will surface particular issues as workshop challenges and explore responses to these challenges as visualizations resting upon interoperability standards and API-oriented open architectures.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {514–515},
numpages = {2},
keywords = {visualization, open learning analytics, interoperability, hackathon, contextual appropriacy, actionable insights},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3029434,
author = {Arnold, Kimberly E. and Karcher, Brandon and Wright, Casey V. and McKay, James},
title = {Student empowerment, awareness, and self-regulation through a quantified-self student tool},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029434},
doi = {10.1145/3027385.3029434},
abstract = {The purpose of this paper is to examine the cross institutional use of a quantified-self application called Pattern, which is designed to promote self-regulation and reflective learning in learners. This paper provides a brief look into how learners report spending their time and react to in-app recommendations. Initial data is encouraging; however, there are limitations of Pattern, and additional research and development must be undertaken.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {526–527},
numpages = {2},
keywords = {self-regulated learning, reflective learning practices, recommendation engine, real-time feedback, quantified-self student, mobile application, learning analytics, higher education},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3029433,
author = {Vigentini, Lorenzo and Le\'{o}n Urrutia, Manuel and Fields, Ben},
title = {FutureLearn data: what we currently have, what we are learning and how it is demonstrating learning in MOOCs},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029433},
doi = {10.1145/3027385.3029433},
abstract = {Compared to other platforms such as Coursera and EdX, FutureLearn is a relatively new player in the MOOC arena and received limited coverage in the Learning Analytics and Educational Data Mining research. Founded by a partnership between the Open University in the UK, the BBC, The British Library and (originally) 12 universities in the UK, FutureLearn has two distinctive features relevant to the way their data is displayed and analyzed: 1) it was designed with a specific educational philosophy in mind which focuses on the social dimension of learning and 2) every learning activity provide opportunities for formal discussion and commenting. This workshop provides an opportunity to invite contributions and connect individual and groups to share their research activities on an international stage. As the first of its kind, this workshop will bring in a number of scholars and practitioners, as well as data scientists and analyst involved in the reporting, researching and developments emerging from the data offered by the platform.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {512–513},
numpages = {2},
keywords = {visualization dashboard, learning analytics, MOOCs},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3029432,
author = {Martinez-Maldonado, Roberto and Hernandez-Leo, Davinia and Pardo, Abelardo and Ogata, Hiroaki},
title = {2nd cross-LAK: learning analytics across physical and digital spaces},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029432},
doi = {10.1145/3027385.3029432},
abstract = {Student's learning happens where the learner is, rather than being constrained to a single physical or digital environment. It is of high relevance for the LAK community to provide analytics support in blended learning scenarios where students can interact at diverse learning spaces and with a variety of educational tools. This workshop aims to gather the sub-community of LAK researchers, learning scientists and researchers in other areas, interested in the intersection between ubiquitous, mobile and/or classroom learning analytics. The underlying concern is how to integrate and coordinate learning analytics seeking to understand the particular pedagogical needs and context constraints to provide learning analytics support across digital and physical spaces. The goals of the workshop are to consolidate the Cross-LAK sub-community and provide a forum for idea generation that can build up further collaborations. The workshop will also serve to disseminate current work in the area by both producing proceedings of research papers and working towards a journal special issue.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {510–511},
numpages = {2},
keywords = {seamless learning, monitoring, learning analytics, integration},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3029431,
author = {Ringtved, Ulla and Milligan, Sandra and Corrin, Linda and Littlejohn, Allison and Law, Nancy},
title = {DesignLAK17: quality metrics and indicators for analytics of assessment design at scale},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029431},
doi = {10.1145/3027385.3029431},
abstract = {Notions of what constitutes quality in design in traditional on-campus or online teaching and learning may not always translate into scaled digital environments. The DesignLAK17 workshop builds on the DesignLAK16 workshop to explore one aspect of this theme, namely the opportunities arising from the use of analytics in scaled assessment design. New paradigms for learning design are exploiting the distinctive characteristics and potentials of analytics, trace data and newer kinds of sensory data usable on digital platforms to transform assessment. But, characteristics of quality assessment design need to be reconsidered, and new metrics for capturing quality are required. This symposium and workshop focuses on what might be appropriate quality metrics and indicators for assessment design in scaled learning. It aims to build a community of interest round the topic, to share perspectives, and to generate design and research ideas.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {508–509},
numpages = {2},
keywords = {scaled courses, learning design, learning at scale, learning analytics, feedback, assessment},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3029430,
author = {Wang, Yuan and Davis, Dan and Chen, Guanliang and Paquette, Luc},
title = {Workshop on integrated learning analytics of MOOC post-course development},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029430},
doi = {10.1145/3027385.3029430},
abstract = {MOOC research is typically limited to evaluations of learner behavior in the context of the learning environment. However, some research has begun to recognize that the impact of MOOCs may extend beyond the confines of the course platform or conclusion of the course time limit. This workshop aims to encourage our community of learning analytics researchers to examine the relationship between performance and engagement within the course and learner behavior and development beyond the course. This workshop intends to build awareness in the community regarding the importance of research measuring multi-platform activity and long-term success after taking a MOOC. We hope to build the community's understanding of what it takes to operationalize MOOC learner success in a novel context by employing data traces across the social web.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {506–507},
numpages = {2},
keywords = {massive online open courses, long-term learning development, learning outcomes, learning analytics, career development},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3029429,
author = {Clow, Doug and Ferguson, Rebecca and Kitto, Kirsty and Cho, Yong-Sang and Sharkey, Mike and Aguerrebere, Cecilia},
title = {Beyond failure: the 2nd LAK Failathon},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029429},
doi = {10.1145/3027385.3029429},
abstract = {The 2nd LAK Failathon will build on the successful event in 2016 and extend the workshop beyond discussing individual experiences of failure to exploring how the field can improve, particularly regarding the creation and use of evidence.Failure in research is an increasingly hot topic, with high-profile crises of confidence in the published research literature in medicine and psychology. Among the major factors in this research crisis are the many incentives to report and publish only positive findings. These incentives prevent the field in general from learning from negative findings, and almost entirely preclude the publication of mistakes and errors. Thus providing an alternative forum for practitioners and researchers to learn from each other's failures can be very productive. The first LAK Failathon, held in 2016, provided just such an opportunity for researchers and practitioners to share their failures and negative findings in a lower-stakes environment, to help participants learn from each other's mistakes. It was very successful, and there was strong support for running it as an annual event. This workshop will build on that success, with twin objectives to provide an environment for individuals to learn from each other's failures, and also to co-develop plans for how we as a field can better build and deploy our evidence base.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {504–505},
numpages = {2},
keywords = {learning from failure, learning analytics, evidence, analytics},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3029428,
author = {Mojarad, Shirin and Lewkow, Nicholas and Essa, Alfred and Zhang, Jie and Feild, Jacqueline},
title = {Quasi-experimental design for causal inference using Python and Apache Spark: a hands-on tutorial},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029428},
doi = {10.1145/3027385.3029428},
abstract = {Educational practitioners and policy makers require evidence supporting claims about educational efficacy. Evidence is often found using causal relationships between education inputs and student learning outcomes. Causal inference covers a wide range of topics in education research, including efficacy studies to prove if a new policy, software, curriculum or intervention is effective in improving student learning outcomes. Randomized controlled trials (RCT) are considered a gold standard method to demonstrate causality. However, these studies are expensive, timely and costly, as well as not being ethical to conduct in many educational contexts. Causality can also be deducted purely from observational data. In this tutorial, we will review methodologies for estimating the causal effects of education inputs on student learning outcomes using observational data. This is an inherently complex task due to many hidden variables and their interrelationships in educational research. In this tutorial, we discuss causal inference in the context of educational research with big data. This is the first tutorial of its kind at Learning Analytics and Knowledge Conference (LAK) that provides a hands-on experience with Python and Apache Spark as a practical tool for educational researchers to conduct causal inference. As a prerequisite, attendees are required to have familiarity with Python.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {502–503},
numpages = {2},
keywords = {quasiexperiment design, parallel computing, causal inference, big data, Python, Apache Spark},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3029427,
author = {Bergner, Yoav and Lang, Charles and Gray, Geraldine},
title = {Workshop on methodology in learning analytics (MLA)},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029427},
doi = {10.1145/3027385.3029427},
abstract = {Learning analytics is an interdisciplinary and inclusive field, a fact which makes the establishment of methodological norms both challenging and important. This community-building workshop intends to convene methodology-focused researchers to discuss new and established approaches, comment on the state of current practice, author pedagogical manuscripts, and co-develop guidelines to help move the field forward with quality and rigor.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {500–501},
numpages = {2},
keywords = {statistics, models, methodology, measurement, evaluation},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3029426,
author = {Macfadyen, Leah P. and Groth, Dennis and Rehrey, George and Shepard, Linda and Greer, Jim and Ward, Douglas and Bennett, Caroline and Kaupp, Jake and Molinaro, Marco and Steinwachs, Matt},
title = {Developing institutional learning analytics 'communities of transformation' to support student success},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029426},
doi = {10.1145/3027385.3029426},
abstract = {Institutional implementation of learning analytics calls for thoughtful management of cultural change. This interactive halfday workshop responds to the LA literature describing the benefits and challenges of institutional LA implementation by offering participants an opportunity to learn about and begin planning for a program to actively engage faculty as leaders of data exploration around the theme of 'student success'. This session will share experiences from five institutions actively engaged in fostering Learning Analytics Communities (LAC) by identifying key issues, sharing lessons learned, and considering structural frameworks that are transferable to other institutional contexts. Structured discussion and activities will engage participants in developing an action plan for establishing an LAC on their own campus.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {498–499},
numpages = {2},
keywords = {student success, learning analytics fellows program, institutional learning analytics, faculty engagement, communities of transformation, change management},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3029425,
author = {Knight, Simon and Allen, Laura and Gibson, Andrew and McNamara, Danielle and Buckingham Shum, Simon},
title = {Writing analytics literacy: bridging from research to practice},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029425},
doi = {10.1145/3027385.3029425},
abstract = {There is untapped potential in achieving the full impact of learning analytics through the integration of tools into practical pedagogic contexts. To meet this potential, more work must be conducted to support educators in developing learning analytics literacy. The proposed workshop addresses this need by building capacity in the learning analytics community and developing an approach to resourcing for building 'writing analytics literacy'.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {496–497},
numpages = {2},
keywords = {writing analytics, practitioner knowledge, learning analytics literacy, learning analytics, automated writing evaluation, analytics for action},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3029424,
author = {Tsai, Yi-Shan and Gasevic, Dragan and Mu\~{n}oz-Merino, Pedro J. and Dawson, Shane},
title = {LA policy: developing an institutional policy for learning analytics using the RAPID outcome mapping approach},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029424},
doi = {10.1145/3027385.3029424},
abstract = {This workshop aims to promote strategic planning for learning analytics in higher education through developing institutional policies. While adoption of learning analytics is predominantly seen in small-scale and bottom-up patterns, it is believed that a systemic implementation can bring the widest impact to the education system and lasting benefits to learners. However, the success of it highly depends on the adopted strategy that meets the needs of various stakeholders and systematically pushes the institution towards achieving its targets. It is imperative to develop a learning analytics policy that ensures a practice that is valid, effective and ethical.The workshop involves two components. The first component includes a set of presentations about the state of learning analytics in higher education, drawing on results from an Australian and a European project examining institutional learning analytics policy and adoption processes. The second component is an interactive session where participants are encouraged to share their motivations for adopting learning analytics and the diversity of challenges they perceive impede analytics adoption in their institution. Using the RAPID Outcome Mapping Approach (ROMA), participants will create a draft policy that articulates how the various challenges can be addressed. This workshop aims to further develop our understanding of how learning analytics operates in an organizational system and promote a cultural change in how such analytics are adopted in higher education.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {494–495},
numpages = {2},
keywords = {policy, learning analytics, higher education},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3027451,
author = {Holstein, Kenneth and McLaren, Bruce M. and Aleven, Vincent},
title = {Intelligent tutors as teachers' aides: exploring teacher needs for real-time analytics in blended classrooms},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027451},
doi = {10.1145/3027385.3027451},
abstract = {Intelligent tutoring systems (ITSs) are commonly designed to enhance student learning. However, they are not typically designed to meet the needs of teachers who use them in their classrooms. ITSs generate a wealth of analytics about student learning and behavior, opening a rich design space for real-time teacher support tools such as dashboards. Whereas real-time dashboards for teachers have become popular with many learning technologies, we are not aware of projects that have designed dashboards for ITSs based on a broad investigation of teachers' needs. We conducted design interviews with ten middle school math teachers to explore their needs for on-the-spot support during blended class sessions, as a first step in a user-centered design process of a real-time dashboard. Based on multi-methods analyses of this interview data, we identify several opportunities for ITSs to better support teachers' needs, noting that the analytics commonly generated by existing teacher support tools do not strongly align with the analytics teachers expect to be most useful. We highlight key tensions and tradeoffs in the design of such real-time supports for teachers, as revealed by "Speed Dating" possible futures with teachers. This paper has implications for our ongoing co-design of a real-time dashboard for ITSs, as well as broader implications for the design of ITSs that can effectively collaborate with teachers in classroom settings.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {257–266},
numpages = {10},
keywords = {teachers, real-time analytics, pedagogical decision-making, intelligent tutoring systems, classrooms, blended learning, adoption},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3027450,
author = {Holstein, Kenneth and McLaren, Bruce M. and Aleven, Vincent},
title = {SPACLE: investigating learning across virtual and physical spaces using spatial replays},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027450},
doi = {10.1145/3027385.3027450},
abstract = {Classroom experiments that evaluate the effectiveness of educational technologies do not typically examine the effects of classroom contextual variables (e.g., out-of-software help-giving and external distractions). Yet these variables may influence students' instructional outcomes. In this paper, we introduce the Spatial Classroom Log Explorer (SPACLE): a prototype tool that facilitates the rapid discovery of relationships between within-software and out-of-software events. Unlike previous tools for retrospective analysis, SPACLE replays moment-by-moment analytics about student and teacher behaviors in their original spatial context. We present a data analysis workflow using SPACLE and demonstrate how this workflow can support causal discovery. We share the results of our initial replay analyses using SPACLE, which highlight the importance of considering spatial factors in the classroom when analyzing ITS log data. We also present the results of an investigation into the effects of student-teacher interactions on student learning in K-12 blended classrooms, using our workflow, which combines replay analysis with SPACLE and causal modeling. Our findings suggest that students' awareness of being monitored by their teachers may promote learning, and that "gaming the system" behaviors may extend outside of educational software use.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {358–367},
numpages = {10},
keywords = {visualizations, teachers, intelligent tutoring systems, classroom, causal modeling, blended learning},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3027449,
author = {Hlosta, Martin and Zdrahal, Zdenek and Zendulka, Jaroslav},
title = {Ouroboros: early identification of at-risk students without models based on legacy data},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027449},
doi = {10.1145/3027385.3027449},
abstract = {This paper focuses on the problem of identifying students, who are at risk of failing their course. The presented method proposes a solution in the absence of data from previous courses, which are usually used for training machine learning models. This situation typically occurs in new courses. We present the concept of a "self-learner" that builds the machine learning models from the data generated during the current course. The approach utilises information about already submitted assessments, which introduces the problem of imbalanced data for training and testing the classification models.There are three main contributions of this paper: (1) the concept of training the models for identifying at-risk students using data from the current course, (2) specifying the problem as a classification task, and (3) tackling the challenge of imbalanced data, which appears both in training and testing data.The results show the comparison with the traditional approach of learning the models from the legacy course data, validating the proposed concept.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {6–15},
numpages = {10},
keywords = {student retention, self-learning, predictive analytics, learning analytics, imbalanced data},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3027448,
author = {Corrin, Linda and de Barba, Paula G. and Bakharia, Aneesha},
title = {Using learning analytics to explore help-seeking learner profiles in MOOCs},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027448},
doi = {10.1145/3027385.3027448},
abstract = {In online learning environments, learners are often required to be more autonomous in their approach to learning. In scaled online learning environments, like Massive Open Online Courses (MOOCs), there are differences in the ability of learners to access teachers and peers to get help with their study than in more traditional educational environments. This exploratory study examines the help-seeking behaviour of learners across several MOOCs with different audiences and designs. Learning analytics techniques (e.g., dimension reduction with t-sne and clustering with affinity propagation) were applied to identify clusters and determine profiles of learners on the basis of their help-seeking behaviours. Five help-seeking learner profiles were identified which provide an insight into how learners' help-seeking behaviour relates to performance. The development of a more in-depth understanding of how learners seek help in large online learning environments is important to inform the way support for learners can be incorporated into the design and facilitation of online courses delivered at scale.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {424–428},
numpages = {5},
keywords = {learning design, learning analytics, help-seeking, MOOCs},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3027447,
author = {Di Mitri, Daniele and Scheffel, Maren and Drachsler, Hendrik and B\"{o}rner, Dirk and Ternier, Stefaan and Specht, Marcus},
title = {Learning pulse: a machine learning approach for predicting performance in self-regulated learning using multimodal data},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027447},
doi = {10.1145/3027385.3027447},
abstract = {Learning Pulse explores whether using a machine learning approach on multimodal data such as heart rate, step count, weather condition and learning activity can be used to predict learning performance in self-regulated learning settings. An experiment was carried out lasting eight weeks involving PhD students as participants, each of them wearing a Fitbit HR wristband and having their application on their computer recorded during their learning and working activities throughout the day. A software infrastructure for collecting multimodal learning experiences was implemented. As part of this infrastructure a Data Processing Application was developed to pre-process, analyse and generate predictions to provide feedback to the users about their learning performance. Data from different sources were stored using the xAPI standard into a cloud-based Learning Record Store. The participants of the experiment were asked to rate their learning experience through an Activity Rating Tool indicating their perceived level of productivity, stress, challenge and abilities. These self-reported performance indicators were used as markers to train a Linear Mixed Effect Model to generate learner-specific predictions of the learning performance. We discuss the advantages and the limitations of the used approach, highlighting further development points.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {188–197},
numpages = {10},
keywords = {wearable enhanced learning, multimodal data, machine learning, learning analytics, biosensors},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3027446,
author = {Wise, Alyssa Friend and Cui, Yi and Jin, Wan Qi},
title = {Honing in on social learning networks in MOOC forums: examining critical network definition decisions},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027446},
doi = {10.1145/3027385.3027446},
abstract = {This study examines the impact of content-based network partitioning and tie definition on social network structures and interpretation for MOOC discussion forums. Using dynamic interrelated post and thread categorization [5] based on a previously developed natural language model [27], 817 threads containing 3124 discussion posts from 567 learners in a MOOC on the use of statistics in medicine were characterized as either related to the learning of course content or not. Content-related, non-content, and unpartitioned interaction networks were constructed based on five different tie definitions: Direct Reply, Star, Direct Reply+Star, Limited Copresence, and Total Copresence. Results showed content-related and non-content networks to have distinct characteristics at the network, community, and individual node levels, validating the usefulness of the content/non-content distinction as an analytic tool. Network properties were less sensitive to differences in tie definition with the exception of Total Copresence, which showed distinct characteristics presenting dangers for general use, but usefulness for detecting inflated social status due to "superthread" initiation.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {383–392},
numpages = {10},
keywords = {tie extraction, social network analysis, network partitioning, massive open online courses, discussion forum},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3027445,
author = {Allen, Laura K. and Perret, Cecile and Likens, Aaron and McNamara, Danielle S.},
title = {What'd you say again? recurrence quantification analysis as a method for analyzing the dynamics of discourse in a reading strategy tutor},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027445},
doi = {10.1145/3027385.3027445},
abstract = {In this study, we investigated the degree to which the cognitive processes in which students engage during reading comprehension could be examined through dynamical analyses of their natural language responses to texts. High school students (n = 142) generated typed self-explanations while reading a science text. They then completed a comprehension test that measured their comprehension at both surface and deep levels. The recurrent patterns of the words in students' self-explanations were first visualized in recurrence plots. These visualizations allowed us to qualitatively analyze the different self-explanation processes of skilled and less skilled readers. These recurrence plots then allowed us to calculate recurrence indices, which represented the properties of these temporal word patterns. Results of correlation and regression analyses revealed that these recurrence indices were significantly related to the students' comprehension scores at both surface- and deep levels. Additionally, when combined with summative metrics of word use, these indices were able to account for 32% of the variance in students' overall text comprehension scores. Overall, our results suggest that recurrence quantification analysis can be utilized to guide both qualitative and quantitative assessments of students' comprehension.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {373–382},
numpages = {10},
keywords = {stealth assessment, reading, natural language processing, intelligent tutoring systems, dynamics, corpus linguistics},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3027444,
author = {Aguerrebere, Cecilia and Cobo, Crist\'{o}bal and Gomez, Marcela and Mateu, Mat\'{\i}as},
title = {Strategies for data and learning analytics informed national education policies: the case of Uruguay},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027444},
doi = {10.1145/3027385.3027444},
abstract = {This work provides an overview of an education and technology monitoring system developed at Plan Ceibal, a nationwide initiative created to enable technology enhanced learning in Uruguay. Plan Ceibal currently offers one-to-one access to technology and connectivity to every student and teacher (from primary and secondary education) as well as a comprehensive set of educational software platforms. All these resources generate massive amounts of data about the progress and style of students learning. This work introduces the conceptual framework, design and preliminary results of the Big Data Center for learning analytics currently being developed at Plan Ceibal. This initiative is focused on exploiting these datasets and conducting advanced analytics to support the educational system. To this aim, a 360 degrees profile will be built including information characterizing the user's online behavior as well as a set of technology enhanced learning factors. These profiles will be studied both at user (e.g., student or teacher) and larger scale levels (e.g., per school or school system), addressing both the need of understanding how technology is being used for learning as well as to provide accurate feedback to support evidence based educational policies.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {449–453},
numpages = {5},
keywords = {technology enhanced learning, plan ceibal, education policies, big data},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3027443,
author = {Lau, Clarissa and Sinclair, Jeanne and Taub, Michelle and Azevedo, Roger and Jang, Eunice Eunhee},
title = {Transitioning self-regulated learning profiles in hypermedia-learning environments},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027443},
doi = {10.1145/3027385.3027443},
abstract = {Self-regulated learning (SRL) is a process that highly fluctuates as students actively deploy their metacognitive and cognitive processes during learning. In this paper, we apply an extension of latent profiling, latent transition analysis (LTA), which investigates the longitudinal development of students' SRL latent class memberships over time. We will briefly review the theoretical foundations of SRL and discuss the value of using LTA to investigate this multidimensional concept. This study is based on college students (n = 75) learning about the human circulatory system while using MetaTutor, an intelligent tutoring system that adaptively supports SRL and targets specific metacognitive SRL processes including judgment of learning (JOL) and content evaluation (CE). Preliminary results identify transitional probabilities of SRL profiles from four distinct events associated with the use of SRL.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {198–202},
numpages = {5},
keywords = {self-regulated learning, metamemory, metacognition, latent transition analysis, information-processing theory},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3027442,
author = {Cross, Sebastian and Waters, Zak and Kitto, Kirsty and Zuccon, Guido},
title = {Classifying help seeking behaviour in online communities},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027442},
doi = {10.1145/3027385.3027442},
abstract = {While help seeking has been extensively studied using self report survey data and models, there is a lack of content analysis techniques that can be directly applied to classify help seeking behaviour. In this preliminary work we propose a coding scheme which is then applied to an open dataset that we have created by carefully selecting sub groups from two popular discussion sites (Reddit and StackExchange). We then explore the possibility for automatically classifying help seeking behaviour using machine learning models. A preliminary model provides good initial results, suggesting that it may indeed be possible to construct student support systems that build off of an accurate classifier.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {419–423},
numpages = {5},
keywords = {open data, machine learning, help seeking, content analysis},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3027441,
author = {Diana, Nicholas and Eagle, Michael and Stamper, John and Grover, Shuchi and Bienkowski, Marie and Basu, Satabdi},
title = {An instructor dashboard for real-time analytics in interactive programming assignments},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027441},
doi = {10.1145/3027385.3027441},
abstract = {Many introductory programming environments generate a large amount of log data, but making insights from these data accessible to instructors remains a challenge. This research demonstrates that student outcomes can be accurately predicted from student program states at various time points throughout the course, and integrates the resulting predictive models into an instructor dashboard. The effectiveness of the dashboard is evaluated by measuring how well the dashboard analytics correctly suggest that the instructor help students classified as most in need. Finally, we describe a method of matching low-performing students with high-performing peer tutors, and show that the inclusion of peer tutors not only increases the amount of help given, but the consistency of help availability as well.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {272–279},
numpages = {8},
keywords = {peer tutors, machine learning, learning analytics, introductory programming, dashboards},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3027440,
author = {Azevedo, Roger and Millar, Garrett C. and Taub, Michelle and Mudrick, Nicholas V. and Bradbury, Amanda E. and Price, Megan J.},
title = {Using data visualizations to foster emotion regulation during self-regulated learning with advanced learning technologies: a conceptual framework},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027440},
doi = {10.1145/3027385.3027440},
abstract = {Emotions play a critical role during learning and problem solving with advanced learning technologies (ALTs). Despite their importance, relatively few attempts have been made to understand learners' emotional monitoring and regulation by using data visualizations of their own (and others') cognitive, affective, metacognitive, and motivational (CAMM) self-regulated learning (SRL) processes to potentially foster their emotion regulation (ER). We present a theoretically based and empirically driven conceptual framework that addresses ER by proposing the use of visualizations of one's own and others' CAMM SRL multichannel data to facilitate learners' monitoring and regulation of emotions during learning with ALTs. We use an example with eye-tracking data to illustrate the mapping between theoretical assumptions, ER strategies, and the types of data visualizations that can enhance learners' ER, including key processes such as emotion flexibility, emotion adaptivity, and emotion efficacy. We conclude with future directions leading to a systematic interdisciplinary research agenda that addresses outstanding ER-related issues by integrating models, theories, methods, and analytical techniques for the cognitive, learning, and affective sciences; human- computer interaction (HCI); data visualization; big data; data mining; and SRL.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {444–448},
numpages = {5},
keywords = {emotions, emotion regulation, advanced learning technologies},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3027439,
author = {Chiu, Ming Ming and Chow, Bonnie Wing-Yin and Joh, Sung Wook},
title = {How to assign students into sections to raise learning},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027439},
doi = {10.1145/3027385.3027439},
abstract = {Grouping students with similar past achievement together (tracking) might affect their reading achievement. Multilevel analyses of 208,057 fourth grade students in 40 countries showed that clustering students in schools by past achievement was linked to higher reading achievement, consistent with the benefits of customized, targeted instruction. Meanwhile, students had higher reading achievement with greater differences (variances) among classmates' past achievement, reading attitudes, or family SES; these results are consistent with the view that greater student differences yield more help opportunities (higher achievers help lower achievers, so that both learn), and foster learning from their different resources, attitudes and behaviors. Also, a student had higher reading achievement when classmates had more resources (SES, home educational resources, reading attitude, past achievement), suggesting that classmates shared their resources and helped one another. Modeling of non-linear relations and achievement subsamples of students supported the above interpretations. Principals can use these results and a simpler version of this methodology to re-allocate students and resources into different course sections at little cost to improve students' reading achievement.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {95–104},
numpages = {10},
keywords = {socioeconomic status, international assessment, inequality, classmates, ability grouping},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3027438,
author = {Slater, Stefan and Baker, Ryan and Almeda, Ma. Victoria and Bowers, Alex and Heffernan, Neil},
title = {Using correlational topic modeling for automated topic identification in intelligent tutoring systems},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027438},
doi = {10.1145/3027385.3027438},
abstract = {Student knowledge modeling is an important part of modern personalized learning systems, but typically relies upon valid models of the structure of the content and skill in a domain. These models are often developed through expert tagging of skills to items. However, content creators in crowdsourced personalized learning systems often lack the time (and sometimes the domain knowledge) to tag skills themselves. Fully automated approaches that rely on the covariance of correctness on items can lead to effective skill-item mappings, but the resultant mappings are often difficult to interpret. In this paper we propose an alternate approach to automatically labeling skills in a crowdsourced personalized learning system using correlated topic modeling, a natural language processing approach, to analyze the linguistic content of mathematics problems. We find a range of potentially meaningful and useful topics within the context of the ASSISTments system for mathematics problem-solving.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {393–397},
numpages = {5},
keywords = {topic modeling, natural language processing, mathematics education, intelligent tutoring systems, correlational topic modeling},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3027437,
author = {Agnihotri, Lalitha and Essa, Alfred and Baker, Ryan},
title = {Impact of student choice of content adoption delay on course outcomes},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027437},
doi = {10.1145/3027385.3027437},
abstract = {It is difficult for a student to succeed in a course without access to course materials and assignments; and yet, some students delay up to a month in obtaining access to these essential materials. Students delay buying material required for their course due to multiple reasons. Out of a concern for students with limited financial resources, some publishers offer a period of free courtesy access. But this may lead to students having access later in the course but then having a lapsed period until they pay for the materials after the courtesy access period ends. Not having key course materials early on probably hurts learning, but how much? In this paper, we investigate the question, "Does lack of access to instructional material impact student performance in blended learning courses?" Specifically, we analyze students who purchased and obtained access to online content at different points in the course. We determine that both types of failure to obtain access to course materials (delaying in signing up for the product, or signing up for a free trial and letting the trial period lapse without purchasing the materials) are associated with substantially worse student outcomes. Students who purchased the product within the first few days of class had the best scores (median 77). Those who waited two weeks before accessing the product did the worst (median 56, effect size Cliff's Delta=0.31 1). We conclude with a discussion of possible interventions and actions that can be taken to ameliorate the situation.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {16–20},
numpages = {5},
keywords = {procrastination, performance, effect size, content adoption delay},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3027436,
author = {Gibson, Andrew and Aitken, Adam and S\'{a}ndor, \'{A}gnes and Buckingham Shum, Simon and Tsingos-Lucas, Cherie and Knight, Simon},
title = {Reflective writing analytics for actionable feedback},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027436},
doi = {10.1145/3027385.3027436},
abstract = {Reflective writing can provide a powerful way for students to integrate professional experience and academic learning. However, writing reflectively requires high quality actionable feedback, which is time-consuming to provide at scale. This paper reports progress on the design, implementation, and validation of a Reflective Writing Analytics platform to provide actionable feedback within a tertiary authentic assessment context. The contributions are: (1) a new conceptual framework for reflective writing; (2) a computational approach to modelling reflective writing, deriving analytics, and providing feedback; (3) the pedagogical and user experience rationale for platform design decisions; and (4) a pilot in a student learning context, with preliminary data on educator and student acceptance, and the extent to which we can evidence that the software provided actionable feedback for reflective writing.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {153–162},
numpages = {10},
keywords = {reflective writing theory, reflective writing analytics, learning analytics, formative feedback},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3027435,
author = {Ocumpaugh, Jaclyn and Baker, Ryan S. and San Pedro, Maria O. C. Z. and Hawn, M. Aaron and Heffernan, Cristina and Heffernan, Neil and Slater, Stefan A.},
title = {Guidance counselor reports of the ASSISTments college prediction model (ACPM)},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027435},
doi = {10.1145/3027385.3027435},
abstract = {Advances in the learning analytics community have created opportunities to deliver early warnings that alert teachers and instructors when a student is at risk of not meeting academic goals [6], [71]. Alert systems have also been developed for school district leaders [33] and for academic advisors in higher education [39], but other professionals in the K-12 system, namely guidance counselors, have not been widely served by these systems. In this study, we use college enrollment models created for the ASSISTments learning system [55] to develop reports that target the needs of these professionals, who often work directly with students, but usually not in classroom settings. These reports are designed to facilitate guidance counselors' efforts to help students to set long term academic and career goals. As such, they provide the calculated likelihood that a student will attend college (the ASSISTments College Prediction Model or ACPM), alongside student engagement and learning measures. Using design principles from risk communication research and student feedback theories to inform a co-design process, we developed reports that can inform guidance counselor efforts to support student achievement.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {479–488},
numpages = {10},
keywords = {student engagement, stakeholder reports, predictive analytics, intelligent tutoring systems, guidance counselors, college attendance},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3027434,
author = {Ez-zaouia, Mohamed and Lavou\'{e}, Elise},
title = {EMODA: a tutor oriented multimodal and contextual emotional dashboard},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027434},
doi = {10.1145/3027385.3027434},
abstract = {Learners' emotional state has proven to be a key factor for successful learning. Visualizing learners' emotions during synchronous on-line learning activities can help tutors in creating and maintaining socio-affective relationships with their learners. However, few dashboards offer emotional information on the learning activity. The current study focuses on synchronous interactions via a videoconferencing tool dedicated to foreign language training. We collected data on learners' emotions in real conditions during ten sessions (five sessions for two learners). We propose to adopt and combine different models of emotions (discrete and dimensional) and to use heterogeneous APIs for measuring learners' emotions from different data sources (audio, video, self-reporting and interaction traces). Based on a thorough data analysis, we propose an approach to combine different cues to infer information on learners' emotional states. We finally present the EMODA dashboard, an affective multimodal and contextual visual analytics dashboard, which allows the tutor to monitor learners' emotions and better understand their evolution during the synchronous learning activity.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {429–438},
numpages = {10},
keywords = {tutor dashboard, multimodal data, learner monitoring, language training, interactive visualizations, emotions},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3027433,
author = {Knight, Simon and Martinez-Maldonado, Roberto and Gibson, Andrew and Buckingham Shum, Simon},
title = {Towards mining sequences and dispersion of rhetorical moves in student written texts},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027433},
doi = {10.1145/3027385.3027433},
abstract = {There is an increasing interest in the analysis of both student's writing and the temporal aspects of learning data. The analysis of higher-level learning features in writing contexts requires analyses of data that could be characterised in terms of the sequences and processes of textual features present. This paper (1) discusses the extant literature on sequential and process analyses of writing; and, based on this and our own first-hand experience on sequential analysis, (2) proposes a number of approaches to both pre-process and analyse sequences in whole-texts. We illustrate how the approaches could be applied to examples drawn from our own datasets of 'rhetorical moves' in written texts, and the potential each approach holds for providing insight into that data. Work is in progress to apply this model to provide empirical insights. Although, similar sequence or process mining techniques have not yet been applied to student writing, techniques applied to event data could readily be operationalised to undercover patterns in texts.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {228–232},
numpages = {5},
keywords = {writing analytics, text mining, temporal analysis, sequence mining, rhetorical moves, process mining, learning analytics, academic writing},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3027432,
author = {Xu, Zhenhua and Woodruff, Earl},
title = {Person-centered approach to explore learner's emotionality in learning within a 3D narrative game},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027432},
doi = {10.1145/3027385.3027432},
abstract = {Emotions form an integral part of our cognitive function. Past research has demonstrated conclusive associations between emotions and learning achievement [7, 26, 27]. This paper used a person-centered approach to explore students' (N = 65) facial behavior, emotions, learner traits and learning. An automatic facial expression recognition system was used to detect both middle school and university students' real-time facial movements while they learned scientific tasks in a 3D narrative video game.The results indicated a strong statistical relationship between three specific facial movements (i.e., outer brow raising, lip tightening and lip pressing), student self-regulatory learning strategy and learning performance. Outer brow raising (AU2) had strong predictive power when a student is confronted with obstacles and does not know how to proceed. Both lip tightening and pressing (AU23 and AU24) were predictive when a student engaged in a task that requires a deep level of incoming information processing and short memory activation. The findings also suggested a correlational relationship between student self-regulatory learning strategy use and neutral state. It is hoped that this study will provide empirical evidence for helping us develop a deeper understanding of the relationship between facial behavior and complex learning especially in educational games.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {439–443},
numpages = {5},
keywords = {scientific reasoning, learner traits, game-based learning, facial expression recognition, emotion, educational video games, complex learning, affect},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3027431,
author = {Mills, Caitlin and Fridman, Igor and Soussou, Walid and Waghray, Disha and Olney, Andrew M. and D'Mello, Sidney K.},
title = {Put your thinking cap on: detecting cognitive load using EEG during learning},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027431},
doi = {10.1145/3027385.3027431},
abstract = {Current learning technologies have no direct way to assess students' mental effort: are they in deep thought, struggling to overcome an impasse, or are they zoned out? To address this challenge, we propose the use of EEG-based cognitive load detectors during learning. Despite its potential, EEG has not yet been utilized as a way to optimize instructional strategies. We take an initial step towards this goal by assessing how experimentally manipulated (easy and difficult) sections of an intelligent tutoring system (ITS) influenced EEG-based estimates of students' cognitive load. We found a main effect of task difficulty on EEG-based cognitive load estimates, which were also correlated with learning performance. Our results show that EEG can be a viable source of data to model learners' mental states across a 90-minute session.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {80–89},
numpages = {10},
keywords = {intelligent tutoring systems, engagement, cognitive load, EEG},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3027430,
author = {Park, Jihyun and Denaro, Kameryn and Rodriguez, Fernando and Smyth, Padhraic and Warschauer, Mark},
title = {Detecting changes in student behavior from clickstream data},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027430},
doi = {10.1145/3027385.3027430},
abstract = {Student clickstream data can provide valuable insights about student activities in an online learning environment and how these activities inform their learning outcomes. However, given the noisy and complex nature of this data, an on-going challenge involves devising statistical techniques that capture clear and meaningful aspects of students' click patterns. In this paper, we utilize statistical change detection techniques to investigate students' online behaviors. Using clickstream data from two large university courses, one face-to-face and one online, we illustrate how this methodology can be used to detect when students change their previewing and reviewing behavior, and how these changes can be related to other aspects of students' activity and performance.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {21–30},
numpages = {10},
keywords = {student clickstream data, regression, poisson models, change detection},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3027429,
author = {Andrade, Alejandro},
title = {Understanding student learning trajectories using multimodal learning analytics within an embodied-interaction learning environment},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027429},
doi = {10.1145/3027385.3027429},
abstract = {The aim of this paper is to show how multimodal learning analytics (MMLA) can help understand how elementary students explore the concept of feedback loops while controlling an embodied simulation of a predator-prey ecosystem using hand movements as an interface with the computer simulation. We represent student motion patterns from fine-grained logs of hands and gaze data, and then map these observed motion patterns against levels of student performance to make inferences about how embodiment plays a role in the learning process. Results show five distinct motion sequences in students' embodied interactions, and these motion patterns are statistically associated with initial and post-tutorial levels of students' understanding of feedback loops. Analysis of student gaze also shows distinctive patterns as to how low- and high-performing students attended to information presented in the simulation. Using MMLA, we show how students' explanations of feedback loops look differently according to cluster membership, which provides evidence that embodiment interacts with conceptual understanding.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {70–79},
numpages = {10},
keywords = {sensing technologies, science education, multimodal learning analytics, learning environments, embodiment, embodied cognition},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3027428,
author = {Scheffel, Maren and Drachsler, Hendrik and Kreijns, Karel and de Kraker, Joop and Specht, Marcus},
title = {Widget, widget as you lead, I am performing well indeed! using results from an exploratory offline study to inform an empirical online study about a learning analytics widget in a collaborative learning environment},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027428},
doi = {10.1145/3027385.3027428},
abstract = {The collaborative learning processes of students in online learning environments can be supported by providing learning analytics-based visualisations that foster awareness and reflection about an individual's as well as the team's behaviour and their learning and collaboration processes. For this empirical study we implemented an activity widget into the online learning environment of a live five-months Master course and investigated the predictive power of the widget indicators towards the students' grades and compared the results to those from an exploratory study with data collected in previous runs of the same course where the widget had not been in use. Together with information gathered from a quantitative as well as a qualitative evaluation of the activity widget during the course, the findings of this current study show that there are indeed predictive relations between the widget indicators and the grades, especially those regarding responsiveness, and indicate that some of the observed differences in the last run could be attributed to the implemented activity widget.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {289–298},
numpages = {10},
keywords = {tool evaluation, statistical analysis, learning analytics},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3027427,
author = {Spann, Catherine A. and Schaeffer, James and Siemens, George},
title = {Expanding the scope of learning analytics data: preliminary findings on attention and self-regulation using wearable technology},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027427},
doi = {10.1145/3027385.3027427},
abstract = {The ability to pay attention and self-regulate is a fundamental skill required of learners of all ages. Learning analytics researchers have to date relied on data generated by a computing system (such as a learning management system, click stream or log data) to examine learners' self-regulatory abilities. The development of wearable computing through fitness trackers, watches, heart rate monitors, and clinical grade devices such as Empatica's E4 wristband now provides researchers with access to biometric data as students interact with learning content or software systems. This level of data collection promises to provide valuable insight into cognitive and affective experiences of individuals, especially when combined with traditional learning analytics data sources. Our study details the use of wearable technologies to assess the relationship between heart rate variability and the self-regulatory abilities of an individual. This is relevant for the field of learning analytics as methods become more complex and the assessment of learner performance becomes more nuanced and attentive to the affective factors that contribute to learner success.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {203–207},
numpages = {5},
keywords = {wearable technology, self-regulation, psychophysiology, heart-rate variability, attention},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3027426,
author = {Huptych, Michal and Bohuslavek, Michal and Hlosta, Martin and Zdrahal, Zdenek},
title = {Measures for recommendations based on past students' activity},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027426},
doi = {10.1145/3027385.3027426},
abstract = {This paper introduces two measures for the recommendation of study materials based on students' past study activity. We use records from the Virtual Learning Environment (VLE) and analyse the activity of previous students. We assume that the activity of past students represents patterns, which can be used as a basis for recommendations to current students.The measures we define are Relevance, for description of a supposed VLE activity derived from previous students of the course, and Effort, that represents the actual effort of individual current students. Based on these measures, we propose a composite measure, which we call Importance.We use data from the previous course presentations to evaluate of the consistency of students' behaviour. We use correlation of the defined measures Relevance and Average Effort to evaluate the behaviour of two different student cohorts and the Root Mean Square Error to measure the deviation of Average Effort and individual student Effort.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {404–408},
numpages = {5},
keywords = {student retention, relevance, recommendation, learning strategy, learning analytics, effort},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3027425,
author = {Peffer, Melanie E. and Kyle, Kristopher},
title = {Assessment of language in authentic science inquiry reveals putative differences in epistemology},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027425},
doi = {10.1145/3027385.3027425},
abstract = {Science epistemology, or beliefs about what it means to do science and how science knowledge is generated, is an integral part of authentic science inquiry. Although the development of a sophisticated science epistemology is critical for attaining science literacy, epistemology remains an elusive construct to precisely and quantitatively evaluate. Previous work has suggested that analysis of student practices in science inquiry, such as their use of language, may be reflective of their underlying epistemologies. Here we describe the usage of a learning analytics tool, TAALES, and keyness analysis to analyze the concluding statements made by students at the end of a computer-based authentic science inquiry experience. Preliminary results indicate that linguistic analysis reveals differences in domain-general lexical sophistication and in domain-specific verb usage that are consistent with the expertise level of the participant. For example, experts tend to use more hedging language such as "may" and "support" during conclusions whereas novices use stronger language such as "cause." Using these differences, a simple, rule-based prediction algorithm with LOOCV achieved prediction accuracies of greater than 80%. These data underscore the potential for the use of learning analytics in simulated authentic inquiry to provide a novel and valuable method of assessing inquiry practices and related epistemologies.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {138–142},
numpages = {5},
keywords = {science practices, science epistemology, science classroom inquiry simulations, lexical sophistication, authentic science inquiry, assessment, TAALES},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3027422,
author = {K\"{a}ser, Tanja and Hallinen, Nicole R. and Schwartz, Daniel L.},
title = {Modeling exploration strategies to predict student performance within a learning environment and beyond},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027422},
doi = {10.1145/3027385.3027422},
abstract = {Modeling and predicting student learning is an important task in computer-based education. A large body of work has focused on representing and predicting student knowledge accurately. Existing techniques are mostly based on students' performance and on timing features. However, research in education, psychology and educational data mining has demonstrated that students' choices and strategies substantially influence learning. In this paper, we investigate the impact of students' exploration strategies on learning and propose the use of a probabilistic model jointly representing student knowledge and strategies. Our analyses are based on data collected from an interactive computer-based game. Our results show that exploration strategies are a significant predictor of the learning outcome. Furthermore, the joint models of performance and knowledge significantly improve the prediction accuracy within the game as well as on external post-test data, indicating that this combined representation provides a better proxy for learning.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {31–40},
numpages = {10},
keywords = {strategies, simulations, probabilistic student models, prediction, learning},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3027421,
author = {Kopeinik, Simone and Lex, Elisabeth and Seitlinger, Paul and Albert, Dietrich and Ley, Tobias},
title = {Supporting collaborative learning with tag recommendations: a real-world study in an inquiry-based classroom project},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027421},
doi = {10.1145/3027385.3027421},
abstract = {In online social learning environments, tagging has demonstrated its potential to facilitate search, to improve recommendations and to foster reflection and learning.Studies have shown that shared understanding needs to be established in the group as a prerequisite for learning. We hypothesise that this can be fostered through tag recommendation strategies that contribute to semantic stabilization. In this study, we investigate the application of two tag recommenders that are inspired by models of human memory: (i) the base-level learning equation BLL and (ii) Minerva. BLL models the frequency and recency of tag use while Minerva is based on frequency of tag use and semantic context. We test the impact of both tag recommenders on semantic stabilization in an online study with 56 students completing a group-based inquiry learning project in school. We find that displaying tags from other group members contributes significantly to semantic stabilization in the group, as compared to a strategy where tags from the students' individual vocabularies are used. Testing for the accuracy of the different recommenders revealed that algorithms using frequency counts such as BLL performed better when individual tags were recommended. When group tags were recommended, the Minerva algorithm performed better. We conclude that tag recommenders, exposing learners to each other's tag choices by simulating search processes on learners' semantic memory structures, show potential to support semantic stabilization and thus, inquiry-based learning in groups.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {409–418},
numpages = {10},
keywords = {technology enhanced learning, semantic stabilization, real-world testing, personalized tag recommendations, minerva, inquiry-based learning, cognitive user models, base-level learning equation},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3027420,
author = {Quigley, David and Ostwald, Jonathan and Sumner, Tamara},
title = {Scientific modeling: using learning analytics to examine student practices and classroom variation},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027420},
doi = {10.1145/3027385.3027420},
abstract = {Modeling has a strong focus in current science learning frameworks as a critical skill for students to learn. However, understanding students' scientific models and their modeling practices at scale is a difficult task that has not been taken up by the research literature. The complex variables involved in classroom learning, such as teacher differences, increase the difficulty of understanding this problem. This work begins with an exploration of the methods used to explore students' scientific modeling in the learning sciences space and the frameworks developed to characterize student modeling practices. Learning analytics can be used to leverage these frameworks of scientific modeling practices to explore questions around students' scientific models and their modeling practices. These analyses are focused around the use of EcoSurvey, a collaborative, digital tool used in high-school biology classrooms to model the local ecosystem. This tool was deployed in ten biology classrooms and used with varying degrees of success. There are significant teacher-level differences found in the activity sequences of students using the EcoSurvey tool. The theoretical metrics around scientific modeling practices and automatically extracted feature sequences were also used in a classification task to automatically determine a particular student's teacher. These results underline the power of learning analytics methods to give insight into how modeling practices are realized in the classroom. This work also informs changes to modeling tools, associated curricula, and supporting professional development around scientific modeling.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {329–338},
numpages = {10},
keywords = {teacher differences, scientific modeling, collaborative modeling, classification},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3027419,
author = {Whitelock-Wainwright, Alexander and Ga\v{s}evi\'{c}, Dragan and Tejeiro, Ricardo},
title = {What do students want? towards an instrument for students' evaluation of quality of learning analytics services},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027419},
doi = {10.1145/3027385.3027419},
abstract = {Quality assurance in any organization is important for ensuring that service users are satisfied with the service offered. For higher education institutes, the use of service quality measures allows for ideological gaps to be both identified and resolved. The learning analytic community, however, has rarely addressed the concept of service quality. A potential outcome of this is the provision of a learning analytics service that only meets the expectations of certain stakeholders (e.g., managers), whilst overlooking those who are most important (e.g., students). In order to resolve this issue, we outline a framework and our current progress towards developing a scale to assess student expectations and perceptions of learning analytics as a service.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {368–372},
numpages = {5},
keywords = {service quality, learning analytics, action research},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3027418,
author = {Koester, Benjamin P. and Fogel, James and Murdock, William and Grom, Galina and McKay, Timothy A.},
title = {Building a transcript of the future},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027418},
doi = {10.1145/3027385.3027418},
abstract = {The pathways and learning outcomes of university students are the culmination of numerous experiences inside and outside of the classroom, with faculty and with other students, in both formal and casual settings. These interactions are guided by the general education requirements of the university and by the learning goals of the student. The only official record and representation of each student's education is captured by their academic transcript: typically a list of courses described by name and number, grades recorded on an A-F scale and summarized by GPA, degrees awarded, and honors received. This limited approach reflects the technological affordances of a 20th century industrial age. In recent years, scholars have begun to imagine a transcript of the future, perhaps combining a richer record of the student experience along with a portfolio of authentic products of student work. In this paper, we concentrate on first, and develop analytic methods for improving measures of both classroom performance and intellectual breadth. In each case, this is done by placing elements of individual transcripts in context using information about their peers. We frame the study by addressing basic questions. Were the courses taken by the student difficult on average? Did the individual stand out from their peers? Were the courses representative of a broad intellectual experience, or did the student delve into detail in the chosen field of study? And with whom did they take courses?},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {299–308},
numpages = {10},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3027417,
author = {Donnelly, Patrick J. and Blanchard, Nathaniel and Olney, Andrew M. and Kelly, Sean and Nystrand, Martin and D'Mello, Sidney K.},
title = {Words matter: automatic detection of teacher questions in live classroom discourse using linguistics, acoustics, and context},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027417},
doi = {10.1145/3027385.3027417},
abstract = {We investigate automatic detection of teacher questions from audio recordings collected in live classrooms with the goal of providing automated feedback to teachers. Using a dataset of audio recordings from 11 teachers across 37 class sessions, we automatically segment the audio into individual teacher utterances and code each as containing a question or not. We train supervised machine learning models to detect the human-coded questions using high-level linguistic features extracted from automatic speech recognition (ASR) transcripts, acoustic and prosodic features from the audio recordings, as well as context features, such as timing and turn-taking dynamics. Models are trained and validated independently of the teacher to ensure generalization to new teachers. We are able to distinguish questions and non-questions with a weighted F1 score of 0.69. A comparison of the three feature sets indicates that a model using linguistic features outperforms those using acoustic-prosodic and context features for question detection, but the combination of features yields a 5% improvement in overall accuracy compared to linguistic features alone. We discuss applications for pedagogical research, teacher formative assessment, and teacher professional development.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {218–227},
numpages = {10},
keywords = {question detection, natural language processing, classroom analytics, automatic speech recognition},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3027416,
author = {Yeomans, Michael and Reich, Justin},
title = {Planning prompts increase and forecast course completion in massive open online courses},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027416},
doi = {10.1145/3027385.3027416},
abstract = {Among all of the learners in Massive Open Online Courses (MOOCs) who intend to complete a course, the majority fail to do so. This intention-action gap is found in many domains of human experience, and research in similar goal pursuit domains suggests that plan-making is a cheap and effective nudge to encourage follow-through. In a natural field experiment in three HarvardX courses, some students received open-ended planning prompts at the beginning of a course. These prompts increased course completion by 29%, and payment for certificates by 40%. This effect was largest for students enrolled in traditional schools. Furthermore, the contents of students' plans could predict which students were least likely to succeed - in particular, students whose plans focused on specific times were unlikely to complete the course. Our results suggest that planning prompts can help learners adopted productive frames of mind at the outset of a learning goal that encourage and forecast student success.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {464–473},
numpages = {10},
keywords = {natural language processing, motivation, learning analytics, goal pursuit, decision-making, MOOCs},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3027415,
author = {Hsiao, I-Han and Huang, Po-Kai and Murphy, Hannah},
title = {Uncovering reviewing and reflecting behaviors from paper-based formal assessment},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027415},
doi = {10.1145/3027385.3027415},
abstract = {In this paper, we study students' learning effectiveness through their use of a homegrown educational technology, Web Programming Grading Assistant (WPGA), which facilitates grading and feedback delivery of paper-based assessments. We designed a classroom study and collected data from a lower-division blended-instruction computer science class. We tracked and modeled students' reviewing and reflecting behaviors from WPGA. The results show that students demonstrated an effort and desire to review assessments regardless of if they were graded for academic performance or for attendance. Hardworking students achieved higher exam scores on average and were found to review their exams and the correct questions frequently. Additionally, student cohorts exhibited similar initial reviewing patterns, but different in-depth reviewing and reflecting strategies. Ultimately, this work contributes to the aggregation of multidimensional learning analytics across the physical and cybersphere.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {319–328},
numpages = {10},
keywords = {reflection, programming learning, orchestration technology, feedback, cross LAK, computing education, blended instruction classes},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3027414,
author = {Hoel, Tore and Griffiths, Dai and Chen, Weiqin},
title = {The influence of data protection and privacy frameworks on the design of learning analytics systems},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027414},
doi = {10.1145/3027385.3027414},
abstract = {Learning analytics open up a complex landscape of privacy and policy issues, which, in turn, influence how learning analytics systems and practices are designed. Research and development is governed by regulations for data storage and management, and by research ethics. Consequently, when moving solutions out the research labs implementers meet constraints defined in national laws and justified in privacy frameworks. This paper explores how the OECD, APEC and EU privacy frameworks seek to regulate data privacy, with significant implications for the discourse of learning, and ultimately, an impact on the design of tools, architectures and practices that now are on the drawing board. A detailed list of requirements for learning analytics systems is developed, based on the new legal requirements defined in the European General Data Protection Regulation, which from 2018 will be enforced as European law. The paper also gives an initial account of how the privacy discourse in Europe, Japan, South-Korea and China is developing and reflects upon the possible impact of the different privacy frameworks on the design of LA privacy solutions in these countries. This research contributes to knowledge of how concerns about privacy and data protection related to educational data can drive a discourse on new approaches to privacy engineering based on the principles of Privacy by Design. For the LAK community, this study represents the first attempt to conceptualise the issues of privacy and learning analytics in a cross-cultural context. The paper concludes with a plan to follow up this research on privacy policies and learning analytics systems development with a new international study.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {243–252},
numpages = {10},
keywords = {privacy frameworks, privacy by design, personal information, learning analytics systems design, learning analytics process requirements, learning analytics, data protection by design, data protection by default, data protection},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3027413,
author = {Avila, Cecilia and Baldiris, Silvia and Fabregat, Ramon and Graf, Sabine},
title = {ATCE: an analytics tool to trace the creation and evaluation of inclusive and accessible open educational resources},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027413},
doi = {10.1145/3027385.3027413},
abstract = {The creation of Inclusive and Accessible Open Educational Resources (IA-OERs) is a challenge for teachers because they have to invest time and effort to create learning contents considering students' learning needs and preferences. An IA-OER is characterized by its alignment with the Universal Design Learning (UDL) principles, the quality on its contents and the web accessibility as a way to address the diversity of students. Creating an IA-OER with these characteristics is not a straightforward task, especially when teachers do not have enough information/feedback to make decisions on how to improve the learning contents. In this paper we introduce ATCE - an Analytics Tool to Trace the Creation and Evaluation of IA-OERs. This tool focuses in particular on the accessibility and quality of the IA-OERs. ATCE was developed as a module within the ATutor Learning Management System (LMS). An analytics dashboard with visualizations related to the teachers' competences in the creation and evaluation of IA-OERs was included as part of the tool. This paper also presents a use case of the visualizations obtained from the creation and evaluation of one IA-OER after using our analytics tool.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {183–187},
numpages = {5},
keywords = {web accessibility, teachers, quality, open educational resources, learning analytics, competences},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3027412,
author = {Adjei, Seth A. and Botelho, Anthony F. and Heffernan, Neil T.},
title = {Sequencing content in an adaptive testing system: the role of choice},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027412},
doi = {10.1145/3027385.3027412},
abstract = {The effect of choice on student achievement and engagement has been an extensively researched area of learning analytics. Current research findings suggest a positive relationship between choice and varied outcome measures, but little has been reported to indicate whether these findings hold in the context of Intelligent Tutoring Systems (ITS). In this paper, we report the results of a randomized controlled experiment in which we investigate the effect of student choice on assignment completion and future achievement in an ITS. The experimental design uses three conditions to observe the effect of choice. In the first condition, students are able to choose the order in which to complete assignments, while in the second condition, students are prescribed an intuitive order in which to complete assignments. Those in the third condition were prescribed a counter-intuitive order in which to complete assignments. Results indicate that allowing students to choose the order in which to work on assignments leads to higher completion rates and better achievement at posttest. A post-hoc analysis also revealed that even considering students with similar completion rates, those given choice had higher posttest scores than those observed in any other condition. These results seem to support the many theories of the positive effect of choice on student achievement.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {178–182},
numpages = {5},
keywords = {student choice, remediation assignments, mastery learning, PLACements, ASSISTments},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3027411,
author = {Davis, Dan and Jivet, Ioana and Kizilcec, Ren\'{e} F. and Chen, Guanliang and Hauff, Claudia and Houben, Geert-Jan},
title = {Follow the successful crowd: raising MOOC completion rates through social comparison at scale},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027411},
doi = {10.1145/3027385.3027411},
abstract = {Social comparison theory asserts that we establish our social and personal worth by comparing ourselves to others. In in-person learning environments, social comparison offers students critical feedback on how to behave and be successful. By contrast, online learning environments afford fewer social cues to facilitate social comparison. Can increased availability of such cues promote effective self-regulatory behavior and achievement in Massive Open Online Courses (MOOCs)? We developed a personalized feedback system that facilitates social comparison with previously successful learners based on an interactive visualization of multiple behavioral indicators. Across four randomized controlled trials in MOOCs (overall N = 33, 726), we find: (1) the availability of social comparison cues significantly increases completion rates, (2) this type of feedback benefits highly educated learners, and (3) learners' cultural context plays a significant role in their course engagement and achievement.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {454–463},
numpages = {10},
keywords = {social comparison, massive open online course, learning analytics, framing, feedback, cultural differences},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3027410,
author = {Lang, Charles},
title = {Opportunities for personalization in modeling students as Bayesian learners},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027410},
doi = {10.1145/3027385.3027410},
abstract = {The following paper is a proof-of-concept demonstration of a novel Bayesian framework for making inferences about individual students and the context in which they are learning. It has implications for both efforts to automate personalized instruction and to probabilistically model educational context. By modelling students as Bayesian learners, individuals who weigh their prior belief against current circumstantial data to reach conclusions, it becomes possible to both generate estimates of performance and the impact of the educational environment in probabilistic terms. This framework is tested through a Bayesian algorithm that can be used to characterize student prior knowledge in course material and predict student performance. This is demonstrated using both simulated data. The algorithm generates estimates that behave qualitatively as expected on simulated data and predict student performance substantially better than chance. A discussion of the results and the conceptual benefits of the framework follow.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {41–45},
numpages = {5},
keywords = {personalization, individualization, context modelling, Bayes},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3027409,
author = {Nguyen, Quan and Rienties, Bart and Toetenel, Lisette},
title = {Unravelling the dynamics of instructional practice: a longitudinal study on learning design and VLE activities},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027409},
doi = {10.1145/3027385.3027409},
abstract = {Substantial progress has been made in understanding how teachers design for learning. However, there remains a paucity of evidence of the actual students' response towards leaning designs. Learning analytics has the power to provide just-in-time support, especially when predictive analytics is married with the way teachers have designed their course, or so-called a learning design. This study investigates how learning designs are configured over time and their impact on student activities by analyzing longitudinal data of 38 modules with a total of 43,099 registered students over 30 weeks at the Open University UK, using social network analysis and panel data analysis. Our analysis unpacked dynamic configurations of learning designs between modules over time, which allows teachers to reflect on their practice in order to anticipate problems and make informed interventions. Furthermore, by controlling for the heterogeneity between modules, our results indicated that learning designs were able to explain up to 60% of the variability in student online activities, which reinforced the importance of pedagogical context in learning analytics.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {168–177},
numpages = {10},
keywords = {social network analysis, panel data analysis, longitudinal, learning design, learning analytics},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3027408,
author = {Mouri, Kousuke and Ogata, Hiroaki and Uosaki, Noriko},
title = {Learning analytics in a seamless learning environment},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027408},
doi = {10.1145/3027385.3027408},
abstract = {This paper describes seamless learning analytics methods of VASCORLL (Visualization and Analysis System for COnnecting Relationships of Learning Logs). VASCORLL is a system for visualizing and analyzing the learning logs collected by the seamless learning system, which supports language learning in the real-world. As far, several studies have been made in the seamless learning environments in order to bridge formal learning over informal learning. However, their focus was the implementation of the seamless learning environment in education. This study focuses on visualizing and analyzing learning logs collected in the seamless learning environment. This paper describes how our analytics could contribute to bridging the gap between formal and informal learning. An experiment was conducted to evaluate 1) whether our developed VASCORLL is effective in connecting the words learned in formal learning to the ones learned in informal learning, 2) which social network algorithm is effective to enhance learning in the seamless learning environment. Twenty international students participated in the evaluation experiment, and they were able to increase their learning opportunities by using VASCORLL. In addition, it was found that the betweenness centrality is useful in finding central words bridging formal and informal learning.1},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {348–357},
numpages = {10},
keywords = {ubiquitous learning, seamless learning, learning analytics},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3027407,
author = {Fu, Xinyu and Shimada, Atsushi and Ogata, Hiroaki and Taniguchi, Yuta and Suehiro, Daiki},
title = {Real-time learning analytics for C programming language courses},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027407},
doi = {10.1145/3027385.3027407},
abstract = {Many universities choose the C programming language (C) as the first one they teach their students, early on in their program. However, students often consider programming courses difficult, and these courses often have among the highest dropout rates of computer science courses offered. It is therefore critical to provide more effective instruction to help students understand the syntax of C and prevent them losing interest in programming. In addition, homework and paper-based exams are still the main assessment methods in the majority of classrooms. It is difficult for teachers to grasp students' learning situation due to the large amount of evaluation work. To facilitate teaching and learning of C, in this article we propose a system---LAPLE (Learning Analytics in Programming Language Education)---that provides a learning dashboard to capture the behavior of students in the classroom and identify the different difficulties faced by different students looking at different knowledge. With LAPLE, teachers may better grasp students' learning situation in real time and better improve educational materials using analysis results. For their part, novice undergraduate programmers may use LAPLE to locate syntax errors in C and get recommendations from educational materials on how to fix them.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {280–288},
numpages = {9},
keywords = {programming education, learning dashboard, learning analytics, information visualization, C programming},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3027406,
author = {Prinsloo, Paul and Slade, Sharon},
title = {An elephant in the learning analytics room: the obligation to act},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027406},
doi = {10.1145/3027385.3027406},
abstract = {As higher education increasingly moves to online and digital learning spaces, we have access not only to greater volumes of student data, but also to increasingly fine-grained and nuanced data. A significant body of research and existing practice are used to convince key stakeholders within higher education of the potential of the collection, analysis and use of student data to positively impact on student experiences in these environments. Much of the recent focus in learning analytics is around predictive modeling and uses of artificial intelligence to both identify learners at risk, and to personalize interventions to increase the chance of success.In this paper we explore the moral and legal basis for the obligation to act on our analyses of student data. The obligation to act entails not only the protection of student privacy and the ethical collection, analysis and use of student data, but also, the effective allocation of resources to ensure appropriate and effective interventions to increase effective teaching and learning.The obligation to act is, however tempered by a number of factors, including inter and intra-departmental operational fragmentation and the constraints imposed by changing funding regimes. Increasingly higher education institutions allocate resources in areas that promise the greatest return. Choosing (not) to respond to the needs of specific student populations then raises questions regarding the scope and nature of the moral and legal obligation to act. There is also evidence that students who are at risk of failing often do not respond to institutional interventions to assist them.In this paper we build and expand on recent research by, for example, the LACE and EP4LA workshops to conceptually map the obligation to act which flows from both higher education's mandate to ensure effective and appropriate teaching and learning and its fiduciary duty to provide an ethical and enabling environment for students to achieve success. We examine how the collection and analysis of student data links to both the availability of resources and the will to act and also to the obligation to act. Further, we examine how that obligation unfolds in two open distance education providers from the perspective of a key set of stakeholders - those in immediate contact with students and their learning journeys - the tutors or adjunct faculty.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {46–55},
numpages = {10},
keywords = {obligation to act ethics, learning analytics},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3027405,
author = {Dawson, Shane and Jovanovic, Jelena and Ga\v{s}evi\'{c}, Dragan and Pardo, Abelardo},
title = {From prediction to impact: evaluation of a learning analytics retention program},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027405},
doi = {10.1145/3027385.3027405},
abstract = {Learning analytics research has often been touted as a means to address concerns regarding student retention outcomes. However, few research studies to date, have examined the impact of the implemented intervention strategies designed to address such retention challenges. Moreover, the methodological rigor of some of the existing studies has been challenged. This study evaluates the impact of a pilot retention program. The study contrasts the findings obtained by the use of different methods for analysis of the effect of the intervention. The pilot study was undertaken between 2012 and 2014 resulting in a combined enrolment of 11,160 students. A model to predict attrition was developed, drawing on data from student information system, learning management system interactions, and assessment. The predictive model identified some 1868 students as academically at-risk. Early interventions were implemented involving learning and remediation support. Common statistical methods demonstrated a positive association between the intervention and student retention. However, the effect size was low. The use of more advanced statistical methods, specifically mixed-effect methods explained higher variability in the data (over 99%), yet found the intervention had no effect on the retention outcomes. The study demonstrates that more data about individual differences is required to not only explain retention but to also develop more effective intervention approaches.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {474–478},
numpages = {5},
keywords = {student retention, predictive models, mixed-effects model, learning analytics, early alert systems},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3027404,
author = {Poquet, Oleksandra and Dawson, Shane and Dowell, Nia},
title = {How effective is your facilitation? group-level analytics of MOOC forums},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027404},
doi = {10.1145/3027385.3027404},
abstract = {The facilitation of interpersonal relationships within a respectful learning climate is an important aspect of teaching practice. However, in large-scale online contexts, such as MOOCs, the number of learners and highly asynchronous nature militates against the development of a sense of belonging and dyadic trust. Given these challenges, instead of conventional instruments that reflect learners' affective perceptions, we suggest a set of indicators that can be used to evaluate social activity in relation to the participation structure. These group-level indicators can then help teachers to gain insights into the evolution of social activity shaped by their facilitation choices. For this study, group-level indicators were derived from measuring information exchange activity between the returning MOOC posters. By conceptualizing this group as an identity-based community, we can apply exponential random graph modelling to explain the network's structure through the configurations of direct reciprocity, triadic-level exchange, and the effect of participants demonstrating super-posting behavior. The findings provide novel insights into network amplification, and highlight the differences between the courses with different facilitation strategies. Direct reciprocation was characteristic of non-facilitated groups. Exchange at the level of triads was more prominent in highly facilitated online communities with instructor's involvement. Super-posting activity was less pronounced in networks with higher triadic exchange, and more pronounced in networks with higher direct reciprocity.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {208–217},
numpages = {10},
keywords = {indicators of social activity, forum, facilitation, MOOCs, ERGM},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3027403,
author = {Bodily, Robert and Verbert, Katrien},
title = {Trends and issues in student-facing learning analytics reporting systems research},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027403},
doi = {10.1145/3027385.3027403},
abstract = {We conducted a literature review on systems that track learning analytics data (e.g., resource use, time spent, assessment data, etc.) and provide a report back to students in the form of visualizations, feedback, or recommendations. This review included a rigorous article search process; 945 articles were identified in the initial search. After filtering out articles that did not meet the inclusion criteria, 94 articles were included in the final analysis. Articles were coded on five categories chosen based on previous work done in this area: functionality, data sources, design analysis, perceived effects, and actual effects. The purpose of this review is to identify trends in the current student-facing learning analytics reporting system literature and provide recommendations for learning analytics researchers and practitioners for future work.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {309–318},
numpages = {10},
keywords = {student-facing systems, literature review, learning analytics dashboards, learning analytics, educational recommender systems},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3027402,
author = {Gadiraju, Ujwal and Dietze, Stefan},
title = {Improving learning through achievement priming in crowdsourced information finding microtasks},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027402},
doi = {10.1145/3027385.3027402},
abstract = {Crowdsourcing has become an increasingly popular means to acquire human input on demand. Microtask crowdsourcing market-places facilitate the access to millions of people (called workers) who are willing to participate in tasks in return for monetary rewards or other forms of compensation. This paradigm presents a unique learning context where workers have to learn to complete tasks on-the-fly by applying their learning immediately through the course of tasks. However, most workers typically dropout early in large batches of tasks, depriving themselves of the opportunity to learn on-the-fly through the course of batch completion. By doing so workers squander a potential chance at improving their performance and completing tasks effectively. In this paper, we propose a novel method to engage and retain workers, to improve their learning in crowdsourced information finding tasks by using achievement priming. Through rigorous experimental findings, we show that it is possible to retain workers in long batches of tasks by triggering their inherent motivation to achieve and excel. As a consequence of increased worker retention, we find that workers learn to perform more effectively, depicting relatively more stable accuracy and lower task completion times in comparison to workers who drop out early.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {105–114},
numpages = {10},
keywords = {retention, microtasks, learning, information finding, crowdsourcing, crowd workers, achievement priming},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3027401,
author = {Martinez-Maldonado, Roberto and Power, Tamara and Hayes, Carolyn and Abdiprano, Adrian and Vo, Tony and Axisa, Carmen and Buckingham Shum, Simon},
title = {Analytics meet patient manikins: challenges in an authentic small-group healthcare simulation classroom},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027401},
doi = {10.1145/3027385.3027401},
abstract = {Healthcare simulations are hands-on learning experiences aimed at allowing students to practice essential skills that they may need when working with real patients in clinical workplaces. Some clinical classrooms are equipped with patient manikins that can respond to actions or that can be programmed to deteriorate over time. Students can perform assessments and interventions, and enhance their critical thinking and communication skills. There is an opportunity to exploit the students' digital traces that these manikins can pervasively capture to make key aspects of the learning process visible. The setting can be augmented with sensors to capture traces of group interaction. These multimodal data can be used to generate visualisations or feedback for students or teachers. This paper reports on an authentic classroom study using analytics to integrate multimodal data of students' interactions with the manikins and their peers in simulation scenarios. We report on the challenges encountered in deploying such analytics 'in the wild', using an analysis framework that considers the social, epistemic and physical dimensions of collocated group activity.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {90–94},
numpages = {5},
keywords = {multimodal, groupwork, face-to-face, classroom, awareness},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3027400,
author = {Tsai, Yi-Shan and Gasevic, Dragan},
title = {Learning analytics in higher education --- challenges and policies: a review of eight learning analytics policies},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027400},
doi = {10.1145/3027385.3027400},
abstract = {This paper presents the results of a review of eight policies for learning analytics of relevance for higher education, and discusses how these policies have tried to address prominent challenges in the adoption of learning analytics, as identified in the literature. The results show that more considerations need to be given to establishing communication channels among stakeholders and adopting pedagogy-based approaches to learning analytics. It also reveals the shortage of guidance for developing data literacy among end-users and evaluating the progress and impact of learning analytics. Moreover, the review highlights the need to establish formalised guidelines to monitor the soundness, effectiveness, and legitimacy of learning analytics. As interest in learning analytics among higher education institutions continues to grow, this review will provide insights into policy and strategic planning for the adoption of learning analytics.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {233–242},
numpages = {10},
keywords = {strategy, policy, learning analytics, higher education, code of practice, challenge},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3027399,
author = {Crossley, Scott and Liu, Ran and McNamara, Danielle},
title = {Predicting math performance using natural language processing tools},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027399},
doi = {10.1145/3027385.3027399},
abstract = {A number of studies have demonstrated links between linguistic knowledge and performance in math. Studies examining these links in first language speakers of English have traditionally relied on correlational analyses between linguistic knowledge tests and standardized math tests. For second language (L2) speakers, the majority of studies have compared math performance between proficient and non-proficient speakers of English. In this study, we take a novel approach and examine the linguistic features of student language while they are engaged in collaborative problem solving within an on-line math tutoring system. We transcribe the students' speech and use natural language processing tools to extract linguistic information related to text cohesion, lexical sophistication, and sentiment. Our criterion variables are individuals' pretest and posttest math performance scores. In addition to examining relations between linguistic features of student language production and math scores, we also control for a number of non-linguistic factors including gender, age, grade, school, and content focus (procedural versus conceptual). Linear mixed effect modeling indicates that non-linguistic factors are not predictive of math scores. However, linguistic features related to cohesion affect and lexical proficiency explained approximately 30% of the variance (R2 = .303) in the math scores.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {339–347},
numpages = {9},
keywords = {sentiment analysis, predictive analytics, on-line tutoring systems, natural language processing, educational data mining},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3027398,
author = {Kovanovi\'{c}, Vitomir and Joksimovi\'{c}, Sre\'{c}ko and Katerinopoulos, Philip and Michail, Charalampos and Siemens, George and Ga\v{s}evi\'{c}, Dragan},
title = {Developing a MOOC experimentation platform: insights from a user study},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027398},
doi = {10.1145/3027385.3027398},
abstract = {In 2011, the phenomenon of MOOCs had swept the world of education and put online education in the focus of the public discourse around the world. Although researchers were excited with the vast amounts of MOOC data being collected, the benefits of this data did not stand to the expectations due to several challenges. The analyses of MOOC data are very time-consuming and labor-intensive, and require and require a highly advanced set of technical skills, often not available to the education researchers. Because of this MOOC data analyses are rarely done before the courses end, limiting the potential of data to impact the student learning outcomes and experience.In this paper we introduce MOOCito (MOOC intervention tool), a user-friendly software platform for the analysis of MOOC data, that focuses on conducting data-informed instructional interventions and course experimentations. We cover important design principles behind MOOCito and provide an overview of the trends in MOOC research leading to its development. Although a work-in-progress, in this paper, we outline the prototype of MOOCito and the results of a user evaluation study that focused on system's perceived usability and ease-of-use. The results of the study are discussed, as well as their practical implications.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {1–5},
numpages = {5},
keywords = {user study, technology acceptance model, controlled experiments, analysis platform, MOOCs, Coursera, A/B testing},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3027397,
author = {Herodotou, Christothea and Rienties, Bart and Boroowa, Avinash and Zdrahal, Zdenek and Hlosta, Martin and Naydenova, Galina},
title = {Implementing predictive learning analytics on a large scale: the teacher's perspective},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027397},
doi = {10.1145/3027385.3027397},
abstract = {In this paper, we describe a large-scale study about the use of predictive learning analytics data with 240 teachers in 10 modules at a distance learning higher education institution. The aim of the study was to illuminate teachers' uses and practices of predictive data, in particular identify how predictive data was used to support students at risk of not completing or failing a module. Data were collected from statistical analysis of 17,033 students' performance by the end of the intervention, teacher usage statistics, and five individual semi-structured interviews with teachers. Findings revealed that teachers endorse the use of predictive data to support their practice yet in diverse ways and raised the need for devising appropriate intervention strategies to support students at risk.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {267–271},
numpages = {5},
keywords = {teachers, retention, predictive analytics, perceptions, higher education},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3027396,
author = {Ferguson, Rebecca and Clow, Doug},
title = {Where is the evidence? a call to action for learning analytics},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027396},
doi = {10.1145/3027385.3027396},
abstract = {Where is the evidence for learning analytics? In particular, where is the evidence that it improves learning in practice? Can we rely on it? Currently, there are vigorous debates about the quality of research evidence in medicine and psychology, with particular issues around statistical good practice, the 'file drawer effect', and ways in which incentives for stakeholders in the research process reward the quantity of research produced rather than the quality. In this paper, we present the Learning Analytics Community Exchange (LACE) project's Evidence Hub, an effort to relate research evidence in learning analytics to four propositions about learning analytics: whether they support learning, support teaching, are deployed widely, and are used ethically. Surprisingly little evidence in this strong, specific sense was found, and very little was negative (7%, N=123), suggesting that learning analytics is not immune from the pressures in other areas. We explore the evidence in one particular area in detail (whether learning analytics improve teaching and learners support in the university sector), and set out some of the weaknesses of the evidence available. We conclude that there is considerable scope for improving the evidence base for learning analytics, and set out some suggestions of ways for various stakeholders to achieve this.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {56–65},
numpages = {10},
keywords = {validity, reliability, learning analytics cycle, generalisability, evidence hub, evidence, ethics, access},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3027395,
author = {Mutahi, Juliet and Kinai, Andrew and Bore, Nelson and Diriye, Abdigani and Weldemariam, Komminist},
title = {Studying engagement and performance with learning technology in an African classroom},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027395},
doi = {10.1145/3027385.3027395},
abstract = {In this paper, we study the engagement and performance of students in a classroom using a system the Cognitive Learning Companion (CLC). CLC is designed to keep track of the relationship between the student, content interaction and learning progression. It also provides evidence-based engagement-oriented actionable insights to teachers by assessing information from a sensor-rich instrumented learning environment in order to infer a learner's cognitive and affective states. Data captured from the instrumented environment is aggregated and analyzed to create interlinked insights helping teachers identify how students engage with learning content and view their performance records on selected assignments. We conducted a 1 month pilot with 27 learners in a primary school in Nairobi, Kenya during their maths and science instructional periods. We present our primary analysis of content-level interactions and engagement at the individual student and classroom level.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {148–152},
numpages = {5},
keywords = {mobile development, learning analytics, engagement, education, developing countries},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3027394,
author = {Ullmann, Thomas Daniel},
title = {Reflective writing analytics: empirically determined keywords of written reflection},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027394},
doi = {10.1145/3027385.3027394},
abstract = {Despite their importance for educational practice, reflective writings are still manually analysed and assessed, posing a constraint on the use of this educational technique. Recently, research started to investigate automated approaches for analysing reflective writing. Foundational to many automated approaches is the knowledge of words that are important for the genre. This research presents keywords that are specific to several categories of a reflective writing model. These keywords have been derived from eight datasets, which contain several thousand instances using the log-likelihood method. Both performance measures, the accuracy and the Cohen's κ, for these keywords were estimated with ten-fold cross validation. The results reached an accuracy of 0.78 on average for all eight categories and a fair to good interrater reliability for most categories even though it did not make use of any sophisticated rule-based mechanisms or machine learning approaches. This research contributes to the development of automated reflective writing analytics that are based on data-driven empirical foundations.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {163–167},
numpages = {5},
keywords = {reflective writing analytics, reflective writing, natural language processing, automated detection of reflection},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3027393,
author = {Brown, Michael Geoffrey and DeMonbrun, R. Matthew and Teasley, Stephanie D.},
title = {Don't call it a comeback: academic recovery and the timing of educational technology adoption},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027393},
doi = {10.1145/3027385.3027393},
abstract = {Recent research using learning analytics data to explore student performance over the course of a term suggests that a substantial percentage of students who are classified as academically struggling manage to recover. In this study, we report the result of a hazard analysis based on students' behavioral engagement with different digital instructional technologies over the course of a semester. We observe substantially different adoption and use behavior between students who did and did not experience academic difficulty in the course. Students who experienced moderate academic difficulty benefited the most from using tools that helped them plan their study behaviors. Students who experienced more severe academic difficulty benefited from tools that helped them prepare for exams. We observed that students adopted most tools and system features before they experienced academic difficulty, and students who adopted early were more likely to recover.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {489–493},
numpages = {5},
keywords = {undergraduate education, technology adoption, educational technology, early warning systems},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3027392,
author = {Arnold, Kimberly E. and Sclater, Niall},
title = {Student perceptions of their privacy in leaning analytics applications},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027392},
doi = {10.1145/3027385.3027392},
abstract = {Over the past five years, ethics and privacy around student data have become major topics of conversation in the learning analytics field. However, the majority of these have been theoretical in nature. The authors of this paper posit that more direct student engagement needs to be undertaken, and initial data from institutions beginning this process is shared. We find that, while the majority of respondents are accepting of the use of their data by their institutions, approval varies depending on the proposed purpose of the analytics. There also appear to be notable variations between students enrolled at United Kingdom and American institutions.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {66–69},
numpages = {4},
keywords = {privacy, learning analytics, higher education, ethics},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3027391,
author = {Boroujeni, Mina Shirvani and Hecking, Tobias and Hoppe, H. Ulrich and Dillenbourg, Pierre},
title = {Dynamics of MOOC discussion forums},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027391},
doi = {10.1145/3027385.3027391},
abstract = {In this integrated study of dynamics in MOOCs discussion forums, we analyze the interplay of temporal patterns, discussion content, and the social structure emerging from the communication using mixed methods. A special focus is on the yet under-explored aspect of time dynamics and influence of the course structure on forum participation. Our analyses show dependencies between the course structure (video opening time and assignment deadlines) and the over-all forum activity whereas such a clear link could only be partially observed considering the discussion content. For analyzing the social dimension we apply role modeling techniques from social network analysis. While the types of user roles based on connection patterns are relatively stable over time, the high fluctuation of active contributors lead to frequent changes from active to passive roles during the course. However, while most users do not create many social connections they can play an important role in the content dimension triggering discussions on the course subject. Finally, we show that forum activity level can be predicted one week in advance based on the course structure, forum activity history and attributes of the communication network which enables identification of periods when increased tutor supports in the forum is necessary.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {128–137},
numpages = {10},
keywords = {temporal analysis, social network, massive open online courses, discussion forum, content analysis, MOOCs},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3027390,
author = {Hubbard, Ryan and Sipolins, Aldis and Zhou, Lin},
title = {Enhancing learning through virtual reality and neurofeedback: a first step},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027390},
doi = {10.1145/3027385.3027390},
abstract = {Virtual reality presents exciting new prospects for the delivery of educational materials to students. By combining this technology with biological sensors, a student in a virtual educational environment can be monitored for physiological markers of engagement or more cognitive states of learning. With this information, the virtual reality environment can be adaptively altered to reflect the student's state, essentially creating a closed-loop feedback system. This paper explores these concepts, and presents preliminary data on a combined EEG-VR working memory experiment as a first step toward a broader implementation of an intelligent adaptive learning system. This first-pass neural time-series and oscillatory data suggest that while an EEG-based neurofeedback system is feasible, more work on removing artifacts and identifying relevant and important features will lead to higher prediction accuracy.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {398–403},
numpages = {6},
keywords = {virtual reality, neurofeedback, human-computer interaction, EEG},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3027389,
author = {Haythornthwaite, Caroline},
title = {An information policy perspective on learning analytics},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027389},
doi = {10.1145/3027385.3027389},
abstract = {Policy for learning analytics joins a stream of initiatives aimed at understanding the expanding world of information collection, storage, processing and dissemination that is being driven by computing technologies. This paper offers a information policy perspective on learning analytics, joining work by others on ethics and privacy in the management of learning analytics data [8], but extending to consider how issues play out across the information lifecycle and in the formation of policy. Drawing on principles from information policy both informs learning analytics and brings learning analytics into the information policy domain. The resulting combination can help inform policy development for educational institutions as they implement and manage learning analytics policy and practices. The paper begins with a brief summary of the information policy perspective, then addresses learning analytics with attention to various categories of consideration for policy development.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {253–256},
numpages = {4},
keywords = {learning analytics, information policy, educational policy},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3027388,
author = {Aghababyan, Ani and Lewkow, Nicholas and Baker, Ryan},
title = {Exploring the asymmetry of metacognition},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027388},
doi = {10.1145/3027385.3027388},
abstract = {People in general and students in particular have a tendency to misinterpret their own abilities. Some tend to underestimate their skills, while others tend to overestimate them. This paper investigates the degree to which metacognition is asymmetric in real-world learning and examines the change of a students' confidence over the course of a semester and its impact on the students' academic performance.Our findings, conducted using 129,644 students learning in eight courses within the LearnSmart platform, indicate that poor or unrealistic metacognition is asymmetric. These students are biased in one direction: they are more likely to be overconfident than underconfident. Additionally, while the examination of the temporal aspects of confidence reveals no significant change throughout the semester, changes are more apparent in the first and the last few weeks of the course. More specifically, there is a sharp increase in underconfidence and a simultaneous decrease in realistic evaluation toward the end of the semester. Finally, both overconfidence and underconfidence seem to be correlated with students' overall course performance. An increase in overconfidence is related to higher overall performance, while an increase in underconfidence is associated with lower overall performance.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {115–119},
numpages = {5},
keywords = {performance, metacognition, learnign analytics, discipline difference, confidence, big data, achievement},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3027387,
author = {Bote-Lorenzo, Miguel L. and G\'{o}mez-S\'{a}nchez, Eduardo},
title = {Predicting the decrease of engagement indicators in a MOOC},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027387},
doi = {10.1145/3027385.3027387},
abstract = {Predicting the decrease of students' engagement in typical MOOC tasks such as watching lecture videos or submitting assignments is key to trigger timely interventions in order to try to avoid the disengagement before it takes place. This paper proposes an approach to build the necessary predictive models using students' data that becomes available during a course. The approach was employed in an experimental study to predict the decrease of three different engagement indicators in a MOOC. The results suggest its feasibility with values of area under the curve for different predictors ranging from 0.718 to 0.914.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {143–147},
numpages = {5},
keywords = {supervised machine learning, engagement, MOOC},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3027386,
author = {Lee, Alwyn Vwen Yen and Tan, Seng Chee},
title = {Temporal analytics with discourse analysis: tracing ideas and impact on communal discourse},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027386},
doi = {10.1145/3027385.3027386},
abstract = {This paper presents a study of temporal analytics and discourse analysis of an online discussion, through investigation of a group of 13 in-service teachers and 2 instructors. A discussion forum consisting of 281 posts on an online collaborative learning environment was investigated. A text-mining tool was used to discover keywords from the discourse, and through social network analysis based on these keywords, a significant presence of relevant and promising ideas within discourse was revealed. However, uncovering the key ideas alone is insufficient to clearly explain students' level of understanding regarding the discussed topics. A more thorough analysis was thus performed by using temporal analytics with step-wise discourse analysis to trace the ideas and determine their impact on communal discourse. The results indicated that most ideas within the discourse could be traced to the origin of a set of improvable ideas, which impacted and also increased the community's level of interest in sharing and discussing ideas through discourse.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {120–127},
numpages = {8},
keywords = {temporality, social network analysis, learning analytics, idea measurement, discourse analysis},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@proceedings{10.1145/3027385,
title = {LAK '17: Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The theme for LAK'17 purposely focused on the transdisciplinary nature of research in learning analytics. This theme extends the work of prior conferences that sought to bring together the diversity of disciplinary fields that now comprise learning analytics. The great diversity of papers submitted for LAK'17 demonstrates that LA research has very much embraced the benefits that can be leveraged from a truly transdisciplinary model of research. While there are inherent complexities in such an approach, the research presented for LAK'17 brings much excitement and promise to the field through the application of novel methods, cutting-edge learning technologies, and actual impact on the learning process. Following this theme, the aim of the conference is to provide a forum for presentation, exchange and discussion of research and practices regarding the transdisciplinary field of Learning Analytics.},
location = {Vancouver, British Columbia, Canada}
}

@inproceedings{10.1145/2883851.2883968,
author = {Berg, Alan and Scheffel, Maren and Drachsler, Hendrik and Ternier, Stefaan and Specht, Marcus},
title = {The dutch xAPI experience},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883968},
doi = {10.1145/2883851.2883968},
abstract = {We present the collected experiences since 2012 of the Dutch Special Interest Group (SIG) for Learning Analytics in the application of the xAPI standard. We have been experimenting and exchanging best practices around the application of xAPI in various contexts. The practices include different design patterns centered around Learning Record Stores. We present three projects that apply xAPI in very different ways and publish a consistent set of xAPI recipes.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {544–545},
numpages = {2},
keywords = {xAPI, learning record store, learning analytics, data standardization, data silos},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883967,
author = {Liu, Ran and Patel, Rony and Koedinger, Kenneth R.},
title = {Modeling common misconceptions in learning process data},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883967},
doi = {10.1145/2883851.2883967},
abstract = {Student mistakes are often not random but, rather, reflect thoughtful yet incorrect strategies. In order for educational technologies to make full use of students' performance data to estimate the knowledge of a student, it is important to model not only the conceptions but also the misconceptions that a student's particular pattern of successes and errors may indicate. The student models that drive the "outer loop" of Intelligent Tutoring Systems typically do not represent or track misconceptions. Here, we present a method of representing misconceptions in the Knowledge Component models, or Q-Matrices, that are used by student models to estimate latent knowledge. We show, in a case study on a fraction arithmetic dataset, that incorporating a misconception into the Knowledge Component model dramatically improves the overall model's fit to data. We also derive qualitative insights from comparing predicted learning curves across models that incorporate varying misconception-related parameters. Finally, we show that the inclusion of a misconception in the Knowledge Component model can yield individual student estimates of misconception strength that are significantly correlated with out-of-tutor measures of student errors.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {369–377},
numpages = {9},
keywords = {q-matrix, misconceptions, knowledge component model, fraction arithmetic, additive factors model},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883966,
author = {Hu, Xiao and Ip, Jason and Sadaful, Koossulraj and Lui, George and Chu, Sam},
title = {Wikiglass: a learning analytic tool for visualizing collaborative wikis of secondary school students},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883966},
doi = {10.1145/2883851.2883966},
abstract = {This demo presents Wikiglass, a learning analytic tool for visualizing the statistics and timelines of collaborative Wikis built by secondary school students during their group project in inquiry-based learning. The tool adopts a modular structure for the flexibility of reuse with different data sources. The client side is built with the Model-View-Controller framework and the AngularJS library whereas the server side manages the database and data sources. The tool is currently used by secondary teachers in Hong Kong and is undergoing evaluation and improvement.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {550–551},
numpages = {2},
keywords = {wiki, visualization, timeline, statistics, collaborative writing},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883965,
author = {Tan, Jennifer Pei-Ling and Yang, Simon and Koh, Elizabeth and Jonathan, Christin},
title = {Fostering 21st century literacies through a collaborative critical reading and learning analytics environment: user-perceived benefits and problematics},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883965},
doi = {10.1145/2883851.2883965},
abstract = {The affordances of learning analytics (LA) are being increasingly harnessed to enhance 21st century (21C) pedagogy and learning. Relatively rare, however, are use cases and empirically based understandings of students' actual experiences with LA tools and environments at fostering 21C literacies, especially in secondary schooling and Asian education contexts. This paper addresses this knowledge gap by 1) presenting a first iteration design of a computer-supported collaborative critical reading and LA environment and its 16-week implementation in a Singapore high school; and 2) foregrounding students' quantitative and qualitative accounts of the benefits and problematics associated with this learning innovation. We focus the analytic lens on the LA dashboard components that provided visualizations of students' reading achievement, 21C learning dispositions, critical literacy competencies and social learning network positioning within the class. The paper aims to provide insights into the potentialities, paradoxes and pathways forward for designing LA that take into consideration the voices of learners as critical stakeholders.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {430–434},
numpages = {5},
keywords = {learning analytics, critical literacy, CSCL, 21st century skills},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883964,
author = {Wang, Xu and Wen, Miaomiao and Ros\'{e}, Carolyn P.},
title = {Towards triggering higher-order thinking behaviors in MOOCs},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883964},
doi = {10.1145/2883851.2883964},
abstract = {With the aim of better scaffolding discussion to improve learning in a MOOC context, this work investigates what kinds of discussion behaviors contribute to learning. We explored whether engaging in higher-order thinking behaviors results in more learning than paying general or focused attention to course materials. In order to evaluate whether to attribute the effect to engagement in the associated behaviors versus persistent characteristics of the students, we adopted two approaches. First, we used propensity score matching to pair students who exhibit a similar level of involvement in other course activities. Second, we explored individual variation in engagement in higher-order thinking behaviors across weeks. The results of both analyses support the attribution of the effect to the behavioral interpretation. A further analysis using LDA applied to course materials suggests that more social oriented topics triggered richer discussion than more biopsychology oriented topics.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {398–407},
numpages = {10},
keywords = {regression analysis, propensity score matching, learning analytics, discussion, coding manual, LDA topic modeling},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883963,
author = {Hu, Xiao and Ng, Tzi-Dong Jeremy and Tian, Lu and Lei, Chi-Un},
title = {Automating assessment of collaborative writing quality in multiple stages: the case of wiki},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883963},
doi = {10.1145/2883851.2883963},
abstract = {This study attempts to investigate to what extent indicators of academic writing and cognitive thinking can help measure the writing quality of group collaborative writings on Wikis. Particularly, comparisons were made on Wiki content in different stages of the projects. Preliminary results from a multiple linear regression analysis reveal that linguistic indicators such as engagement markers and self-mention were significant predictors in earlier stages to the projects, whereas verbs indicating cognitive thinking in the evaluation level were significant in later project stages.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {518–519},
numpages = {2},
keywords = {wiki, metadiscourse, automated assessment},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883961,
author = {K\"{a}ser, Tanja and Klingler, Severin and Gross, Markus},
title = {When to stop? towards universal instructional policies},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883961},
doi = {10.1145/2883851.2883961},
abstract = {The adaptivity of intelligent tutoring systems relies on the accuracy of the student model and the design of the instructional policy. Recently an instructional policy has been presented that is compatible with all common student models. In this work we present the next step towards a universal instructional policy. We introduce a new policy that is applicable to an even wider range of student models including DBNs modeling skill topologies and forgetting. We theoretically and empirically compare our policy to previous policies. Using synthetic and real world data sets we show that our policy can effectively handle wheel-spinning students as well as forgetting across a wide range of student models.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {289–298},
numpages = {10},
keywords = {wheel-spinning, student modeling, noisy data, instructional policies, individualization},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883960,
author = {Dillon, John and Ambrose, G. Alex and Wanigasekara, Nirandika and Chetlur, Malolan and Dey, Prasenjit and Sengupta, Bikram and D'Mello, Sidney K.},
title = {Student affect during learning with a MOOC},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883960},
doi = {10.1145/2883851.2883960},
abstract = {This paper presents affect data collected from periodic emotion detection surveys throughout an introductory Statistics MOOC called "I Heart Stats." This is the first MOOC, to our knowledge, to capture valuable student affect data through self-reported surveys. To collect student affect, we used two self-reporting methods: (1) The Self-Assessment Manikin and (2) A discrete emotion list. We found that the most common reported MOOC emotion was Hope followed by Enjoyment and Contentment. There were substantial shifts in affective states over the course, notably with Anxiety and Pride. The most valuable result of our study is a preliminary description of the methods for collecting self-reported student affect at scale in a MOOC setting.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {528–529},
numpages = {2},
keywords = {technology and learning, data collection, affect},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883959,
author = {Hu, Xiao and Zhang, Yinfei and Chu, Samuel K. W. and Ke, Xiaobo},
title = {Towards personalizing an e-quiz bank for primary school students: an exploration with association rule mining and clustering},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883959},
doi = {10.1145/2883851.2883959},
abstract = {Given the importance of reading proficiency and habits for young students, an online e-quiz bank, Reading Battle, was launched in 2014 to facilitate reading improvement for primary-school students. With more than ten thousand questions in both English and Chinese, the system has attracted nearly five thousand learners who have made about half a million question answering records. In an effort towards delivering personalized learning experience to the learners, this study aims to discover potentially useful knowledge from learners' reading and question answering records in the Reading Battle system, by applying association rule mining and clustering analysis. The results show that learners could be grouped into three clusters based on their self-reported reading habits. The rules mined from different learner clusters can be used to develop personalized recommendations to the learners. Implications of the results on evaluating and further improving the Reading Battle system are also discussed.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {25–29},
numpages = {5},
keywords = {reading, e-quiz bank, clustering, association rule mining},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883958,
author = {Hagood, Danielle and Ching, Cynthia Carter and Schaefer, Sara},
title = {Integrating physical activity data in videogames with user-centered dashboards},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883958},
doi = {10.1145/2883851.2883958},
abstract = {To promote healthy awareness and activity learning, we gave 12-to 14-year-old youth activity monitors (Fitbits) to track their physical activity, which was then integrated into a videogame we created. The players' real-world steps transform into in-game resources needed for gameplay. In addition to requiring real-world steps for various in-game activities, a dashboard in this game presents visual representations of activity patterns, ostensibly informing students about patterns of their own activity. In this paper and poster, we discuss challenges in initial designs of our dashboard. We present findings and challenges in the process of creating a user-centered dashboard and conclude with our future design goals.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {530–531},
numpages = {2},
keywords = {sociocultural theory, quantified self, health, dashboards, activity monitors (Fitbit)},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883957,
author = {Koedinger, Kenneth R. and McLaughlin, Elizabeth A. and Jia, Julianna Zhuxin and Bier, Norman L.},
title = {Is the doer effect a causal relationship? how can we tell and why it's important},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883957},
doi = {10.1145/2883851.2883957},
abstract = {The "doer effect" is an association between the number of online interactive practice activities students' do and their learning outcomes that is not only statistically reliable but has much higher positive effects than other learning resources, such as watching videos or reading text. Such an association suggests a causal interpretation--more doing yields better learning--which requires randomized experimentation to most rigorously confirm. But such experiments are expensive, and any single experiment in a particular course context does not provide rigorous evidence that the causal link will generalize to other course content. We suggest that analytics of increasingly available online learning data sets can complement experimental efforts by facilitating more widespread evaluation of the generalizability of claims about what learning methods produce better student learning outcomes. We illustrate with analytics that narrow in on a causal interpretation of the doer effect by showing that doing within a course unit predicts learning of that unit content more than doing in units before or after. We also provide generalizability evidence across four different courses involving over 12,500 students that the learning effect of doing is about six times greater than that of reading.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {388–397},
numpages = {10},
keywords = {prediction, learning engineering, learn by doing, doer effect, course effectiveness},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883956,
author = {Milligan, Sandra and He, Jiazhen and Bailey, James and Zhang, Rui and Rubinstein, Benjamin I. P},
title = {Validity: a framework for cross-disciplinary collaboration in mining indicators of learning from MOOC forums},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883956},
doi = {10.1145/2883851.2883956},
abstract = {Two research teams from the University of Melbourne's Learning Analytics Research Group used validation as applied in educational measurement to provide a framework for collaboration. One team was focussed on defining and building measures of learning capability of MOOCs participants, and the other on using topic modelling to discover topics in MOOC forums. The collaboration explored the suitability of items discovered from MOOC forums using topic modelling as measures of learning capability of participants in MOOCs.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {546–547},
numpages = {2},
keywords = {validity, topic modelling, rasch analysis, non-negative matrix factorisation, measurement theory, learning progression, learning analytics, learner performance, crowd-sourced learning, collaborative learning, MOOC},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883955,
author = {Buckingham Shum, Simon and S\'{a}ndor, \'{A}gnes and Goldsmith, Rosalie and Wang, Xiaolong and Bass, Randall and McWilliams, Mindy},
title = {Reflecting on reflective writing analytics: assessment challenges and iterative evaluation of a prototype tool},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883955},
doi = {10.1145/2883851.2883955},
abstract = {When used effectively, reflective writing tasks can deepen learners' understanding of key concepts, help them critically appraise their developing professional identity, and build qualities for lifelong learning. As such, reflecting writing is attracting substantial interest from universities concerned with experiential learning, reflective practice, and developing a holistic conception of the learner. However, reflective writing is for many students a novel genre to compose in, and tutors may be inexperienced in its assessment. While these conditions set a challenging context for automated solutions, natural language processing may also help address the challenge of providing real time, formative feedback on draft writing. This paper reports progress in designing a writing analytics application, detailing the methodology by which informally expressed rubrics are modelled as formal rhetorical patterns, a capability delivered by a novel web application. This has been through iterative evaluation on an independently human-annotated corpus, showing improvements from the first to second version. We conclude by discussing the reasons why classifying reflective writing has proven complex, and reflect on the design processes enabling work across disciplinary boundaries to develop the prototype to its current state.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {213–222},
numpages = {10},
keywords = {writing analytics, rhetoric, reflection, natural language processing, metadiscourse, learning analytics, education},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883953,
author = {Hicks, Drew and Eagle, Michael and Rowe, Elizabeth and Asbell-Clarke, Jodi and Edwards, Teon and Barnes, Tiffany},
title = {Using game analytics to evaluate puzzle design and level progression in a serious game},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883953},
doi = {10.1145/2883851.2883953},
abstract = {Our previous work has demonstrated that players who perceive a game as more challenging are likely to perceive greater learning from that game [8]. However, this may not be the case for all sources of challenge. In this study of a Science learning game called Quantum Spectre, we found that students' progress through the first zone of the game seemed to encounter a "roadblock" during gameplay, dropping out when they cannot (or do not want to) progress further. Previously we had identified two primary types of errors in the learning game, Quantum Spectre: Science Errors related to the game's core educational content; and Puzzle Errors related to rules of the game but not to science knowledge. Using this prior analysis, alongside Survival Analysis techniques for analyzing time-series data and drop-out rates, we explored players' gameplay patterns to help us understand player dropout in Quantum Spectre. These results demonstrate that modeling player behavior can be useful for both assessing learning and for designing complex problem solving content for learning environments.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {440–448},
numpages = {9},
keywords = {survival analysis, serious games, learning analytics, educational data mining, complex problem solving},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883951,
author = {Chen, Ye and Yu, Bei and Zhang, Xuewei and Yu, Yihan},
title = {Topic modeling for evaluating students' reflective writing: a case study of pre-service teachers' journals},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883951},
doi = {10.1145/2883851.2883951},
abstract = {Journal writing is an important and common reflective practice in education. Students' reflection journals also offer a rich source of data for formative assessment. However, the analysis of the textual reflections in class of large size presents challenges. Automatic analysis of students' reflective writing holds great promise for providing adaptive real time support for students. This paper proposes a method based on topic modeling techniques for the task of themes exploration and reflection grade prediction. We evaluated this method on a sample of journal writings from pre-service teachers. The topic modeling method was able to discover the important themes and patterns emerged in students' reflection journals. Weekly topic relevance and word count were identified as important indicators of their journal grades. Based on the patterns discovered by topic modeling, prediction models were developed to automate the assessing and grading of reflection journals. The findings indicate the potential of topic modeling in serving as an analytic tool for teachers to explore and assess students' reflective thoughts in written journals.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {1–5},
numpages = {5},
keywords = {topic modeling, text mining, reflection, learning analytics, journal writing, education, automated grading, LDA},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883950,
author = {Kovanovi\'{c}, Vitomir and Joksimovi\'{c}, Sre\'{c}ko and Waters, Zak and Ga\v{s}evi\'{c}, Dragan and Kitto, Kirsty and Hatala, Marek and Siemens, George},
title = {Towards automated content analysis of discussion transcripts: a cognitive presence case},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883950},
doi = {10.1145/2883851.2883950},
abstract = {In this paper, we present the results of an exploratory study that examined the problem of automating content analysis of student online discussion transcripts. We looked at the problem of coding discussion transcripts for the levels of cognitive presence, one of the three main constructs in the Community of Inquiry (CoI) model of distance education. Using Coh-Metrix and LIWC features, together with a set of custom features developed to capture discussion context, we developed a random forest classification system that achieved 70.3% classification accuracy and 0.63 Cohen's kappa, which is significantly higher than values reported in the previous studies. Besides improvement in classification accuracy, the developed system is also less sensitive to overfitting as it uses only 205 classification features, which is around 100 times less features than in similar systems based on bag-of-words features. We also provide an overview of the classification features most indicative of the different phases of cognitive presence that gives an additional insights into the nature of cognitive presence learning cycle. Overall, our results show great potential of the proposed approach, with an added benefit of providing further characterization of the cognitive presence coding scheme.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {15–24},
numpages = {10},
keywords = {text classification, online discussions, content analytics, content analysis, community of inquiry (CoI) model},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883949,
author = {Pardos, Zachary A. and Xu, Yanbo},
title = {Improving efficacy attribution in a self-directed learning environment using prior knowledge individualization},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883949},
doi = {10.1145/2883851.2883949},
abstract = {Models of learning in EDM and LAK are pushing the boundaries of what can be measured from large quantities of historical data. When controlled randomization is present in the learning platform, such as randomized ordering of problems within a problem set, natural quasi-randomized controlled studies can be conducted, post-hoc. Difficulty and learning gain attribution are among factors of interest that can be studied with secondary analyses under these conditions. However, much of the content that we might like to evaluate for learning value is not administered as a random stimulus to students but instead is being self-selected, such as a student choosing to seek help in the discussion forums, wiki pages, or other pedagogically relevant material in online courseware. Help seekers, by virtue of their motivation to seek help, tend to be the ones who have the least knowledge. When presented with a cohort of students with a bi-modal or uniform knowledge distribution, this can present problems with model interpretability when a single point estimation is used to represent cohort prior knowledge. Since resource access is indicative of a low knowledge student, a model can tend towards attributing the resources with low or negative learning gain in order to better explain performance given the higher average prior point estimate. In this paper we present several individualized prior strategies and demonstrate how learning efficacy attribution validity and prediction accuracy improve as a result. Level of education attained, relative past assessment performance, and the prior per student cold start heuristic were employed and compared as prior knowledge individualization strategies.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {435–439},
numpages = {5},
keywords = {self-selection bias, self-directed learning, prior knowledge, massive open online courses (MOOCs), individualization, efficacy attribution, education, Bayesian knowledge tracing},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883947,
author = {Ruip\'{e}rez-Valiente, Jos\'{e} A. and Mu\~{n}oz-Merino, Pedro J. and Kloos, Carlos Delgado},
title = {Analyzing students' intentionality towards badges within a case study using Khan academy},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883947},
doi = {10.1145/2883851.2883947},
abstract = {One of the most common gamification techniques in education is the use of badges as a reward for making specific student actions. We propose two indicators to gain insight about students' intentionality towards earning badges and use them with data from 291 students interacting with Khan Academy courses. The intentionality to earn badges was greater for repetitive badges, and this can be related to the fact that these are easier to achieve. We provide the general distribution of students depending on these badge indicators, obtaining different profiles of students which can be used for adaptation purposes.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {536–537},
numpages = {2},
keywords = {modelling behavior, learning analytics, badges, Khan academy},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883946,
author = {Cooper, Martyn and Ferguson, Rebecca and Wolff, Annika},
title = {What can analytics contribute to accessibility in e-learning systems and to disabled students' learning?},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883946},
doi = {10.1145/2883851.2883946},
abstract = {This paper explores the potential of analytics for improving accessibility of e-learning and supporting disabled learners in their studies. A comparative analysis of completion rates of disabled and non-disabled students in a large five-year dataset is presented and a wide variation in comparative retention rates is characterized. Learning analytics enable us to identify and understand such discrepancies and, in future, could be used to focus interventions to improve retention of disabled students. An agenda for onward research, focused on Critical Learning Paths, is outlined. This paper is intended to stimulate a wider interest in the potential benefits of learning analytics for institutions as they try to assure the accessibility of their e-learning and provision of support for disabled students.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {99–103},
numpages = {5},
keywords = {technology enhanced learning, metrics, learning analytics, higher education, accessibility, HCI},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883945,
author = {McPherson, Jen and Tong, Huong Ly and Fatt, Scott J. and Liu, Danny Y. T.},
title = {Student perspectives on data provision and use: starting to unpack disciplinary differences},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883945},
doi = {10.1145/2883851.2883945},
abstract = {How can we best align learning analytics practices with disciplinary knowledge practices in order to support student learning? Although learning analytics itself is an interdisciplinary field, it tends to take a 'one-size-fits-all' approach to the collection, measurement, and reporting of data, overlooking disciplinary knowledge practices. In line with a recent trend in higher education research, this paper considers the contribution of a realist sociology of education to the field of learning analytics, drawing on findings from recent student focus groups at an Australian university. It examines what learners say about their data needs with reference to organizing principles underlying knowledge practices within their disciplines. The key contribution of this paper is a framework that could be used as the basis for aligning the provision and/or use of data in relation to curriculum, pedagogy, and assessment with disciplinary knowledge practices. The framework extends recent research in Legitimation Code Theory, which understands disciplinary differences in terms of the principles that underpin knowledge-building. The preliminary analysis presented here both provides a tool for ensuring a fit between learning analytics practices and disciplinary practices and standards for achievement, and signals disciplinarity as an important consideration in learning analytics practices.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {158–167},
numpages = {10},
keywords = {student needs, sociology of education, legitimation code theory, learning analytics, knowledge, disciplinary differences},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883944,
author = {Bakharia, Aneesha and Corrin, Linda and de Barba, Paula and Kennedy, Gregor and Ga\v{s}evi\'{c}, Dragan and Mulder, Raoul and Williams, David and Dawson, Shane and Lockyer, Lori},
title = {A conceptual framework linking learning design with learning analytics},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883944},
doi = {10.1145/2883851.2883944},
abstract = {In this paper we present a learning analytics conceptual framework that supports enquiry-based evaluation of learning designs. The dimensions of the proposed framework emerged from a review of existing analytics tools, the analysis of interviews with teachers, and user scenarios to understand what types of analytics would be useful in evaluating a learning activity in relation to pedagogical intent. The proposed framework incorporates various types of analytics, with the teacher playing a key role in bringing context to the analysis and making decisions on the feedback provided to students as well as the scaffolding and adaptation of the learning design. The framework consists of five dimensions: temporal analytics, tool-specific analytics, cohort dynamics, comparative analytics and contingency. Specific metrics and visualisations are defined for each dimension of the conceptual framework. Finally the development of a tool that partially implements the conceptual framework is discussed.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {329–338},
numpages = {10},
keywords = {learning design, learning analytics, intervention design},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883943,
author = {Karkalas, Sokratis and Mavrikis, Manolis and Labs, Oliver},
title = {Towards analytics for educational interactive e-books: the case of the reflective designer analytics platform (RDAP)},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883943},
doi = {10.1145/2883851.2883943},
abstract = {This paper presents an analytics dashboard that has been developed for designers of interactive e-books. This is part of the EU-funded MC Squared project that is developing a platform for authoring interactive educational e-books. The primary objective is to develop technologies and resources that enhance creative thinking for both designers (authors) and learners. The learning material is expected to offer learners opportunities to engage creatively with mathematical problems and develop creative mathematical thinking. The analytics dashboard is designed to increase authors' awareness so that they can make informed decisions on how to redesign and improve the e-books. This paper presents architectural and design decisions on key features of the dashboard and discusses future steps with respect to the potential for exploratory data analysis.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {143–147},
numpages = {5},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883942,
author = {Manai, Ouajdi and Yamada, Hiroyuki and Thorn, Christopher},
title = {Real-time indicators and targeted supports: using online platform data to accelerate student learning},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883942},
doi = {10.1145/2883851.2883942},
abstract = {Statway® is one of the Community College Pathways initiatives designed to promote students' success in their developmental math sequence and reduce the time required to earn college credit. A recent causal analysis confirmed that Statway dramatically increased students' success rates in half the time across two different cohorts. These impressive results were also obtained across gender and race/ethnicity groups. However, there is still room for improvement. Students who did not succeed in Statway often did not complete the first of the two-course sequence. Therefore, the objective of this study is to formulate a series of indicators from self-report and online learning system data, alerting instructors to students' progress during the first weeks of the first course in the Statway sequence.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {183–187},
numpages = {5},
keywords = {online engagement, networked improvement community, learning analytics, hierarchical linear modeling, community college developmental mathematics, cognitive and non-cognitive factors},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883941,
author = {Kevan, Jonathan M. and Menchaca, Michael P. and Hoffman, Ellen S.},
title = {Designing MOOCs for success: a student motivation-oriented framework},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883941},
doi = {10.1145/2883851.2883941},
abstract = {Considerable literature exists regarding MOOCs. Evaluations of MOOCs range from ringing endorsements to its vilification as a delivery model. Much evaluation focuses on completion rates and/or participant satisfaction. Overall, MOOCs are ill-defined and researchers struggle with appropriate evaluation criteria beyond attrition rates. In this paper, we provide a brief history of MOOCs, a summary of some evaluation research, and we propose a new model for evaluation with an example from a previously-delivered MOOC. Measurement of the MOOC success framework through four student satisfaction types is proposed in this paper with a model for informal learning satisfaction, one of the proposed types, theorized and tested. Results indicated theoretical underpinnings, while intended to improve instruction, might not have influenced the same satisfaction construct. Therefore, future research into alternative satisfaction factor models is needed.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {274–278},
numpages = {5},
keywords = {structural equation modeling, motivation, learning analytics, framework, confirmatory factor analysis, MOOC},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883940,
author = {Jayaprakash, Sandeep M. and Laur\'{\i}a, Eitel J. M. and Gandhi, Pritesh and Mendhe, Dinesh},
title = {Benchmarking student performance and engagement in an early alert predictive system using interactive radar charts},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883940},
doi = {10.1145/2883851.2883940},
abstract = {This poster synthesizes the design features of a visualization layer applied on the Open Academic Analytics Initiative (OAAI), an open source academic early alert system based on predictive analytics. The poster explores ways to convey the predictive model outputs and benchmark student performances using visually intuitive radar plots.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {526–527},
numpages = {2},
keywords = {visualization, predictive analytics, open source, learning analytics, interventions, intervention, instructional assessment, information visualizations, data mining, benchmarking},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883939,
author = {Allen, Laura K. and Mills, Caitlin and Jacovina, Matthew E. and Crossley, Scott and D'Mello, Sidney and McNamara, Danielle S.},
title = {Investigating boredom and engagement during writing using multiple sources of information: the essay, the writer, and keystrokes},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883939},
doi = {10.1145/2883851.2883939},
abstract = {Writing training systems have been developed to provide students with instruction and deliberate practice on their writing. Although generally successful in providing accurate scores, a common criticism of these systems is their lack of personalization and adaptive instruction. In particular, these systems tend to place the strongest emphasis on delivering accurate scores, and therefore, tend to overlook additional indices that may contribute to students' success, such as their affective states during writing practice. This study takes an initial step toward addressing this gap by building a predictive model of students' affect using information that can potentially be collected by computer systems. We used individual difference measures, text indices, and keystroke analyses to predict engagement and boredom in 132 writing sessions. The results suggest that these three categories of indices were successful in modeling students' affective states during writing. Taken together, indices related to students' academic abilities, text properties, and keystroke logs were able classify high and low engagement and boredom in writing sessions with accuracies between 76.5% and 77.3%. These results suggest that information readily available in writing training systems can inform affect detectors and ultimately improve student models within intelligent tutoring systems.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {114–123},
numpages = {10},
keywords = {writing, stealth assessment, natural language processing, intelligent tutoring systems, corpus linguistics},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883936,
author = {Grawemeyer, Beate and Mavrikis, Manolis and Holmes, Wayne and Gutierrez-Santos, Sergio and Wiedmann, Michael and Rummel, Nikol},
title = {Affecting off-task behaviour: how affect-aware feedback can improve student learning},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883936},
doi = {10.1145/2883851.2883936},
abstract = {This paper describes the development and evaluation of an affect-aware intelligent support component that is part of a learning environment known as iTalk2Learn. The intelligent support component is able to tailor feedback according to a student's affective state, which is deduced both from speech and interaction. The affect prediction is used to determine which type of feedback is provided and how that feedback is presented (interruptive or non-interruptive). The system includes two Bayesian networks that were trained with data gathered in a series of ecologically-valid Wizard-of-Oz studies, where the effect of the type of feedback and the presentation of feedback on students' affective states was investigated. This paper reports results from an experiment that compared a version that provided affect-aware feedback (affect condition) with one that provided feedback based on performance only (non-affect condition). Results show that students who were in the affect condition were less bored and less off-task, with the latter being statically significant. Importantly, students in both conditions made learning gains that were statistically significant, while students in the affect condition had higher learning gains than those in the non-affect condition, although this result was not statistically significant in this study's sample. Taken all together, the results point to the potential and positive impact of affect-aware intelligent support.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {104–113},
numpages = {10},
keywords = {feedback, exploratory learning environments, affect},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883935,
author = {Mostafavi, Behrooz and Barnes, Tiffany},
title = {Data-driven proficiency profiling: proof of concept},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883935},
doi = {10.1145/2883851.2883935},
abstract = {Data-driven methods have previously been used in intelligent tutoring systems to improve student learning outcomes and predict student learning methods. We have been incorporating data-driven methods for feedback and problem selection into Deep Thought, a logic tutor where students practice constructing deductive logic proofs. In this latest study we have implemented our data-driven proficiency profiler (DDPP) into Deep Thought as a proof of concept. The DDPP determines student proficiency without expert involvement by comparing relevant student rule scores to previous students who behaved similarly in the tutor and successfully completed it. The results show that the DDPP did improve in performance with additional data and proved to be an effective proof of concept.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {324–328},
numpages = {5},
keywords = {tutoring system, student classification, data-driven, clustering},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883934,
author = {Zhu, Mengxiao and Bergner, Yoav and Zhang, Yan and Baker, Ryan and Wang, Yuan and Paquette, Luc},
title = {Longitudinal engagement, performance, and social connectivity: a MOOC case study using exponential random graph models},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883934},
doi = {10.1145/2883851.2883934},
abstract = {This paper explores a longitudinal approach to combining engagement, performance and social connectivity data from a MOOC using the framework of exponential random graph models (ERGMs). The idea is to model the social network in the discussion forum in a given week not only using performance (assignment scores) and overall engagement (lecture and discussion views) covariates within that week, but also on the same person-level covariates from adjacent previous and subsequent weeks. We find that over all eight weekly sessions, the social networks constructed from the forum interactions are relatively sparse and lack the tendency for preferential attachment. By analyzing data from the second week, we also find that individuals with higher performance scores from current, previous, and future weeks tend to be more connected in the social network. Engagement with lectures had significant but sometimes puzzling effects on social connectivity. However, the relationships between social connectivity, performance, and engagement weakened over time, and results were not stable across weeks.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {223–230},
numpages = {8},
keywords = {network analysis, learning, forum participation, exponential random graph model, MOOC, ERGM},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883933,
author = {Drachsler, Hendrik and Hoel, Tore and Cooper, Adam and Kismih\'{o}k, G\'{a}bor and Berg, Alan and Scheffel, Maren and Chen, Weiqin and Ferguson, Rebecca},
title = {Ethical and privacy issues in the design of learning analytics applications},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883933},
doi = {10.1145/2883851.2883933},
abstract = {Issues related to Ethics and Privacy have become a major stumbling block in application of Learning Analytics technologies on a large scale. Recently, the learning analytics community at large has more actively addressed the EP4LA issues, and we are now starting to see learning analytics solutions that are designed not only as an afterthought, but also with these issues in mind. The 2nd EP4LA@LAK16 workshop will bring the discussion on ethics and privacy for learning analytics to a the next level, helping to build an agenda for organizational and technical design of LA solutions, addressing the different processes of a learning analytics workflow.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {492–493},
numpages = {2},
keywords = {surveillance, privacy, legal rights, learning analytics, ethics, data ownership},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883932,
author = {Robinson, Carly and Yeomans, Michael and Reich, Justin and Hulleman, Chris and Gehlbach, Hunter},
title = {Forecasting student achievement in MOOCs with natural language processing},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883932},
doi = {10.1145/2883851.2883932},
abstract = {Student intention and motivation are among the strongest predictors of persistence and completion in Massive Open Online Courses (MOOCs), but these factors are typically measured through fixed-response items that constrain student expression. We use natural language processing techniques to evaluate whether text analysis of open responses questions about motivation and utility value can offer additional capacity to predict persistence and completion over and above information obtained from fixed-response items. Compared to simple benchmarks based on demographics, we find that a machine learning prediction model can learn from unstructured text to predict which students will complete an online course. We show that the model performs well out-of-sample, compared to a standard array of demographics. These results demonstrate the potential for natural language processing to contribute to predicting student success in MOOCs and other forms of open online learning.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {383–387},
numpages = {5},
keywords = {motivation, learning analytics, MOOCS},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883931,
author = {Crossley, Scott and Paquette, Luc and Dascalu, Mihai and McNamara, Danielle S. and Baker, Ryan S.},
title = {Combining click-stream data with NLP tools to better understand MOOC completion},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883931},
doi = {10.1145/2883851.2883931},
abstract = {Completion rates for massive open online classes (MOOCs) are notoriously low. Identifying student patterns related to course completion may help to develop interventions that can improve retention and learning outcomes in MOOCs. Previous research predicting MOOC completion has focused on click-stream data, student demographics, and natural language processing (NLP) analyses. However, most of these analyses have not taken full advantage of the multiple types of data available. This study combines click-stream data and NLP approaches to examine if students' on-line activity and the language they produce in the online discussion forum is predictive of successful class completion. We study this analysis in the context of a subsample of 320 students who completed at least one graded assignment and produced at least 50 words in discussion forums, in a MOOC on educational data mining. The findings indicate that a mix of click-stream data and NLP indices can predict with substantial accuracy (78%) whether students complete the MOOC. This predictive power suggests that student interaction data and language data within a MOOC can help us both to understand student retention in MOOCs and to develop automated signals of student success.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {6–14},
numpages = {9},
keywords = {sentiment analysis, predictive analytics, natural language processing, educational success, educational data mining, click-stream data, MOOC},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883930,
author = {Schwendimann, Beat A. and Rodr\'{\i}guez-Triana, Mar\'{\i}a Jes\'{u}s and Vozniuk, Andrii and Prieto, Luis P. and Boroujeni, Mina Shirvani and Holzer, Adrian and Gillet, Denis and Dillenbourg, Pierre},
title = {Understanding learning at a glance: an overview of learning dashboard studies},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883930},
doi = {10.1145/2883851.2883930},
abstract = {Research on learning dashboards aims to identify what data is meaningful to different stakeholders in education, and how data can be presented to support sense-making processes. This paper summarizes the main outcomes of a systematic literature review on learning dashboards, in the fields of Learning Analytics and Educational Data Mining. The query was run in five main academic databases and enriched with papers coming from GScholar, resulting in 346 papers out of which 55 were included in the final analysis. Our review distinguishes different kinds of research studies as well as different aspects of learning dashboards and their maturity in terms of evaluation. As the research field is still relatively young, many of the studies are exploratory and proof-of-concept. Among the main open issues and future lines of work in the area of learning dashboards, we identify the need for longitudinal research in authentic settings, as well as studies that systematically compare different dashboard design options.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {532–533},
numpages = {2},
keywords = {systematic review, learning analytics, information visualization, educational data mining, dashboards},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883928,
author = {Joksimovi\'{c}, Sre\'{c}ko and Manataki, Areti and Ga\v{s}evi\'{c}, Dragan and Dawson, Shane and Kovanovi\'{c}, Vitomir and de Kereki, In\'{e}s Friss},
title = {Translating network position into performance: importance of centrality in different network configurations},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883928},
doi = {10.1145/2883851.2883928},
abstract = {As the field of learning analytics continues to mature, there is a corresponding evolution and sophistication of the associated analytical methods and techniques. In this regard social network analysis (SNA) has emerged as one of the cornerstones of learning analytics methodologies. However, despite the noted importance of social networks for facilitating the learning process, it remains unclear how and to what extent such network measures are associated with specific learning outcomes. Motivated by Simmel's theory of social interactions and building on the argument that social centrality does not always imply benefits, this study aimed to further contribute to the understanding of the association between students' social centrality and their academic performance. The study reveals that learning analytics research drawing on SNA should incorporate both - descriptive and statistical methods to provide a more comprehensive and holistic understanding of a students' network position. In so doing researchers can undertake more nuanced and contextually salient inferences about learning in network settings. Specifically, we show how differences in the factors framing students' interactions within two instances of a MOOC affect the association between the three social network centrality measures (i.e., degree, closeness, and betweenness) and the final course outcome.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {314–323},
numpages = {10},
keywords = {social processes, social network analysis, learning, academic achievement, MOOC, ERGM},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883927,
author = {Prieto, Luis P. and Sharma, Kshitij and Dillenbourg, Pierre and Jes\'{u}s, Mar\'{\i}a},
title = {Teaching analytics: towards automatic extraction of orchestration graphs using wearable sensors},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883927},
doi = {10.1145/2883851.2883927},
abstract = { 'Teaching analytics' is the application of learning analytics techniques to understand teaching and learning processes, and eventually enable supportive interventions. However, in the case of (often, half-improvised) teaching in face-to-face classrooms, such interventions would require first an understanding of what the teacher actually did, as the starting point for teacher reflection and inquiry. Currently, such teacher enactment characterization requires costly manual coding by researchers. This paper presents a case study exploring the potential of machine learning techniques to automatically extract teaching actions during classroom enactment, from five data sources collected using wearable sensors (eye-tracking, EEG, accelerometer, audio and video). Our results highlight the feasibility of this approach, with high levels of accuracy in determining the social plane of interaction (90%, κ=0.8). The reliable detection of concrete teaching activity (e.g., explanation vs. questioning) accurately still remains challenging (67%, κ=0.56), a fact that will prompt further research on multimodal features and models for teaching activity extraction, as well as the collection of a larger multimodal dataset to improve the accuracy and generalizability of these methods.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {148–157},
numpages = {10},
keywords = {wearable sensors, teaching analytics, teacher reflection, multimodal learning analytics, activity detection},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883926,
author = {Papamitsiou, Zacharoula and Karapistoli, Eirini and Economides, Anastasios A.},
title = {Applying classification techniques on temporal trace data for shaping student behavior models},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883926},
doi = {10.1145/2883851.2883926},
abstract = {Differences in learners' behavior have a deep impact on their educational performance. Consequently, there is a need to detect and identify these differences and build suitable learner models accordingly. In this paper, we report on the results from an alternative approach for dynamic student behavioral modeling based on the analysis of time-based student-generated trace data. The goal was to unobtrusively classify students according to their time-spent behavior. We applied 5 different supervised learning classification algorithms on these data, using as target values (class labels) the students' performance score classes during a Computer-Based Assessment (CBA) process, and compared the obtained results. The proposed approach has been explored in a study with 259 undergraduate university participant students. The analysis of the findings revealed that a) the low misclassification rates are indicative of the accuracy of the applied method and b) the ensemble learning (treeBagger) method provides better classification results compared to the others. These preliminary results are encouraging, indicating that a time-spent driven description of the students' behavior could have an added value towards dynamically reshaping the respective models.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {299–303},
numpages = {5},
keywords = {supervised learning classification, learner behavioral modeling, computer-based testing, assessment analytics},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883925,
author = {Oster, Meghan and Lonn, Steven and Pistilli, Matthew D. and Brown, Michael G.},
title = {The learning analytics readiness instrument},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883925},
doi = {10.1145/2883851.2883925},
abstract = {Little is known about the processes institutions use when discerning their readiness to implement learning analytics. This study aims to address this gap in the literature by using survey data from the beta version of the Learning Analytics Readiness Instrument (LARI) [1]. Twenty-four institutions were surveyed and 560 respondents participated. Five distinct factors were identified from a factor analysis of the results: Culture; Data Management Expertise; Data Analysis Expertise; Communication and Policy Application; and, Training. Data were analyzed using both the role of those completing the survey and the Carnegie classification of the institutions as lenses. Generally, information technology professionals and institutions classified as Research Universities--Very High research activity had significantly different scores on the identified factors. Working within a framework of organizational learning, this paper details the concept of readiness as a reflective process, as well as how the implementation and application of analytics should be done so with ethical considerations in mind. Limitations of the study, as well as next steps for research in this area, are also discussed.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {173–182},
numpages = {10},
keywords = {survey design, reflection, readiness, learning analytics, higher education, ethics},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883924,
author = {Hecking, Tobias and Chounta, Irene-Angelica and Hoppe, H. Ulrich},
title = {Investigating social and semantic user roles in MOOC discussion forums},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883924},
doi = {10.1145/2883851.2883924},
abstract = {This paper describes the analysis of the social and semantic structure of discussion forums in massive open online courses (MOOCs) in terms of information exchange and user roles. To that end, we analyse a network of forum users based on information-giving relations extracted from the forum data. Connection patterns that appear in the information exchange network of forum users are used to define specific user roles in a social context. Semantic roles are derived by identifying thematic areas in which an actor seeks for information (problem areas) and the areas of interest in which an actor provides information to others (expertise). The interplay of social and semantic roles is analysed using a socio-semantic blockmodelling approach. The results show that social and semantic roles are not strongly interdependent. This indicates that communication patterns and interests of users develop simultaneously only to a moderate extent. In addition to the case study, the methodological contribution is in combining traditional blockmodelling with semantic information to characterise participant roles.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {198–207},
numpages = {10},
keywords = {socio-semantic analysis, discussion forums, blockmodeling, MOOCs},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883923,
author = {Harrison, Scott and Villano, Renato and Lynch, Grace and Chen, George},
title = {Measuring financial implications of an early alert system},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883923},
doi = {10.1145/2883851.2883923},
abstract = {The prevalence of early alert systems (EAS) at tertiary institutions is increasing. These systems are designed to assist with targeted student support in order to improve student retention. They also require considerable human and capital resources to implement, with significant costs involved. It is therefore an imperative that the systems can demonstrate quantifiable financial benefits to the institution. The purpose of this paper is to report on the financial implications of implementing an EAS at an Australian university as a case study. The case study institution implemented an EAS in 2011 using data generated from a data warehouse. The data set is comprised of 16,124 students enrolled between 2011 and 2013. Using a treatment effects approach, the study found that the cost of a student discontinuing was on average $4,687. Students identified by the EAS remained enrolled for longer, with the institution benefiting with approximately an additional $4,004 in revenue per student over the length of enrolment. All schools had a significant positive effect associated with the EAS and the EAS showed significant value to the institution regardless of the timing when the student was identified. The results indicate that EAS had significant financial benefits to this institution and that the benefits extended to the entire institution beyond the first year of enrolment.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {241–248},
numpages = {8},
keywords = {student retention, financial, evaluation, early alert systems},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883922,
author = {Koile, Kimberle and Rubin, Andee and Chapman, Steve and Kliman, Marlene and Ko, Lily},
title = {Using machine analysis to make elementary students' mathematical thinking visible},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883922},
doi = {10.1145/2883851.2883922},
abstract = {The INK-12: Teaching and Learning Using Interactive Ink Inscriptions in K-12 project has been developing and investigating the use of pen-based technology in elementary math classes. This paper reports on progress made on machine analysis of students' visual representations created using digital tools developed to support learning multiplication and division. The goal of the analysis is to make student thinking visible in order to (a) better understand how students learn multiplication and division, and (b) provide feedback to teachers, e.g., about strategies students use to solve problems. Student work from a five-week trial in a third grade class provides a corpus for development and evaluation of the machine analysis routines. Preliminary findings indicate that the routines can reproduce human analyses.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {524–525},
numpages = {2},
keywords = {visual representations, pen-based computing, mathematics, learning analytics, formative assessment, elementary education},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883921,
author = {Muslim, Arham and Chatti, Mohamed Amine and Mahapatra, Tanmaya and Schroeder, Ulrik},
title = {A rule-based indicator definition tool for personalized learning analytics},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883921},
doi = {10.1145/2883851.2883921},
abstract = {In the last few years, there has been a growing interest in learning analytics (LA) in technology-enhanced learning (TEL). Generally, LA deals with the development of methods that harness educational data sets to support the learning process. Recently, the concept of open learning analytics (OLA) has received a great deal of attention from LA community, due to the growing demand for self-organized, networked, and lifelong learning opportunities. A key challenge in OLA is to follow a personalized and goal-oriented LA model that tailors the LA task to the needs and goals of multiple stakeholders. Current implementations of LA rely on a predefined set of questions and indicators. There is, however, a need to adopt a personalized LA approach that engages end users in the indicator definition process by supporting them in setting goals, posing questions, and self-defining the indicators that help them achieve their goals. In this paper, we address the challenge of personalized LA and present the conceptual, design, and implementation details of a rule-based indicator definition tool to support flexible definition and dynamic generation of indicators to meet the needs of different stakeholders with diverse goals and questions in the LA exercise.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {264–273},
numpages = {10},
keywords = {personalized learning analytics, open learning analytics, learning analytics, indicator},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883920,
author = {Spikol, Daniel and Avramides, Katerina and Cukurova, Mutlu and Vogel, Bahtijar and Luckin, Rose and Ruffaldi, Emanuele and Mavrikis, Manolis},
title = {Exploring the interplay between human and machine annotated multimodal learning analytics in hands-on STEM activities},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883920},
doi = {10.1145/2883851.2883920},
abstract = {This poster explores how to develop a working framework for STEM education that uses both human annotated and machine data across a purpose-built learning environment. Our dual approach is to develop a robust framework for analysis and investigate how to design a learning analytics system to support hands-on engineering design tasks. Data from the first user tests are presented along with the framework for discussion.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {522–523},
numpages = {2},
keywords = {mobile, learning analytics, CSCL},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883919,
author = {Oleksandra, Poquet and Shane, Dawson},
title = {Untangling MOOC learner networks},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883919},
doi = {10.1145/2883851.2883919},
abstract = {Research in formal education has repeatedly offered evidence of the importance of social interactions for student learning. However, it remains unclear whether the development of such interpersonal relationships has the same influence on learning in the context of large-scale open online learning. For instance, in MOOCs group members frequently change and the volume of interactions can quickly amass to chaos, therefore impeding an individual's propensity to foster meaningful relationships. This paper examined a MOOC for its potential to develop social processes. As it is exceedingly difficult to establish a relationship with somebody who seldom accesses a MOOC discussion, we singled out a cohort defined by its participants' regularity of forum presence. The study, analysed this 'cohort' and its development, in comparison to the entire MOOC learner network. Mixed methods of social network analysis (SNA), content analysis and statistical network modelling, revealed the potential for unfolding social processes among a more persistent group of learners in the MOOC setting.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {208–212},
numpages = {5},
keywords = {networked learning, interpersonal interactions, forums, MOOCs},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883918,
author = {Clow, Doug and Ferguson, Rebecca and Macfadyen, Leah and Prinsloo, Paul and Slade, Sharon},
title = {LAK failathon},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883918},
doi = {10.1145/2883851.2883918},
abstract = {As in many fields, most papers in the learning analytics literature report success or, at least, read as if they are reporting success. This is almost certainly not because learning analytics research and activity are always successful. Generally, we report our successes widely, but keep our failures to ourselves. As Bismarck is alleged to have said: it is wise to learn from the mistakes of others. This workshop offers an opportunity for researchers and practitioners to share their failures in a lower-stakes environment, to help them learn from each other's mistakes.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {509–511},
numpages = {3},
keywords = {publication bias, positive results, negative results, failure},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883916,
author = {Wise, Alyssa Friend and Cui, Yi and Vytasek, Jovita},
title = {Bringing order to chaos in MOOC discussion forums with content-related thread identification},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883916},
doi = {10.1145/2883851.2883916},
abstract = {This study addresses the issues of overload and chaos in MOOC discussion forums by developing a model to categorize and identify threads based on whether or not they are substantially related to the course content. Content-related posts were defined as those that give/seek help for the learning of course material and share/comment on relevant resources. A linguistic model was built based on manually-coded starting posts in threads from a statistics MOOC (n=837) and tested on thread starting posts from the second offering of the same course (n=304) and a different statistics course (n=298). The number of views and votes threads received were tested to see if they helped classification. Results showed that content-related posts in the statistics MOOC had distinct linguistic features which appeared to be unrelated to the subject-matter domain; the linguistic model demonstrated good cross-course reliability (all recall and precision &gt; .77) and was useful across all time segments of the courses; number of views and votes were not helpful for classification.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {188–197},
numpages = {10},
keywords = {social interaction, natural language processing, massive open online courses, machine learning, discussion forum},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883915,
author = {Hsiao, I-Han and Pandhalkudi Govindarajan, Sesha Kumar and Lin, Yi-Ling},
title = {Semantic visual analytics for today's programming courses},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883915},
doi = {10.1145/2883851.2883915},
abstract = {We designed and studied an innovative semantic visual learning analytics for orchestrating today's programming classes. The visual analytics integrates sources of learning activities by their content semantics. It automatically processs paper-based exams by associating sets of concepts to the exam questions. Results indicated the automatic concept extraction from exams were promising and could be a potential technological solution to address a real world issue. We also discovered that indexing effectiveness was especially prevalent for complex content by covering more comprehensive semantics. Subjective evaluation revealed that the dynamic concept indexing provided teachers with immediate feedback on producing more balanced exams.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {48–53},
numpages = {6},
keywords = {visual analytics, semantic analytics, programming, orchestration technology, intelligent authoring, dashboard, auto grading},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883914,
author = {Koh, Elizabeth and Shibani, Antonette and Tan, Jennifer Pei-Ling and Hong, Helen},
title = {A pedagogical framework for learning analytics in collaborative inquiry tasks: an example from a teamwork competency awareness program},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883914},
doi = {10.1145/2883851.2883914},
abstract = {Many pedagogical models in the field of learning analytics are implicit and do not overtly direct learner behavior. While this allows flexibility of use, this could also result in misaligned practice, and there are calls for more explicit pedagogical models in learning analytics. This paper presents an explicit pedagogical model, the Team and Self Diagnostic Learning (TSDL) framework, in the context of collaborative inquiry tasks. Key informing theories include experiential learning, collaborative learning, and the learning analytics process model. The framework was trialed through a teamwork competency awareness program for 14 year old students. A total of 272 students participated in the program. This paper foregrounds students' and teachers' evaluative accounts of the program. Findings reveal positive perceptions of the stages of the TSDL framework, despite identified challenges, which points to its potential usefulness for teaching and learning. The TSDL framework aims to provide theoretical clarity of the learning process, and foster alignment between learning analytics and the learning design. The current work provides trial outcomes of a teamwork competency awareness program that used dispositional analytics, and further efforts are underway to develop the discourse layer of the analytic engine. Future work will also be dedicated to application and refinement of the framework for other contexts and participants, both learners and teachers alike.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {74–83},
numpages = {10},
keywords = {twenty-first century skills, teamwork competency, teamwork, pedagogical model, learning design, evaluation, dispositional analytics, collaboration, assessment},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883913,
author = {Ochoa, Xavier and Worsley, Marcelo and Weibel, Nadir and Oviatt, Sharon},
title = {Multimodal learning analytics data challenges},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883913},
doi = {10.1145/2883851.2883913},
abstract = {This is a proposal for organizing a Multimodal Learning Analytics (MLA) data challenge as part of the workshop offering of the Learning Analytics and Knowledge (LAK) conference. It explains the motivation of the event, its objectives, target groups, expected format, organization, dissemination strategy and schedule.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {498–499},
numpages = {2},
keywords = {multimodal datasets, multimodal, data challenge},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883912,
author = {Jo, Yohan and Tomar, Gaurav and Ferschke, Oliver and Ros\'{e}, Carolyn Penstein and Ga\v{s}evi\'{c}, Dragan},
title = {Pipeline for expediting learning analytics and student support from data in social learning},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883912},
doi = {10.1145/2883851.2883912},
abstract = {An important research problem in learning analytics is to expedite the cycle of data leading to the analysis of student progress and the improvement of student support. For this goal in the context of social learning, we propose a pipeline that includes data infrastructure, learning analytics, and intervention, along with computational models for individual components. Next, we describe an example of applying this pipeline to real data in a case study, whose goal is to investigate the positive effects that goal-setting students have on their peers, which suggests ways in which we might foster these social benefits through intervention.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {542–543},
numpages = {2},
keywords = {social learning, learning analytics},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883911,
author = {Khan, Imran and Pardo, Abelardo},
title = {Data2U: scalable real time student feedback in active learning environments},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883911},
doi = {10.1145/2883851.2883911},
abstract = {The majority of applications and products that use learning analytics to understand and improve learning experiences assume the creation of actionable items that will affect students through an intermediary. Much less focus is devoted to exploring how to provide insight directly to students. Furthermore, student engagement has always been a relevant aspect to increase the quality of a learning experience. Learning analytics techniques can be used to provide real-time insight tightly integrated with the learning outcomes directly to the students. This paper describes a case study deployed in a first year engineering course using a flipped learning strategy to explore the behavior of students interacting with a dashboard updated in real time providing indicators of their engagement with the course activities. The results show different patterns of use and their evolution throughout the experience and shed some light on how students perceived this resource.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {249–253},
numpages = {5},
keywords = {visualizations, learning analytics, feedback, dashboard},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883910,
author = {Wang, Yan and Ostrow, Korinn and Beck, Joseph and Heffernan, Neil},
title = {Enhancing the efficiency and reliability of group differentiation through partial credit},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883910},
doi = {10.1145/2883851.2883910},
abstract = {The focus of the learning analytics community bridges the gap between controlled educational research and data mining. Online learning platforms can be used to conduct randomized controlled trials to assist in the development of interventions that increase learning gains; datasets from such research can act as a treasure trove for inquisitive data miners. The present work employs a data mining approach on randomized controlled trial data from ASSISTments, a popular online learning platform, to assess the benefits of incorporating additional student performance data when attempting to differentiate between two user groups. Through a resampling technique, we show that partial credit, defined as an algorithmic combination of binary correctness, hint usage, and attempt count, can benefit assessment and group differentiation. Partial credit reduces sample sizes required to reliably differentiate between groups that are known to differ by 58%, and reduces sample sizes required to reliably differentiate between less distinct groups by 9%.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {454–458},
numpages = {5},
keywords = {resampling, randomized controlled trial, partial credit, group differentiation, data mining},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883909,
author = {Mavrikis, Manolis and Gutierrez-Santos, Sergio and Poulovassilis, Alex},
title = {Design and evaluation of teacher assistance tools for exploratory learning environments},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883909},
doi = {10.1145/2883851.2883909},
abstract = {We present our approach to designing and evaluating tools that can assist teachers in classroom settings where students are using Exploratory Learning Environments (ELEs), using as our case study the MiGen system, which targets 1114 year old students' learning of algebra. We discuss the challenging role of teachers in exploratory learning settings and motivate the need for visualisation and notification tools that can assist teachers in focusing their attention across the whole class and inform their interventions. We present the design and evaluation approach followed during the development of MiGen's Teacher Assistance tools, drawing parallels with the recently proposed LATUX workflow but also discussing how we go beyond this to include a large number of teacher participants in our evaluation activities, so as to gain the benefit of different view points. We discuss the results of the evaluations, which show that participants appreciated the capabilities of the tools and were mostly able to use them quickly and accurately.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {168–172},
numpages = {5},
keywords = {teacher assistance tools, exploratory learning},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883908,
author = {Feng, Mingyu and Krumm, Andrew E. and Bowers, Alex J. and Podkul, Timothy},
title = {Elaborating data intensive research methods through researcher-practitioner partnerships},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883908},
doi = {10.1145/2883851.2883908},
abstract = {Technologies used by teachers and students generate vast amounts of data that can be analyzed to provide insights into improving teaching and learning. However, practitioners are left out of the process. We describe the development of an approach by which researchers and practitioners can work together to use data intensive research methods to launch improvement efforts within schools. This paper describes elements of the first year of a researcher-practitioner partnership, highlighting initial findings, challenges, and strategies for overcoming these challenges.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {540–541},
numpages = {2},
keywords = {researcher-practitioner partnership, learning analytics, data intensive research},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883907,
author = {Brown, Michael Geoffrey and DeMonbrun, R. Matthew and Lonn, Steven and Aguilar, Stephen J. and Teasley, Stephanie D.},
title = {What and when: the role of course type and timing in students' academic performance},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883907},
doi = {10.1145/2883851.2883907},
abstract = {In this paper we discuss the results of a study of students' academic performance in first year general education courses. Using data from 566 students who received intensive academic advising as part of their enrollment in the institution's pre-major/general education program, we investigate individual student, organizational, and disciplinary factors that might predict a students' potential classification in an Early Warning System as well as factors that predict improvement and decline in their academic performance. Disciplinary course type (based on Biglan's [7] typology) was significantly related to a student's likelihood to enter below average performance classifications. Students were the most likely to enter a classification in fields like the natural science, mathematics, and engineering in comparison to humanities courses. We attribute these disparities in academic performance to disciplinary norms around teaching and assessment. In particular, the timing of assessments played a major role in students' ability to exit a classification. Implications for the design of Early Warning analytics systems as well as academic course planning in higher education are offered.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {459–468},
numpages = {10},
keywords = {undergraduate education, time based learning analytics, early warning system, disciplinary fields},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883904,
author = {Beheshitha, Sanam Shirazi and Hatala, Marek and Ga\v{s}evi\'{c}, Dragan and Joksimovi\'{c}, Sre\'{c}ko},
title = {The role of achievement goal orientations when studying effect of learning analytics visualizations},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883904},
doi = {10.1145/2883851.2883904},
abstract = {When designing learning analytics tools for use by learners we have an opportunity to provide tools that consider a particular learner's situation and the learner herself. To afford actual impact on learning, such tools have to be informed by theories of education. Particularly, educational research shows that individual differences play a significant role in explaining students' learning process. However, limited empirical research in learning analytics has investigated the role of theoretical constructs, such as motivational factors, that are underlying the observed differences between individuals. In this work, we conducted a field experiment to examine the effect of three designed learning analytics visualizations on students' participation in online discussions in authentic course settings. Using hierarchical linear mixed models, our results revealed that effects of visualizations on the quantity and quality of messages posted by students with differences in achievement goal orientations could either be positive or negative. Our findings highlight the methodological importance of considering individual differences and pose important implications for future design and research of learning analytics visualizations.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {54–63},
numpages = {10},
keywords = {visualizations, online discussions, learning analytics, dashboards, achievement goal orientation},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883903,
author = {Freeman, J. D.},
title = {Demonstration of the Unizin sentiment visualizer},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883903},
doi = {10.1145/2883851.2883903},
abstract = {While much promise has been demonstrated in the learning analytics field with sentiment analysis, the analyses are typically post hoc. The Unizin Sentiment Visualizer demonstrates that the application of sentiment analysis in real-time provides a powerful new tool to support students in complex learning environments.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {552–553},
numpages = {2},
keywords = {text mining, student, sentiment analysis, real-time, real time, natural language processing, learning analytics, intervention, discussion, Unizin},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883902,
author = {Sharma, Kshitij and Alavi, Hamed S. and Jermann, Patrick and Dillenbourg, Pierre},
title = {A gaze-based learning analytics model: in-video visual feedback to improve learner's attention in MOOCs},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883902},
doi = {10.1145/2883851.2883902},
abstract = {In the context of MOOCs, "With-me-ness" refers to the extent to which the learner succeeds in following the teacher, specifically in terms of looking at the area in the video that the teacher is explaining. In our previous works, we employed eye-tracking methods to quantify learners' With-me-ness and showed that it is positively correlated with their learning gains. In this contribution, we describe a tool that is designed to improve With-me-ness by providing a visual-aid superimposed on the video. The position of the visual-aid is suggested by the teachers' dialogue and deixis, and it is displayed when the learner's With-me-ness is under the average value, which is computed from the other students' gaze behavior. We report on a user-study that examines the effectiveness of the proposed tool. The results show that it significantly improves the learning gain and it significantly increases the extent to which the students follow the teacher. Finally, we demonstrate how With-me-ness can create a complete theoretical framework for conducting gaze-based learning analytics in the context of MOOCs.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {417–421},
numpages = {5},
keywords = {video based learning, student attention, eye-tracking, MOOCs},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883901,
author = {Martori, Francesc and Cuadros, Jordi and Gonz\'{a}lez-Sabat\'{e}, Lucinio},
title = {Studying the relationship between BKT fitting error and the skill difficulty index},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883901},
doi = {10.1145/2883851.2883901},
abstract = {Bayesian Knowledge Tracing (BKT) is one of the most popular knowledge inference models due to its interpretability and ability to infer student knowledge. A proper student modeling can help guide the behavior of a cognitive tutor system and provide insight to researchers on understanding how students learn. Using four different datasets we study the relationship between the error coming from fitting the parameters and the difficulty index of the skills and the effect of the size of the dataset in this relationship. The relationship between the fitting error and the difficulty index can be very easy modeled and might be indicating some problems with BKTs performance. However, large datasets are required to clearly see this connection as there is an important sample size effect.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {364–368},
numpages = {5},
keywords = {educational data mining, difficulty index, RMSE modeling, BKT-BF, BKT},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883900,
author = {Cukurova, Mutlu and Avramides, Katerina and Spikol, Daniel and Luckin, Rose and Mavrikis, Manolis},
title = {An analysis framework for collaborative problem solving in practice-based learning activities: a mixed-method approach},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883900},
doi = {10.1145/2883851.2883900},
abstract = {Systematic investigation of the collaborative problem solving process in open-ended, hands-on, physical computing design tasks requires a framework that highlights the main process features, stages and actions that then can be used to provide 'meaningful' learning analytics data. This paper presents an analysis framework that can be used to identify crucial aspects of the collaborative problem solving process in practice-based learning activities. We deployed a mixed-methods approach that allowed us to generate an analysis framework that is theoretically robust, and generalizable. Additionally, the framework is grounded in data and hence applicable to real-life learning contexts. This paper presents how our framework was developed and how it can be used to analyse data. We argue for the value of effective analysis frameworks in the generation and presentation of learning analytics for practice-based learning activities.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {84–88},
numpages = {5},
keywords = {problem solving, practice-based learning, collaborative learning, analysis framework},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883899,
author = {Greer, Jim and Molinaro, Marco and Ochoa, Xavier and McKay, Timothy},
title = {Learning analytics for curriculum and program quality improvement (PCLA 2016)},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883899},
doi = {10.1145/2883851.2883899},
abstract = {This workshop on Learning Analytics for Curriculum and Program Quality Improvement investigates how LAK can drive improvements in teaching practices, instructional and curricular design, and academic program delivery. This workshop brings forward research and examples of how LAK can help build the case for instructional, curricular, or programmatic change and further how LAK can be used to foster acceptance of change processes by teachers, administrators, and other stakeholders in the educational enterprise.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {494–495},
numpages = {2},
keywords = {text tagging, LATEX, ACM proceedings},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883898,
author = {Giannakos, Michail N. and Sampson, Demetrios G. and Kidzi\'{n}ski, \L{}ukasz and Pardo, Abelardo},
title = {Smart environments and analytics on video-based learning},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883898},
doi = {10.1145/2883851.2883898},
abstract = {The International Workshop of Smart Environments and Analytics on Video-Based Learning (SE@VBL) aims to connect research efforts on Video-Based Learning with Smart Environments and Analytics to create synergies between these fields. The main objective is to build a research community around the intersection of these topical areas. In particular, SE@VBL aims to develop a critical discussion about the next generation of video-based learning environments and their analytics, the form of these analytics and the way they can be analyzed in order to help us to better understand and improve the value of educational videos to support teaching and learning. SE@VBL is based on the rationale that combining and analyzing learners' interactions with other available data obtained from learners, new avenues for research on video-based learning have emerged. This can have a significant impact in current educational trends such as Massive Open Online Courses (MOOCs) and Flipped Classroom.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {502–504},
numpages = {3},
keywords = {visual analytics, video-based learning, smart environments, learning analytics, interaction design},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883897,
author = {Pijeira-D\'{\i}az, H\'{e}ctor J. and Drachsler, Hendrik and J\"{a}rvel\"{a}, Sanna and Kirschner, Paul A.},
title = {Investigating collaborative learning success with physiological coupling indices based on electrodermal activity},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883897},
doi = {10.1145/2883851.2883897},
abstract = {Collaborative learning is considered a critical 21st century skill. Much is known about its contribution to learning, but still investigating a process of collaboration remains a challenge. This paper approaches the investigation on collaborative learning from a psychophysiological perspective. An experiment was set up to explore whether biosensors can play a role in analysing collaborative learning. On the one hand, we identified five physiological coupling indices (PCIs) found in the literature: 1) Signal Matching (SM), 2) Instantaneous Derivative Matching (IDM), 3) Directional Agreement (DA), 4) Pearson's correlation coefficient (PCC) and the 5) Fisher's z-transform (FZT) of the PCC. On the other hand, three collaborative learning measurements were used: 1) collaborative will (CW), 2) collaborative learning product (CLP) and 3) dual learning gain (DLG). Regression analyses showed that out of the five PCIs, IDM related the most to CW and was the best predictor of the CLP. Meanwhile, DA predicted DLG the best. These results play a role in determining informative collaboration measures for designing a learning analytics, biofeedback dashboard.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {64–73},
numpages = {10},
keywords = {physiological coupling indices, learning analytics, electrodermal activity, collaborative learning, biosensors},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883896,
author = {Epp, Carrie Demmans},
title = {English language learner experiences of formal and informal learning environments},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883896},
doi = {10.1145/2883851.2883896},
abstract = {Many people who do not know English have moved to English-speaking countries to learn English. Once there, they learn English through formal and informal methods. While considerable work has studied the experiences of English language learners in different learning environments, we have yet to see analytics that detail the experiences of this population within formal and informal learning environments. This study used the experience sampling methodology to capture the information that is needed to detail the communication and affective experiences of advanced English language learners. The collected data reveals differences in how English language learners perceived their communication success based on their learning context, with higher levels of communicative success experienced in formal learning settings. No such differences were found for learners', highly negative, affect. The data suggest a need for additional emotional support within formal and informal learning environments as well as a need for oral communication support within informal contexts.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {231–235},
numpages = {5},
keywords = {language learning, informal learning, formal learning, experience sampling methodology (ESM), communication, analytics, affect},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883895,
author = {Taraghi, Behnam and Saranti, Anna and Legenstein, Robert and Ebner, Martin},
title = {Bayesian modelling of student misconceptions in the one-digit multiplication with probabilistic programming},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883895},
doi = {10.1145/2883851.2883895},
abstract = {One-digit multiplication errors are one of the most extensively analysed mathematical problems. Research work primarily emphasises the use of statistics whereas learning analytics can go one step further and use machine learning techniques to model simple learning misconceptions. Probabilistic programming techniques ease the development of probabilistic graphical models (bayesian networks) and their use for prediction of student behaviour that can ultimately influence learning decision processes.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {449–453},
numpages = {5},
keywords = {probabilistic programming, one-digit multiplication, learning analytics, Bayesian modelling},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883894,
author = {Wells, Marc and Wollenschlaeger, Alex and Lefevre, David and Magoulas, George D. and Poulovassilis, Alexandra},
title = {Analysing engagement in an online management programme and implications for course design},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883894},
doi = {10.1145/2883851.2883894},
abstract = {We analyse engagement and performance data arising from participants' interactions with an in-house LMS at Imperial College London while a cohort of students follow two courses on a new online postgraduate degree in Management. We identify and investigate two main questions relating to the relationships between engagement and performance, drawing recommendations for improved guidelines to inform the design of such courses.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {236–240},
numpages = {5},
keywords = {predicting student performance, engagement and performance, analysing interaction data},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883893,
author = {Drachsler, Hendrik and Greller, Wolfgang},
title = {Privacy and analytics: it's a DELICATE issue a checklist for trusted learning analytics},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883893},
doi = {10.1145/2883851.2883893},
abstract = {The widespread adoption of Learning Analytics (LA) and Educational Data Mining (EDM) has somewhat stagnated recently, and in some prominent cases even been reversed following concerns by governments, stakeholders and civil rights groups about privacy and ethics applied to the handling of personal data. In this ongoing discussion, fears and realities are often indistinguishably mixed up, leading to an atmosphere of uncertainty among potential beneficiaries of Learning Analytics, as well as hesitations among institutional managers who aim to innovate their institution's learning support by implementing data and analytics with a view on improving student success. In this paper, we try to get to the heart of the matter, by analysing the most common views and the propositions made by the LA community to solve them. We conclude the paper with an eight-point checklist named DELICATE that can be applied by researchers, policy makers and institutional managers to facilitate a trusted implementation of Learning Analytics.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {89–98},
numpages = {10},
keywords = {trust, privacy, legal aspects, learning analytics, implementation, ethics, educational data mining, data management},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883892,
author = {Molenaar, Inge and van Campen, Carolien Knoop},
title = {Learning analytics in practice: the effects of adaptive educational technology Snappet on students' arithmetic skills},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883892},
doi = {10.1145/2883851.2883892},
abstract = {Even though the recent influx of tablets in primary education goes together with the vision that educational technology empowered with learning analytics will revolutionize education, empirical results supporting this claim are scares. Adaptive educational technology Snappet combines extracted and embedded learning analytics daily in classrooms. While students make exercises on the tablet this technology displays real-time data of learner performance in a teacher dashboard (extracted analytics). At the same time, learner performance is used to adaptively adjust exercises to students' progress (embedded analytics). This quasiexperimental study compares the development of students' arithmetic skills over one schoolyear (grade 2 and 4) in a traditional paper based setting to learning with the adaptive educational technology Snappet. The results indicate that students in the Snappet condition make significantly more progress on arithmetic skills in grade 4. Moreover, in this grade students with a high ability level, benefit the most from working with this adaptive educational technology. Overall the development pattern of students with different abilities was more divergent in the AET condition compared to the control condition. These results indicate that adaptive educational technologies combining extracted and embedded learning analytics are indeed creating new education scenarios that contribute to personalized learning in primary education.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {538–539},
numpages = {2},
keywords = {primary education, educational technologies, arithmetic's, ability levels},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883890,
author = {Bos, Nynke and Brand-Gruwel, Saskia},
title = {Student differences in regulation strategies and their use of learning resources: implications for educational design},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883890},
doi = {10.1145/2883851.2883890},
abstract = {The majority of the learning analytics research focuses on the prediction of course performance and modeling student behaviors with a focus on identifying students who are at risk of failing the course. Learning analytics should have a stronger focus on improving the quality of learning for all students, not only identifying at risk students. In order to do so, we need to understand what successful patterns look like when reflected in data and subsequently adjust the course design to avoid unsuccessful patterns and facilitate successful patterns.However, when establishing these successful patterns, it is important to account for individual differences among students since previous research has shown that not all students engage with learning resources to the same extent. Regulation strategies seem to play an important role in explaining the different usage patterns students' display when using digital learning recourses. When learning analytics research incorporates contextualized data about student regulation strategies we are able to differentiate between students at a more granular level.The current study examined if regulation strategies could account for differences in the use of various learning resources. It examines how students regulated their learning process and subsequently used the different learning resources throughout the course and established how this use contributes to course performance.The results show that students with different regulation strategies use the learning resources to the same extent. However, the use of learning resources influences course performance differently for different groups of students. This paper recognizes the importance of contextualization of learning data resources with a broader set of indicators to understand the learning process. With our focus on differences between students, we strive for a shift within learning analytics from identifying at risk students towards a contribution of learning analytics in the educational design process and enhance the quality of learning; for all students.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {344–353},
numpages = {10},
keywords = {regulation strategies, learning dispositions, individual differences, cluster analysis, blended learning},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883888,
author = {Ruiz, Samara and Charleer, Sven and Urretavizcaya, Maite and Klerkx, Joris and Fern\'{a}ndez-Castro, Isabel and Duval, Erik},
title = {Supporting learning by considering emotions: tracking and visualization a case study},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883888},
doi = {10.1145/2883851.2883888},
abstract = {The adequate emotional state of students has proved to be essential for favoring learning. This paper explores the possibility of obtaining students' feedback about the emotions they feel in class in order to discover potential emotion patterns that might indicate learning fails. This paper presents a visual dashboard that allows students to track their emotions and follow up on their evolution during the course. We have compiled the principal classroom related emotions and developed a two-phase inquiry process to: verify the possibility to measure students' emotions in classroom; discover how emotions can be displayed to promote self-reflection; and confirm the impact of emotions on learning performance. Our results suggest that students' emotions in class are related to evaluation marks. This shows that early information about students' emotions can be useful for teachers and students to improve classroom results and learning outcomes.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {254–263},
numpages = {10},
keywords = {visualization, visual dashboards, students' emotions, self-reflection, quantified-self, face to face interactions},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883886,
author = {Rienties, Bart and Boroowa, Avinash and Cross, Simon and Farrington-Flint, Lee and Herodotou, Christothea and Prescott, Lynda and Mayles, Kevin and Olney, Tom and Toetenel, Lisette and Woodthorpe, John},
title = {Reviewing three case-studies of learning analytics interventions at the open university UK},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883886},
doi = {10.1145/2883851.2883886},
abstract = {This study provides a conceptual framework how organizations may adopt evidence-based interventions at scale, and how institutions may evaluate the costs and benefits of such interventions. Building on a new conceptual model developed by the Open University UK (OU), we will analyse three case-studies of evidence-based interventions. By working with 90+ large-scale modules for a period of two years across the five faculties and disciplines within the OU, Analytics4Action provides a bottom-up-approach for working together with key stakeholders within their respective contexts. Using principles of embedded case-study approaches by Yin [1], by comparing the learning behavior, satisfaction and performance of 11079 learners the findings indicated that each of the three learning designs led to satisfied students and average to good student retention. In the second part we highlighted that the three module teams made in-presentation interventions based upon real-time analytics, whereby initial user data indicated VLE behaviour in line with expectations. In 2-5 years, we hope that a rich, robust evidence-base will be presented to show how learning analytics can help teachers to make informed, timely and successful interventions that will help learners to achieve their learning outcomes.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {534–535},
numpages = {2},
keywords = {online learning settings, distance learning, collaborative learning},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883885,
author = {David, Yossi Ben and Segal, Avi and Gal, Ya'akov (Kobi)},
title = {Sequencing educational content in classrooms using Bayesian knowledge tracing},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883885},
doi = {10.1145/2883851.2883885},
abstract = {Despite the prevalence of e-learning systems in schools, most of today's systems do not personalize educational data to the individual needs of each student. This paper proposes a new algorithm for sequencing questions to students that is empirically shown to lead to better performance and engagement in real schools when compared to a baseline approach. It is based on using knowledge tracing to model students' skill acquisition over time, and to select questions that advance the student's learning within the range of the student's capabilities, as determined by the model. The algorithm is based on a Bayesian Knowledge Tracing (BKT) model that incorporates partial credit scores, reasoning about multiple attempts to solve problems, and integrating item difficulty. This model is shown to outperform other BKT models that do not reason about (or reason about some but not all) of these features. The model was incorporated into a sequencing algorithm and deployed in two classes in different schools where it was compared to a baseline sequencing algorithm that was designed by pedagogical experts. In both classes, students using the BKT sequencing approach solved more difficult questions and attributed higher performance than did students who used the expert-based approach. Students were also more engaged using the BKT approach, as determined by their interaction time and number of log-ins to the system, as well as their reported opinion. We expect our approach to inform the design of better methods for sequencing and personalizing educational content to students that will meet their individual learning needs.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {354–363},
numpages = {10},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883884,
author = {Papou\v{s}ek, Jan and Stanislav, V\'{\i}t and Pel\'{a}nek, Radek},
title = {Evaluation of an adaptive practice system for learning geography facts},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883884},
doi = {10.1145/2883851.2883884},
abstract = {Computerized educational systems are increasingly provided as open online services which provide adaptive personalized learning experience. To fully exploit potential of such systems, it is necessary to thoroughly evaluate different design choices. However, both openness and adaptivity make proper evaluation difficult. We provide a detailed report on evaluation of an online system for adaptive practice of geography, and use this case study to highlight methodological issues with evaluation of open online learning systems, particularly attrition bias. To facilitate evaluation of learning, we propose to use randomized reference questions. We illustrate application of survival analysis and learning curves for declarative knowledge. The result provide an interesting insight into the impact of adaptivity on learner behaviour and learning.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {134–142},
numpages = {9},
keywords = {survival analysis, learning curve, evaluation, engagement, computerized adaptive practice, attrition bias},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883883,
author = {Pardo, Abelardo and Han, Feifei and Ellis, Robert A.},
title = {Exploring the relation between self-regulation, online activities, and academic performance: a case study},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883883},
doi = {10.1145/2883851.2883883},
abstract = {The areas of educational data mining and learning analytics focus on the extraction of knowledge and actionable items from data sets containing detailed information about students. However, the potential impact from these techniques is increased when properly contextualized within a learning environment. More studies are needed to explore the connection between student interactions, approaches to learning, and academic performance. Self-regulated learning (SRL) is defined as the extent to which a student is able to motivationally, metacognitively, and cognitively engage in a learning experience. SRL has been the focus of research in traditional classroom learning and is also argued to play a vital role in the online or blended learning contexts. In this paper, we study how SRL affects students' online interactions with various learning activities and its influence in academic performance. The results derived from a naturalistic experiment among a cohort of first year engineering students showed that positive self-regulated strategies (PSRS) and negative self-regulated strategies (NSRS) affected both the interaction with online activities and academic performance. NSRS directly predicted academic outcomes, whereas PSRS only contributed indirectly to academic performance via the interactions with online activities. These results point to concrete avenues to promote self-regulation among students in this type of learning contexts.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {422–429},
numpages = {8},
keywords = {self-regulation, learning analytics, higher education, SEM},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883882,
author = {Bakharia, Aneesha and Kitto, Kirsty and Pardo, Abelardo and Ga\v{s}evi\'{c}, Dragan and Dawson, Shane},
title = {Recipe for success: lessons learnt from using xAPI within the connected learning analytics toolkit},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883882},
doi = {10.1145/2883851.2883882},
abstract = {An ongoing challenge for Learning Analytics research has been the scalable derivation of user interaction data from multiple technologies. The complexities associated with this challenge are increasing as educators embrace an ever growing number of social and content-related technologies. The Experience API (xAPI) alongside the development of user specific record stores has been touted as a means to address this challenge, but a number of subtle considerations must be made when using xAPI in Learning Analytics. This paper provides a general overview to the complexities and challenges of using xAPI in a general systemic analytics solution - called the Connected Learning Analytics (CLA) toolkit. The importance of design is emphasised, as is the notion of common vocabularies and xAPI Recipes. Early decisions about vocabularies and structural relationships between statements can serve to either facilitate or handicap later analytics solutions. The CLA toolkit case study provides us with a way of examining both the strengths and the weaknesses of the current xAPI specification, and we conclude with a proposal for how xAPI might be improved by using JSON-LD to formalise Recipes in a machine readable form.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {378–382},
numpages = {5},
keywords = {xAPI, learning record store, learning analytics, architecture, CLRecipe, CLA toolkit},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883881,
author = {Kitto, Kirsty and Bakharia, Aneesha and Lupton, Mandy and Mallet, Dann and Banks, John and Bruza, Peter and Pardo, Abelardo and Buckingham Shum, Simon and Dawson, Shane and Ga\v{s}evi\'{c}, Dragan and Siemens, George and Lynch, Grace},
title = {The connected learning analytics toolkit},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883881},
doi = {10.1145/2883851.2883881},
abstract = {This demonstration introduces the Connected Learning Analytics (CLA) Toolkit. The CLA toolkit harvests data about student participation in specified learning activities across standard social media environments, and presents information about the nature and quality of the learning interactions.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {548–549},
numpages = {2},
keywords = {social learning analytics, sensemaking, dashboards},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883879,
author = {Brooks, Christopher A. and Thompson, Craig and Kovanovi\'{c}, Vitomir},
title = {Introduction to data mining for educational researchers},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883879},
doi = {10.1145/2883851.2883879},
abstract = {The goal of this tutorial is to share data mining tools and techniques used by computer scientists with educational social scientists. We broadly define educational social scientists as being made up of people with backgrounds in the learning sciences, cognitive psychology, and educational research. The learning analytics community is heavily populated with researchers of these backgrounds, and we believe those that find themselves at the intersection of research, theory, and practice have a particular interest in expanding their knowledge of datadriven tools and techniques.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {505–506},
numpages = {2},
keywords = {predictive modeling, learning analytics, data mining},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883878,
author = {Ferguson, Rebecca and Clow, Doug},
title = {Learning analytics community exchange: evidence hub},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883878},
doi = {10.1145/2883851.2883878},
abstract = {This poster sets out the background and development of the LACE Evidence Hub, a site that gathers evidence about learning analytics in an accessible form. The poster also describes the functionality of the site, summarises its quantitative and thematic content to date, and assesses the state of evidence. In addition, it encourages people to add to and make use of the Hub.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {520–521},
numpages = {2},
keywords = {teaching, take-up, learning analytics, learning, evidence, ethics},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883877,
author = {Grover, Shuchi and Bienkowski, Marie and Tamrakar, Amir and Siddiquie, Behjat and Salter, David and Divakaran, Ajay},
title = {Multimodal analytics to study collaborative problem solving in pair programming},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883877},
doi = {10.1145/2883851.2883877},
abstract = {Collaborative problem solving (CPS) is seen as a key skill in K-12 education---in computer science as well as other subjects. Efforts to introduce children to computing rely on pair programming as a way of having young learners engage in CPS. Characteristics of quality collaboration are joint exploring or understanding, joint representation, and joint execution. We present a data driven approach to assessing and elucidating collaboration through modeling of multimodal student behavior and performance data.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {516–517},
numpages = {2},
keywords = {pair programming, multimodal analytics, kinect, k-12 computer science education, collaborative problem solving, collaboration},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883876,
author = {Renz, Jan and Hoffmann, Daniel and Staubitz, Thomas and Meinel, Christoph},
title = {Using A/B testing in MOOC environments},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883876},
doi = {10.1145/2883851.2883876},
abstract = {In recent years, Massive Open Online Courses (MOOCs) have become a phenomenon offering the possibility to teach thousands of participants simultaneously. In the same time the platforms used to deliver these courses are still in their fledgling stages. While course content and didactics of those massive courses are the primary key factors for the success of courses, still a smart platform may increase or decrease the learners experience and his learning outcome. The paper at hand proposes the usage of an A/B testing framework that is able to be used within an micro-service architecture to validate hypotheses about how learners use the platform and to enable data-driven decisions about new features and settings. To evaluate this framework three new features (Onboarding Tour, Reminder Mails and a Pinboard Digest) have been identified based on a user survey. They have been implemented and introduced on two large MOOC platforms and their influence on the learners behavior have been measured. Finally this paper proposes a data driven decision workflow for the introduction of new features and settings on e-learning platforms.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {304–313},
numpages = {10},
keywords = {microservice, e-learning, controlled online tests, MOOC, A/B testing},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883875,
author = {Rienties, Bart and Toetenel, Lisette},
title = {The impact of 151 learning designs on student satisfaction and performance: social learning (analytics) matters},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883875},
doi = {10.1145/2883851.2883875},
abstract = {An increasing number of researchers are taking learning design into consideration when predicting learning behavior and outcomes across different modules. This study builds on preliminary learning design work that was presented at LAK2015 by the Open University UK. In this study we linked 151 modules and 111.256 students with students' satisfaction and performance using multiple regression models. Our findings strongly indicate the importance of learning design in predicting and understanding performance of students in blended and online environments. In line with proponents of social learning analytics, our primary predictor for academic retention was the amount of communication activities, controlling for various institutional and disciplinary factors. Where possible, appropriate communication tasks that align with the learning objectives of the course may be a way forward to enhance academic retention.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {339–343},
numpages = {5},
keywords = {distance learning, data analytics, collaborative learning},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883874,
author = {van Leeuwen, Anouschka},
title = {Learning analytics in a flipped university course},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883874},
doi = {10.1145/2883851.2883874},
abstract = {In this poster, we describe the design of a university course with a blended learning character. Learning analytics were used both within the course to facilitate effective teacher-student interaction, as well as after the course to examine patterns between students' activities during the course and their performance on the test and the group assignment at the end of the course.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {514–515},
numpages = {2},
keywords = {web lectures, learning analytics, higher education, formative assessment, blended learning},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883873,
author = {Martinez-Maldonado, Roberto and Schneider, Bertrand and Charleer, Sven and Buckingham Shum, Simon and Klerkx, Joris and Duval, Erik},
title = {Interactive surfaces and learning analytics: data, orchestration aspects, pedagogical uses and challenges},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883873},
doi = {10.1145/2883851.2883873},
abstract = {The proliferation of varied types of multi-user interactive surfaces (such as digital whiteboards, tabletops and tangible interfaces) is opening a new range of applications in face-to-face (f2f) contexts. They offer unique opportunities for Learning Analytics (LA) by facilitating multi-user sensemaking of automatically captured digital footprints of students' f2f interactions. This paper presents an analysis of current research exploring learning analytics associated with the use of surface devices. We use a framework to analyse our first-hand experiences, and the small number of related deployments according to four dimensions: the orchestration aspects involved; the phases of the pedagogical practice that are supported; the target actors; and the levels of iteration of the LA process. The contribution of the paper is twofold: 1) a synthesis of conclusions that identify the degree of maturity, challenges and pedagogical opportunities of the existing applications of learning analytics and interactive surfaces; and 2) an analysis framework that can be used to characterise the design space of similar areas and LA applications.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {124–133},
numpages = {10},
keywords = {visualizations, studies in the wild, groupware, face-to-face, design, dashboard, awareness},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883872,
author = {Ostrow, Korinn S. and Selent, Doug and Wang, Yan and Van Inwegen, Eric G. and Heffernan, Neil T. and Williams, Joseph Jay},
title = {The assessment of learning infrastructure (ALI): the theory, practice, and scalability of automated assessment},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883872},
doi = {10.1145/2883851.2883872},
abstract = {Researchers invested in K-12 education struggle not just to enhance pedagogy, curriculum, and student engagement, but also to harness the power of technology in ways that will optimize learning. Online learning platforms offer a powerful environment for educational research at scale. The present work details the creation of an automated system designed to provide researchers with insights regarding data logged from randomized controlled experiments conducted within the ASSISTments TestBed. The Assessment of Learning Infrastructure (ALI) builds upon existing technologies to foster a symbiotic relationship beneficial to students, researchers, the platform and its content, and the learning analytics community. ALI is a sophisticated automated reporting system that provides an overview of sample distributions and basic analyses for researchers to consider when assessing their data. ALI's benefits can also be felt at scale through analyses that crosscut multiple studies to drive iterative platform improvements while promoting personalized learning.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {279–288},
numpages = {10},
keywords = {universal data reporting, tools for learning analytics, the assistments testbed, randomized controlled experiments at scale, automated analysis, assessment of learning infrastructure},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883871,
author = {The, Benedict and Mavrikis, Manolis},
title = {A study on eye fixation patterns of students in higher education using an online learning system},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883871},
doi = {10.1145/2883851.2883871},
abstract = {We study how the use of online learning systems stimulate cognitive activities, by conducting an experiment with the use of eye tracking technology to monitor eye fixations of 60 final year students engaging in online interactive tutorials at the start of their Final Year Project module. Our findings show that the students' visual scanning behaviours fall into three different types of eye fixation patterns, and the data corresponding to the different types relates to the performance of the students in other related academic modules. We conclude that this method of studying eye fixation patterns can identify different types of learners with respect to cognitive activities and academic potentials, allowing educators to understand how their instructional design using online learning environments can stimulate higher-order cognitive activities.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {408–416},
numpages = {9},
keywords = {online learning, instructional design, human-computer interaction, eye tracking, cognitive activity},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883870,
author = {Pardo, Abelardo and Mirriahi, Negin and Martinez-Maldonado, Roberto and Jovanovic, Jelena and Dawson, Shane and Ga\v{s}evi\'{c}, Dragan},
title = {Generating actionable predictive models of academic performance},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883870},
doi = {10.1145/2883851.2883870},
abstract = {The pervasive collection of data has opened the possibility for educational institutions to use analytics methods to improve the quality of the student experience. However, the adoption of these methods faces multiple challenges particularly at the course level where instructors and students would derive the most benefit from the use of analytics and predictive models. The challenge lies in the knowledge gap between how the data is captured, processed and used to derive models of student behavior, and the subsequent interpretation and the decision to deploy pedagogical actions and interventions by instructors. Simply put, the provision of learning analytics alone has not necessarily led to changing teaching practices. In order to support pedagogical change and aid interpretation, this paper proposes a model that can enable instructors to readily identify subpopulations of students to provide specific support actions. The approach was applied to a first year course with a large number of students. The resulting model classifies students according to their predicted exam scores, based on indicators directly derived from the learning design.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {474–478},
numpages = {5},
keywords = {recursive partitioning, personalization, learning analytics, feedback},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883868,
author = {Pel\'{a}nek, Radek and Rih\'{a}k, Jir\'{\i} and Papou\v{s}ek, Jan},
title = {Impact of data collection on interpretation and evaluation of student models},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883868},
doi = {10.1145/2883851.2883868},
abstract = {Student modeling techniques are evaluated mostly using historical data. Researchers typically do not pay attention to details of the origin of the used data sets. However, the way data are collected can have important impact on evaluation and interpretation of student models. We discuss in detail two ways how data collection in educational systems can influence results: mastery attrition bias and adaptive choice of items. We systematically discuss previous work related to these biases and illustrate the main points using both simulated and real data. We summarize specific consequences for practice -- not just for doing evaluation of student models, but also for data collection and publication of data sets.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {40–47},
numpages = {8},
keywords = {student modeling, parameter fitting, evaluation, data sets, bias, attition},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883867,
author = {Adjei, Seth A. and Botelho, Anthony F. and Heffernan, Neil T.},
title = {Predicting student performance on post-requisite skills using prerequisite skill data: an alternative method for refining prerequisite skill structures},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883867},
doi = {10.1145/2883851.2883867},
abstract = {Prerequisite skill structures have been closely studied in past years leading to many data-intensive methods aimed at refining such structures. While many of these proposed methods have yielded success, defining and refining hierarchies of skill relationships are often difficult tasks. The relationship between skills in a graph could either be causal, therefore, a prerequisite relationship (skill A must be learned before skill B). The relationship may be non-causal, in which case the ordering of skills does not matter and may indicate that both skills are prerequisites of another skill. In this study, we propose a simple, effective method of determining the strength of pre-to-post-requisite skill relationships. We then compare our results with a teacher-level survey about the strength of the relationships of the observed skills and find that the survey results largely confirm our findings in the data-driven approach.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {469–473},
numpages = {5},
keywords = {skill relationships, refinements, prerequisite structures, placements, learning maps},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883865,
author = {Chen, Bodong and Wise, Alyssa F. and Knight, Simon and Cheng, Britte Haugan},
title = {Putting temporal analytics into practice: the 5th international workshop on temporality in learning data},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883865},
doi = {10.1145/2883851.2883865},
abstract = {Interest in temporal analytics---analytics that probe temporal aspects of learning so as to gain insights into the processes through which learning occurs---continues to grow. The relationships of temporal patterns to learning outcomes is a central area of interest. However, while the literature on temporal analyses is developing, there has been less consideration of the methods by which temporal analyses might be translated to actionable insights and thus, put into use in educational practice. Emerging temporal analysis techniques present both theoretical and practical challenges for producing and interpreting results. Synergetic actions are needed in order to support practitioners.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {488–489},
numpages = {2},
keywords = {temporality, practitioner knowledge, learning analytics, analytics for action, CSCL},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883864,
author = {Wolff, Annika and Moore, John and Zdrahal, Zdenek and Hlosta, Martin and Kuzilek, Jakub},
title = {Data literacy for learning analytics},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883864},
doi = {10.1145/2883851.2883864},
abstract = {This workshop explores how data literacy impacts on learning analytics both for practitioners and for end users. The term data literacy is used to broadly describe the set of abilities around the use of data as part of everyday thinking and reasoning for solving real-world problems. It is a skill required both by learning analytics practitioners to derive actionable insights from data and by the intended end users, such that it affects their ability to accurately interpret and critique presented analysis of data. The latter is particularly important, since learning analytics outcomes can be targeted at a wide range of end users, some of whom will be young students and many of whom are not data specialists.Whilst data literacy is rarely an end goal of learning analytics projects, this workshop aims to find where issues related to data literacy have impacted on project outcomes and where important insights have been gained. This workshop will further encourage the sharing of knowledge and experience through practical activities with datasets and visualisations. This workshop aims to highlight the need for a greater understanding of data literacy as a field of study, especially with regard to communicating around large, complex, data sets.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {500–501},
numpages = {2},
keywords = {visualization, learning analytics, data literacy, communication, analysis},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883860,
author = {Ley, Tobias and Klamma, Ralf and Lindstaedt, Stefanie and Wild, Fridolin},
title = {Learning analytics for workplace and professional learning},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883860},
doi = {10.1145/2883851.2883860},
abstract = {Recognizing the need for addressing the rather fragmented character of research in this field, we have held a workshop on learning analytics for workplace and professional learning at the Learning Analytics and Knowledge (LAK) Conference. The workshop has taken a broad perspective, encompassing approaches from a number of previous traditions, such as adaptive learning, professional online communities, workplace learning and performance analytics. Being co-located with the LAK conference has provided an ideal venue for addressing common challenges and for benefiting from the strong research on learning analytics in other sectors that LAK has established. Learning Analytics for Workplace and Professional Learning is now on the research agenda of several ongoing EU projects, and therefore a number of follow-up activities are planned for strengthening integration in this emerging field.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {484–485},
numpages = {2},
keywords = {workplace learning, learning analytics},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883859,
author = {Mol, Stefan and Kobayashi, Vladimer and Kismih\'{o}k, G\'{a}bor and Zhao, Catherine},
title = {Learning through goal setting},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883859},
doi = {10.1145/2883851.2883859},
abstract = {Despite the mounting evidence supporting the role that goal setting has on the learning process, there seems to be only a handful of studies that directly investigate goal setting in the context of Learning Analytics (LA). Although investigations have incorporated elements of goal setting, the attention afforded to theory and operationalization have been modest. In this workshop we plan to position goal setting at the forefront of LA research. The workshop will serve as a venue to bring together researchers interested in advancing Goal Setting (GS) research in the LA field. Topics include: (1) GS theory and measurement; (2) analysis and visualization of GS data; (3) strategies for integrating GS in the learning experience; and (4) implementation of GS technologies. Participants who need tools to execute their GS ideas and those who already have tools and are exploring better ways to integrate a goal setting feature can gain a lot from this workshop. Moreover, participants will have the opportunity to contribute to the conceptualization and staging of GS ideas in LA research.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {512–513},
numpages = {2},
keywords = {learning record store, learning analytics, goal setting},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883858,
author = {Whyte, Anthony and Nayak, Prashant and Johnston, John},
title = {LAK16 workshop: extending IMS caliper analytics™ with learning activity profiles},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883858},
doi = {10.1145/2883851.2883858},
abstract = {Educational institutions are evolving away from the one-application-fits-all learning management system to a loosely connected digital learning ecosystem comprising diverse services that increasingly leverage data analytics to drive pedagogical innovation. Yet an ecosystem rich in services but lacking a common approach to measuring learning activity will find data collection, aggregation and analysis time-consuming and costly. The IMS Caliper Analytics™ specification addresses the need for data and semantic interoperability by providing an extensible information model, controlled vocabularies and an API for instrumenting learning applications and systems that log learning events. However, many learning activities have yet to be modeled by the Caliper working group. Engaging the SoLAR community directly in this effort will help ensure that the needs of researchers and other consumers of learning analytics data will inform future versions of the specification. The LAK16 Caliper workshop is being offered with this goal in mind. The half-day session, facilitated by members of Team Caliper, will provide LAK16 participants with an opportunity to extend the Caliper specification by modeling new learning activity profiles. New profiles, new connections and new friendships are expected outcomes.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {490–491},
numpages = {2},
keywords = {learning analytics, learning activity profiles, information modeling, controlled vocabularies, IMS caliper},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883857,
author = {Agnihotri, Lalitha and Mojarad, Shirin and Lewkow, Nicholas and Essa, Alfred},
title = {Educational data mining with Python and Apache spark: a hands-on tutorial},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883857},
doi = {10.1145/2883851.2883857},
abstract = {Enormous amount of educational data has been accumulated through Massive Open Online Courses (MOOCs), as well as commercial and non-commercial learning platforms. This is in addition to the educational data released by US government since 2012 to facilitate disruption in education by making data freely available. The high volume, variety and velocity of collected data necessitate use of big data tools and storage systems such as distributed databases for storage and Apache Spark for analysis.This tutorial will introduce researchers and faculty to real-world applications involving data mining and predictive analytics in learning sciences. In addition, the tutorial will introduce statistics required to validate and accurately report results. Topics will cover how big data is being used to transform education. Specifically, we will demonstrate how exploratory data analysis, data mining, predictive analytics, machine learning, and visualization techniques are being applied to educational big data to improve learning and scale insights driven from millions of student's records.The tutorial will be held over a half day and will be hands on with pre-posted material. Due to the interdisciplinary nature of work, the tutorial appeals to researchers from a wide range of backgrounds including big data, predictive analytics, learning sciences, educational data mining, and in general, those interested in how big data analytics can transform learning. As a prerequisite, attendees are required to have familiarity with at least one programming language.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {507–508},
numpages = {2},
keywords = {visualization, spark, simulation, python, predictive analytics, parallel computing, machine learning, learning analytics, exploratory data analysis, educational data mining, data mining, big data},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883856,
author = {Ringtved, Ulla and Milligan, Sandra and Corrin, Linda},
title = {Learning design and feedback processes at scale: stocktaking emergent theory and practice},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883856},
doi = {10.1145/2883851.2883856},
abstract = {Design for learning in scaled courses is shifting away from replication of traditional on-campus or online teaching towards exploiting the distinctive characteristic and potentials of scale to transform both teaching and learning. Scaled learning environments such as MOOCs may represent a new paradigm for teaching. This workshop involves consideration of the how learning occurs in scaled environments, and how learning designers and analysts can assist. It will explore questions at the heart of effective learning design, using expert panelists and collaborative knowledge-building techniques to arrive at a stocktake of thinking.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {479–480},
numpages = {2},
keywords = {scaled courses, management performance, learning design, learning at scale, learning analytics, feedback, crowd-sourced learning},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883855,
author = {Martinez-Maldonado, Roberto and Hernandez-Leo, Davinia and Pardo, Abelardo and Suthers, Dan and Kitto, Kirsty and Charleer, Sven and Aljohani, Naif Radi and Ogata, Hiroaki},
title = {Cross-LAK: learning analytics across physical and digital spaces},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883855},
doi = {10.1145/2883851.2883855},
abstract = {It is of high relevance to the LAK community to explore blended learning scenarios where students can interact at diverse digital and physical learning spaces. This workshop aims to gather the sub-community of LAK researchers, learning scientists and researchers from other communities, interested in ubiquitous, mobile and/or face-to-face learning analytics. An overarching concern is how to integrate and coordinate learning analytics to provide continued support to learning across digital and physical spaces. The goals of the workshop are to share approaches and identify a set of guidelines to design and connect Learning Analytics solutions according to the pedagogical needs and contextual constraints to provide support across digital and physical learning spaces.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {486–487},
numpages = {2},
keywords = {seamless learning, monitoring, learning analytics, integration},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883854,
author = {Buckingham Shum, Simon and Knight, Simon and McNamara, Danielle and Allen, Laura and Bektik, Duygu and Crossley, Scott},
title = {Critical perspectives on writing analytics},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883854},
doi = {10.1145/2883851.2883854},
abstract = {Writing Analytics focuses on the measurement and analysis of written texts for the purpose of understanding writing processes and products, in their educational contexts, and improving the teaching and learning of writing. This workshop adopts a critical, holistic perspective in which the definition of "the system" and "success" is not restricted to IR metrics such as precision and recall, but recognizes the many wider issues that aid or obstruct analytics adoption in educational settings, such as theoretical and pedagogical grounding, usability, user experience, stakeholder design engagement, practitioner development, organizational infrastructure, policy and ethics.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {481–483},
numpages = {3},
keywords = {writing, natural language processing, education},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883853,
author = {Bull, Susan and Ginon, Blandine and Boscolo, Clelia and Johnson, Matthew},
title = {Introduction of learning visualisations and metacognitive support in a persuadable open learner model},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883853},
doi = {10.1145/2883851.2883853},
abstract = {This paper describes open learner models as visualisations of learning for learners, with a particular focus on how information about their learning can be used to help them reflect on their skills, identify gaps in their skills, and plan their future learning. We offer an approach that, in addition to providing visualisations of their learning, allows learners to propose changes to their learner model. This aims to help improve the accuracy of the learner model by taking into account student viewpoints on their learning, while also promoting learner reflection on their learning as part of a discussion of the content of their learner model. This aligns well with recent calls for learning analytics for learners. Building on previous research showing that learners will use open learner models, we here investigate their initial reactions to open learner model features to identify the likelihood of uptake in contexts where an open learner model is offered on an optional basis. We focus on university students' perceptions of a range of visualisations and their stated preferences for a facility to view evidence for the learner model data and to propose changes to the values.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {30–39},
numpages = {10},
keywords = {visual learning analytics, persuading the learner model, open learner models, learning analytics for learners},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883852,
author = {Bull, S. and Ginon, B. and Kay, J. and Kickmeier-Rust, M. and Johnson, M. D.},
title = {LAL workshop: learning analytics for learners},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883852},
doi = {10.1145/2883851.2883852},
abstract = {With the arrival of 'big data; in education, the potential was recognised for learning analytics to track students' learning, to reveal patterns in their learning, or to identify at-risk students, in addition to guiding reform and supporting educators in improving teaching and learning processes [1]. Learning Analytics dashboards have been used at all levels, including institutional, regional and national level [2]. In classroom use, while learning visualisations are often based on counts of activity data or interaction patterns, there is increasing recognition that learning analytics relate to learning, and should therefore provide pedagogically useful information [3]. While increasing numbers of technology-enhanced learning applications are embracing the potential of learning analytics at the classroom level, often these are aimed at teachers. However, learners can also benefit from learning analytics data (e.g. [4][5]).},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {496–497},
numpages = {2},
keywords = {visual learning analytics, open learner models, learning data for learners, learning analytics for learners, dashboards},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@proceedings{10.1145/2883851,
title = {LAK '16: Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {We are pleased to welcome you to the 6th International Conference on Learning Analytics and Knowledge (LAK16). This year's conference is held in the beautiful city of Edinburgh, Scotland, April 25-29. For the first time, the international Learning Analytics and Knowledge conference is co-located with ACM Learning @ Scale 2016. The LAK16 conference is organized by the Society for Learning Analytics Research (SoLAR), and this year is hosted by the University of Edinburgh, a university with a long and rich history of innovation and research in teaching, learning and technologies. Building on the momentum generated from previous LAK conferences, we have extend invitations to practitioners, researchers, administrators, government and industry groups alike, interested in the field of learning analytics and related disciplines. This annual conference provides a multidisciplinary forum for addressing the critical issues and challenges confronting the education sector today. A particular emphasis of this year's program is enhancing our impact through synergistic connections with other related research communities.The field of learning analytics is rapidly growing in all facets of its research, application into practice and theoretical contributions. The theme for the 6th International Learning Analytics and Knowledge (LAK16) conference aims to explore the multidisciplinary connections that effectively illustrate how learning analytics can provide critical insights into the individual and collective learning process. This year's theme particularly highlights the multidisciplinary nature of the field and embraces the convergence of these disciplines to provide theoretical and practical insights that will further advance the field - through research, adoption and implementation and ultimately provide a foundation for informing government and institutional policy. We invite research and practice papers that address the "convergence of communities" in LAK and bring a novel perspective and approach for reflecting on the field. This theme is reflected in the workshops, papers, posters, panels, and especially our keynote talks. The conference will culminate with a leadership panel featuring leaders from a spectrum of research societies dedicated to advancing technology in service of education.},
location = {Edinburgh, United Kingdom}
}

@inproceedings{10.1145/2723576.2723665,
author = {Harrer, Andreas and G\"{o}hnert, Tilman},
title = {Integrated representations and small data: towards contextualized and embedded analytics tools for learners},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723665},
doi = {10.1145/2723576.2723665},
abstract = {We present an approach to support learners by means of visualization and contextualization of learning analytics interventions in the learning process. We follow up on conceptual work of colleagues and derive further design principles oriented towards learners as recipients of LA results. These are shown with implementations in two distinct projects to fulfill learners information in collaborative learning processes.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {406–407},
numpages = {2},
keywords = {learner-centered analytics, embedded visualizations, design principles for interventions},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723663,
author = {Xu, Zhenhua and Makos, Alexandra},
title = {Investigating the impact of a notification system on student behaviors in a discourse-intensive hybrid course: a case study},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723663},
doi = {10.1145/2723576.2723663},
abstract = {This study investigated the effects of students' opting to use notification tools in a collaborative discourse-intensive online graduate course. Social constructivism and self-expectancy theory were applied to frame our understanding of the interactive relationship between the use of the notification tools, student's online contribution behavior and student's self-expectancy. Log-data from a 12-week hybrid (online and face-to-face) graduate course at a Canadian faculty of education was analyzed. Findings from the correlation, mediation and ANOVA analyses suggested that activation of the notification tool system positively affected students' contribution behavior and that the influence of the use of notification tools on student contribution behavior was partially mediated by student's self-expectancy.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {402–403},
numpages = {2},
keywords = {student online behavior, self-expectancy, notification system},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723662,
author = {Corrin, Linda and de Barba, Paula},
title = {How do students interpret feedback delivered via dashboards?},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723662},
doi = {10.1145/2723576.2723662},
abstract = {Providing feedback directly to students on their engagement and performance in educational activities is important to supporting students' learning. However, questions have been raised whether such data representations are adequate to inform reflection, planning and monitoring of students' learning strategies. In this poster we present an investigation of how students interpret feedback delivered via learning analytics dashboards. The findings indicated that most students were able to articulate an interpretation of the feedback presented through the dashboard to identify gaps between their expected and actual performance to inform changes to their study strategies. However, there was also evidence of uncertain interpretation both in terms of the format of the visualization of the feedback and their inability to understand the connection between the feedback and their current strategies. The findings have been used to inform recommendations for ways to enhance the effectiveness of the delivery of feedback through dashboards to provide value to students in developing effective learning strategies to meet their educational goals.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {430–431},
numpages = {2},
keywords = {self-regulated learning, learning analytics, feedback, dashboards},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723661,
author = {Shehata, Shady and Arnold, Kimberly E.},
title = {Measuring student success using predictive engine},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723661},
doi = {10.1145/2723576.2723661},
abstract = {A basic challenge in delivering global education is improving student success. Institutions of education are increasingly focused on improving graduation and retention rates of their students. In this poster, we describe Student Success System (S3) that can measure student performance starting from the first weeks of the semester and the adoption process for S3 by University of Wisconsin System (UWS).},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {416–417},
numpages = {2},
keywords = {student success, regression analysis, predictive modeling, machine learning, learning analytics, data mining, algorithms},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723660,
author = {Niemann, Katja},
title = {Increasing the accessibility of learning objects by automatic tagging},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723660},
doi = {10.1145/2723576.2723660},
abstract = {Data sets coming from the educational domain often suffer from sparsity. Hence, they might comprise potentially useful learning objects that are not findable by the users. In order to address this problem, we present a new way to automatically assign tags and classifications to learning objects offered by educational web portals that is solely based on the objects' usage.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {414–415},
numpages = {2},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723659,
author = {Worsley, Marcelo and Blikstein, Paulo},
title = {Using learning analytics to study cognitive disequilibrium in a complex learning environment},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723659},
doi = {10.1145/2723576.2723659},
abstract = {Cognitive disequilibrium has received significant attention for its role in fostering student learning in intelligent tutoring systems and in complex learning environments. In this paper, we both add to and extend this discussion by analyzing the emergence of four affective states associated with disequilibrium: joy, surprise, neutrality and confusion; in a collaborative hands-on, engineering design task. Specifically, we conduct a comparison between two learning strategies to make salient how the strategies are associated with different affective states. This comparison is grounded in the construction of a probabilistic model of student affective state as defined by the frequency of each state, and the rate of transition between affective states. Through this comparison we confirm prior research that highlights the importance of confusion as a marker of knowledge construction, but put to question the notion that surprise is a significant mediator of cognitive disequilibrium. Overall, we show how modeling learner affect is useful for understanding and improving learning in complex, hands-on learning environments.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {426–427},
numpages = {2},
keywords = {learning sciences, cognition, affect},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723658,
author = {Monroy, Carlos and Rangel, Virginia Snodgrass and Bell, Elizabeth R. and Whitaker, Reid},
title = {A learning analytics approach to characterize and analyze inquiry-based pedagogical processes},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723658},
doi = {10.1145/2723576.2723658},
abstract = {Here we describe the use of learning analytics (LA) for investigating inquiry-based science instruction. We define several variables that quantify curriculum usage and leverage tools from process mining to examine inquiry-based pedagogical processes. These are initial steps toward measuring and modeling fidelity of implementation of a science curriculum. We use data from one school district's use of an online science curriculum (N=1,021 teachers and nearly 330,000 page views).},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {398–399},
numpages = {2},
keywords = {process mining, learninformatics, inquiry-based pedagogy},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723657,
author = {Dodge, Bernie and Whitmer, John and Frazee, James P.},
title = {Improving undergraduate student achievement in large blended courses through data-driven interventions},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723657},
doi = {10.1145/2723576.2723657},
abstract = {This pilot study applied Learning Analytics methods to identify students at-risk of not succeeding in two high enrollment courses with historically low pass rates at San Diego State University: PSY 101 and STAT 119. With input from instructors, targeted interventions were developed and sent to participating students (n=882) suggesting ways to improve their performance. An experimental design was used with half of the students randomly assigned to receive these interventions via email and the other half being analyzed for at-risk triggers but receiving no intervention. Pre-course surveys on student motivation [4] and prior subject matter knowledge were conducted, and students were asked to maintain weekly logs of their activity online and offline connected to the courses. Regression analyses, incorporating feature selection methods to account for student demographic data, were used to compare the impact of the interventions between the control and experimental groups. Results showed that the interventions were associated with a higher final grade in one course, but only for a particular demographic group.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {412–413},
numpages = {2},
keywords = {time logs, motivation, learning management systems, learning analytics, large enrollment courses, interventions, blended learning, at-risk student prediction},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723656,
author = {Mor, Dalit and Laks, Hagar and Hershkovitz, Arnon},
title = {Predicting post-training readiness to work with computers: the predominance of log-based variables},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723656},
doi = {10.1145/2723576.2723656},
abstract = {In today's job market, computer skills are part of the prerequisites for many jobs. In this paper, we report on a study of readiness to work with computers (the dependent variable) among unemployed women (N=54) after participating in a unique training focused on computer skills and empowerment. Associations were explored between this variable and 17 variables from four categories: log-based, computer literacy and experience, job-seeking motivation and practice, and training satisfaction. Only two variables were associated with the dependent variable: Knowledge post-test duration and satisfaction with content. Building a prediction model of the dependent variable, another feature was highlighted: Total number of actions in the course website along the course. Our analyses highlight the predominance of the log-based variables over the variables from the other categories, and we thoroughly discuss this finding.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {400–401},
numpages = {2},
keywords = {working with computers, work readiness, decision tree},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723655,
author = {Maruya, Kazushi and Watanabe, Junji and Takahashi, Hiroyuki and Hashiba, Shoji},
title = {A learning system utilizing learners' active tracing behaviors},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723655},
doi = {10.1145/2723576.2723655},
abstract = {A monitoring system that does not disturb learners' motivation and attention is important, especially in online learning with massive numbers of participants. We propose a learning system, called the finger trail learning system (FTLS), that can monitor participants' learning attitude by means of their finger movements. On the display of the FTLS, letters are presented with low contrast in the initial state, and the contrast of the letters changes to high when they are traced by learners. We implemented the FTLS as an iOS application and confirmed that the software can be utilized to monitor learners' attitudes. In addition, we compared trails of finger movements between participants with high and low performance. The results show that the trail of finger movements recorded by the FTLS can be an index of learners' attitudes.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {418–419},
numpages = {2},
keywords = {interpersonal interaction, finger tracing, dynamic text display},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723654,
author = {Absar, Rafa and Gruzd, Anatoliy and Haythornthwaite, Caroline and Paulin, Drew},
title = {Media multiplexity in connectivist MOOCs},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723654},
doi = {10.1145/2723576.2723654},
abstract = {In this poster, we present work on exploring use of multiple social media platforms for learning in two connectivist MOOCs (or cMOOCs) to develop and evaluate methods for learning analytics to detect and study collaborative learning processes.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {424–425},
numpages = {2},
keywords = {social networks, social media, learning, connectivism, MOOCs},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723653,
author = {Ye, Shiwei and Sun, Yuan and Wang, Haobo and Sun, Yi},
title = {Minimum information entropy based q-matrix learning in DINA model},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723653},
doi = {10.1145/2723576.2723653},
abstract = {Cognitive diagnosis models (CDMs) are of growing interest in test development and measurement of learners' performance. The DINA (deterministic input, noisy, and gate) model is one of the most widely used models in CDM. In this paper, we propose a new method and present an alternating recursive algorithm to learn Q-matrix and uncertainty variables, slip and guessing parameters, based on Boolean Matrix Factorization (BMF) and Minimized Information Entropy (MIE) respectively for the DINA model. Simulation results show that our algorithm for Q-matrix learning has fast convergence to the local optimal solutions for Q-matrix and students' knowledge states A matrix. This is especially important and applicable when the method is extended to big data.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {404–405},
numpages = {2},
keywords = {rule space, q-matrix, approximation algorithm, Boolean matrix factorization},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723652,
author = {Hawn, Aaron},
title = {The bridge report: bringing learning analytics to low-income, urban schools},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723652},
doi = {10.1145/2723576.2723652},
abstract = {Widespread adoption of learning analytics for risk prediction faces different challenges at low-income secondary schools than at post-secondary institutions, where such methods have been more widely adopted. To leverage the benefits of learning analytics for under-resourced communities, educators must overcome the barriers to adoption faced by local schools: internet access, data integration, data interpretation, and local alignment. We present the case study of an enhanced reporting tool for parents and teachers, the Bridge Report, locally designed to meet the needs of a low-income secondary school in New York City. Parent and Teacher focus groups suggest that addressing local obstacles to learning analytics can create conditions for enthusiastic adoption by parents and teachers.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {410–411},
numpages = {2},
keywords = {risk prediction, predictive analytics, learning analytics, instructor support},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723651,
author = {Nwanganga, Frederick and Aguiar, Everaldo and Ambrose, G. Alex and Goodrich, Victoria and Chawla, Nitesh V.},
title = {Qualitatively exploring electronic portfolios: a text mining approach to measuring student emotion as an early warning indicator},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723651},
doi = {10.1145/2723576.2723651},
abstract = {The collection and analysis of student-level data is quickly becoming the norm across school campuses. More and more institutions are starting to use this resource as a window into better understanding the needs of their student population. In previous work, we described the use of electronic portfolio data as a proxy to measuring student engagement, and showed how it can be predictive of student retention. This paper highlights our ongoing efforts to explore and measure the valence of positive and negative emotions in student reflections and how they can serve as an early warning indicator of student disengagement.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {422–423},
numpages = {2},
keywords = {text mining, reflecting learning, quantified self, predictive analytics, natural language processing, emotions, analytic approaches &amp; methods, affect},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723650,
author = {Barmaki, Roghayeh and Hughes, Charles E.},
title = {A case study to track teacher gestures and performance in a virtual learning environment},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723650},
doi = {10.1145/2723576.2723650},
abstract = {As part of normal interpersonal communication, people send and receive messages with their body, especially with their hands. Gestures play an important role in teacher-student classroom interactions. In the domain of education, many research projects have focused on the study of such gestures either in real classrooms or in tutorial settings with experienced teachers. Novice teachers especially need to understand the messages they are sending through nonverbal communication as this can have a major effect on their ability to manage behaviors and deliver content. Such learning should optimally occur before experiencing the real classroom. To assist in this process, we have developed a virtual classroom environment- TeachLivE- and used it for teacher practice, reflection and assessment. This paper investigates the way teachers use gestures in the virtual classroom settings of TeachLivE. Biology and algebra teachers were evaluated in our study. Analysis of video recordings from real and virtual environment seems to indicate that algebra teachers gesture significantly more often than biology teachers. These results have implications for providing useful feedback to participant teachers.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {420–421},
numpages = {2},
keywords = {virtual learning environment, teacher preparation, professional development, grounding, gesture},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723649,
author = {Rogers, Tim and Colvin, Cassandra and West, Deborah and Dawson, Shane},
title = {Learning analytics in Oz: what's happening now, what's planned, and where could it (and should it) go?},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723649},
doi = {10.1145/2723576.2723649},
abstract = {This poster outlines the process and purpose of two related Australian Office for Learning and Teaching (OLT) commissioned grants to investigate the current usage and future potential of learning analytics in Australian Higher Education, with a view to developing resources to guide Australian universities in their adoption of learning analytics. The commissioned grants run from February 2014 to June 2015. Preliminary results will be available for LAK 15.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {432–433},
numpages = {2},
keywords = {student retention, maturity model, learning analytics, institutional preparedness},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723648,
author = {Shibani, Antonette and Koh, Elizabeth and Hong, Helen},
title = {Text mining approach to automate teamwork assessment in group chats},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723648},
doi = {10.1145/2723576.2723648},
abstract = {The increasing use of chat tools for learning and collaboration emphasizes the need for automating assessment. We propose a text mining approach to automate teamwork assessment in chat data. This supervised training approach can be extended to other domains for efficient assessment.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {434–435},
numpages = {2},
keywords = {text mining, teamwork, collaboration, chat, assessment},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723647,
author = {Dascalu, Mihai and Stavarache, Larise L. and Trausan-Matu, Stefan and Dessus, Philippe and Bianco, Maryse and McNamara, Danielle S.},
title = {ReaderBench: An Integrated Tool Supporting both Individual and Collaborative Learning},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723647},
doi = {10.1145/2723576.2723647},
abstract = {The core of our ReaderBench software framework exposes a unified vision for predicting and assessing comprehension in both individual and collaborative learning scenarios. ReaderBench aims to improve both the quality and the classification of the analyzed documents by using an expanded range of criteria such as: morphology, semantics, discourse analysis with emphasis on polyphony and dialogism, thus providing reliable support for both tutors and students across a range of educational settings. ReaderBench uses a unitary cohesion-based representation of discourse applied into three major directions, all tightly connected by the underlying model and the Natural Language Processing (NLP) computations: reading strategies, textual complexity, and collaboration evaluation in Computer Supported Collaborative Learning (CSCL) conversations.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {436–437},
numpages = {2},
keywords = {Textual Complexity, Reading Strategies, Participation/Collaboration Assessment, Learning Analytics, Discourse Analysis},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723645,
author = {Zushi, Mitsumasa and Miyazaki, Yoshinori and Norizuki, Ken},
title = {Analysis of learners' study logs: mouse trajectories to identify the occurrence of hesitation in solving word-reordering problems},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723645},
doi = {10.1145/2723576.2723645},
abstract = {In this paper, we describe a Web application we have been developing in order to help both teachers and learners notice the crucial aspects of solving word-reordering problems (WRPs). Also, we discuss ways to analyze the recorded mouse trajectories, response time, and drag and drop (D&amp;D) logs, because these records are potential indicators of the degree of learners' understanding.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {428–429},
numpages = {2},
keywords = {study logs, occurrence of hesitation, mouse trajectory, information retrieving tool, e-learning},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723644,
author = {Kang, Raymond and Radinsky, Josh and Lyons, Leilah},
title = {Frequent sequential interactions as opportunities to engage in temporal reasoning with an online GIS},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723644},
doi = {10.1145/2723576.2723644},
abstract = {Temporal reasoning (i.e., reasoning about relationships across time) is complex and difficult, particularly when engaged through complex media such as online Geographic Information System (GIS) applications. Partnering with Social Explorer (SE), a Web-based GIS application that allows users to create interactive visualizations of large sociological datasets, we engaged in frequent sequential pattern mining of a database of users' interactions with SE. The resulting frequent sequences provide initial descriptions of how SE affords opportunities to engage in temporal reasoning.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {408–409},
numpages = {2},
keywords = {temporal reasoning, opportunities to learn, GIS},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723643,
author = {Duval, Erik and Verbert, Katrien and Klerkx, Joris and Wolpers, Martin and Pardo, Abelardo and Govaerts, Sten and Gillet, Denis and Ochoa, Xavier and Parra, Denis},
title = {VISLA: visual aspects of learning analytics},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723643},
doi = {10.1145/2723576.2723643},
abstract = {In this paper, we briefly describe the goal and activities of the LAK15 workshop on Visual Aspects of Learning analytics.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {394–395},
numpages = {2},
keywords = {visual analytics, learning analytics, information visualisation},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723642,
author = {Drachsler, Hendrik and Hoel, Tore and Scheffel, Maren and Kismih\'{o}k, G\'{a}bor and Berg, Alan and Ferguson, Rebecca and Chen, Weiqin and Cooper, Adam and Manderveld, Jocelyn},
title = {Ethical and privacy issues in the application of learning analytics},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723642},
doi = {10.1145/2723576.2723642},
abstract = {The large-scale production, collection, aggregation, and processing of information from various learning platforms and online environments have led to ethical and privacy concerns regarding potential harm to individuals and society. In the past, these types of concern have impacted on areas as diverse as computer science, legal studies and surveillance studies. Within a European consortium that brings together the EU project LACE, the SURF SIG Learning Analytics, the Apereo Foundation and the EATEL SIG dataTEL, we aim to understand the issues with greater clarity, and to find ways of overcoming the issues and research challenges related to ethical and privacy aspects of learning analytics practice. This interactive workshop aims to raise awareness of major ethics and privacy issues. It will also be used to develop practical solutions to advance the application of learning analytics technologies.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {390–391},
numpages = {2},
keywords = {surveillance, privacy, legal rights, learning analytics, ethics, data ownership},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723641,
author = {Drachsler, Hendrik and Dietze, Stefan and Herder, Eelco and d'Aquin, Mathieu and Taibi, Davide and Scheffel, Maren},
title = {The 3rd LAK data competition},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723641},
doi = {10.1145/2723576.2723641},
abstract = {The LAK Data Challenge 2015 continues the research efforts of the previous data competitions in 2013 and 2014 by stimulating research on the evolving fields Learning Analytics (LA) and Educational Data Mining (EDM). Building on a series of activities of the LinkedUp project, the challenge aims to generate new insights and analysis on the LA &amp; EDM disciplines and is supported through the LAK Dataset - a unique corpus of LA &amp; EDM literature, exposed in structured and machine-readable formats.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {396–397},
numpages = {2},
keywords = {visualization, linked data, learning analytics, data mining},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723639,
author = {Hickey, Daniel and Jovanovic, Jelena and Lonn, Steve and Willis, James E.},
title = {2nd int'l workshop on open badges in education (OBIE 2015): from learning evidence to learning analytics},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723639},
doi = {10.1145/2723576.2723639},
abstract = {Open digital badges are Web-enabled tokens of learning and accomplishment. Unlike traditional grades, certificates, and transcripts, badges include specific claims about learning accomplishments and detailed evidence in support of those claims. Considering the richness of data associated with Open Badges, it is reasonable to expect a very powerful predictive element at the intersection of Open Badges and Learning Analytics. This could have substantial implications for recommending and exposing students to a variety of curricular and co-curricular pathways utilizing data sources far more nuanced than grades and achievement tests. Therefore, this workshop was aimed at: i) examining the potentials of Open Badges (including the associated data and resources) to provide new and potentially unprecedented data for analysis; ii) examining the kinds of Learning Analytics methods and techniques that could be suitable for gaining valuable insights from and/or making predictions based on the evidence (data and resources) associated with badges, and iii) connecting Open Badges communities, aiming to allow for the exchange of experiences and learning from different cultures and communities.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {392–393},
numpages = {2},
keywords = {open badges, online learning, learning analytics, education},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723638,
author = {Knight, Simon and Wise, Alyssa F. and Chen, Bodong and Cheng, Britte Haugan},
title = {It's about time: 4th international workshop on temporal analyses of learning data},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723638},
doi = {10.1145/2723576.2723638},
abstract = {Interest in analyses that probe the temporal aspects of learning continues to grow. The study of common and consequential sequences of events (such as learners accessing resources, interacting with other learners and engaging in self-regulatory activities) and how these are associated with learning outcomes, as well as the ways in which knowledge and skills grow or evolve over time are both core areas of interest. Learning analytics datasets are replete with fine-grained temporal data: click streams; chat logs; document edit histories (e.g. wikis, etherpads); motion tracking (e.g. eye-tracking, Microsoft Kinect), and so on. However, the emerging area of temporal analysis presents both technical and theoretical challenges in appropriating suitable techniques and interpreting results in the context of learning. The learning analytics community offers a productive focal ground for exploring and furthering efforts to address these challenges. This workshop, the fourth in a series on temporal analysis of learning, provides a focal point for analytics researchers to consider issues around and approaches to temporality in learning analytics.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {388–389},
numpages = {2},
keywords = {temporality, sequence mining, learning analytics, knowledge building, discourse analytics, CSCL},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723637,
author = {Ferguson, Rebecca and Cooper, Adam and Drachsler, Hendrik and Kismih\'{o}k, G\'{a}bor and Boyer, Anne and Tammets, Kairit and Mon\'{e}s, Alejandra Mart\'{\i}nez},
title = {Learning analytics: European perspectives},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723637},
doi = {10.1145/2723576.2723637},
abstract = {Since the emergence of learning analytics in North America, researchers and practitioners have worked to develop an international community. The organization of events such as SoLAR Flares and LASI Locals, as well as the move of LAK in 2013 from North America to Europe, has supported this aim. There are now thriving learning analytics groups in North American, Europe and Australia, with smaller pockets of activity emerging on other continents. Nevertheless, much of the work carried out outside these forums, or published in languages other than English, is still inaccessible to most people in the community. This panel, organized by Europe's Learning Analytics Community Exchange (LACE) project, brings together researchers from five European countries to examine the field from European perspectives. In doing so, it will identify the benefits and challenges associated with sharing and developing practice across national boundaries.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {69–72},
numpages = {4},
keywords = {learning analytics, learning, international, education, LACE project, Europe},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723636,
author = {Xiong, Xiaolu and Wang, Yan and Beck, Joseph Barbosa},
title = {Improving students' long-term retention performance: a study on personalized retention schedules},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723636},
doi = {10.1145/2723576.2723636},
abstract = {Traditional practices of spacing and expanding retrieval practices have typically fixed their spacing intervals to one or few predefined schedules [5, 7]. Few have explored the advantages of using personalized expanding intervals and scheduling systems to adapt to the knowledge levels and learning patterns of individual students. In this work, we are concerned with estimating the effects of personalized expanding intervals on improving students' long-term mastery level of skills. We developed a Personalized Adaptive Scheduling System (PASS) in ASSISTments' retention and relearning workflow. After implementing the PASS, we conducted a study to investigate the impact of personalized scheduling on long-term retention by comparing results from 97 classes in the summer of 2013 and 2014. We observed that students in PASS outperformed students in traditional scheduling systems on long-term retention performance (p = 0.0002), and that in particular, students with medium level of knowledge demonstrated reliable improvement (p = 0.0209) with an effect size of 0.27. In addition, the data we gathered from this study also helped to expose a few issues we have with the new system. These results suggest personalized knowledge retrieval schedules are more effective than fixed schedules and we should continue our future work on examining approaches to optimize PASS.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {325–329},
numpages = {5},
keywords = {spacing effect, retrieval practice, personalization, knowledge retention, intelligent tutoring system},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723635,
author = {Gibson, Andrew and Kitto, Kirsty},
title = {Analysing reflective text for learning analytics: an approach using anomaly recontextualisation},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723635},
doi = {10.1145/2723576.2723635},
abstract = {Reflective writing is an important learning task to help foster reflective practice, but even when assessed it is rarely analysed or critically reviewed due to its subjective and affective nature. We propose a process for capturing subjective and affective analytics based on the identification and recontextualisation of anomalous features within reflective text. We evaluate 2 human supervised trials of the process, and so demonstrate the potential for an automated Anomaly Recontextualisation process for Learning Analytics.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {275–279},
numpages = {5},
keywords = {reflective text, learning analytics, affective computing},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723634,
author = {Hickey, Daniel T. and Quick, Joshua D. and Shen, Xinyi},
title = {Formative and summative analyses of disciplinary engagement and learning in a big open online course},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723634},
doi = {10.1145/2723576.2723634},
abstract = {Situative theories of knowing and participatory approaches to learning and assessment were used to offer a big open online course on Educational Assessment using Google CourseBuilder in 2013. The course was started by 160 students and completed by 60, with relatively extensive instructor interaction with individual learners. This yielded much higher levels of engagement and learning than are typical of open or conventional online courses. The course was further refined and offered a second time in 2014, where it was started by 76 students and completed by 22, with a much lower level of support. Comparable levels of engagement and learning were obtained, suggesting that this participatory approach to learning and assessment can indeed be managed with more typical instructor support. Nonetheless, additional automation and streamlining is called for if the model is to eventually be used in massive online courses with thousands of students or as an autonomous self-paced open course.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {310–314},
numpages = {5},
keywords = {social learning analysis, personalized learning, learning analytics, assessment, analytic approaches},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723633,
author = {Vogelsang, Tim and Ruppertz, Lara},
title = {On the validity of peer grading and a cloud teaching assistant system},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723633},
doi = {10.1145/2723576.2723633},
abstract = {We introduce a new grading system, the Cloud Teaching Assistant System (CTAS), as an additional element to instructor grading, peer grading and automated validation in massive open online courses (MOOCs). The grading distributions of the different approaches are compared in an experiment consisting of 476 exam participants. 25 submissions were graded by all four methods. 451 submissions were graded only by peer grading and automated validation. The results of the experiment suggest that both CTAS and peer grading do not simulate instructor grading (Pearson's correlations: 0.36, 0.39). If the CTAS and not the instructor is assumed to deliver accurate grading, peer grading is concluded to be a valid grading method (Pearson's correlation: 0.76).},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {41–50},
numpages = {10},
keywords = {validity, peer grading, iversity, MOOCs, CTAS},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723632,
author = {Holman, Caitlin and Aguilar, Stephen J. and Levick, Adam and Stern, Jeff and Plummer, Benjamin and Fishman, Barry},
title = {Planning for success: how students use a grade prediction tool to win their classes},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723632},
doi = {10.1145/2723576.2723632},
abstract = {Gameful course designs require a significant shift in approach for both students and instructors. Transforming a standard course into a good game involves fundamentally altering how the course functions, most notably by giving students greater control over their work. We have developed an application, GradeCraft, to support this shift in pedagogy. A key feature of the application is the Grade Predictor, where students can explore coursework options and plan pathways to success. We observed students in two gameful courses with differing designs using the Grade Predictor in similar ways: they spent similar amounts of time per session, increased usage when assignments were due and before making significant course decisions, predicted different types of assignments at different rates, and made more predictions in preparation for the end of semester. This study describes how students plan their coursework using the GradeCraft Grade Predictor tool.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {260–264},
numpages = {5},
keywords = {learning analytics, higher education, gameful instruction, design-research},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723631,
author = {Rogers, Tim},
title = {Critical realism and learning analytics research: epistemological implications of an ontological foundation},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723631},
doi = {10.1145/2723576.2723631},
abstract = {Learning analytics is a broad church that incorporates a range of topics and methodologies. As the field has developed some tension has emerged regarding a perceived contradiction between the implied constructivist ethos of the field and prevalent empirical practices that have been characterised as 'behaviourist' and 'positivist'. This paper argues that this tension is a sign of deeper metatheoretical faultlines that have plagued the social sciences more broadly. Critical realism is advanced as a philosophy of science that can help reconcile the apparent contradictions between the constructivist aims and the empirical practices of learning analytics and simultaneously can justify learning analytics' current methodological tolerance. The paper concludes that learning analytics, arrayed in realist terms, is essentially longitudinal and multimethodological, concerned with the socio-technical systems of learning and the problems of implementation, and has the potential to be emancipatory. Some methodological implications for learning analytics practice are discussed.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {223–230},
numpages = {8},
keywords = {theory, philosophy of science, critical realism},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723630,
author = {Eagle, Michael and Hicks, Drew and Peddycord, Barry and Barnes, Tiffany},
title = {Exploring networks of problem-solving interactions},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723630},
doi = {10.1145/2723576.2723630},
abstract = {Intelligent tutoring systems and other computer-aided learning environments produce large amounts of transactional data on student problem-solving behavior, in previous work we modeled the student-tutor interaction data as a complex network, and successfully generated automated next-step hints as well as visualizations for educators. In this work we discuss the types of tutoring environments that are best modeled by interaction networks, and how the empirical observations of problem-solving result in common network features. We find that interaction networks exhibit the properties of scale-free networks such as vertex degree distributions that follow power law. We compare data from two versions of a propositional logic tutor, as well as two different representations of data from an educational game on programming. We find that statistics such as degree assortativity and the scale-free metric allow comparison of the network structures across domains, and provide insight into student problem solving behavior.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {21–30},
numpages = {10},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723629,
author = {Scheffel, Maren and Drachsler, Hendrik and Specht, Marcus},
title = {Developing an evaluation framework of quality indicators for learning analytics},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723629},
doi = {10.1145/2723576.2723629},
abstract = {This paper presents results from the continuous process of developing an evaluation framework of quality indicators for learning analytics (LA). Building on a previous study, a group concept mapping approach that uses multidimensional scaling and hierarchical clustering, the study presented here applies the framework to a collection of LA tools in order to evaluate the framework. Using the quantitative and qualitative results of this study, the first version of the framework was revisited so as to allow work towards an improved version of the evaluation framework of quality indicators for LA.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {16–20},
numpages = {5},
keywords = {quality indicators, group concept mapping, evaluation framework, assessment of learning analytics tools},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723628,
author = {Beheshitha, Sanam Shirazi and Ga\v{s}evi\'{c}, Dragan and Hatala, Marek},
title = {A process mining approach to linking the study of aptitude and event facets of self-regulated learning},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723628},
doi = {10.1145/2723576.2723628},
abstract = {Research on self-regulated learning has taken main two paths: self-regulated learning as aptitudes and more recently, self-regulated learning as events. This paper proposes the use of the Fuzzy miner process mining technique to examine the relationship between students' self-reported aptitudes (i.e., achievement goal orientation and approaches to learning) and strategies followed in self-regulated learning. A pilot study is conducted to probe the method and the preliminary results are reported.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {265–269},
numpages = {5},
keywords = {self-regulated learning, process mining, learning patterns, clustering},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723627,
author = {Kitto, Kirsty and Cross, Sebastian and Waters, Zak and Lupton, Mandy},
title = {Learning analytics beyond the LMS: the connected learning analytics toolkit},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723627},
doi = {10.1145/2723576.2723627},
abstract = {We present a Connected Learning Analytics (CLA) toolkit, which enables data to be extracted from social media and imported into a Learning Record Store (LRS), as defined by the new xAPI standard. A number of implementation issues are discussed, and a mapping that will enable the consistent storage and then analysis of xAPI verb/object/activity statements across different social media and online environments is introduced. A set of example learning activities are proposed, each facilitated by the Learning Analytics beyond the LMS that the toolkit enables.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {11–15},
numpages = {5},
keywords = {xAPI, integration, data ownership, connected learning},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723626,
author = {Suthers, Dan},
title = {From contingencies to network-level phenomena: multilevel analysis of activity and actors in heterogeneous networked learning environments},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723626},
doi = {10.1145/2723576.2723626},
abstract = {Learning in social settings is a complex phenomenon that involves multiple processes at individual and collective levels of agency. Thus, a richer understanding of learning in socio-technical networks will be furthered by analytic methods that can move between and coordinate analyses of individual, small group and network level phenomena. This paper outlines Traces, an analytic framework designed to address these and other needs, and gives examples of the framework's practical utility using data from the Tapped In educator professional network. The Traces framework identifies observable contingencies between events and uses these to build more abstract models of interaction and ties represented as graphs. Applications are illustrated to identification of sessions and key participants in the sessions, relations between sessions as mediated by participants, and longer-term participant roles.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {368–377},
numpages = {10},
keywords = {social network analysis, networked learning environments, learning analytics, interaction analysis},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723625,
author = {Pardo, Abelardo and Ellis, Robert A. and Calvo, Rafael A.},
title = {Combining observational and experiential data to inform the redesign of learning activities},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723625},
doi = {10.1145/2723576.2723625},
abstract = {A main goal for learning analytics is to inform the design of a learning experience to improve its quality. The increasing presence of solutions based on big data has even questioned the validity of current scientific methods. Is this going to happen in the area of learning analytics? In this paper we postulate that if changes are driven solely by a digital footprint, there is a risk of focusing only on factors that are directly connected to numeric methods. However, if the changes are complemented with an understanding about how students approach their learning, the quality of the evidence used in the redesign is significantly increased. This reasoning is illustrated with a case study in which an initial set of activities for a first year engineering course were shaped based only on the student's digital footprint. These activities were significantly modified after collecting qualitative data about the students approach to learning. We conclude the paper arguing that the interpretation of the meaning of learning analytics is improved when combined with qualitative data which reveals how and why students engaged with the learning tasks in qualitatively different ways, which together provide a more informed basis for designing learning activities.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {305–309},
numpages = {5},
keywords = {mixed methods analysis, learning analytics, interventions, approaches to learning, active learning},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723624,
author = {Worsley, Marcelo and Blikstein, Paulo},
title = {Leveraging multimodal learning analytics to differentiate student learning strategies},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723624},
doi = {10.1145/2723576.2723624},
abstract = {Multimodal analysis has had demonstrated effectiveness in studying and modeling several human-human and human-computer interactions. In this paper, we explore the role of multimodal analysis in the service of studying complex learning environments. We compare uni-modal and multimodal; manual and semi-automated methods for examining how students learn in a hands-on, engineering design context. Specifically, we compare human annotations, speech, gesture and electro-dermal activation data from a study (N=20) where student participating in two different experimental conditions. The experimental conditions have already been shown to be associated with differences in learning gains and design quality. Hence, one objective of this paper is to identify the behavioral practices that differed between the two experimental conditions, as this may help us better understand how the learning interventions work. An additional objective is to provide examples of how to conduct learning analytics research in complex environments and compare how the same algorithm, when used with different forms of data can provide complementary results.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {360–367},
numpages = {8},
keywords = {learning sciences, data mining, constructionist, computational},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723623,
author = {Kovanovi\'{c}, Vitomir and Ga\v{s}evi\'{c}, Dragan and Dawson, Shane and Joksimovi\'{c}, Sre\'{c}ko and Baker, Ryan S. and Hatala, Marek},
title = {Penetrating the black box of time-on-task estimation},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723623},
doi = {10.1145/2723576.2723623},
abstract = {All forms of learning take time. There is a large body of research suggesting that the amount of time spent on learning can improve the quality of learning, as represented by academic performance. The wide-spread adoption of learning technologies such as learning management systems (LMSs), has resulted in large amounts of data about student learning being readily accessible to educational researchers. One common use of this data is to measure time that students have spent on different learning tasks (i.e., time-on-task). Given that LMS systems typically only capture times when students executed various actions, time-on-task measures are estimated based on the recorded trace data. LMS trace data has been extensively used in many studies in the field of learning analytics, yet the problem of time-on-task estimation is rarely described in detail and the consequences that it entails are not fully examined.This paper presents the results of a study that examined the effects of different time-on-task estimation methods on the results of commonly adopted analytical models. The primary goal of this paper is to raise awareness of the issue of accuracy and appropriateness surrounding time-estimation within the broader learning analytics community, and to initiate a debate about the challenges of this process. Furthermore, the paper provides an overview of time-on-task estimation methods in educational and related research fields.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {184–193},
numpages = {10},
keywords = {time on task, moodle, measurement, learning management systems (LMS), learning analytics, higher education},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723622,
author = {Mostafavi, Behrooz and Eagle, Michael and Barnes, Tiffany},
title = {Towards data-driven mastery learning},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723622},
doi = {10.1145/2723576.2723622},
abstract = {We have developed a novel data-driven mastery learning system to improve learning in complex procedural problem solving domains. This new system was integrated into an existing logic proof tool, and assigned as homework in a deductive logic course. Student performance and dropout were compared across three systems: The Deep Thought logic tutor, Deep Thought with integrated hints, and Deep Thought with our data-driven mastery learning system. Results show that the data-driven mastery learning system increases mastery of target tutor-actions, improves tutor scores, and lowers the rate of tutor dropout over Deep Thought, with or without provided hints.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {270–274},
numpages = {5},
keywords = {problem selection, logic proof, knowledge tracing, data-driven mastery learning},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723621,
author = {Harrison, Scott and Villano, Renato and Lynch, Grace and Chen, George},
title = {Likelihood analysis of student enrollment outcomes using learning environment variables: a case study approach},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723621},
doi = {10.1145/2723576.2723621},
abstract = {Tertiary institutions are increasing the emphasis on generating, collecting and analyzing student data as a means of targeting student support services. This study utilizes a data set from a regional Australian university to conduct logistic regression analyzing the student enrollment outcomes. The results indicate that demographic factors have a minor effect while institutional and learning environment variables play a more significant role in determining student enrollment outcomes. Using grade distribution compared to grade point average provides better estimates as to the effect particular grades have on enrollment outcomes. Moreover, the effect of an early alert system on enrollment outcomes shows that early identification has a significant relationship to a student's choice to stay enrolled versus discontinuing, lapsing or being inactive in their enrollment. These results are vital in the targeting of student support services at the case study institution. The significant results indicate the importance of learning environment variables in understanding student enrollment outcomes at tertiary institutions. This analysis forms part of a much larger research project analyzing student retention at the institution.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {141–145},
numpages = {5},
keywords = {student retention, logistic regression, early alert systems},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723620,
author = {Davies, Randall and Nyland, Rob and Chapman, John and Allen, Gove},
title = {Using transaction-level data to diagnose knowledge gaps and misconceptions},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723620},
doi = {10.1145/2723576.2723620},
abstract = {The role of assessment in learning is to evaluate student comprehension and ability. Assessment instruments often function at the task level. What is rarely considered is the process students go through to reach the final solution. This often allows knowledge component gaps and misconceptions to go undetected. This research identified higher levels of knowledge component gaps and misunderstandings when assessing transaction-level knowledge component data than task-level final solution data. Final solution data showed little evidence that students had any misunderstanding or knowledge gaps about the use of absolute references. However, when analyzing these data at the transaction level we found evidence that far more students struggled than the analysis of the final solutions suggested.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {113–117},
numpages = {5},
keywords = {transaction level data, knowledge components, assessment},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723619,
author = {Aguiar, Everaldo and Lakkaraju, Himabindu and Bhanpuri, Nasir and Miller, David and Yuhas, Ben and Addison, Kecia L.},
title = {Who, when, and why: a machine learning approach to prioritizing students at risk of not graduating high school on time},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723619},
doi = {10.1145/2723576.2723619},
abstract = {Several hundred thousand students drop out of high school every year in the United States. Interventions can help those who are falling behind in their educational goals, but given limited resources, such programs must focus on the right students, at the right time, and with the right message. In this paper, we describe an incremental approach that can be used to select and prioritize students who may be at risk of not graduating high school on time, and to suggest what may be the predictors of particular students going off-track. These predictions can then be used to inform targeted interventions for these students, hopefully leading to better outcomes.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {93–102},
numpages = {10},
keywords = {student retention, secondary education, predictive analytics, learning analytics, early intervention},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723618,
author = {Wang, Yutao and Heffernan, Neil T. and Heffernan, Cristina},
title = {Towards better affect detectors: effect of missing skills, class features and common wrong answers},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723618},
doi = {10.1145/2723576.2723618},
abstract = {The well-studied Baker et al., affect detectors on boredom, frustration, confusion and engagement concentration with ASSISTments dataset were used to predict state tests scores, college enrollment, and even whether a student majored in a STEM field. In this paper, we present three attempts to improve upon current affect detectors. The first attempt analyzed the effect of missing skill tags in the dataset to the accuracy of the affect detectors. The results show a small improvement after correctly tagging the missing skill values. The second attempt added four features related to student classes for feature selection. The third attempt added two features that described information about student common wrong answers for feature selection. Result showed that two out of the four detectors were improved by adding the new features.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {31–35},
numpages = {5},
keywords = {missing skill, measurement, learning analytics, common wrong answers, class features, affect detection},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723617,
author = {Allen, Laura K. and Snow, Erica L. and McNamara, Danielle S.},
title = {Are you reading my mind? modeling students' reading comprehension skills with natural language processing techniques},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723617},
doi = {10.1145/2723576.2723617},
abstract = {This study builds upon previous work aimed at developing a student model of reading comprehension ability within the intelligent tutoring system, iSTART. Currently, the system evaluates students' self-explanation performance using a local, sentence-level algorithm and does not adapt content based on reading ability. The current study leverages natural language processing tools to build models of students' comprehension ability from the linguistic properties of their self-explanations. Students (n = 126) interacted with iSTART across eight training sessions where they self-explained target sentences from complex science texts. Coh-Metrix was then used to calculate the linguistic properties of their aggregated self-explanations. The results of this study indicated that the linguistic indices were predictive of students' reading comprehension ability, over and above the current system algorithms. These results suggest that natural language processing techniques can inform stealth assessments and ultimately improve student models within intelligent tutoring systems.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {246–254},
numpages = {9},
keywords = {stealth assessment, reading comprehension, natural language processing, intelligent tutoring systems, corpus linguistics},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723616,
author = {Van Inwegen, Eric and Adjei, Seth and Wang, Yan and Heffernan, Neil},
title = {An analysis of the impact of action order on future performance: the fine-grain action model},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723616},
doi = {10.1145/2723576.2723616},
abstract = {To better model students' learning, user modelling should be able to use the detailed sequence of student actions to model student knowledge, not just their right/wrong scores. Our goal is to analyze the question: "Does it matter when a hint is used?". We look at students who use identical attempt counts to get the right answer and look for the impact of help use and action order on future performance. We conclude that students who use hints too early do worse than students who use hints later. However, students who use hints, at times, may perform as well as students who do not use hints. This paper makes a novel contribution showing for the first time that paying attention to the precise sequence of hints and attempts allows better prediction of students' performance, as well as to definitively show that, when we control for the number of attempts and hints, students that attempt problems before asking for hints show higher performance on the next question. This analysis shows that the pattern of hints and attempts, not just their numbers, is important.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {320–324},
numpages = {5},
keywords = {tabling, prediction of future success, hint use, data mining, binning, action order},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723615,
author = {Hansen, John D. and Reich, Justin},
title = {Socioeconomic status and MOOC enrollment: enriching demographic information with external datasets},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723615},
doi = {10.1145/2723576.2723615},
abstract = {To minimize barriers to entry, massive open online course (MOOC) providers collect minimal demographic information about users. In isolation, these data are insufficient to address important questions about socioeconomic status (SES) and MOOC enrollment and performance. We demonstrate the use of third-party datasets to enrich demographic portraits of MOOC students and answer fundamental questions about SES and MOOC enrollment. We derive demographic information from registrants' geographic location by matching self-reported mailing addresses with data available from Esri at the census block group level and the American Community Survey at the zip code level. We then use these data to compare neighborhood income and parental education for US registrants in HarvardX courses to the US population as a whole. Overall, HarvardX registrants tend to reside in more affluent neighborhoods. Registrants on average live in neighborhoods with median incomes approximately. 45 standard deviations higher than the US population. Higher levels of parental education are also associated with a higher likelihood of registration.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {59–63},
numpages = {5},
keywords = {socioeconomic status, demographics, MOOCs, GIS},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723614,
author = {Brooks, Christopher and Chavez, Omar and Tritz, Jared and Teasley, Stephanie},
title = {Reducing selection bias in quasi-experimental educational studies},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723614},
doi = {10.1145/2723576.2723614},
abstract = {In this paper we examine the issue of selection bias in quasi-experimental (non-randomly controlled) educational studies. We provide background about common sources of selection bias and the issues involved in evaluating the outcomes of quasi-experimental studies. We describe two methods, matched sampling and propensity score matching, that can be used to overcome this bias. Using these methods, we describe their application through one case study that leverages large educational datasets drawn from higher education institutional data warehouses. The contribution of this work is the recommendation of a methodology and case study that educational researchers can use to understand, measure, and reduce selection bias in real-world educational interventions.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {295–299},
numpages = {5},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723613,
author = {Hsiao, I-Han and Awasthi, Piyush},
title = {Topic facet modeling: semantic visual analytics for online discussion forums},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723613},
doi = {10.1145/2723576.2723613},
abstract = {In this paper, we propose a novel Topic Facet Model (TFM), a probabilistic topic model that assumes all words in single sentence are generated from one topic facet. The model is applied to automatically extract forum posts semantics for uncovering the content latent structures. We further prototype a visual analytics interface to present online discussion forum semantics. We hypothesize that the semantic modeling through analytics on open online discussion forums can help users examine the post content by viewing the summarized topic facets. Our preliminary results demonstrated that TFM can be a promising method to extract topic specificity from conversational and relatively short texts in online programming discussion forums.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {231–235},
numpages = {5},
keywords = {topic modeling, programming, learning analytics, discussion forums, discourse analytics, automated assessment, TFM, SLDA, LDA},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723611,
author = {Pardo, Abelardo and Mirriahi, Negin and Dawson, Shane and Zhao, Yu and Zhao, An and Ga\v{s}evi\'{c}, Dragan},
title = {Identifying learning strategies associated with active use of video annotation software},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723611},
doi = {10.1145/2723576.2723611},
abstract = {The higher education sector has seen a shift in teaching approaches over the past decade with an increase in the use of video for delivering lecture content as part of a flipped classroom or blended learning model. Advances in video technologies have provided opportunities for students to now annotate videos as a strategy to support their achievement of the intended learning outcomes. However, there are few studies exploring the relationship between video annotations, student approaches to learning, and academic performance. This study seeks to narrow this gap by investigating the impact of students' use of video annotation software coupled with their approaches to learning and academic performance in the context of a flipped learning environment. Preliminary findings reveal a significant positive relationship between annotating videos and exam results. However, negative effects of surface approaches to learning, cognitive strategy use and test anxiety on midterm grades were also noted. This indicates a need to better promote and scaffold higher order cognitive strategies and deeper learning with the use of video annotation software.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {255–259},
numpages = {5},
keywords = {video annotation software, learning strategies, learning analytics},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723610,
author = {San Pedro, Maria O. Z. and Baker, Ryan S. and Heffernan, Neil T. and Ocumpaugh, Jaclyn L.},
title = {Exploring college major choice and middle school student behavior, affect and learning: what happens to students who game the system?},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723610},
doi = {10.1145/2723576.2723610},
abstract = {Choosing a college major is a major life decision. Interests stemming from students' ability and self-efficacy contribute to eventual college major choice. In this paper, we consider the role played by student learning, affect and engagement during middle school, using data from an educational software system used as part of regular schooling. We use predictive analytics to leverage automated assessments of student learning and engagement, investigating which of these factors are related to a chosen college major. For example, we already know that students who game the system in middle school mathematics are less likely to major in science or technology, but what majors are they more likely to select? Using data from 356 college students who used the ASSISTments system during their middle school years, we find significant differences in student knowledge, performance, and off-task and gaming behaviors between students who eventually choose different college majors.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {36–40},
numpages = {5},
keywords = {predictive analytics, knowledge modeling, engagement, educational data mining, college major choice, affect detection},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723609,
author = {Joksimovi\'{c}, Sre\'{c}ko and Kovanovi\'{c}, Vitomir and Jovanovi\'{c}, Jelena and Zouaq, Amal and Ga\v{s}evi\'{c}, Dragan and Hatala, Marek},
title = {What do cMOOC participants talk about in social media? a topic analysis of discourse in a cMOOC},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723609},
doi = {10.1145/2723576.2723609},
abstract = {Creating meaning from a wide variety of available information and being able to choose what to learn are highly relevant skills for learning in a connectivist setting. In this work, various approaches have been utilized to gain insights into learning processes occurring within a network of learners and understand the factors that shape learners' interests and the topics to which learners devote a significant attention. This study combines different methods to develop a scalable analytic approach for a comprehensive analysis of learners' discourse in a connectivist massive open online course (cMOOC). By linking techniques for semantic annotation and graph analysis with a qualitative analysis of learner-generated discourse, we examined how social media platforms (blogs, Twitter, and Facebook) and course recommendations influence content creation and topics discussed within a cMOOC. Our findings indicate that learners tend to focus on several prominent topics that emerge very quickly in the course. They maintain that focus, with some exceptions, throughout the course, regardless of readings suggested by the instructor. Moreover, the topics discussed across different social media differ, which can likely be attributed to the affordances of different media. Finally, our results indicate a relatively low level of cohesion in the topics discussed which might be an indicator of a diversity of the conceptual coverage discussed by the course participants.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {156–165},
numpages = {10},
keywords = {content analysis, connectivism, cMOOC, SNA},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723608,
author = {Gweon, G.-H. and Lee, Hee-Sun and Dorsey, Chad and Tinker, Robert and Finzer, William and Damelin, Daniel},
title = {Tracking student progress in a game-like learning environment with a Monte Carlo Bayesian knowledge tracing model},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723608},
doi = {10.1145/2723576.2723608},
abstract = {The Bayesian Knowledge Tracing (BKT) model is a popular model used for tracking student progress in learning systems such as an intelligent tutoring system. However, the model is not free of problems. Well-recognized problems include the identifiability problem and the empirical degeneracy problem. Unfortunately, these problems are still poorly understood and how they should be dealt with in practice is unclear. Here, we analyze the mathematical structure of the BKT model, identify a source of the difficulty, and construct a simple Monte Carlo BKT model to analyze the problem in real data. Using the student activity data obtained from the ramp task module at the Concord Consortium, we find that the Monte Carlo BKT analysis is capable of detecting the identifiability problem and the empirical degeneracy problem, and, more generally, gives an excellent summary of the student learning data. In particular, the student activity monitoring parameter M emerges as the central parameter.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {166–170},
numpages = {5},
keywords = {educational data mining, Monte Carlo, Bayesian knowledge tracing},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723607,
author = {Miller, William L. and Baker, Ryan S. and Labrum, Matthew J. and Petsche, Karen and Liu, Yu-Han and Wagner, Angela Z.},
title = {Automated detection of proactive remediation by teachers in reasoning mind classrooms},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723607},
doi = {10.1145/2723576.2723607},
abstract = {Among the most important tasks of the teacher in a classroom using the Reasoning Mind blended learning system is proactive remediation: dynamically planned interventions conducted by the teacher with one or more students. While there are several examples of detectors of student behavior within an online learning environment, most have focused on behaviors occurring fully within the context of the system, and on student behaviors. In contrast, proactive remediation is a teacher-driven activity that occurs outside of the system, and its occurrence is not necessarily related to the student's current task within the Reasoning Mind system. We present a sensor-free detector of proactive remediation, which is able to distinguish these activities from other behaviors involving idle time, such as on-task conversation related to immediate learning activities and off-task behavior.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {290–294},
numpages = {5},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723606,
author = {Ferguson, Rebecca and Clow, Doug},
title = {Examining engagement: analysing learner subpopulations in massive open online courses (MOOCs)},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723606},
doi = {10.1145/2723576.2723606},
abstract = {Massive open online courses (MOOCs) are now being used across the world to provide millions of learners with access to education. Many learners complete these courses successfully, or to their own satisfaction, but the high numbers who do not finish remain a subject of concern for platform providers and educators. In 2013, a team from Stanford University analysed engagement patterns on three MOOCs run on the Coursera platform. They found four distinct patterns of engagement that emerged from MOOCs based on videos and assessments. However, not all platforms take this approach to learning design. Courses on the FutureLearn platform are underpinned by a social-constructivist pedagogy, which includes discussion as an important element. In this paper, we analyse engagement patterns on four FutureLearn MOOCs and find that only two clusters identified previously apply in this case. Instead, we see seven distinct patterns of engagement: Samplers, Strong Starters, Returners, Mid-way Dropouts, Nearly There, Late Completers and Keen Completers. This suggests that patterns of engagement in these massive learning environments are influenced by decisions about pedagogy. We also make some observations about approaches to clustering in this context.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {51–58},
numpages = {8},
keywords = {learning analytics, learner engagement patterns, MOOCs},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723605,
author = {Vahdati, Sahar and Lange, Christoph and Auer, S\"{o}ren},
title = {OpenCourseWare observatory: does the quality of OpenCourseWare live up to its promise?},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723605},
doi = {10.1145/2723576.2723605},
abstract = {A vast amount of OpenCourseWare (OCW) is meanwhile being published online to make educational content accessible to larger audiences. The awareness of such courses among users and the popularity of systems providing such courses are increasing. However, from a subjective experience, OCW is frequently cursory, outdated or non-reusable. In order to obtain a better understanding of the quality of OCW, we assess the quality in terms of fitness for use. Based on three OCW use case scenarios, we define a range of dimensions according to which the quality of courses can be measured. From the definition of each dimension a comprehensive list of quality metrics is derived. In order to obtain a representative overview of the quality of OCW, we performed a quality assessment on a set of 100 randomly selected courses obtained from 20 different OCW repositories. Based on this assessment we identify crucial areas in which OCW needs to improve in order to deliver up to its promises.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {73–82},
numpages = {10},
keywords = {quality metrics, quality assessment, educational content, OpenCourseWare},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723604,
author = {Joksimovi\'{c}, Sre\'{c}ko and Dowell, Nia and Skrypnyk, Oleksandra and Kovanovi\'{c}, Vitomir and Ga\v{s}evi\'{c}, Dragan and Dawson, Shane and Graesser, Arthur C.},
title = {How do you connect? analysis of social capital accumulation in connectivist MOOCs},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723604},
doi = {10.1145/2723576.2723604},
abstract = {Connections established between learners via interactions are seen as fundamental for connectivist pedagogy. Connections can also be viewed as learning outcomes, i.e. learners' social capital accumulated through distributed learning environments. We applied linear mixed effects modeling to investigate whether the social capital accumulation interpreted through learners' centrality to course interaction networks, is influenced by the language learners use to express and communicate in two connectivist MOOCs. Interactions were distributed across the three social media, namely Twitter, blog and Facebook. Results showed that learners in a cMOOC connect easier with the individuals who use a more informal, narrative style, but still maintain a deeper cohesive structure to their communication.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {64–68},
numpages = {5},
keywords = {social processes, social network analysis, social capital, learning, language, automated text analysis, MOOCs, Coh-Metrix},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723603,
author = {Simsek, Duygu and S\'{a}ndor, \'{A}gnes and Buckingham Shum, Simon and Ferguson, Rebecca and De Liddo, Anna and Whitelock, Denise},
title = {Correlations between automated rhetorical analysis and tutors' grades on student essays},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723603},
doi = {10.1145/2723576.2723603},
abstract = {When assessing student essays, educators look for the students' ability to present and pursue well-reasoned and strong arguments. Such scholarly argumentation is often articulated by rhetorical metadiscourse. Educators will be necessarily examining metadiscourse in students' writing as signals of the intellectual moves that make their reasoning visible. Therefore students and educators could benefit from available powerful automated textual analysis that is able to detect rhetorical metadiscourse. However, there is a need to validate such technologies in higher education contexts, since they were originally developed in non-educational applications. This paper describes an evaluation study of a particular language analysis tool, the Xerox Incremental Parser (XIP), on undergraduate social science student essays, using the mark awarded as a measure of the quality of the writing. As part of this exploration, the study presented in this paper seeks to assess the quality of the XIP through correlational studies and multiple regression analysis.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {355–359},
numpages = {5},
keywords = {writing analytics, rhetorical parsing, natural language processing, metadiscourse, learning analytics, argumentation, academic writing analytics, academic writing},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723602,
author = {Xing, Wanli and Goggins, Sean},
title = {Learning analytics in outer space: a Hidden Na\"{\i}ve Bayes model for automatic student off-task behavior detection},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723602},
doi = {10.1145/2723576.2723602},
abstract = {Learning analytics (LA) has invested much effort in the investigation of students' behavior and performance within learning systems. This paper expands the influence of LA to students' behavior outside of learning systems and describes a novel machine learning model which automatically detects students' off-task behavior as students interact with a learning system, ASSISTments, based solely on log file data. We first operationalize social cognitive theory to introduce two new variables, affect states and problem set, both of which can be automatically derived from the logs, and can be considered to have a major influence on students' behavior. These two variables further work as the feature vector data for a K-means clustering algorithm in order to quantify students' different behavioral characteristics. This quantified variable representing student behavior type expands the feature space and contributes to the improvement of the various model performance compared with only time- and performance-related features. In addition, an advanced Hidden Na\"{\i}ve Bayes (HNB) algorithm is coded for off-task behavior detection and to show the best performance compared with traditional modeling techniques. Implications of the study are then discussed.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {176–183},
numpages = {8},
keywords = {social cognitive theory, off-task behavior, learning analytics, assessment, ITS, Hidden Na\"{\i}ve Bayes},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723601,
author = {Gross, Eric and Wshah, Safwan and Simmons, Isaiah and Skinner, Gary},
title = {A handwriting recognition system for the classroom},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723601},
doi = {10.1145/2723576.2723601},
abstract = {The Xerox Ignite™ Educator Support System (henceforth referred to simply as Ignite™) is a data collection, analysis, and visualization workflow and software solution to assist K-12 educators. To illustrate, suppose a third-grade teacher wants to know how well her class has grasped a lesson on fractions. She would first scan her students' homework and/or exams into the Ignite system via a range of multifunctional input devices. Xerox Ignite™ reads, interprets, and analyzes the students' work in minutes. Then the teacher can select how to view the data by choosing from numerous reports. Examples are; an "at a glance" class summary that shows who needs extra help in what areas and who is ready to move on; a "context" report showing how each skill for each student is progressing over time; a grade-level performance report that helps third-grade teachers share best practices and cluster students into learning groups; and a student feedback report that tells each student what he/she needs to improve upon. Ignite™ intent is also to make it easier for districts to administer, score and evaluate content based on academic goals set for schools and students. The scanning and 'mark lifting' technology embedded into Ignite™ reduces the time needed to correct papers and frees time for the teacher to apply detailed insights to their day-to-day instruction tasks. Critical to this function is the automated reading of student marks, including handwriting, to enable the digitization of student performance at a detailed level. In this paper we present a system level description of the Ignite™ handwriting recognition module and describe the challenges and opportunities presented in an educational environment.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {218–222},
numpages = {5},
keywords = {learning analytics, intelligent character recognition, handwriting, Xerox Ignite™},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723600,
author = {Rienties, Bart and Toetenel, Lisette and Bryan, Annie},
title = {"Scaling up" learning design: impact of learning design activities on LMS behavior and performance},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723600},
doi = {10.1145/2723576.2723600},
abstract = {While substantial progress has been made in terms of predictive modeling in the Learning Analytics Knowledge (LAK) community, one element that is often ignored is the role of learning design. Learning design establishes the objectives and pedagogical plans which can be evaluated against the outcomes captured through learning analytics. However, no empirical study is available linking learning designs of a substantial number of courses with usage of Learning Management Systems (LMS) and learning performance. Using cluster- and correlation analyses, in this study we compared how 87 modules were designed, and how this impacted (static and dynamic) LMS behavior and learning performance. Our findings indicate that academics seem to design modules with an "invisible" blueprint in their mind. Our cluster analyses yielded four distinctive learning design patterns: constructivist, assessment-driven, balanced-variety and social constructivist modules. More importantly, learning design activities strongly influenced how students were engaging online. Finally, learning design activities seem to have an impact on learning performance, in particular when modules rely on assimilative activities. Our findings indicate that learning analytics researchers need to be aware of the impact of learning design on LMS data over time, and subsequent academic performance.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {315–319},
numpages = {5},
keywords = {learning design, learning analytics, academic retention},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723599,
author = {Whitelock, Denise and Twiner, Alison and Richardson, John T. E. and Field, Debora and Pulman, Stephen},
title = {OpenEssayist: a supply and demand learning analytics tool for drafting academic essays},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723599},
doi = {10.1145/2723576.2723599},
abstract = {This paper focuses on the use of a natural language analytics engine to provide feedback to students when preparing an essay for summative assessment. OpenEssayist is a real-time learning analytics tool, which operates through the combination of a linguistic analysis engine that processes the text in the essay, and a web application that uses the output of the linguistic analysis engine to generate the feedback. We outline the system itself and present analysis of observed patterns of activity as a cohort of students engaged with the system for their module assignments. We report a significant positive correlation between the number of drafts submitted to the system and the grades awarded for the first assignment. We can also report that this cohort of students gained significantly higher overall grades than the students in the previous cohort, who had no access to OpenEssayist. As a system that is content free, OpenEssayist can be used to support students working in any domain that requires the writing of essays.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {208–212},
numpages = {5},
keywords = {online distance education, natural language processing, educational performance, automated formative feedback, academic essay writing},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723598,
author = {Mouri, Kousuke and Ogata, Hiroaki and Uosaki, Noriko},
title = {Ubiquitous learning analytics in the context of real-world language learning},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723598},
doi = {10.1145/2723576.2723598},
abstract = {This paper describes a method of the visualization and analysis for mining useful learning logs from numerous learning experiences that learners have accumulated in the real world as the ubiquitous learning logs. Ubiquitous Learning Log (ULL) is defined as a digital record of what learners have learned in the daily life using ubiquitous technologies. It allows learners to log their learning experiences with photos, audios, videos, location, RFID tag and sensor data, and to share and reuse ULL with others. By constructing real-world corpora which comprise of accumulated ULLs with information such as what, when, where, and how learners have learned in the real world and by analyzing them, we can support learners to learn more effectively. The proposed system will predict their future learning opportunities including their learning patterns and trends by analyzing their past ULLs. The prediction is made possible both by network analysis based on ULL information such as learners, knowledge, place and time and by learners' self-analysis using time-map. By predicting what they tend to learn next in their learning paths, it provides them with more learning opportunities. Accumulated data are so big and the relationships among the data are so complicated that it is difficult to grasp how closely the ULLs are related each other. Therefore, this paper proposes a system to help learners to grasp relationships among learners, knowledge, place and time, using network graphs and network analysis.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {378–382},
numpages = {5},
keywords = {ubiquitous learning log, time map, network graph, network analysis},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723597,
author = {Jim\'{e}nez-G\'{o}mez, Manuel \'{A}ngel and Luna, Jos\'{e} Mar\'{\i}a and Romero, Crist\'{o}bal and Ventura, Sebasti\'{a}n},
title = {Discovering clues to avoid middle school failure at early stages},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723597},
doi = {10.1145/2723576.2723597},
abstract = {The use of data mining techniques in educational domains helps to find new knowledge about how students learn and how to improve the resources management. Using these techniques for predicting school failure is very useful in order to carry out actions to avoid drop out. With this purpose, we try to determine the earliest stage when the quality of the results allows for clarifying the possibility of school failure. We process real information from a Spanish high school by structuring the whole data in incremental datasets, which represent how students' academic records grow. Our study reveals an early and robust detection of the risky cases of school failure at the end of the first out of four courses.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {300–304},
numpages = {5},
keywords = {school failure, educational data mining, early prediction, drop-out},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723596,
author = {Milligan, Sandra},
title = {Crowd-sourced learning in MOOCs: learning analytics meets measurement theory},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723596},
doi = {10.1145/2723576.2723596},
abstract = {This paper illustrated the promise of the combination of measurement theory and learning analytics for understanding effective MOOC learning. It reports findings from a study of whether and how MOOC log file data can assist in understanding how MOOC participants use (often) messy, chaotic forums to support complex, unpredictable, contingent learning processes. It is argued that descriptions of posting, voting and viewing behaviours do not in and of themselves provide insights about how learning is generated in MOOC forums. Rather, it is hypothesised that there is a skill involved in using forums to learn; that theory-informed descriptions of this skill illustrate how MOOC participants use forums differently as they progress from novice to expert; that the skill progression can be validated through the use of forum log file data; and that log file data can also be used to assess an individual MOOC participant's position in relation to this progression -- that is, to measure an individual's skill in learning through forums and similar educational settings. These hypotheses were examined using data drawn from forums in a large MOOC run at the University of Melbourne in 2013.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {151–155},
numpages = {5},
keywords = {rasch analysis, on-line forums, measurement theory, learning progression, learning analytics, learner performance, crowd-sourced learning, collaborative learning, analytics tools, MOOC, 21st century skills},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723595,
author = {Crossley, Scott and Allen, Laura K. and Snow, Erica L. and McNamara, Danielle S.},
title = {Pssst... textual features... there is more to automatic essay scoring than just you!},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723595},
doi = {10.1145/2723576.2723595},
abstract = {This study investigates a new approach to automatically assessing essay quality that combines traditional approaches based on assessing textual features with new approaches that measure student attributes such as demographic information, standardized test scores, and survey results. The results demonstrate that combining both text features and student attributes leads to essay scoring models that are on par with state-of-the-art scoring models. Such findings expand our knowledge of textual and non-textual features that are predictive of writing success.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {203–207},
numpages = {5},
keywords = {natural language processing, intelligent tutoring systems, individual differences, data mining, corpus linguistics, automatic essay scoring},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723594,
author = {Sekiya, Takayuki and Matsuda, Yoshitatsu and Yamaguchi, Kazunori},
title = {Curriculum analysis of CS departments based on CS2013 by simplified, supervised LDA},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723594},
doi = {10.1145/2723576.2723594},
abstract = {The curricula higher educational institutions offer is a key asset in enabling them to systematically educate their students. We have been developing a curriculum analysis method that can help to find out differences among curricula. On the basis of "Computing Science Curricula CS2013", a report released by the ACM and IEEE Computer Society, we applied our method to analyzing 10 computer science (CS) related curricula offered by CS departments of universities in the United States. Using the method enables us to compare courses across universities. Through an analysis of course syllabi distribution, we found that CS2013 uniformly covered a wide area of computer science. Some universities emphasized human factors, while others attached greater importance to theoretical ones. We also found that some CS departments offered not only a CS curriculum but also an electrical engineering one, and those departments showed a tendency to have more "Architecture and Organization (AR)" related curricula. Furthermore, we found that even though "Information Assurance and Security (IAS)" has not yet become a very popular field, some universities are already offering IAS related courses.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {330–339},
numpages = {10},
keywords = {syllabus, supervised LDA, curriculum analysis, curriculum},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723593,
author = {Kennedy, Gregor and Coffrin, Carleton and de Barba, Paula and Corrin, Linda},
title = {Predicting success: how learners' prior knowledge, skills and activities predict MOOC performance},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723593},
doi = {10.1145/2723576.2723593},
abstract = {While MOOCs have taken the world by storm, questions remain about their pedagogical value and high rates of attrition. In this paper we argue that MOOCs which have open entry and open curriculum structures, place pressure on learners to not only have the requisite knowledge and skills to complete the course, but also the skills to traverse the course in adaptive ways that lead to success. The empirical study presented in the paper investigated the degree to which students' prior knowledge and skills, and their engagement with the MOOC as measured through learning analytics, predict end-of-MOOC performance. The findings indicate that prior knowledge is the most significant predictor of MOOC success followed by students' ability to revise and revisit their previous work.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {136–140},
numpages = {5},
keywords = {prior knowledge, learning analytics, engagement, MOOCs},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723592,
author = {Snow, Erica L. and Allen, Laura K. and Jacovina, Matthew E. and Perret, Cecile A. and McNamara, Danielle S.},
title = {You've got style: detecting writing flexibility across time},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723592},
doi = {10.1145/2723576.2723592},
abstract = {Writing researchers have suggested that students who are perceived as strong writers (i.e., those who generate texts that are rated as high quality) demonstrate flexibility in their writing style. While anecdotally this has been a commonly held belief among researchers, scientists, and educators, there is little empirical research to support this claim. This study investigates this hypothesis by examining how students vary in their use of linguistic features across 16 prompt-based essays. Forty-five high school students wrote 16 essays across 8 sessions within an Automated Writing Evaluation (AWE) system. Natural language processing (NLP) techniques and Entropy analyses were used to calculate how rigid or flexible students were in their use of narrative linguistic features over time and how this trait related to individual differences in literacy ability and essay quality. Additional analyses indicated that NLP and Entropy reliably detected narrative flexibility (or rigidity) after session 2 and was related to students' prior literacy skills. These exploratory methodologies are important for researchers and educators, as they indicate that writing flexibility is indeed a trait of strong writers and can be detected rather quickly using the combination of textual features and dynamic analyses.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {194–202},
numpages = {9},
keywords = {writing quality, narrativity, individual differences, flexibility, entropy},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723591,
author = {Zhu, Mengxiao and Feng, Gary},
title = {An exploratory study using social network analysis to model eye movements in mathematics problem solving},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723591},
doi = {10.1145/2723576.2723591},
abstract = {Eye tracking is a useful tool to understand students' cognitive process during problem solving. This paper offers a unique perspective by applying techniques from social network analysis to eye movement patterns in mathematics problem solving. We construct and visualize transition networks using eye-tracking data collected from 37 8th grade students while solving linear function problems. By applying network analysis on the constructed transition networks, we find general transition patterns between areas of interest (AOIs) for all students, and we also compare patterns for high- and low-performing students. Our results show that even though students share general transition patterns during problem solving, high-performing students made more strategic transitions among AOI triples than low-performing students.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {383–387},
numpages = {5},
keywords = {problem solving, network analysis, eye tracking},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723590,
author = {Elbadrawy, Asmaa and Studham, R. Scott and Karypis, George},
title = {Collaborative multi-regression models for predicting students' performance in course activities},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723590},
doi = {10.1145/2723576.2723590},
abstract = {Methods that accurately predict the grade of a student at a given activity or course can identify students that are at risk in failing a course and allow their educational institution to take corrective actions. Though a number of prediction models have been developed, they either estimate a single model for all students based on their past course performance and interactions with learning management systems (LMS), or estimate student-specific models that do not take into account LMS interactions; thus, failing to exploit fine-grain information related to a student's engagement. In this work we present a class of collaborative multi-regression models that are personalized to each student and also take into account features related to student's past performance, engagement and course characteristics. These models use all historical information to estimate a small number of regression models shared by all students along with student-specific combination weights. This allows for information sharing and also generating personalized predictions. Our experimental evaluation on a large set of students, courses, and activities shows that these models are capable of improving the performance prediction accuracy by over 20%. In addition, we show that by analyzing the estimated models and the student-specific combination functions we can gain insights on the effectiveness of the educational material that is made available at the courses of different departments.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {103–107},
numpages = {5},
keywords = {predicting student performance, collaborative multi-regression models, analyzing student behavior},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723589,
author = {Ezen-Can, Aysu and Boyer, Kristy Elizabeth and Kellogg, Shaun and Booth, Sherry},
title = {Unsupervised modeling for understanding MOOC discussion forums: a learning analytics approach},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723589},
doi = {10.1145/2723576.2723589},
abstract = {Massively Open Online Courses (MOOCs) have gained attention recently because of their great potential to reach learners. Substantial empirical study has focused on student persistence and their interactions with the course materials. However, most MOOCs include a rich textual dialogue forum, and these textual interactions are largely unexplored. Automatically understanding the nature of discussion forum posts holds great promise for providing adaptive support to individual students and to collaborative groups. This paper presents a study that applies unsupervised student understanding models originally developed for synchronous tutorial dialogue to MOOC forums. We use a clustering approach to group similar posts, compare the clusters with manual annotations by MOOC researchers, and further investigate clusters qualitatively. This paper constitutes a step toward applying unsupervised models to asynchronous communication, which can enable massive-scale automated discourse analysis and mining to better support students' learning.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {146–150},
numpages = {5},
keywords = {text-based learning analytics, online learning, MOOCs},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723588,
author = {Ezen-Can, Aysu and Grafsgaard, Joseph F. and Lester, James C. and Boyer, Kristy Elizabeth},
title = {Classifying student dialogue acts with multimodal learning analytics},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723588},
doi = {10.1145/2723576.2723588},
abstract = {Supporting learning with rich natural language dialogue has been the focus of increasing attention in recent years. Many adaptive learning environments model students' natural language input, and there is growing recognition that these systems can be improved by leveraging multimodal cues to understand learners better. This paper investigates multimodal features related to posture and gesture for the task of classifying students' dialogue acts within tutorial dialogue. In order to accelerate the modeling process by eliminating the manual annotation bottleneck, a fully unsupervised machine learning approach is utilized for this task. The results indicate that these unsupervised models are significantly improved with the addition of automatically extracted posture and gesture information. Further, even in the absence of any linguistic features, a model that utilizes posture and gesture features alone performed significantly better than a majority class baseline. This work represents a step toward achieving better understanding of student utterances by incorporating multimodal features within adaptive learning environments. Additionally, the technique presented here is scalable to very large student datasets.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {280–289},
numpages = {10},
keywords = {tutorial dialogue, text-based learning analytics, multimodal learning analytics, dialogue act modeling},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723587,
author = {Lee, Hee-Sun and Gweon, Gey-Hong and Dorsey, Chad and Tinker, Robert and Finzer, William and Damelin, Daniel and Kimball, Nathan and Pallant, Amy and Lord, Trudi},
title = {How does Bayesian knowledge tracing model emergence of knowledge about a mechanical system?},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723587},
doi = {10.1145/2723576.2723587},
abstract = {An interactive learning task was designed in a game format to help high school students acquire knowledge about a simple mechanical system involving a car moving on a ramp. This ramp game consisted of five challenges that addressed individual knowledge components with increasing difficulty. In order to investigate patterns of knowledge emergence during the ramp game, we applied the Monte Carlo Bayesian Knowledge Tracing (BKT) algorithm to 447 game segments produced by 64 student groups in two physics teachers' classrooms. Results indicate that, in the ramp game context, (1) the initial knowledge and guessing parameters were significantly highly correlated, (2) the slip parameter was interpretable monotonically, (3) low guessing parameter values were associated with knowledge emergence while high guessing parameter values were associated with knowledge maintenance, and (4) the transition parameter showed the speed of knowledge emergence. By applying the k-means clustering to ramp game segments represented in the three dimensional space defined by guessing, slip, and transition parameters, we identified seven clusters of knowledge emergence. We characterize these clusters and discuss implications for future research as well as for instructional game design.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {171–175},
numpages = {5},
keywords = {physics learning, game-based learning, Bayesian knowledge tracing},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723586,
author = {Molenaar, Inge and Chiu, Ming Ming},
title = {Effects of sequences of socially regulated learning on group performance},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723586},
doi = {10.1145/2723576.2723586},
abstract = {Past research shows that regulative activities (metacognitive or relational) can aid learning and that sequences of cognitive, metacognitive and relational activities affect subsequent cognition. Extending this research, this study examines whether sequences of socially regulated learning differ across low, medium or high performing groups. Scaffolded by a computer avatar, 54 primary school students (working in 18 groups of 3) discussed writing a report about a foreign country for 51,338 turns. Statistical discourse analysis (SDA) of these sequences of talk showed that in high performing groups, high cognition was preceded more often by high cognition and less often by denials or low cognition. In medium performing groups, high cognition was preceded more often by high cognition or planning. As these results indicate that different sequences among students' cognitive, metacognitive and relational activities are linked to levels of performance, they can inform a micro-temporal theory of socially shared regulation.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {236–240},
numpages = {5},
keywords = {temporal analysis, scaffolding, metacognition, elementary education, discourse analysis, collaborative learning},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723585,
author = {Prinsloo, Paul and Slade, Sharon},
title = {Student privacy self-management: implications for learning analytics},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723585},
doi = {10.1145/2723576.2723585},
abstract = {Optimizing the harvesting and analysis of student data promises to clear the fog surrounding the key drivers of student success and retention, and provide potential for improved student success. At the same time, concerns are increasingly voiced around the extent to which individuals are routinely and progressively tracked as they engage online. The Internet, the very thing that promised to open up possibilities and to break down communication barriers, now threatens to narrow it again through the panopticon of mass surveillance.Within higher education, our assumptions and understanding of issues surrounding student attitudes to privacy are influenced both by the apparent ease with which the public appear to share the detail of their lives and our paternalistic institutional cultures. As such, it can be easy to allow our enthusiasm for the possibilities offered by learning analytics to outweigh consideration of issues of privacy.This paper explores issues around consent and the seemingly simple choice to allow students to opt-in or opt-out of having their data tracked. We consider how 3 providers of massive open online courses (MOOCs) inform users of how their data is used, and discuss how higher education institutions can work toward an approach which engages and more fully informs students of the implications of learning analytics on their personal data.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {83–92},
numpages = {10},
keywords = {opting out, opt out, learning analytics, informed consent, ethics},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723584,
author = {Chen, Bodong and Chen, Xin and Xing, Wanli},
title = {"Twitter Archeology" of learning analytics and knowledge conferences},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723584},
doi = {10.1145/2723576.2723584},
abstract = {The goal of the present study was to uncover new insights about the learning analytics community by analyzing Twitter archives from the past four Learning Analytics and Knowledge (LAK) conferences. Through descriptive analysis, interaction network analysis, hashtag analysis, and topic modeling, we found: extended coverage of the community over the years; increasing interactions among its members regardless of peripheral and in-persistent participation; increasingly dense, connected and balanced social networks; and more and more diverse research topics. Detailed inspection of semantic topics uncovered insights complementary to the analysis of LAK publications in previous research.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {340–349},
numpages = {10},
keywords = {topic modeling, social network, learning analytics, hashtag analysis, Twitter analytics, Twitter},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723583,
author = {Martinez-Maldonado, Roberto and Pardo, Abelardo and Mirriahi, Negin and Yacef, Kalina and Kay, Judy and Clayphan, Andrew},
title = {The LATUX workflow: designing and deploying awareness tools in technology-enabled learning settings},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723583},
doi = {10.1145/2723576.2723583},
abstract = {Designing, deploying and validating learning analytics tools for instructors or students is a challenge requiring techniques and methods from different disciplines, such as software engineering, human-computer interaction, educational design and psychology. Whilst each of these disciplines has consolidated design methodologies, there is a need for more specific methodological frameworks within the cross-disciplinary space defined by learning analytics. In particular there is no systematic workflow for producing learning analytics tools that are both technologically feasible and truly underpin the learning experience. In this paper, we present the LATUX workflow, a five-stage workflow to design, deploy and validate awareness tools in technology-enabled learning environments. LATUX is grounded on a well-established design process for creating, testing and re-designing user interfaces. We extend this process by integrating the pedagogical requirements to generate visual analytics to inform instructors' pedagogical decisions or intervention strategies. The workflow is illustrated with a case study in which collaborative activities were deployed in a real classroom.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {1–10},
numpages = {10},
keywords = {visualisations, groupware, design, dashboard, awareness},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723582,
author = {Bergner, Yoav and Colvin, Kimberly and Pritchard, David E.},
title = {Estimation of ability from homework items when there are missing and/or multiple attempts},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723582},
doi = {10.1145/2723576.2723582},
abstract = {Scoring of student item response data from online courses and especially massively open online courses (MOOCs) is complicated by two challenges, potentially large amounts of missing data and allowances for multiple attempts to answer. Approaches to ability estimation with respect to both of these issues are considered using data from a large-enrollment electrical engineering MOOC. The allowance of unlimited multiple attempts sets up a range of observed score and latent-variable approaches to scoring the constructed response homework. With respect to missing data, two classical approaches are discussed, treating omitted items as incorrect or missing at random (MAR). These treatments turn out to have slightly different interpretations depending on the scoring model. In all, twelve different homework scores are proposed based on combinations of scoring model and missing data handling. The scores are computed and correlations between each score and the final exam score are compared, with attention to different populations of course participants.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {118–125},
numpages = {8},
keywords = {psychometrics, missing data, ability estimation, MOOCs},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723581,
author = {Brooks, Christopher and Thompson, Craig and Teasley, Stephanie},
title = {A time series interaction analysis method for building predictive models of learners using log data},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723581},
doi = {10.1145/2723576.2723581},
abstract = {As courses become bigger, move online, and are deployed to the general public at low cost (e.g. through Massive Open Online Courses, MOOCs), new methods of predicting student achievement are needed to support the learning process. This paper presents a novel method for converting educational log data into features suitable for building predictive models of student success. Unlike cognitive modelling or content analysis approaches, these models are built from interactions between learners and resources, an approach that requires no input from instructional or domain experts and can be applied across courses or learning environments.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {126–135},
numpages = {10},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723580,
author = {Mandran, Nadine and Ortega, Michael and Luengo, Vanda and Bouhineau, Denis},
title = {DOP8: merging both data and analysis operators life cycles for technology enhanced learning},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723580},
doi = {10.1145/2723576.2723580},
abstract = {This paper presents DOP8: a Data Mining Iterative Cycle that improves the classical data life cycle. While the latter only combines the data production and data analysis phases, DOP8 also integrates the analysis operators life cycle. In this cycle, data life cycle and operators life cycle processing meet in the data analysis step. This paper also presents a reification of DOP8 in a new computing platform: UnderTracks. The latter provides a flexibility on storing and sharing data, operators and analysis processes. Undertracks is compared with three types of platform 'Storage platform', 'Analysis platform' and 'Storage and Analysis platform'. Several real TEL analysis scenarios are present into the platform, (1) to test Undertracks flexibility on storing data and operators and (2) to test Undertracks flexibility on designing analysis processes.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {213–217},
numpages = {5},
keywords = {sharing, process analysis, operators life cycle, flexibility, data life cycle, computing platform},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723579,
author = {Asif, Raheela and Merceron, Agathe and Pathan, Mahmood Khan},
title = {Investigating performance of students: a longitudinal study},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723579},
doi = {10.1145/2723576.2723579},
abstract = {This paper, investigates how academic performance of students evolves over the years in their studies during a study programme. To determine typical progression patterns over the years, students are described by a 4 tuple (e.g. x1, x2, x3, x4), these being the clusters' mean to which a student belongs to in each year of the degree. For this purpose, two consecutive cohorts have been analyzed using X-means clustering. Interestingly the patterns found in both cohorts show that a substantial number of students stay in the same kind of groups during their studies.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {108–112},
numpages = {5},
keywords = {students' performance, progression, X-means clustering},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723578,
author = {Dascalu, Mihai and Trausan-Matu, Stefan and Dessus, Philippe and McNamara, Danielle S.},
title = {Discourse cohesion: a signature of collaboration},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723578},
doi = {10.1145/2723576.2723578},
abstract = {As Computer Supported Collaborative Learning (CSCL) becomes increasingly adopted as an alternative to classic educational scenarios, we face an increasing need for automatic tools designed to support tutors in the time consuming process of analyzing conversations and interactions among students. Therefore, building upon a cohesion-based model of the discourse, we have validated ReaderBench, a system capable of evaluating collaboration based on a social knowledge-building perspective. Through the inter-twining of different participants' points of view, collaboration emerges and this process is reflected in the identified cohesive links between different speakers. Overall, the current experiments indicate that textual cohesion successfully detects collaboration between participants as ideas are shared and exchanged within an ongoing conversation.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {350–354},
numpages = {5},
keywords = {learning analytics, discourse analysis, computer supported collaborative learning, collaboration assessment, cohesion},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723577,
author = {Knight, Simon and Littleton, Karen},
title = {Developing a multiple-document-processing performance assessment for epistemic literacy},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723577},
doi = {10.1145/2723576.2723577},
abstract = {The LAK15 theme "shifts the focus from data to impact", noting the potential for Learning Analytics based on existing technologies to have scalable impact on learning for people of all ages. For such demand and potential in scalability to be met the challenges of addressing higher-order thinking skills should be addressed. This paper discuses one such approach--the creation of an analytic and task model to probe epistemic cognition in complex literacy tasks. The research uses existing technologies in novel ways to build a conceptually grounded model of trace-indicators for epistemic-commitments in information seeking behaviors. We argue that such an evidence centered approach is fundamental to realizing the potential of analytics, which should maintain a strong association with learning theory.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {241–245},
numpages = {5},
keywords = {social learning analytics, learning analytics, epistemic cognition, educational assessment, discourse analytics},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@proceedings{10.1145/2723576,
title = {LAK '15: Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to Learning Analytics &amp; Knowledge 2015 (LAK15), the fifth edition of this international conference. This year LAK takes place at Marist College in Poughkeepsie, a city located on the banks of the majestic Hudson River, midway between Albany, the state capital, and New York City. The theme of LAK15, Scaling Up: Big Data to Big Impact, reflects our growing community of researchers, practitioners, and learners and our success in leveraging the power of "big data" to create substantial impact within higher education and learning at increasingly larger scales, while simultaneously reflecting the need for analytics to be effective at the level of individual learning (n of 1). Building on the momentum generated in earlier conferences and in recognition of our growth and of our mission 'inventing effective means to improve the way students learn', we have introduced a practitioner track this year. We hope that addressing and discussing the learning from two points of view; that of the researcher/learner and the practitioner, will provide each of us some impulse to think diversely about the many stakeholders in learning analytics and to open new perspectives on the intersection between research and the practice.},
location = {Poughkeepsie, New York}
}

@inproceedings{10.1145/2567574.2576773,
author = {Gruzd, Anatoliy and Haythornthwaite, Caroline and Paulin, Drew and Absar, Rafa and Huggett, Michael},
title = {Learning analytics for the social media age},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2576773},
doi = {10.1145/2567574.2576773},
abstract = {In just a short period of time, social media have altered many aspects of our daily lives, from how we form and maintain social relationships to how we discover, access and share information online. Now social media are also beginning to affect how we teach and learn in this increasingly interconnected and information-rich world. The panelists will discuss their ongoing work that seeks to understand the affordances and potential roles of social media in learning, as well as to determine and provide methods that can help researchers and educators evaluate the use of social media for teaching and learning based on automated analyses of social media texts and networks. The panel will focus on the first phase of this five-year research initiative "Learning Analytics for the Social Media Age" funded by the Social Science and Humanites Research Council of Canada (2013--2018).},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {254–256},
numpages = {3},
keywords = {social media, online communities, learning networks, learning analytics},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/2567574.2567633,
author = {Gasevic, Dragan and Rose, Carolyn and Siemens, George and Wolff, Annika and Zdrahal, Zdenek},
title = {Learning analytics and machine learning},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567633},
doi = {10.1145/2567574.2567633},
abstract = {Learning analytics (LA) as a field remains in its infancy. Many of the techniques now prominent from practitioners have been drawn from various fields, including HCI, statistics, computer science, and learning sciences. In order for LA to grow and advance as a discipline, two significant challenges must be met: 1) development of analytics methods and techniques that are native to the LA discipline, and 2) practitioners in LA to develop algorithms and models that reflect the social and computational dimensions of analytics. This workshop introduces researchers in learning analytics to machine learning (ML) and the opportunities that ML can provide in building next generation analysis models.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {287–288},
numpages = {2},
keywords = {theory, machine learning, learning analytics, collaboration},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/2567574.2567632,
author = {Hoppe, H. Ulrich and Suthers, Daniel D.},
title = {Computational approaches to connecting levels of analysis in networked learning communities},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567632},
doi = {10.1145/2567574.2567632},
abstract = {The focus of this workshop is on the potential benefits and challenges of using specific computational methods to analyze interactions in networked learning environments, particularly with respect to integrating multiple analytic approaches towards understanding learning at multiple levels of agency, from individual to collective. The workshop is designed for researchers interested in analytical studies of collaborative and networked learning in socio-technical networks, using data-intensive computational methods of analysis (including social-network analysis, log-file analysis, information extraction and data mining). The workshop may also be of interest to pedagogical professionals and educational decision makers who want to evaluate the potential of learning analytics techniques to better inform their decisions regarding learning in technology-rich environments.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {285–286},
numpages = {2},
keywords = {networked learning, levels of analysis, computational interaction analysis, CSCL},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/2567574.2567631,
author = {Ferguson, Rebecca and De Liddo, Anna and Whitelock, Denise and de Laat, Maarten and Buckingham Shum, Simon},
title = {DCLA14: second international workshop on discourse-centric learning analytics},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567631},
doi = {10.1145/2567574.2567631},
abstract = {The first international workshop on discourse-centric learning analytics (DCLA) took place at LAK13 in Leuven, Belgium. That workshop succeeded in its aim of catalysing ideas and building community connections between those working in this field of social learning analytics. It also proposed a mission statement for DCLA: to devise and validate analytics that look beyond surface measures in order to quantify linguistic proxies for deeper learning. This year, the focus of the second international DCLA workshop, like that of LAK14, is on the intersection of learning analytics research, theory and practice. Once researchers have developed and validated discourse-centric analytics, how can these be successfully deployed at scale to support learning?},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {283–284},
numpages = {2},
keywords = {visualisation, social learning analytics, learning analytics, discourse, dialogue, deliberation, argumentation},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/2567574.2567630,
author = {Drachsler, Hendrik and Dietze, Stefan and Herder, Eelco and d'Aquin, Mathieu and Taibi, Davide},
title = {The learning analytics &amp; knowledge (LAK) data challenge 2014},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567630},
doi = {10.1145/2567574.2567630},
abstract = {The LAK Data Challenge 2014 continues the research efforts of the second edition by stimulating research on the evolving fields Learning Analytics (LA) and Educational Data Mining (EDM). Building on a series of activities of the LinkedUp project, the challenge aims to generate new insights and analysis on the LA &amp; EDM disciplines and is supported through the LAK Dataset - a unique corpus of LA &amp; EDM literature, exposed in structured and machine-readable formats.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {289–290},
numpages = {2},
keywords = {visualization, linked data, learning analytics, data mining},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/2567574.2567629,
author = {Rogers, Tim and Colvin, Cassandra and Chiera, Belinda},
title = {Modest analytics: using the index method to identify students at risk of failure},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567629},
doi = {10.1145/2567574.2567629},
abstract = {Regression is the tool of choice for developing predictive models of student risk of failure. However, the forecasting literature has demonstrated the predictive equivalence of much simpler methods. We directly compare one simple tabulation technique, the index method, to a linear multiple regression approach for identifying students at risk. The broader purpose is to explore the plausibility of a flexible method that is conducive to adoption and diffusion. In this respect this paper fits within the ambit of the modest computing agenda, and suggests the possibility of a modest analytics. We built both regression and index method models on 2011 student data and applied these to 2012 student data. The index method was comparable in terms of predictive accuracy of student risk. We suggest that the context specificity of learning environments makes the index method a promising tool for educators who want a situated risk algorithm that is flexible and adaptable.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {118–122},
numpages = {5},
keywords = {regression, predictive models for student performance, modest computing, index method},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/2567574.2567628,
author = {Mendiburo, Maria and Sulcer, Brian and Hasselbring, Ted},
title = {Interaction design for improved analytics},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567628},
doi = {10.1145/2567574.2567628},
abstract = {In this paper, we explain a portion of the design research process that we used to develop the learning analytics for a manipulative-based fractions intervention program. In particular, we highlight a set of qualitative interviews that we conducted with individual students after a short study in which students in three classes at the same school learned to use virtual manipulatives to compare pairs of proper fractions and order groups of 3 proper fractions. These qualitative interviews provided us with considerable information that helped us improve the interactions students have with the virtual manipulatives and produce more sophisticated and informative analytics. We emphasize the importance of using mixed-methods during the iterative cycles of development that define design research.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {78–82},
numpages = {5},
keywords = {virtual manipulatives, fractions, design research},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/2567574.2567627,
author = {Santos, Jose Luis and Klerkx, Joris and Duval, Erik and Gago, David and Rodr\'{\i}guez, Luis},
title = {Success, activity and drop-outs in MOOCs an exploratory study on the UNED COMA courses},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567627},
doi = {10.1145/2567574.2567627},
abstract = {This paper presents an exploratory study about two language learning MOOCs deployed in the UNED COMA platform. The study identifies three research questions: a) How does activity evolve in these MOOCs? b) Are all learning activities relevant?, and c) Does the use of the target language influence?. We conclude that the MOOC activity drops not only due to the drop-outs. When students skips around 10% of the proposed activities, the percentage of passing the course decrease in a 25%. Forum activity is a useful indicator for success, however the participation in active threads is not. Finally, the use of the target language course is not an indicator to predict success.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {98–102},
numpages = {5},
keywords = {visualisation, participation, learning analytics, MOOCs},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/2567574.2567626,
author = {Hickey, Daniel T. and Kelley, Tara Alana and Shen, Xinyi},
title = {Small to big before massive: scaling up participatory learning analytics},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567626},
doi = {10.1145/2567574.2567626},
abstract = {This case study describes how course features and individual &amp; social learning analytics were scaled up to support "participatory" learning. An existing online course was turned into a "big open online course" (BOOC) offered to hundreds. Compared to typical open courses, relatively high levels of persistence, individual &amp; social engagement, and achievement were obtained. These results suggest that innovative learning analytics might best be scaled (a) incrementally, (b) using design-based research methods, (c) focusing on engagement in consequential &amp; contextual knowledge, (d) using emerging situative assessment theories.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {93–97},
numpages = {5},
keywords = {social learning analysis, personalized learning, learning analytics, assessment, analytic approaches},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/2567574.2567625,
author = {Aguilar, Stephen and Lonn, Steven and Teasley, Stephanie D.},
title = {Perceptions and use of an early warning system during a higher education transition program},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567625},
doi = {10.1145/2567574.2567625},
abstract = {This paper reports findings from the implementation of a learning analytics-powered Early Warning System (EWS) by academic advisors who were novice users of data-driven learning analytics tools. The information collected from these users sheds new light on how student analytic data might be incorporated into the work practices of advisors working with university students. Our results indicate that advisors predominantly used the EWS during their meetings with students---despite it being designed as a tool to provide information to prepare for meetings and identify students who are struggling academically. This introduction of an unintended audience brings significant design implications to bear that are relevant for learning analytics innovations.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {113–117},
numpages = {5},
keywords = {learning analytics, higher education, design-research, academic advising},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/2567574.2567624,
author = {Yuan, Yueran and Chang, Kai-min and Taylor, Jessica Nelson and Mostow, Jack},
title = {Toward unobtrusive measurement of reading comprehension using low-cost EEG},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567624},
doi = {10.1145/2567574.2567624},
abstract = {Assessment of reading comprehension can be costly and obtrusive. In this paper, we use inexpensive EEG to detect reading comprehension of readers in a school environment. We use EEG signals to produce above-chance predictors of student performance on end-of-sentence cloze questions. We also attempt (unsuccessfully) to distinguish among student mental states evoked by distracters that violate either syntactic, semantic, or contextual constraints. In total, this work investigates the practicality of classroom use of inexpensive EEG devices as an unobtrusive measure of reading comprehension.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {54–58},
numpages = {5},
keywords = {reading comprehension, intelligent tutoring systems, EEG},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/2567574.2567623,
author = {Waddington, Richard Joseph and Nam, SungJin},
title = {Practice exams make perfect: incorporating course resource use into an early warning system},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567623},
doi = {10.1145/2567574.2567623},
abstract = {Early Warning Systems (EWSs) are being developed and used more frequently to aggregate multiple sources of data and provide timely information to stakeholders about students in need of academic support. As these systems grow more complex, there is an increasing need to incorporate relevant and real-time course-related information that could be predictors of a student's success or failure. This paper presents an investigation of how to incorporate students' use of course resources from a Learning Management System (LMS) into an existing EWS. Specifically, we focus our efforts on understanding the relationship between course resource use and a student's final course grade. Using ten semesters of LMS data from a requisite Chemistry course, we categorized course resources into four categories. We used a multinomial logistic regression model with semester fixed-effects to estimate the relationship between course resource use and the likelihood that a student receives an "A" or "B" in the course versus a "C." Results suggest that students who use Exam Preparation or Lecture resources to a greater degree than their peers are more likely to receive an "A" or "B" as a final grade. We discuss the implications of our results for the further development of this EWS and EWSs in general.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {188–192},
numpages = {5},
keywords = {multinomial logistic regression, modeling, learning management systems, learning analytics, early warning systems, data mining, data integration, data analysis},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/2567574.2567622,
author = {Grann, Jeff and Bushway, Deborah},
title = {Competency map: visualizing student learning to promote student success},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567622},
doi = {10.1145/2567574.2567622},
abstract = {Adult students often struggle to appreciate the relevance of their higher educational experiences to their careers. Capella University's competency map is a dashboard that visually indicates each student's status relative to specific assessed competencies. MBA students who utilize their competency map demonstrate competencies at slightly higher levels and persist in their program at greater rates, even after statistically controlling for powerful covariates, such as course engagement.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {168–172},
numpages = {5},
keywords = {visualization, learning analytics, evaluation, competency},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/2567574.2567621,
author = {Arnold, Kimberly E. and Lonn, Steven and Pistilli, Matthew D.},
title = {An exercise in institutional reflection: the learning analytics readiness instrument (LARI)},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567621},
doi = {10.1145/2567574.2567621},
abstract = {While the landscape of learning analytics is relatively well defined, the extent to which institutions are ready to embark on an analytics implementation is less known. Further, while work has been done on measuring the maturity of an institution's implementation, this work fails to investigate how an institution that has not implemented analytics to date might become mature over time. To that end, the authors developed and piloted a survey, the Learning Analytics Readiness Instrument (LARI), in an attempt to help institutions successfully prepare themselves for a successfully analytics implementation. The LARI is comprised of 90 items encompassing five factors related to a learning analytics implementation: (1) Ability, (2) Data, (3) Culture and Process, (4) Governance and Infrastructure, and, (5) Overall Readiness Perception. Each of the five factors has a high internal consistency, as does the overall tool. This paper discusses the need for a survey such as the LARI, the tool's psychometric properties, the authors' broad interpretations of the findings, and next steps for the LARI and the research in this field.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {163–167},
numpages = {5},
keywords = {survey design, readiness, learning analytics, higher education},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/2567574.2567620,
author = {Leeman-Munk, Samuel P. and Wiebe, Eric N. and Lester, James C.},
title = {Assessing elementary students' science competency with text analytics},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567620},
doi = {10.1145/2567574.2567620},
abstract = {Real-time formative assessment of student learning has become the subject of increasing attention. Students' textual responses to short answer questions offer a rich source of data for formative assessment. However, automatically analyzing textual constructed responses poses significant computational challenges, and the difficulty of generating accurate assessments is exacerbated by the disfluencies that occur prominently in elementary students' writing. With robust text analytics, there is the potential to accurately analyze students' text responses and predict students' future success. In this paper, we present WriteEval, a hybrid text analytics method for analyzing student-composed text written in response to constructed response questions. Based on a model integrating a text similarity technique with a semantic analysis technique, WriteEval performs well on responses written by fourth graders in response to short-text science questions. Further, it was found that WriteEval's assessments correlate with summative analyses of student performance.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {143–147},
numpages = {5},
keywords = {writing assessment, text-based learning analytics, formative assessment, constructed response analysis, automated assessment},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/2567574.2567617,
author = {Drachsler, Hendrik and Stoyanov, Slavi and Specht, Marcus},
title = {The impact of learning analytics on the dutch education system},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567617},
doi = {10.1145/2567574.2567617},
abstract = {The article reports the findings of a Group Concept Mapping study that was conducted within the framework of the Learning Analytics Summer Institute (LASI) in the Netherlands. Learning Analytics are expected to be beneficial for students and teacher empowerment, personalization, research on learning design, and feedback for performance. The study depicted some management and economics issues and identified some possible treats. No differences were found between novices and experts on how important and feasible are changes in education triggered by Learning Analytics.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {158–162},
numpages = {5},
keywords = {learning analytics, group concept mapping, focus group, community building},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/2567574.2567616,
author = {Jo, Il-Hyun and Kim, Dongho and Yoon, Meehyun},
title = {Analyzing the log patterns of adult learners in LMS using learning analytics},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567616},
doi = {10.1145/2567574.2567616},
abstract = {In this paper, we describe a process of constructing proxy variables that represent adult learners' time management strategies in an online course. Based upon previous research, three values were selected from a data set. According to the result of empirical validation, an (ir)regularity of the learning interval was proven to be correlative with and predict learning performance. As indicated in previous research, regularity of learning is a strong indicator to explain learners' consistent endeavors. This study demonstrates the possibility of using learning analytics to address a learner's specific competence on the basis of a theoretical background. Implications for the learning analytics field seeking a pedagogical theory-driven approach are discussed.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {183–187},
numpages = {5},
keywords = {time management strategy, log data, learning analytics, big-data mining, adult education},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/2567574.2567615,
author = {Fancsali, Stephen E. and Ritter, Steven},
title = {Context personalization, preferences, and performance in an intelligent tutoring system for middle school mathematics},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567615},
doi = {10.1145/2567574.2567615},
abstract = {Learners often think math is unrelated to their own interests. Instructional software has the potential to provide personalized instruction that responds to individuals' interests. Carnegie Learning's MATHia™ software for middle school mathematics asks learners to specify domains of their interest (e.g., sports &amp; fitness, arts &amp; music), as well as names of friends/classmates, and uses this information to both choose and personalize word problems for individual learners. Our analysis of MATHia's relatively coarse-grained personalization contrasts with more finegrained analysis in previous research on word problems in the Cognitive Tutor (e.g., finding effects on performance in parts of problems that depend on more difficult skills), and we explore associations of aggregate preference "honoring" with learner performance. To do so, we define a notion of "strong" learner interest area preferences and find that honoring such preferences has a small negative association with performance. However, learners that both merely express preferences (either interest area preferences or setting names of friends/classmates), and those that express strong preferences, tend to perform in ways that are associated with better learning compared to learners that do not express such preferences. We consider several explanations of these findings and suggest important topics for future research.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {73–77},
numpages = {5},
keywords = {preferences, personalization, non-cognitive factors, mathematics education, intelligent tutoring systems},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/2567574.2567614,
author = {Taraghi, Behnam and Ebner, Martin and Saranti, Anna and Sch\"{o}n, Martin},
title = {On using markov chain to evidence the learning structures and difficulty levels of one digit multiplication},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567614},
doi = {10.1145/2567574.2567614},
abstract = {Understanding the behavior of learners within learning applications and analyzing the factors that may influence the learning process play a key role in designing and optimizing learning applications. In this work we focus on a specific application named "1x1 trainer" that has been designed for primary school children to learn one digit multiplications. We investigate the database of learners' answers to the asked questions (N &gt; 440000) by applying the Markov chains. We want to understand whether the learners' answers to the already asked questions can affect the way they will answer the subsequent asked questions and if so, to what extent. Through our analysis we first identify the most difficult and easiest multiplications for the target learners by observing the probabilities of the different answer types. Next we try to identify influential structures in the history of learners' answers considering the Markov chain of different orders. The results are used to identify pupils who have difficulties with multiplications very soon (after couple of steps) and to optimize the way questions are asked for each pupil individually.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {68–72},
numpages = {5},
keywords = {primary school, one digit multiplication, math, markov chain, learning analytics, elearning, difficulty level},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/2567574.2567613,
author = {Swenson, Jenni},
title = {Establishing an ethical literacy for learning analytics},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567613},
doi = {10.1145/2567574.2567613},
abstract = {This paper borrows multiple frameworks from the field of technical communication in order to review theory, research, practice, and ethics of the Learning Analytics and Knowledge (LAK) discipline. These frameworks also guide discussion on the ethics of learning analytics "artifacts" (data visualizations, dashboards, and methodology), and the ethical consequences of using learning analytics (classification, social power moves, and absence of voice). Finally, the author suggests a literacy for learning analytics that includes an ethical viewpoint.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {246–250},
numpages = {5},
keywords = {literacy, learning analytics, higher education, ethics},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/2567574.2567612,
author = {Brooks, Christopher and Greer, Jim},
title = {Explaining predictive models to learning specialists using personas},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567612},
doi = {10.1145/2567574.2567612},
abstract = {This paper describes a method we have developed to convert statistical predictive models into visual narratives which explain student classifications. Building off of the work done within the user experience community, we apply the concept of personas to predictive models. These personas provide familiar and memorable descriptions of the learners identified by data mining activities, and bridge the gap between the data scientist and the learning specialist.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {26–30},
numpages = {5},
keywords = {sensemaking, predictive modelling, personas, learning designers},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/2567574.2567611,
author = {Coopey, Eric and Shapiro, R. Benjamin and Danahy, Ethan},
title = {Collaborative spatial classification},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567611},
doi = {10.1145/2567574.2567611},
abstract = {Interactive technologies have become an important part of teaching and learning. However, the data that these systems generate is increasingly unstructured, complex, and therefore difficult of which to make sense of. Current computationally driven methods (e.g., latent semantic analysis or learning based image classifiers) for classifying student contributions don't include the ability to function on multimodal artifacts (e.g., sketches, videos, or annotated images) that new technologies enable. We have developed and implemented a classifcation algorithm based on learners' interactions with the artifacts they create. This new form of semi-automated concept classification, coined Collaborative Spatial Classification, leverages the spatial arrangement of artifacts to provide a visualization that generates summary level data about about idea distribution. This approach has two benefits. First, students learn to identify and articulate patterns and connections among classmates ideas. Second, the teacher receives a high-level view of the distribution of ideas, enabling them to decide how to shift their instructional practices in real-time.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {138–142},
numpages = {5},
keywords = {spatial arrangement, minimum spanning tree, interaction techniques, collaboration, clustering, classification, audience response system},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/2567574.2567610,
author = {Gibson, Andrew and Kitto, Kirsty and Willis, Jill},
title = {A cognitive processing framework for learning analytics},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567610},
doi = {10.1145/2567574.2567610},
abstract = {Incorporating a learner's level of cognitive processing into Learning Analytics presents opportunities for obtaining rich data on the learning process. We propose a framework called COPA that provides a basis for mapping levels of cognitive operation into a learning analytics system. We utilise Bloom's taxonomy, a theoretically respected conceptualisation of cognitive processing, and apply it in a flexible structure that can be implemented incrementally and with varying degree of complexity within an educational organisation. We outline how the framework is applied, and its key benefits and limitations. Finally, we apply COPA to a University undergraduate unit, and demonstrate its utility in identifying key missing elements in the structure of the course.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {212–216},
numpages = {5},
keywords = {learning analytics, curriculum design, cognitive processing, Bloom's revised taxonomy},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/2567574.2567609,
author = {Papamitsiou, Zacharoula K. and Terzis, Vasileios and Economides, Anastasios A.},
title = {Temporal learning analytics for computer based testing},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567609},
doi = {10.1145/2567574.2567609},
abstract = {Predicting student's performance is a challenging, yet complicated task for institutions, instructors and learners. Accurate predictions of performance could lead to improved learning outcomes and increased goal achievement. In this paper we explore the predictive capabilities of student's time-spent on answering (in-)correctly each question of a multiple-choice assessment quiz, along with student's final quiz-score, in the context of computer-based testing. We also explore the correlation between the time-spent factor (as defined here) and goal-expectancy. We present a case study and investigate the value of using this parameter as a learning analytics factor for improving prediction of performance during computer-based testing. Our initial results are encouraging and indicate that the temporal dimension of learning analytics should be further explored.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {31–35},
numpages = {5},
keywords = {temporal learning analytics, prediction of performance, prediction, goal-expectancy, educational data mining, computer-based testing, computer based assessment},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/2567574.2567608,
author = {Vozniuk, Andrii and Holzer, Adrian and Gillet, Denis},
title = {Peer assessment based on ratings in a social media course},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567608},
doi = {10.1145/2567574.2567608},
abstract = {Peer assessment is seen as a powerful supporting tool to achieve scalability in the evaluation of complex assignments in large courses, possibly virtual ones, as in the context of massive open online courses (MOOCs). However, the adoption of peer assessment is slow due in part to the lack of ready-to-use systems. Furthermore, the validity of peer assessment is still under discussion. In this paper, in order to tackle some of these issues, we present as a proof-of-concept of a novel extension of Graasp, a social media platform, to setup a peer assessment activity. We then report a case study of peer assessment using Graasp in a Social Media course with 60 master's level university students and analyze the level of agreement between students and instructors in the evaluation of short individual reports. Finally, to see if both instructor and student evaluations were based on appearance of project reports rather than on content, we conducted a study with 40 kids who rated reports solely on their look. Our results convey the fact that unlike the kid evaluation, which shows a low level of agreement with instructors, student assessment is reliable since the level of agreement between instructors and students was high.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {133–137},
numpages = {5},
keywords = {social media, ratings, peer assessment, learning activities, MOOCs},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/2567574.2567607,
author = {Harfield, Timothy D.},
title = {Teaching the unteachable: on the compatibility of learning analytics and humane education},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567607},
doi = {10.1145/2567574.2567607},
abstract = {This paper is an exploratory effort to find a place for learning analytics in humane education. After distinguishing humane education from training on the basis of the Aristotelian model of intellectual capabilities, and arguing that humane education is distinct by virtue of its interest in cultivating prudence, which is unteachable, an account of three key characteristics of humane education is provided. Appealing to thinkers of the Italian Renaissance, it is argued that ingenium, eloquence, and self-knowledge constitute the what, how, and why of humane education. Lastly, looking to several examples from recent learning analytics literature, it is demonstrated that learning analytics is not only helpful as set of aids for ensuring success in scientific and technical disciplines, but in the humanities as well. In order to function effectively as an aid to humane education, however, learning analytics must be embedded within a context that encourages continuous reflection, responsiveness, and personal responsibility for learning.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {241–245},
numpages = {5},
keywords = {renaissance, pedagogy, learning analytics, humanities, humanism, aristotle},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/2567574.2567606,
author = {Chen, Bodong and Resendes, Monica},
title = {Uncovering what matters: analyzing transitional relations among contribution types in knowledge-building discourse},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567606},
doi = {10.1145/2567574.2567606},
abstract = {Temporality matters for analysis of collaborative learning. The present study attempts to uncover temporal patterns that distinguish "productive" threads of knowledge building inquiry. Using a rich knowledge building discourse dataset, in which notes' contribution types and threads' productivity have been coded, a secondary temporal analysis was conducted. In particular, Lag-sequential Analysis was conducted to identify transitional patterns among different contribution types that distinguish productive threads from "improvable" ones. Results indicated that productive inquiry threads involved significantly more transitions among questioning, theorizing, obtaining information, and working with information; in contrast, responding to questions and theories by merely giving opinions was not sufficient to achieve knowledge progress. This study highlights the importance of investigating temporality in collaborative learning and calls for attention to developing and testing temporal analysis methods in learning analytics research.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {226–230},
numpages = {5},
keywords = {temporal analysis, sequential analysis, lag-sequential analysis, knowledge building, evidence-based research, discourse analysis, collaborative learning},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/2567574.2567605,
author = {Almeda, Ma. Victoria and Scupelli, Peter and Baker, Ryan S. and Weber, Mimi and Fisher, Anna},
title = {Clustering of design decisions in classroom visual displays},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567605},
doi = {10.1145/2567574.2567605},
abstract = {In this paper, we investigate the patterns of design choices made by classroom teachers for decorating their classroom walls, using cluster analysis to see which design decisions go together. Classroom visual design has been previously studied, but not in terms of the systematic patterns adopted by teachers in selecting what materials to place on classroom walls, or in terms of the actual semantic content of what is placed on walls. This is potentially important, as classroom walls are continuously seen by students, and form a continual off-task behavior option, available to students at all times. Using the k-means clustering algorithm, we find four types of visual classroom environments (one of them an outlier within our data set), representing teachers' strategies in classroom decoration. Our results indicate that the degree to which teachers place content-related decorations on the walls, is a feature of particular importance for distinguishing which approach teachers are using. Similarly, the type of school (e.g. whether private or charter) appeared to be another significant factor in determining teachers' design choices for classroom walls. The present findings begin the groundwork to better understand the impact of teacher decisions and choices in classroom design that lead to better outcomes in terms of engagement and learning, and finally towards developing classroom designs that are more effective and engaging for learners.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {44–48},
numpages = {5},
keywords = {teacher, design decisions, clustering, classroom decorations},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/2567574.2567604,
author = {Bogar\'{\i}n, Alejandro and Romero, Crist\'{o}bal and Cerezo, Rebeca and S\'{a}nchez-Santill\'{a}n, Miguel},
title = {Clustering for improving educational process mining},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567604},
doi = {10.1145/2567574.2567604},
abstract = {In this paper, we propose to use clustering to improve educational process mining. We want to improve both the performance and comprehensibility of the models obtained. We have used data from 84 undergraduate students who followed an online course using Moodle 2.0. We propose to group students firstly starting from data about Moodle's usage summary and/or the students' final marks in the course. Then, we propose to use data from Moodle's logs about each cluster/group of students separately in order to be able to obtain more specific and accurate models of students' behaviour. The results show that the fitness of the specific models is greater than the general model obtained using all the data, and the comprehensibility of the models can be also improved in some cases.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {11–15},
numpages = {5},
keywords = {process mining, learning analytics, educational data mining, clustering},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/2567574.2567603,
author = {Clow, Doug},
title = {Data wranglers: human interpreters to help close the feedback loop},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567603},
doi = {10.1145/2567574.2567603},
abstract = {Closing the feedback loop to improve learning is at the heart of good learning analytics practice. However, the quantity of data, and the range of different data sources, can make it difficult to take systematic action on that data. Previous work in the literature has emphasised the need for and value of human meaning-making in the process of interpretation of data to transform it in to actionable intelligence.This paper describes a programme of human Data Wranglers deployed at the Open University, UK, charged with making sense of a range of data sources related to learning, analysing that data in the light of their understanding of practice in individual faculties/departments, and producing reports that summarise the key points and make actionable recommendations.The evaluation of and experience in this programme of work strongly supports the value of human meaning-makers in the learning analytics process, and suggests that barriers to organisational change in this area can be mitigated by embedding learning analytics work within strategic contexts, and working at an appropriate level and granularity of analysis.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {49–53},
numpages = {5},
keywords = {systems, organizational learning, meaning-making, learning analytics, interpretation, data wrangling},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/2567574.2567602,
author = {DeBoer, Jennifer and Stump, Glenda S.},
title = {National differences in an international classroom},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567602},
doi = {10.1145/2567574.2567602},
abstract = {The virtual classrooms of open online courses include students from a vast array of individual, social, economic, and educational contexts. Detailed data were collected for the first course MIT ran on the edX platform, including student behavior, performance, and background information. In this paper, we estimate the systematic differences in average performance, distribution of performance, and performance conditional on behaviors for countries with different characteristics (e.g., language, income).},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {281–282},
numpages = {2},
keywords = {comparative education, MOOCs},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/2567574.2567601,
author = {Whitmer, John and Schiorring, Eva and James, Pat},
title = {Patterns of persistence: what engages students in a remedial english writing MOOC?},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567601},
doi = {10.1145/2567574.2567601},
abstract = {MOOCs have the potential to help institutions and students needing remedial English language instruction in two ways. First, with their capacity to use a wide range of instructional approaches and to emphasize contextualized and visual learning, MOOCS can offer potentially more effective pedagogical approaches for remedial students. Second, if students increase success meeting college-level English competencies, MOOCS can help institutions and students conserve their limited resources. Similarly, MOOCs offer domestically and international employers opportunities to provide professional development to workers both in ways that are flexible, affordable and interactive.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {279–280},
numpages = {2},
keywords = {under-represented minority students, retention, persistence, massive open online courses (MOOCs), learning analytics, language learning, higher education},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/2567574.2567598,
author = {Tritz, Jared and Michelotti, Nicole and Shultz, Ginger and McKay, Tim and Mohapatra, Barsaa},
title = {Peer evaluation of student generated content},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567598},
doi = {10.1145/2567574.2567598},
abstract = {We will present three similar studies that examine online peer evaluation of student-generated explanations for missed exam problems in introductory physics. In the first study, students created video solutions using YouTube and in the second two studies, they created written solutions using Google documents. All peer evaluations were performed using a tournament module as part of the interactive online coaching system called E2Coach[4] at the University of Michigan. With the theme of LAK 2014 being "intersection of learning analytics research, theory and practice", we think this poster will provide an accessible example that combines a classroom experiment with rigorous analysis to understand outcomes.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {277–278},
numpages = {2},
keywords = {videos, tournaments, peer evaluation, google docs, blended learning},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/2567574.2567597,
author = {Greenberg, Anne K. and Gross, Melissa and Wright, Mary C.},
title = {Effects of image-based and text-based activities on student learning outcomes},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567597},
doi = {10.1145/2567574.2567597},
abstract = {Research on benefits of visual learning has relied primarily on lecture-based pedagogy, not accounting for the processing time students need to make sense of both visual and verbal material[8]. In this study, we investigate the potential differential effects of text-based and image-based student learning activities on student learning outcomes in a functional anatomy course. When controlling for demographics and prior GPA, participation in in-class image-based activities is significantly correlated with performance on associated exam questions, while text-based engagement is not. Additionally, students rated activities as helpful for seeing images of key ideas and as being significantly less mentally taxing than text-based activities.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {275–276},
numpages = {2},
keywords = {visualizer-verbalizer, dual coding, assessment, active learning, LectureTools},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/2567574.2567596,
author = {Cerezo, Rebeca and Suarez, Natalia and N\'{u}\~{n}ez, J. Carlos and S\'{a}nchez-Santill\'{a}n, Miguel},
title = {eGraph tool: graphing the learning process in LMSs},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567596},
doi = {10.1145/2567574.2567596},
abstract = {eGraph is a virtual tool developed with the aim of make easier to track the students' learning process in Learning Management Systems like Moodle. It is based in the log files that the learning platform records when the students are interacting with and allows teachers, students, and researchers to track the learning route that learners have followed during a particular time span.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {273–274},
numpages = {2},
keywords = {learning process visualization, graph, educational data mining, adaptive feedback},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/2567574.2567595,
author = {Chen, Bodong},
title = {Visualizing semantic space of online discourse: the knowledge forum case},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567595},
doi = {10.1145/2567574.2567595},
abstract = {This poster presents an early experimentation of applying topic modeling and visualization techniques to analyze online discourse. In particular, Latent Dirichlet Allocation was used to convert discourse into a high-dimensional semantic space. To explore meaningful visualizations of the space, Locally Linear Embedding was performed reducing it to two-dimensional. Further, Time Series Analysis was applied to track evolution of topics in the space. This work will lead to new analytic tools for collaborative learning.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {271–272},
numpages = {2},
keywords = {text mining, semantic analysis, knowledge building, discourse analysis, collaborative learning, LDA},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/2567574.2567594,
author = {Yu, Taeho and Jo, Il-Hyun},
title = {Educational technology approach toward learning analytics: relationship between student online behavior and learning performance in higher education},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567594},
doi = {10.1145/2567574.2567594},
abstract = {The aim of this study is to suggest more meaningful components for learning analytics in order to help learners improving their learning achievement continuously through an educational technology approach. Multiple linear regression analysis is conducted to determine which factors influence student's academic achievement. 84 undergraduate students in a women's university in South Korea participated in this study. The six-predictor model was able to account for 33.5% of the variance in final grade, F(6, 77) = 6.457, p &lt; .001, R2 = .335. Total studying time in LMS, interaction with peers, regularity of learning interval in LMS, and number of downloads were determined to be significant factors for students' academic achievement in online learning environment. These four controllable variables not only predict learning outcomes significantly but also can be changed if learners put more effort to improve their academic performance. The results provide a rationale for the treatment for student time management effort.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {269–270},
numpages = {2},
keywords = {learning analytics, higher education, educational technology, e-learning},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/2567574.2567593,
author = {Arnold, Kimberly E. and Lynch, Grace and Huston, Daniel and Wong, Lorna and Jorn, Linda and Olsen, Christopher W.},
title = {Building institutional capacities and competencies for systemic learning analytics initiatives},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567593},
doi = {10.1145/2567574.2567593},
abstract = {The last five years have brought an explosion of research in the learning analytics field. However, much of what has emerged has been small scale or tool-centric. While these efforts are vitally important to the development of the field, in order to truly transform education, learning analytics must scale and become institutionalized at multiple levels throughout an educational system. Many institutions are currently undertaking this grand challenge and this panel will highlight cases from: the University of Wisconsin System, the Society for Learning Analytics Research, the University of New England, and Rio Salado College.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {257–260},
numpages = {4},
keywords = {systemic application, sustainability, learning analytics, leadership, higher education, cultural change, capacity building},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/2567574.2567592,
author = {Ferguson, Rebecca and Clow, Doug and Macfadyen, Leah and Essa, Alfred and Dawson, Shane and Alexander, Shirley},
title = {Setting learning analytics in context: overcoming the barriers to large-scale adoption},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567592},
doi = {10.1145/2567574.2567592},
abstract = {Once learning analytics have been successfully developed and tested, the next step is to implement them at a larger scale -- across a faculty, an institution or an educational system. This introduces a new set of challenges, because education is a stable system, resistant to change. Implementing learning analytics at scale involves working with the entire technological complex that exists around technology-enhanced learning (TEL). This includes the different groups of people involved -- learners, educators, administrators and support staff -- the practices of those groups, their understandings of how teaching and learning take place, the technologies they use and the specific environments within which they operate. Each element of the TEL Complex requires explicit and careful consideration during the process of implementation, in order to avoid failure and maximise the chances of success. In order for learning analytics to be implemented successfully at scale, it is crucial to provide not only the analytics and their associated tools but also appropriate forms of support, training and community building.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {251–253},
numpages = {3},
keywords = {technology-enhanced learning, teaching, learning analytics, learning, implementation, higher education, education, change management, change, administration, TEL complex},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/2567574.2567591,
author = {M\'{e}ndez, Gonzalo and Ochoa, Xavier and Chiluiza, Katherine},
title = {Techniques for data-driven curriculum analysis},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567591},
doi = {10.1145/2567574.2567591},
abstract = {One of the key promises of Learning Analytics research is to create tools that could help educational institutions to gain a better insight of the inner workings of their programs, in order to tune or correct them. This work presents a set of simple techniques that applied to readily available historical academic data could provide such insights. The techniques described are real course difficulty estimation, dependance estimation, curriculum coherence, dropout paths and load/performance graph. The description of these techniques is accompanied by its application to real academic data from a Computer Science program. The results of the analysis are used to obtain recommendations for curriculum re-design.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {148–157},
numpages = {10},
keywords = {learning analytics, curriculum design},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/2567574.2567590,
author = {Ga\v{s}evi\'{c}, Dragan and Mirriahi, Negin and Dawson, Shane},
title = {Analytics of the effects of video use and instruction to support reflective learning},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567590},
doi = {10.1145/2567574.2567590},
abstract = {Although video annotation software is no longer considered as a new innovation, its application in promoting student self-regulated learning and reflection skills has only begun to emerge in the research literature. Advances in text and video analytics provide the capability of investigating students' use of the tool and the psychometrics and linguistic processes evident in their written annotations. This paper reports on a study exploring students' use of a video annotation tool when two different instructional approaches were deployed -- graded and non-graded self-reflection annotations within two courses in the performing arts. In addition to counts and temporal locations of self-reflections, the Linguistic Inquiry and Word Counts (LIWC) framework was used for the extraction of variables indicative of the linguistic and psychological processes associated with self-reflection annotations of videos. The results indicate that students in the course with graded self-reflections adopted more linguistic and psychological related processes in comparison to the course with non-graded self-reflections. In general, the effect size of the graded reflections was lower for students who took both courses in parallel. Consistent with prior research, the study identified that students tend to make the majority of their self-reflection annotations early in the video time line. The paper also provides several suggestions for future research to better understand the application of video annotations in facilitating student learning.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {123–132},
numpages = {10},
keywords = {text analysis, self-reflections, metacognition, learning analytics},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/2567574.2567589,
author = {Nam, SungJin and Lonn, Steven and Brown, Thomas and Davis, Cinda-Sue and Koch, Darryl},
title = {Customized course advising: investigating engineering student success with incoming profiles and patterns of concurrent course enrollment},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567589},
doi = {10.1145/2567574.2567589},
abstract = {Every college student registers for courses from a catalog of numerous offerings each term. Selecting the courses in which to enroll, and in what combinations, can dramatically impact each student's chances for academic success. Taking inspiration from the STEM Academy, we wanted to identify the characteristics of engineering students who graduate with 3.0 or above grade point average. The overall goal of the Customized Course Advising project is to determine the optimal term-by-term course selections for all engineering students based on their incoming characteristics and previous course history and performance, paying particular attention to concurrent enrollment. We found that ACT Math, SAT Math, and Advanced Placement exam can be effective measures to measure the students' academic preparation level. Also, we found that some concurrent course-enrollment patterns are highly predictive of first-term and overall academic success.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {16–25},
numpages = {10},
keywords = {learning analytics, data mining, data analysis, course advising},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/2567574.2567588,
author = {Wise, Alyssa Friend},
title = {Designing pedagogical interventions to support student use of learning analytics},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567588},
doi = {10.1145/2567574.2567588},
abstract = {This article addresses a relatively unexplored area in the emerging field of learning analytics, the design of learning analytics interventions. A learning analytics intervention is defined as the surrounding frame of activity through which analytic tools, data, and reports are taken up and used. It is a soft technology that involves the orchestration of the human process of engaging with the analytics as part of the larger teaching and learning activity. This paper first makes the case for the overall importance of intervention design, situating it within the larger landscape of the learning analytics field, and then considers the specific issues of intervention design for student use of learning analytics. Four principles of pedagogical learning analytics intervention design that can be used by teachers and course developers to support the productive use of learning analytics by students are introduced: Integration, Agency, Reference Frame and Dialogue. In addition three core processes in which to engage students are described: Grounding, Goal-Setting and Reflection. These principles and processes are united in a preliminary model of pedagogical learning analytics intervention design for students, presented as a starting point for further inquiry.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {203–211},
numpages = {9},
keywords = {student participation, learning analytics, intervention design},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/2567574.2567587,
author = {Xing, Wanli and Wadholm, Bob and Goggins, Sean},
title = {Learning analytics in CSCL with a focus on assessment: an exploratory study of activity theory-informed cluster analysis},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567587},
doi = {10.1145/2567574.2567587},
abstract = {In this paper we propose an automated strategy to assess participation in a multi-mode math discourse environment called Virtual Math Teams with Geogrebra (VMTwG). A holistic participation clustering algorithm is applied through the lens of activity theory. Our activity theory-informed algorithm is a step toward accelerating heuristic approaches to assessing collaborative work in synchronous technology mediated environments like VMTwG. Our Exploratory findings provide an example of a novel, time-efficient, valid, and reliable participatory learning assessment tool for teachers in computer mediated learning environments. Scaling online learning with a combination of computation and theory is the overall goal of the work this paper is situated within.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {59–67},
numpages = {9},
keywords = {learning analytics, educational assessment, assessment, activity theory, CSCL},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/2567574.2567586,
author = {Coffrin, Carleton and Corrin, Linda and de Barba, Paula and Kennedy, Gregor},
title = {Visualizing patterns of student engagement and performance in MOOCs},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567586},
doi = {10.1145/2567574.2567586},
abstract = {In the last five years, the world has seen a remarkable level of interest in Massive Open Online Courses, or MOOCs. A consistent message from universities participating in MOOC delivery is their eagerness to understand students' online learning processes. This paper reports on an exploratory investigation of students' learning processes in two MOOCs which have different curriculum and assessment designs. When viewed through the lens of common MOOC learning analytics, the high level of initial student interest and, ultimately, the high level of attrition, makes these two courses appear very similar to each other, and to MOOCs in general. With the goal of developing a greater understanding of students' patterns of learning behavior in these courses, we investigated alternative learning analytic approaches and visual representations of the output of these analyses. Using these approaches we were able to meaningfully classify student types and visualize patterns of student engagement which were previously unclear. The findings from this research contribute to the educational community's understanding of students' engagement and performance in MOOCs, and also provide the broader learning analytics community with suggestions of new ways to approach learning analytic data analysis and visualization.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {83–92},
numpages = {10},
keywords = {visualizations, visualization, prior knowledge, online learning, learning analytics, learner engagement patterns, completion rate, MOOC},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/2567574.2567585,
author = {Dawson, Shane and Ga\v{s}evi\'{c}, Dragan and Siemens, George and Joksimovic, Srecko},
title = {Current state and future trends: a citation network analysis of the learning analytics field},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567585},
doi = {10.1145/2567574.2567585},
abstract = {This paper provides an evaluation of the current state of the field of learning analytics through analysis of articles and citations occurring in the LAK conferences and identified special issue journals. The emerging field of learning analytics is at the intersection of numerous academic disciplines, and therefore draws on a diversity of methodologies, theories and underpinning scientific assumptions. Through citation analysis and structured mapping we aimed to identify the emergence of trends and disciplinary hierarchies that are influencing the development of the field to date. The results suggest that there is some fragmentation in the major disciplines (computer science and education) regarding conference and journal representation. The analyses also indicate that the commonly cited papers are of a more conceptual nature than empirical research reflecting the need for authors to define the learning analytics space. An evaluation of the current state of learning analytics provides numerous benefits for the development of the field, such as a guide for under-represented areas of research and to identify the disciplines that may require more strategic and targeted support and funding opportunities.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {231–240},
numpages = {10},
keywords = {social network analysis, learning analytics, citation analysis, author networks},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/2567574.2567584,
author = {Hecking, Tobias and Ziebarth, Sabrina and Hoppe, H. Ulrich},
title = {Analysis of dynamic resource access patterns in a blended learning course},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567584},
doi = {10.1145/2567574.2567584},
abstract = {This paper presents an analysis of resource access patterns in a recently conducted master level university course. The specialty of the course was that it followed a new teaching approach by providing additional learning resources such as wikis, self-tests and videos. To gain deeper insights into the usage of the provided learning material we have built dynamic bipartite student -- resource networks based on event logs of resource access. These networks are analysed using methods adapted from social network analysis. In particular we uncover bipartite clusters of students and resources in those networks and propose a method to identify patterns and traces of their evolution over time.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {173–182},
numpages = {10},
keywords = {social network analysis, learning resources, learning analytics, MOOCs},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/2567574.2567583,
author = {Aguiar, Everaldo and Chawla, Nitesh V. and Brockman, Jay and Ambrose, G. Alex and Goodrich, Victoria},
title = {Engagement vs performance: using electronic portfolios to predict first semester engineering student retention},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567583},
doi = {10.1145/2567574.2567583},
abstract = {As providers of higher education begin to harness the power of big data analytics, one very fitting application for these new techniques is that of predicting student attrition. The ability to pinpoint students who might soon decide to drop out of a given academic program allows those in charge to not only understand the causes for this undesired outcome, but it also provides room for the development of early intervention systems. While making such inferences based on academic performance data alone is certainly possible, we claim that in many cases there is no substantial correlation between how well a student performs and his or her decision to withdraw. This is specially true when the overall set of students has a relatively similar academic performance. To address this issue, we derive measurements of engagement from students' electronic portfolios and show how these features can be effectively used to augment the quality of predictions.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {103–112},
numpages = {10},
keywords = {student retention, predictive analytics, learning analytics, electronic portfolios, early intervention, data fusion},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/2567574.2567582,
author = {Piety, Philip J. and Hickey, Daniel T. and Bishop, M. J.},
title = {Educational data sciences: framing emergent practices for analytics of learning, organizations, and systems},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567582},
doi = {10.1145/2567574.2567582},
abstract = {In this paper, we develop a conceptual framework for organizing emerging analytic activities involving educational data that can fall under broad and often loosely defined categories, including Academic/Institutional Analytics, Learning Analytics/Educational Data Mining, Learner Analytics/Personalization, and Systemic Instructional Improvement. While our approach is substantially informed by both higher education and K-12 settings, this framework is developed to apply across all educational contexts where digital data are used to inform learners and the management of learning. Although we can identify movements that are relatively independent of each other today, we believe they will in all cases expand from their current margins to encompass larger domains and increasingly overlap. The growth in these analytic activities leads to the need to find ways to synthesize understandings, find common language, and develop frames of reference to help these movements develop into a field.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {193–202},
numpages = {10},
keywords = {tools for sense-making in learning analytics, theories and theoretical concepts for understanding learning, methods, learning analytics, learner analytics, educational data science, educational data mining, data-driven decisions, big data, analytic approaches},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/2567574.2567581,
author = {Raca, Mirko and Tormey, Roland and Dillenbourg, Pierre},
title = {Sleepers' lag - study on motion and attention},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567581},
doi = {10.1145/2567574.2567581},
abstract = {Human body-language is one of the richest and most obscure sources of information in inter-personal communication which we aim to re-introduce into the classroom's ecosystem. In this paper we present our observations of student-to-student influence and measurements. We show parallels with previous theories and formulate a new concept for measuring the level of attention based on synchronization of student actions. We observed that the students with lower levels of attention are slower to react then focused students, a phenomenon we named "sleepers' lag".},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {36–43},
numpages = {8},
keywords = {orchestration, motion lag, classroom orchestration, classroom attention, audience synchronization},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/2567574.2567580,
author = {Chiu, Ming Ming and Fujita, Nobuko},
title = {Statistical discourse analysis of online discussions: informal cognition, social metacognition and knowledge creation},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567580},
doi = {10.1145/2567574.2567580},
abstract = {To statistically model large data sets of knowledge processes during asynchronous, online forums, we must address analytic difficulties involving the whole data set (missing data, nested data and the tree structure of online messages), dependent variables (multiple, infrequent, discrete outcomes and similar adjacent messages), and explanatory variables (sequences, indirect effects, false positives, and robustness). Statistical discourse analysis (SDA) addresses all of these issues, as shown in an analysis of 1,330 asynchronous messages written and self-coded by 17 students during a 13-week online educational technology course. The results showed how attributes at multiple levels (individual and message) affected knowledge creation processes. Men were more likely than women to theorize. Asynchronous messages created a micro-sequence context; opinions and asking about purpose preceded new information; anecdotes, opinions, different opinions, elaborating ideas, and asking about purpose or information preceded theorizing. These results show how informal thinking precedes formal thinking and how social metacognition affects knowledge creation.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {217–225},
numpages = {9},
keywords = {statistical discourse analysis, social metacognition, knowledge creation, informal cognition},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/2567574.2567579,
author = {Okada, Masaya and Tada, Masahiro},
title = {Formative assessment method of real-world learning by integrating heterogeneous elements of behavior, knowledge, and the environment},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567579},
doi = {10.1145/2567574.2567579},
abstract = {Real-world learning in a field is an important educational area for experience-based activities. Formative assessment by constant monitoring of the intellectual achievement of real-world learners is essential for adaptive learning support, but no assessment methodology has yet been developed. We consider a method to systematically integrate heterogeneous factors of real-world learning: learners' internal situations, their external situations, and their learning field. Then, we propose a method for formatively assessing the situation of real-world learning. The method enables us to recognize the sequence of characteristic stay behavior and the associated body posture of a learner, and to estimate the 3D location of his/her interest. The method enables the estimation of not only the learning topic that a learner is currently examining in a field but also the prospective topics that he/she should learn. Our assessment method is the basis for context-aware support to promote the emergence of new knowledge from intellectual collaboration in the world.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {1–10},
numpages = {10},
keywords = {spatial information distribution, real-world learning, multimodal sensing, formative assessment, context estimation},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/2567574.2567578,
author = {Jayaprakash, Sandeep M. and Laur\'{\i}a, Eitel J. M.},
title = {Open academic early alert system: technical demonstration},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567578},
doi = {10.1145/2567574.2567578},
abstract = {This paper synthesizes some of the technical decisions, design strategies &amp; concepts developed during the execution of Open Academic Analytics Initiative (OAAI), a research program aimed at improving student retention rates in colleges, by deploying an open-source academic early alert system to identify the students at academic risk. The paper explains the prototype demonstration of the system, detailing several dimensions of data mining &amp; analysis such as: data integration, predictive modelling and scoring with reporting. The paper should be relevant to practitioners and academicians who want to better understand the implementation of an OAAI academic early-alert system.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {267–268},
numpages = {2},
keywords = {retention, portability, open source, learning analytics, interventions, intervention, data mining, course management systems},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/2567574.2567577,
author = {Simsek, Duygu and Buckingham Shum, Simon and De Liddo, Anna and Ferguson, Rebecca and S\'{a}ndor, \'{A}gnes},
title = {Visual analytics of academic writing},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567577},
doi = {10.1145/2567574.2567577},
abstract = {This paper describes a novel analytics dashboard which visualises the key features of scholarly documents. The Dashboard aggregates the salient sentences of scholarly papers, their rhetorical types and the key concepts mentioned within these sentences. These features are extracted from papers through a Natural Language Processing (NLP) technology, called Xerox Incremental Parser (XIP). The XIP Dashboard is a set of visual analytics modules based on the XIP output. In this paper, we briefly introduce the XIP technology and demonstrate an example visualisation of the XIP Dashboard.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {265–266},
numpages = {2},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/2567574.2567576,
author = {Samson, Perry J.},
title = {Analyzing student notes and questions to create personalized study guides},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567576},
doi = {10.1145/2567574.2567576},
abstract = {In the foreseeable future it will be technically possible for instructors, advisors and other delegated representatives of a college or university to access student participation and performance data in near-real time. One potential benefit of this increased data flow could include an improved ability to identify students at risk of academic failure or withdrawal. The availability of these data could also lead to creation of new adaptive learning measures that can automatically provide students personalized guidance.This demonstration will describe how the student notes and questions are being mined to provide student study guides that automatically link to outside resources. The demonstration will also report on how these new study guides have been received by the students and how they are at least partially responsible for a significant increase in student outcomes.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {263–264},
numpages = {2},
keywords = {student engagement, learning analytics, data mining},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/2567574.2567575,
author = {Kuo, Chin-Hwa and Peng, Jian-Wen and Chang, Wen-Chen},
title = {Hanzi handwriting acquisition with automatic feedback},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567575},
doi = {10.1145/2567574.2567575},
abstract = {One of the most crucial distinctions between Chinese and Western languages is that the former is based on ideograms, whereas the latter is based on phonograms. Due to this distinction, Western learners of Chinese often experience more difficulties in grasping correct character stroke sequence and/or stroke direction relative to native Chinese speakers. In this paper, we designed a HanZi writing environment with automatic feedback to address the above issue. Before the collection of HanZi characters on a massive scale, we conducted a pilot study to collect handwritten Chinese samples from 160 college students in the U.S. The findings from this study enabled us to further refine the learning environment and design optimal learning and teaching strategies for learners and teachers.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {261–262},
numpages = {2},
keywords = {personalization, learning analytics, chinese character handwriting},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@proceedings{10.1145/2567574,
title = {LAK '14: Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our pleasure to welcome you to LAK 2014. This is the fourth meeting of Learning Analytics and Knowledge, and this conference has quickly become an established meeting place for top researchers from learning sciences, data mining, learning analytics, computer sciences and other fields. Since the first conference held in 2011, the Learning Analytics community has grown and matured. The annual meetings of LAK are convened by an international professional society, the Society for Learning Analytics Research (SoLAR), which has also just this year launched a new scholarly journal, the Journal of Learning Analytics. The rapid growth of this field and the public attention drawn to the explosion of online technology platforms and services has convinced many of us that we are engaged in an exciting area that holds great promise for innovating teaching and learning.},
location = {Indianapolis, Indiana, USA}
}

@inproceedings{10.1145/2460296.2460360,
author = {Vatrapu, Ravi and Reimann, Peter and Halb, Wolfgang and Bull, Susan},
title = {Second International Workshop on Teaching Analytics},
year = {2013},
isbn = {9781450317856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2460296.2460360},
doi = {10.1145/2460296.2460360},
abstract = {Teaching Analytics is conceived as a subfield of learning analytics that focuses on the design, development, evaluation, and education of visual analytics methods and tools for teachers in primary, secondary, and tertiary educational settings. The Second International Workshop on Teaching Analytics (IWTA) 2013 seeks to bring together researchers and practitioners in the fields of education, learning sciences, learning analytics, and visual analytics to investigate the design, development, use, evaluation, and impact of visual analytical methods and tools for teachers' dynamic diagnostic decision-making in real-world settings.},
booktitle = {Proceedings of the Third International Conference on Learning Analytics and Knowledge},
pages = {287–289},
numpages = {3},
keywords = {affordances, teaching analytics, open learner models representational guidance, learning analytics, computer supported collaborative learning (CSCL)},
location = {Leuven, Belgium},
series = {LAK '13}
}

@inproceedings{10.1145/2460296.2460359,
author = {Sicilia, Miguel-Angel and Ochoa, Xavier and Stoitsis, Giannis and Klerkx, Joris},
title = {Learning object analytics for collections, repositories &amp; federations},
year = {2013},
isbn = {9781450317856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2460296.2460359},
doi = {10.1145/2460296.2460359},
abstract = {A large number of curated digital collections containing learning resources of a various kind has emerged in the last year. These include referatories containing descriptions for resources in the Web (as MERLOT), aggregated collections (as Organic.Edunet), concrete initiatives as Khan Academy, repositories hosting and versioning modular content (as Connexions) and meta-aggregators (as Globe and Learning Registry). Also, OpenCourseware and other OER initiatives have contributed to making this ecosystem of resources richer. Very interesting insights can be extracted when studying the usage and social data that are produced within the learning collections, repositories and federations. At the same time, concerns for the quality and sustainability of these collections have been raised, which has lead to research on quality measurement and metrics. The Workshop attempts to bring studies and demonstrations for any kind of analysis done on learning resource collections, from an interdisciplinary perspective. We consider digital collections not as merely IT deployments but as social systems with contributors, owners, evaluators and users forming patterns of interactions on top of portals or through search systems embedded in other learning technology components. This is in coherence of considering these social systems under a Web Science approach (http://webscience.org/).},
booktitle = {Proceedings of the Third International Conference on Learning Analytics and Knowledge},
pages = {285–286},
numpages = {2},
keywords = {social data, metadata, learning repositories, analytics},
location = {Leuven, Belgium},
series = {LAK '13}
}

@inproceedings{10.1145/2460296.2460358,
author = {Giannakos, Michail N. and Chorianopoulos, Konstantinos and Ronchetti, Marco and Szegedi, Peter and Teasley, Stephanie D.},
title = {Analytics on video-based learning},
year = {2013},
isbn = {9781450317856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2460296.2460358},
doi = {10.1145/2460296.2460358},
abstract = {The International Workshop on Analytics on Video-based Learning (WAVe2013) aims to connect research efforts on Video-based Learning with Learning Analytics to create visionary ideas and foster synergies between the two fields. The main objective of WAVe is to build a research community around the topical area of Analytics on video-based learning. In particular, WAVe aims to develop a critical discussion about the next generation of analytics employed on video learning tools, the form of these analytics and the way they can be analyzed in order to help us to better understand and improve the value of video-based learning. WAVe is based on the rationale that combining and analyzing learners' interactions with other available data obtained from learners, new avenues for research on video-based learning have emerged.},
booktitle = {Proceedings of the Third International Conference on Learning Analytics and Knowledge},
pages = {283–284},
numpages = {2},
keywords = {video based learning, learning analytics, interaction design, MOOCs},
location = {Leuven, Belgium},
series = {LAK '13}
}

@inproceedings{10.1145/2460296.2460357,
author = {Buckingham Shum, Simon and de Laat, Maarten and De Liddo, Anna and Ferguson, Rebecca and Kirschner, Paul and Ravenscroft, Andrew and S\'{a}ndor, \'{A}gnes and Whitelock, Denise},
title = {DCLA13: 1st International Workshop on Discourse-Centric Learning Analytics},
year = {2013},
isbn = {9781450317856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2460296.2460357},
doi = {10.1145/2460296.2460357},
abstract = {This workshop anticipates that an important class of learning analytic will emerge at the intersection of research into learning dynamics, online discussion platforms, and computational linguistics. Written discourse is arguably the primary class of data that can give us insights into deeper learning and higher order qualities such as critical thinking, argumentation, mastery of complex ideas, empathy, collaboration and interpersonal skills. Moreover, the ability to write in a scholarly manner is a core competence, often taking the form of discourse with oneself and the literature. Computational linguistics research has developed a rich array of tools for machine interpretation of human discourse, but work to develop these tools in the context of learning is at a relatively early stage. Moreover, there is a significant difference between designing tools to assist researchers in discourse analysis, and their deployment on platforms to provide meaningful analytics for the learners and educators who are conducting that discourse. This workshop aims to catalyse ideas and build community connections among those who want to shape this field.},
booktitle = {Proceedings of the Third International Conference on Learning Analytics and Knowledge},
pages = {282},
numpages = {1},
keywords = {visualization, learning analytics, discourse, dialogue, deliberation, argumentation},
location = {Leuven, Belgium},
series = {LAK '13}
}

@inproceedings{10.1145/2460296.2460355,
author = {Buckingham Shum, Simon and Hawksey, Martin and Baker, Ryan S. J. D. and Jeffery, Naomi and Behrens, John T. and Pea, Roy},
title = {Educational data scientists: a scarce breed},
year = {2013},
isbn = {9781450317856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2460296.2460355},
doi = {10.1145/2460296.2460355},
abstract = {The Educational Data Scientist is currently a poorly understood, rarely sighted breed. Reports vary: some are known to be largely nocturnal, solitary creatures, while others have been reported to display highly social behaviour in broad daylight. What are their primary habits? How do they see the world? What ecological niches do they occupy now, and will predicted seismic shifts transform the landscape in their favour? What survival skills do they need when running into other breeds? Will their numbers grow, and how might they evolve? In this panel, the conference will hear and debate not only broad perspectives on the terrain, but will have been exposed to some real life specimens, and caught glimpses of the future ecosystem.},
booktitle = {Proceedings of the Third International Conference on Learning Analytics and Knowledge},
pages = {278–281},
numpages = {4},
keywords = {learning analytics, educational data scientist, educational data mining, data science},
location = {Leuven, Belgium},
series = {LAK '13}
}

@inproceedings{10.1145/2460296.2460354,
author = {Baer, Linda L. and Duin, Ann Hill and Norris, Donald and Brodnick, Robert},
title = {Crafting transformative strategies for personalized learning/analytics},
year = {2013},
isbn = {9781450317856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2460296.2460354},
doi = {10.1145/2460296.2460354},
abstract = {Personalized learning environments and learning analytics hold the promise to transform learning experiences, enhance and accelerate student success, and "open up" student learning to resources and experiences from outside individual institutions. To achieve their potential, personalized learning projects must move beyond individual, stand-alone projects or innovations to reshaping the institutional experience.Learning science must connect with learning pedagogy and design. Learners and institutions must have access to tools and resources that assist in customizing student progress and supplemental learning needs. Teachers and faculty must be empowered to provide teaching and learning environments that allow individual students to thrive. All this will require unique partnerships and collaborations within and across institutions, incorporating the best learning science findings and bridging with public and private entities developing the learning and analytic tools to support personalized learning.Crafting a strategy to embrace and sustain the transformative power of personalized learning systems will require strong leadership and clear planning models to align with institutional planning and future investments.},
booktitle = {Proceedings of the Third International Conference on Learning Analytics and Knowledge},
pages = {275–277},
numpages = {3},
keywords = {strategic planning, personalized learning, learning analytics, collaboration, academic analytics},
location = {Leuven, Belgium},
series = {LAK '13}
}

@inproceedings{10.1145/2460296.2460352,
author = {Slotta, James D. and Tissenbaum, Mike and Lui, Michelle},
title = {Orchestrating of complex inquiry: three roles for learning analytics in a smart classroom infrastructure},
year = {2013},
isbn = {9781450317856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2460296.2460352},
doi = {10.1145/2460296.2460352},
abstract = {This paper presents our research of a pedagogical model known as Knowledge Community and Inquiry (KCI), focusing on our design of a technological infrastructure for the orchestration of the complex CSCL scripts that characterize KCI curricula. We first introduce the KCI model including some basic design principles, and describe its dependency on real time learning analytics. Next, we describe our technology, known as SAIL Smart Space (S3), which provides scaffolding and analytic support of sequenced interactions amongst people, materials, tools and environments. We outline the critical role of the teacher in our designs and describe how S3 supports their active role in orchestration. Finally we outline two implementations of KCI/S3 and the role of learning analytics, in supporting dynamic collective visualizations, real time orchestrational logic, and ambient displays.},
booktitle = {Proceedings of the Third International Conference on Learning Analytics and Knowledge},
pages = {270–274},
numpages = {5},
keywords = {smart classrooms, orchestration, intelligent agents, ambient display},
location = {Leuven, Belgium},
series = {LAK '13}
}

@inproceedings{10.1145/2460296.2460351,
author = {Raca, Mirko and Dillenbourg, Pierre},
title = {System for assessing classroom attention},
year = {2013},
isbn = {9781450317856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2460296.2460351},
doi = {10.1145/2460296.2460351},
abstract = {In this paper we give a preview of our system for automatically evaluating attention in the classroom. We demonstrate our current behaviour metrics and preliminary observations on how they reflect the reactions of people to the given lecture. We also introduce foundations of our hypothesis on peripheral awareness of students during lectures.},
booktitle = {Proceedings of the Third International Conference on Learning Analytics and Knowledge},
pages = {265–269},
numpages = {5},
keywords = {orchestration, computer vision, classroom orchestration, behavioural observation, attention},
location = {Leuven, Belgium},
series = {LAK '13}
}

@inproceedings{10.1145/2460296.2460350,
author = {Holman, Caitlin and Aguilar, Stephen and Fishman, Barry},
title = {GradeCraft: what can we learn from a game-inspired learning management system?},
year = {2013},
isbn = {9781450317856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2460296.2460350},
doi = {10.1145/2460296.2460350},
abstract = {The "gamification" of courses (i.e., designing courses that leverage motivational mechanisms found in videogames) is a movement that is gaining traction in educational research communities and universities. Two game-inspired courses were developed at a high-enrollment public university in an effort to increase student engagement, and to provide students with more personalized learning experiences. We designed a learning management system, GradeCraft, to foreground the affordances of these grading systems, and to enhance the "game-like" experience for students. Along with serving as a translation layer for the grading systems of these courses, GradeCraft is also designed with an eye towards learning analytics, and captures information that can be described as student "process" data. Currently this data includes what types of assignments students choose to complete; how students assign percentage weights to their chosen assignments; how often and how accurately students check or model their course grades; and how successfully assignments are completed by students individually and the class as a whole across a structured grading rubric. We hope GradeCraft will give instructors new insight into student engagement, and provide data-driven ideas about how to tailor courses to student needs.},
booktitle = {Proceedings of the Third International Conference on Learning Analytics and Knowledge},
pages = {260–264},
numpages = {5},
keywords = {syllabus design, learning analytics, gamification, game-inspired instruction},
location = {Leuven, Belgium},
series = {LAK '13}
}

@inproceedings{10.1145/2460296.2460348,
author = {Harrer, Andreas},
title = {Analytics of collaborative planning in Metafora: architecture, data, and analytic methods},
year = {2013},
isbn = {9781450317856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2460296.2460348},
doi = {10.1145/2460296.2460348},
abstract = {This paper describes our approach for learning analytics in the Metafora system, a collaborative learning framework that supports self-regulated and constructionist activities in groups. Our specific interest in analysis is the nature of collaborative planning behaviour and aspects of learning to learn together (L2L2). For that end we will describe the architecture supporting diverse analytic components across all the tools used in Metafora, the data formats, storage and access methods, and the analytic principles we designed and implemented. We will also describe our first insights using these methods on real Metafora data collected during practical experimentation in schools.},
booktitle = {Proceedings of the Third International Conference on Learning Analytics and Knowledge},
pages = {255–259},
numpages = {5},
keywords = {collaborative learning, analytics architecture, analysis indicators},
location = {Leuven, Belgium},
series = {LAK '13}
}

@inproceedings{10.1145/2460296.2460347,
author = {Renzel, Dominik and Klamma, Ralf},
title = {From micro to macro: analyzing activity in the ROLE Sandbox},
year = {2013},
isbn = {9781450317856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2460296.2460347},
doi = {10.1145/2460296.2460347},
abstract = {Current learning services are increasingly based on standard Web technologies and concepts. As by-product of service operation, Web logs capture and contextualize user interactions in a generic manner, in high detail, and on a massive scale. At the same time, we face inventions of data standards for capturing and encoding learner interactions tailored to learning analytics purposes. However, such standards are often focused on institutional and management perspectives or biased by their intended use. In this paper, we argue for Web logs as valuable data sources for learning analytics on all levels of Bronfenbrenner's Ecological System Theory and introduce a simple framework for Web log data enrichment, processing and further analysis. Based on an example data set from a management service for widget-based Personal Learning Environments, we illustrate our approach and discuss the applicability of different analysis techniques along with their particular benefits for learners.},
booktitle = {Proceedings of the Third International Conference on Learning Analytics and Knowledge},
pages = {250–254},
numpages = {5},
keywords = {web logs, processing pipeline, personal learning environment, learning analytics, ecological systems theory},
location = {Leuven, Belgium},
series = {LAK '13}
}

@inproceedings{10.1145/2460296.2460345,
author = {Niemann, Katja and Wolpers, Martin and Stoitsis, Giannis and Chinis, Georgios and Manouselis, Nikos},
title = {Aggregating social and usage datasets for learning analytics: data-oriented challenges},
year = {2013},
isbn = {9781450317856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2460296.2460345},
doi = {10.1145/2460296.2460345},
abstract = {Recent work has studied real-life social and usage datasets from educational applications, highlighting the opportunity to combine or merge them. It is expected that being able to put together different datasets from various applications will make it possible to support learning analytics of a much larger scale and across different contexts. We examine how this can be achieved from a practical perspective by carrying out a study that focuses on three real datasets. More specifically, we combine social data that has been collected from the users of three learning portals and reflect on how they should be handled. We start by studying the data types and formats that these portals use to represent and store social and usage data. Then we develop crosswalks between the different schemas, so that merged versions of the source datasets may be created. The results of this bottom-up, hands-on investigation reveal several interesting issues that need to be overcome before aggregated sets of social and usage data can be actually used to support learning analytics research or services.},
booktitle = {Proceedings of the Third International Conference on Learning Analytics and Knowledge},
pages = {245–249},
numpages = {5},
keywords = {usage data formats, experimental investigation, education, dataset, data-driven analysis},
location = {Leuven, Belgium},
series = {LAK '13}
}

@inproceedings{10.1145/2460296.2460344,
author = {Prinsloo, Paul and Slade, Sharon},
title = {An evaluation of policy frameworks for addressing ethical considerations in learning analytics},
year = {2013},
isbn = {9781450317856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2460296.2460344},
doi = {10.1145/2460296.2460344},
abstract = {Higher education institutions have collected and analysed student data for years, with their focus largely on reporting and management needs. A range of institutional policies exist which broadly set out the purposes for which data will be used and how data will be protected. The growing advent of learning analytics has seen the uses to which student data is put expanding rapidly. Generally though the policies setting out institutional use of student data have not kept pace with this change.Institutional policy frameworks should provide not only an enabling environment for the optimal and ethical harvesting and use of data, but also clarify: who benefits and under what conditions, establish conditions for consent and the de-identification of data, and address issues of vulnerability and harm. A directed content analysis of the policy frameworks of two large distance education institutions shows that current policy frameworks do not facilitate the provision of an enabling environment for learning analytics to fulfil its promise.},
booktitle = {Proceedings of the Third International Conference on Learning Analytics and Knowledge},
pages = {240–244},
numpages = {5},
keywords = {policy, learning analytics, ethics, distance learning},
location = {Leuven, Belgium},
series = {LAK '13}
}

@inproceedings{10.1145/2460296.2460343,
author = {Lonn, Steven and Aguilar, Stephen and Teasley, Stephanie D.},
title = {Issues, challenges, and lessons learned when scaling up a learning analytics intervention},
year = {2013},
isbn = {9781450317856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2460296.2460343},
doi = {10.1145/2460296.2460343},
abstract = {This paper describes an intra-institutional partnership between a research team and a technology service group that was established to facilitate the scaling up of a learning analytics intervention. Our discussion focuses on the benefits and challenges that arose from this partnership in order to provide useful information for similar partnerships developed to support scaling up learning analytics interventions.},
booktitle = {Proceedings of the Third International Conference on Learning Analytics and Knowledge},
pages = {235–239},
numpages = {5},
keywords = {scale, learning analytics, higher education, design-research},
location = {Leuven, Belgium},
series = {LAK '13}
}

