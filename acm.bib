@inproceedings{10.1145/3706468.3706493,
author = {Liu, Qinyi and Shakya, Ronas and Khalil, Mohammad and Jovanovic, Jelena},
title = {Advancing privacy in learning analytics using differential privacy},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706493},
doi = {10.1145/3706468.3706493},
abstract = {This paper addresses the challenge of balancing learner data privacy with the use of data in learning analytics (LA) by proposing a novel framework by applying Differential Privacy (DP). The need for more robust privacy protection keeps increasing, driven by evolving legal regulations and heightened privacy concerns, as well as traditional anonymization methods being insufficient for the complexities of educational data. To address this, we introduce the first DP framework specifically designed for LA and provide practical guidance for its implementation. We demonstrate the use of this framework through a LA usage scenario and validate DP in safeguarding data privacy against potential attacks through an experiment on a well-known LA dataset. Additionally, we explore the trade-offs between data privacy and utility across various DP settings. Our work contributes to the field of LA by offering a practical DP framework that can support researchers and practitioners in adopting DP in their works.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {181–191},
numpages = {11},
keywords = {Differential Privacy (DP), Learning Analytics, Privacy-Enhanced Technologies, Privacy-Preserving, framework},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706491,
author = {Wang, Zuo and Lin, Weiyue and Hu, Xiao},
title = {Self-service Teacher-facing Learning Analytics Dashboard with Large Language Models},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706491},
doi = {10.1145/3706468.3706491},
abstract = {With the rise of online learning platforms, the need for effective learning analytics (LA) has become critical for teachers. However, the development of traditional LA dashboards often requires technical expertise and a certain level of data literacy, preventing many teachers from integrating LA dashboards effectively and flexibly into their teaching practice. This paper explores the development of a self-service teacher-facing learning analytics dashboard powered by large language models (LLMs), for improving teaching practices. By leveraging LLMs, the self-service system aims to simplify the implementation of data queries and visualizations, allowing teachers to create personalized LA dashboards using natural languages. This study also investigates the capabilities of LLMs in generating charts for LA dashboards and evaluates the effectiveness of the self-service system through usability tests with 15 teachers. Preliminary findings suggest that LLMs demonstrate high capabilities in generating charts for LA dashboards, and the LLM-powered self-service system can effectively address participating teachers’ pedagogical needs for LA. This research contributes to the ongoing research on the intersection of LLMs and education, emphasizing the potential of self-service systems to empower teachers in daily teaching practices.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {824–830},
numpages = {7},
keywords = {data visualization, large language models, learning analytics dashboard, self-service learning analytics},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706525,
author = {Li, Zaibei and Yamaguchi, Shunpei and Spikol, Daniel},
title = {OpenMMLA: an IoT-based Multimodal Data Collection Toolkit for Learning Analytics},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706525},
doi = {10.1145/3706468.3706525},
abstract = {Multimodal Learning Analytics (MMLA) expands traditional learning analytics into the digital and physical learning environment, using diverse sensors and systems to collect information about education in more real-world environments. Challenges remain in making these technologies practical for capturing data in authentic learning situations. With the advent of readily accessible powerful artificial intelligence that includes multimodal large language models, new opportunities are available. However, few approaches allow access to these technologies, and most systems are developed for specific environments. Recent work has begun to make toolkits with access to collecting data from sensors, processing, and analyzing, yet these tools are challenging to integrate into a system. This paper introduces OpenMMLA, a toolkit approach that provides programming interfaces for harnessing these technologies into an MMLA platform with prebuilt pipelines, including the audio analyzer, indoor positioning, and video frame analyzer, offering multimodal data collection and visualizations and analytics. The paper provides an initial evaluation of the functionalities of the toolkit in data capturing and the implemented pipelines’ performances.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {872–879},
numpages = {8},
keywords = {Multimodal Learning Analytics, Group Work, Internet of Thing, Smart Badges},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706543,
author = {Ahrar, Arash and Doroodian, Mohammadreza and Hatala, Marek},
title = {Exploring Eye-tracking Features to Understand Students' Sensemaking of Learning Analytics Dashboards},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706543},
doi = {10.1145/3706468.3706543},
abstract = {Learning analytics dashboards (LADs) are widely used in learning analytics as visual tools to present information about learning activities and outcomes. However, only few studies have explored how students make sense from LAD elements and what cognitive processes follow after viewing each element. In this study, we explore how eye-tracking data can help researchers to identify salient LAD elements critical to students’ sensemaking process. Our findings reveal that the eye-tracking derived features, including fixation duration and eye movement patterns, are highly indicative of students’ social comparison tendencies and offer valuable insights into their sensemaking processes.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {931–937},
numpages = {7},
keywords = {learning analytics dashboards, eye tracking, sensemaking, motivation},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3636555.3636856,
author = {Yan, Lixiang and Martinez-Maldonado, Roberto and Gasevic, Dragan},
title = {Generative Artificial Intelligence in Learning Analytics: Contextualising Opportunities and Challenges through the Learning Analytics Cycle},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636856},
doi = {10.1145/3636555.3636856},
abstract = {Generative artificial intelligence (GenAI), exemplified by ChatGPT, Midjourney, and other state-of-the-art large language models and diffusion models, holds significant potential for transforming education and enhancing human productivity. While the prevalence of GenAI in education has motivated numerous research initiatives, integrating these technologies within the learning analytics (LA) cycle and their implications for practical interventions remain underexplored. This paper delves into the prospective opportunities and challenges GenAI poses for advancing LA. We present a concise overview of the current GenAI landscape and contextualise its potential roles within Clow’s generic framework of the LA cycle. We posit that GenAI can play pivotal roles in analysing unstructured data, generating synthetic learner data, enriching multimodal learner interactions, advancing interactive and explanatory analytics, and facilitating personalisation and adaptive interventions. As the lines blur between learners and GenAI tools, a renewed understanding of learners is needed. Future research can delve deep into frameworks and methodologies that advocate for human-AI collaboration. The LA community can play a pivotal role in capturing data about human and AI contributions and exploring how they can collaborate most effectively. As LA advances, it is essential to consider the pedagogical implications and broader socioeconomic impact of GenAI for ensuring an inclusive future.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {101–111},
numpages = {11},
keywords = {ChatGPT, Midjourney, educational technology, generative artificial intelligence, human-AI collaboration, learning analytics},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636923,
author = {Asatryan, Hayk and Tousside, Basile and Mohr, Janis and Neugebauer, Malte and Bijl, Hildo and Spiegelberg, Paul and Frohn-Schauf, Claudia and Frochte, J\"{o}rg},
title = {Exploring Student Expectations and Confidence in Learning Analytics},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636923},
doi = {10.1145/3636555.3636923},
abstract = {Learning Analytics (LA) is nowadays ubiquitous in many educational systems, providing the ability to collect and analyze student data in order to understand and optimize learning and the environments in which it occurs. On the other hand, the collection of data requires to comply with the growing demand regarding privacy legislation. In this paper, we use the Student Expectation of Learning Analytics Questionnaire (SELAQ) to analyze the expectations and confidence of students from different faculties regarding the processing of their data for Learning Analytics purposes. This allows us to identify four clusters of students through clustering algorithms: Enthusiasts, Realists, Cautious and Indifferents. This structured analysis provides valuable insights into the acceptance and criticism of Learning Analytics among students.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {892–898},
numpages = {7},
keywords = {Clustering, Data Protection, Learning Analytics, Survey},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3706468.3706489,
author = {Khalil, Mohammad and Prinsloo, Paul},
title = {The lack of generalisability in learning analytics research: why, how does it matter, and where to?},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706489},
doi = {10.1145/3706468.3706489},
abstract = {Concerns about the lack of impact of learning analytics (LA) research has been part of the evolution of the field since its emergence as a research focus and practice in 2011. The preponderance of small-scale and exploratory nature of much of LA research are well-documented as contributing factors to the lack of generalisability, transferability, replicability and scalability. Through an analysis of 144 full research papers published in the conference proceedings of the Learning Analytics &amp; Knowledge (LAK) Conference '22, 23 and 24, this paper provides an overview of the extent and contours of the lack of generalisability in LA research and pointers for making LA research more generalisable. The inductive and deductive analysis of the recent three LAK conferences provide evidence that a significant percentage (46%) of the corpus papers do not refer at all to generalisability or transferability, while few papers report on the scalability of their research findings. While the crisis of replicability/reproducibility is a wider concern in the broader context of research, considering and reporting on generalisability and transferability is integral to the scientific rigour. We conclude our paper with a range of pointers for addressing the lack of generalisability in LA research including, but not limited to expanding data, methodological adaptation and the potential of open science.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {170–180},
numpages = {11},
keywords = {generalisability, impact, learning analytics, replicability, scalability, transferability},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706553,
author = {Prieto, Luis P. and Alfredo, Riordan and D\'{\i}az-Chavarr\'{\i}a, Henry Benjam\'{\i}n and Martinez-Maldonado, Roberto and Echeverr\'{\i}a, Vanessa},
title = {VALA/AID: A Method for Rapid, Participatory Value-sensitive Learning Analytics and Artificial Intelligence Design},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706553},
doi = {10.1145/3706468.3706553},
abstract = {The adoption of learning analytics (LA) and artificial intelligence (AI) in education has long been a challenge, in part due to the ethical issues it engenders (e.g., the alignment of technology-embedded and human stakeholder values). Value-sensitive design (VSD) is a human-centered and theory-grounded approach to technology design that explicitly elicits and accounts for human values. However, there is scant concrete guidance on how to involve students in co-designing LA technologies from a VSD perspective, in an efficient manner. This paper presents a novel method, called VALA/AID, to elicit student values, challenges and motivations, in the early stages of an LA design process. We briefly illustrate the application of the method and the kind of evidence and design insights that can be distilled from it, for a relatively under-explored context in LA research: doctoral education.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {950–956},
numpages = {7},
keywords = {Human-centered design, Value-sensitive design, Participatory design, Learning analytics, Artificial intelligence in education.},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3636555.3636878,
author = {Poquet, Oleksandra and Joksimovic, Srecko and Brams, Pernille},
title = {The Role of Gender in Citation Practices of Learning Analytics Research},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636878},
doi = {10.1145/3636555.3636878},
abstract = {Mounting evidence indicates that modern citation practices contribute to inequalities in who receives citations. In response to this evidence, our paper investigates citation practices in learning analytics (LA). We analyse citations in papers published over ten years at the Learning Analytics and Knowledge conference (LAK). Our analysis examines the gender composition of authored and cited papers in LA, estimating various factors that explain why one paper cites another, and if the citation rates differ across different author teams. Results indicate an overall increase in the number of women authors at LAK, while the ratio of men to women remains stable. Citation patterns in LAK are influenced by the seniority of authors, paper age, topic, and team size. We found that LAK papers with women as the last author are under-cited, but papers where the first author is a woman and the last author is a man are over-cited. Author teams with different gender composition also vary in who they over- and under-cite. Upon presenting the empirical results, the paper reflects on the role of mindful citation practices and reviews existing measures proposed to promote diversity in citations.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {807–813},
numpages = {7},
keywords = {citations, equity, gender, learning analytics},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3706468.3706541,
author = {Claassen, Alrike and Mirriahi, Negin and Kovanovi\'{c}, Vitomir and Dawson, Shane},
title = {From Data to Design: Integrating Learning Analytics into Educational Design for Effective Decision-Making: From Data to Design},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706541},
doi = {10.1145/3706468.3706541},
abstract = {Learning Analytics (LA) aims to provide university instructors with meaningful data and insights that can be used to improve courses. However, instructors are often met with challenges that arise when wanting to use LA to inform their educational design decisions. For instance, there may be a misalignment between instructors’ needs and the data and insights LA systems provide. Further research is required to understand instructors’ expectations of LA and how it can support the diversity of educational designs. This case study addresses this gap by investigating the role of LA in instructors’ educational decision-making processes. The study employs self-determination theory's constructs to examine instructors’ existing practices when using LA to support their decision-making. The study reveals that LA enables instructors to make data-informed iterative educational design decisions, supporting their need for competence and relatedness. The emotional aspect of LA is an important consideration that can easily lead to demotivation and avoidance of LA. Support is needed to address instructors’ psychological needs so instructors can fully utilise LA to make effective educational design decisions. The findings inform a framework for considering how instructors’ data-informed educational decision-making can be understood. The implications of our findings and opportunities for the future are discussed.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {558–567},
numpages = {10},
keywords = {Learning analytics, data-informed decision-making, educational design, higher education, self-determination theory},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706598.3713395,
author = {Echeverria, Vanessa and Zhao, Linxuan and Alfredo, Riordan and Milesi, Mikaela E and Jin, Yueqiao and Abel, Sophie and Fan, Jie Xiang and Yan, Lixiang and Dix, Samantha and Wotherspoon, Rosie and Li, Xinyu and Jaggard, Hollie A and Osborne, Abra and Buckingham Shum, Simon and Gasevic, Dragan and Martinez-Maldonado, Roberto},
title = {TeamVision: An AI-powered Learning Analytics System for Supporting Reflection in Team-based Healthcare Simulation},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713395},
doi = {10.1145/3706598.3713395},
abstract = {Healthcare simulations help learners develop teamwork and clinical skills in a risk-free setting, promoting reflection on real-world practices through structured debriefs. However, despite video’s potential, it is hard to use, leaving a gap in providing concise, data-driven summaries for supporting effective debriefing. Addressing this, we present TeamVision, an AI-powered multimodal learning analytics (MMLA) system that captures voice presence, automated transcriptions, body rotation, and positioning data, offering educators a dashboard to guide debriefs immediately after simulations. We conducted an in-the-wild study with 56 teams (221 students) and recorded debriefs led by six teachers using TeamVision. Follow-up interviews with 15 students and five teachers explored perceptions of its usefulness, accuracy, and trustworthiness. This paper examines: i) how TeamVision was used in debriefing, ii) what educators found valuable/challenging, and iii) perceptions of its effectiveness. Results suggest TeamVision enables flexible debriefing and highlights the challenges and implications of using AI-powered systems in healthcare simulation.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {309},
numpages = {22},
keywords = {teamwork, large-language models, AI, sensors, healthcare, learning analytics},
location = {
},
series = {CHI '25}
}

@proceedings{10.1145/3706468,
title = {LAK '25: Proceedings of the 15th International Learning Analytics and Knowledge Conference},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@inproceedings{10.1145/3706468.3706555,
author = {Liu, Yiming and Ma, Zhengyang and Ng, Jeremy Tzi Dong and Hu, Xiao},
title = {Multimodal learning analytics for game-based assessment of collaborative problem solving skills among young students},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706555},
doi = {10.1145/3706468.3706555},
abstract = {Collaborative Problem Solving (CPS) has emerged as a key competence for the 21st century. In support of this, valid assessments of CPS skills have become critical. However, limited research has designed and developed CPS assessments for young students. Based on multimodal learning analytics, we aim to develop and validate a game-based assessment of CPS for primary school students. In this study, evidence centered design approach was used to design and develop the game-based CPS assessment. Specifically, we designed and developed a mobile multiplayer online 3D role-playing game on CPS and a coding scheme for coding students’ gameplay data (i.e., game logs and voice chat) based on the ATC21S CPS framework. A total of 32 primary 5 students participated in this study to play the game in a group of four and complete a questionnaire of CPS skills. The gameplay data were coded based on our coding scheme. Correlation analysis between the coded results and the CPS questionnaire data supported the criterion validity of our game-based assessment measure. Additionally, the results of expert interview facilitated our understanding of assessment design and data use. This study will make methodological and practical contributions to the integration of MMLA into game-based CPS assessments.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {963–969},
numpages = {7},
keywords = {Collaborative problem solving, Game-based assessment, K-12 education, Multimodal learning analytics},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3636555.3636847,
author = {Alfredo, Riordan and Echeverria, Vanessa and Jin, Yueqiao and Swiecki, Zachari and Ga\v{s}evi\'{c}, Dragan and Martinez-Maldonado, Roberto},
title = {SLADE: A Method for Designing Human-Centred Learning Analytics Systems},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636847},
doi = {10.1145/3636555.3636847},
abstract = {There is a growing interest in creating Learning Analytics (LA) systems that incorporate student perspectives. Yet, many LA systems still lean towards a technology-centric approach, potentially overlooking human values and the necessity of human oversight in automation. Although some recent LA studies have adopted a human-centred design stance, there is still limited research on establishing safe, reliable, and trustworthy systems during the early stages of LA design. Drawing from a newly proposed framework for human-centred artificial intelligence, we introduce SLADE, a method for ideating and identifying features of human-centred LA systems that balance human control and computer automation. We illustrate SLADE’s application in designing LA systems to support collaborative learning in healthcare. Twenty-one third-year students participated in design sessions through SLADE’s four steps: i) identifying challenges and corresponding LA systems; ii) prioritising these LA systems; iii) ideating human control and automation features; and iv) refining features emphasising safety, reliability, and trustworthiness. Our results demonstrate SLADE’s potential to assist researchers and designers in: 1) aligning authentic student challenges with LA systems through both divergent ideation and convergent prioritisation; 2) understanding students’ perspectives on personal agency and delegation to teachers; and 3) fostering discussions about the safety, reliability, and trustworthiness of LA solutions.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {24–34},
numpages = {11},
keywords = {Design Thinking, Double Diamond, Human-centered AI, Human-centered learning analytics},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3706468.3706542,
author = {Li, Fanjie and Wise, Alyssa Friend},
title = {From Filling Gaps to Amplifying Strengths: Exploring an Asset-Based Approach to Learning Analytics},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706542},
doi = {10.1145/3706468.3706542},
abstract = {This paper explores how an asset-based lens can help expand the design space of learning analytics beyond a deficit-oriented approach focused mainly on identifying and remedying gaps to one that elevates every student’s strengths and potentials. To do so, we draw on the rich history of asset-based pedagogies to consider expansive possibilities for the kinds of information that analytics uncover, the processes that analytics support, and the outcomes that analytics seek to engender. To explore the value and feasibility of an asset-based lens for learning analytics, this paper instantiates the approach with a proof-of-concept prototype designed to support teachers’ noticing of student contributions to discussions in a K-12 science learning classroom. The illustrative case demonstrates ways in which the analytic capabilities of large language models can be leveraged to make visible, and create opportunities for teachers to build upon, the funds of knowledge that students bring to the classroom, thereby creating spaces for minoritized students’ agentive engagement.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {924–930},
numpages = {7},
keywords = {Educational equity, Generative AI, Value-sensitive design, Classroom discourse analytics},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3636555.3636844,
author = {Sloan-Lynch, Jay and Morse, Robert},
title = {Equity-Forward Learning Analytics: Designing a Dashboard to Support Marginalized Student Success},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636844},
doi = {10.1145/3636555.3636844},
abstract = {Student outcomes in US higher education exhibit deep and persistent inequities. The continued underperformance of historically marginalized students remains a serious concern across higher education, reflected in increasing efforts among institutions to infuse diversity, equity, and inclusion into their academic and social communities. Yet despite widespread recognition of these inequities, few studies in the learning analytics literature engage in practical ways with issues of educational equity or DEI considerations. In this paper, we share our work supporting a large college's strategic DEI goals through the creation of a Course Diversity Dashboard informed by research into how students’ study behaviors and performance interact with their gender and ethnic identities to impact course outcomes. The dashboard enables users to explore inequalities in course outcomes and take concrete actions to improve student study strategies, time management, and prior knowledge. Results from our research revealed the existence of previously hidden learner inequities in all courses included in our study as well as critical differences in underrepresented minority students’ prior knowledge. And while we did not find evidence of meaningful differences in the study behaviors of student subgroups, our findings further validate the effectiveness of evidence-informed study strategies in an authentic educational setting.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {1–11},
numpages = {11},
keywords = {DEI, educational equity, learning analytics dashboards, study strategies},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3706468.3706549,
author = {Mun, Soyeon and Jo, Il-Hyun},
title = {Evolving the 4C/ID model through learning analytics approaches: Teaching and learning system design framework for supporting learners' complex problem-solving},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706549},
doi = {10.1145/3706468.3706549},
abstract = {In recent years, there has been a surge in studies focusing on learning analytics (LA) aimed to collecting and analyzing learning trace data from various digital learning platforms. However, there is a need for these platforms to be designed from the ground up by LA experts, ensuring that data collection and analysis procedures align with the objectives and activities of teaching and learning as informed by established instructional theories. As a result, we developed a novel framework that integrates the 4C/ID model with LA approaches to enhance learners’ complex problem-solving abilities within online and blended learning environments. In developing this framework, we focused on the four core components of the original 4C/ID model along with the P-A-S cycle to determine optimal timing and methods for data collection and analysis. Our aim is to propose a framework that not only revisits and reinterprets the 4C/ID model but also fosters the development of a LA system embedded within digital learning platforms and closely tied to educational goals and learning activities. The findings of this study can serve as a valuable resource for designing and constructing adaptive teaching and learning systems, ultimately supporting learners in effectively cultivating their problem-solving skills.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {611–619},
numpages = {9},
keywords = {4C/ID model, Adaptive learning system design, Complex problem-solving, Data-informed decision-making, Learning analytics},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3636555.3636914,
author = {Jung, Yeonji and Wise, Alyssa Friend},
title = {Probing Actionability in Learning Analytics: The Role of Routines, Timing, and Pathways},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636914},
doi = {10.1145/3636555.3636914},
abstract = {Actionability is a critical, but understudied, issue in learning analytics for driving impact on learning. This study investigated access and action-taking of 91 students in an online undergraduate statistics course who received analytics designed for actionability twice a week for five weeks in the semester. Findings showed high levels of access, but little direct action through the provided links. The major contribution of the study was the identification of unexpected indirect actions taken by students in response to the analytics which requires us to think (and look for evidence of impact) more broadly than has been done previously. The study also found that integrating analytics into existing learning tools and routines can increase access rates to the analytics, but may not guarantee meaningful engagement without better strategies to manage analytic timing. Together, this study advances an understanding of analytic actionability, calling for a broader examination of both direct and indirect actions within a larger learning ecosystem.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {871–877},
numpages = {7},
keywords = {Actionability, data-informed learning, formative feedback, learning analytics implementation},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636865,
author = {Milesi, Mikaela Elizabeth and Martinez-Maldonado, Roberto},
title = {Data Storytelling in Learning Analytics? A Qualitative Investigation into Educators’ Perceptions of Benefits and Risks},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636865},
doi = {10.1145/3636555.3636865},
abstract = {Emerging research has begun to explore the incorporation of data storytelling (DS) elements to enhance the design of learning analytics (LA) dashboards. This involves using visual features, such as text annotations and visual highlights, to help educators and learners focus their attention on key insights derived from data and act upon them. Previous studies have often overlooked the perspectives of educators and other stakeholders on the potential value and risks associated with implementing DS in LA to guide attention. We address this gap by presenting a case study examining how educators perceive the: i) potential value of DS features for teaching and learning design; ii) role of the visualisation designer in delivering a contextually appropriate data story; and iii) ethical implications of utilising DS to communicate insights. We asked educators from a first-year undergraduate program to explore and discuss DS and the visualisation designer by reviewing sample data stories using their students’ data and crafting their own data stories. Our findings suggest that educators were receptive to DS features, especially meaningful use of annotations and highlighting important data points to easily identify critical information. Every participant acknowledged the potential for DS features to be exploited for harmful or self-serving purposes.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {167–177},
numpages = {11},
keywords = {data storytelling, information visualisation, learning analytics},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3576050.3576141,
author = {Divjak, Blazenka and Svetec, Barbi and Horvat, Damir},
title = {Learning analytics dashboards: What do students actually ask for?},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576141},
doi = {10.1145/3576050.3576141},
abstract = {Learning analytics (LA) has been opening new opportunities to support learning in higher education (HE). LA dashboards are an important tool in providing students with insights into their learning progress, and predictions, leading to reflection and adaptation of learning plans and habits. Based on a human-centered approach, we present a perspective of students, as essential stakeholders, on LA dashboards. We describe a longitudinal study, based on survey methodology. The study included two iterations of a survey, conducted with second-year ICT students in 2017 (N = 222) and 2022 (N = 196). The study provided insights into the LA dashboard features the students find the most useful to support their learning. The students highly appreciated features related to short-term planning and organization of learning, while they were cautious about comparison and competition with other students, finding such features possibly demotivating. We compared the 2017 and 2022 results to establish possible changes in the students’ perspectives with the COVID-19 pandemic. The students’ awareness of the benefits of LA has increased, which may be related to the strong focus on online learning during the pandemic. Finally, a factor analysis yielded a dashboard model with five underlying factors: comparison, planning, predictions, extracurricular, and teachers.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {44–56},
numpages = {13},
keywords = {dashboard, higher education, human-centered, learning analytics, students’ perspectives},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576060,
author = {van Haastrecht, Max and Brinkhuis, Matthieu and Peichl, Jessica and Remmele, Bernd and Spruit, Marco},
title = {Embracing Trustworthiness and Authenticity in the Validation of Learning Analytics Systems},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576060},
doi = {10.1145/3576050.3576060},
abstract = {Learning analytics sits in the middle space between learning theory and data analytics. The inherent diversity of learning analytics manifests itself in an epistemology that strikes a balance between positivism and interpretivism, and knowledge that is sourced from theory and practice. In this paper, we argue that validation approaches for learning analytics systems should be cognisant of these diverse foundations. Through a systematic review of learning analytics validation research, we find that there is currently an over-reliance on positivistic validity criteria. Researchers tend to ignore interpretivistic criteria such as trustworthiness and authenticity. In the 38 papers we analysed, researchers covered positivistic validity criteria 221 times, whereas interpretivistic criteria were mentioned 37 times. We motivate that learning analytics can only move forward with holistic validation strategies that incorporate “thick descriptions” of educational experiences. We conclude by outlining a planned validation study using argument-based validation, which we believe will yield meaningful insights by considering a diverse spectrum of validity criteria.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {552–558},
numpages = {7},
keywords = {authenticity, interpretivism, learning analytics, trustworthiness, validation},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3706468.3706545,
author = {Jin, Yueqiao and Yang, Kaixun and Yan, Lixiang and Echeverria, Vanessa and Zhao, Linxuan and Alfredo, Riordan and Milesi, Mikaela and Fan, Jie Xiang and Li, Xinyu and Gasevic, Dragan and Martinez-Maldonado, Roberto},
title = {Chatting with a Learning Analytics Dashboard: The Role of Generative AI Literacy on Learner Interaction with Conventional and Scaffolding Chatbots},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706545},
doi = {10.1145/3706468.3706545},
abstract = {Learning analytics dashboards (LADs) simplify complex learner data into accessible visualisations, providing actionable insights for educators and students. However, their educational effectiveness has not always matched the sophistication of the technology behind them. Explanatory and interactive LADs, enhanced by generative AI (GenAI) chatbots, hold promise by enabling dynamic, dialogue-based interactions with data visualisations and offering personalised feedback through text. Yet, the effectiveness of these tools may be limited by learners’ varying levels of GenAI literacy, a factor that remains underexplored in current research. This study investigates the role of GenAI literacy in learner interactions with conventional (reactive) versus scaffolding (proactive) chatbot-assisted LADs. Through a comparative analysis of 81 participants, we examine how GenAI literacy is associated with learners’ ability to interpret complex visualisations and their cognitive processes during interactions with chatbot-assisted LADs. Results show that while both chatbots significantly improved learner comprehension, those with higher GenAI literacy benefited the most, particularly with conventional chatbots, demonstrating diverse prompting strategies. Findings highlight the importance of considering learners’ GenAI literacy when integrating GenAI chatbots in LADs and educational technologies. Incorporating scaffolding techniques within GenAI chatbots can be an effective strategy, offering a more guided experience that reduces reliance on learners’ GenAI literacy.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {579–590},
numpages = {12},
keywords = {learning analytics dashboard, generative AI literacy, generative AI chatbots, data visualisation},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3636555.3636886,
author = {Alcock, Sarah and Rienties, Bart and Aristeidou, Maria and Kouadri Most\'{e}faoui, Soraya},
title = {How do visualizations and automated personalized feedback engage professional learners in a Learning Analytics Dashboard?},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636886},
doi = {10.1145/3636555.3636886},
abstract = {Learning Analytics Dashboards (LAD) are the subject of research in a multitude of schools and higher education institutions, but a lack of research into learner-facing dashboards in professional learning has been identified. This study took place in an authentic professional learning context and aims to contribute insights into LAD design by using an academic approach in a practice-based environment. An existing storytelling LAD created to support 81 accountants was evaluated using Technology Acceptance Model, finding a learner expectation for clarity, conciseness, understanding and guidance on next steps. High usage levels and a ‘take what you need’ approach was identified, with all visualizations and automated personalized feedback being considered useful although to varying degrees. Professional learners in this study focus on understanding and acting upon weaknesses rather than celebrating strengths. The lessons for LAD design are to offer choice and create elements which support learners to take action to improve performance at a multitude of time points and levels of success.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {316–325},
numpages = {10},
keywords = {LAD, Learning Analytics Dashboard, Technology Acceptance Model, accountancy, assessment, data storytelling, feedback, personalization, professional learning},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636874,
author = {Dang, Belle and Nguyen, Andy and J\"{a}rvel\"{a}, Sanna},
title = {The Unspoken Aspect of Socially Shared Regulation in Collaborative Learning: AI-Driven Learning Analytics Unveiling ‘Silent Pauses’},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636874},
doi = {10.1145/3636555.3636874},
abstract = {Socially Shared Regulation (SSRL) contributes to collaborative learning success. Recent advancements in Artificial Intelligence (AI) and Learning Analytics (LA) have enabled examination of this phenomenon’s temporal and cyclical complexities. However, most of these studies focus on students’ verbalised interactions, not accounting for the intertwined ’silent pauses’ that can index learners’ internal cognitive and emotional processes, potentially offering insight into regulation’s core mental processes. To address this gap, we employed AI-driven LA to explore the deliberation tactics among ten triads of secondary students during a face-to-face collaborative task (2,898 events). Discourse was coded for deliberative interactions for SSRL. With the micro-annotation of ‘silent pause’ added, sequences were analysed with the Optimal Matching algorithm, Ward’s Clustering and Lag Sequential Analysis. Three distinct deliberation tactics with different patterns and characteristics involving silent pauses emerged: i) Elaborated deliberation, ii) Coordinated deliberation, and iii) Solitary deliberation. Our findings highlight the role of ‘silent pauses’ in revealing not only the pattern but also the dynamics and characteristics of each deliberative interaction. This study illustrates the potential of AI-driven LA to tap into granular data points that enrich discourse analysis, presenting theoretical, methodological, and practical contributions and implications.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {231–240},
numpages = {10},
keywords = {Artificial Intelligence, Collaborative Learning, Deliberative Interactions, Learning Analytics, Socially Shared Regulation},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636921,
author = {Liu, Qinyi and Khalil, Mohammad and Jovanovic, Jelena and Shakya, Ronas},
title = {Scaling While Privacy Preserving: A Comprehensive Synthetic Tabular Data Generation and Evaluation in Learning Analytics},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636921},
doi = {10.1145/3636555.3636921},
abstract = {Privacy poses a significant obstacle to the progress of learning analytics (LA), presenting challenges like inadequate anonymization and data misuse that current solutions struggle to address. Synthetic data emerges as a potential remedy, offering robust privacy protection. However, prior LA research on synthetic data lacks thorough evaluation, essential for assessing the delicate balance between privacy and data utility. Synthetic data must not only enhance privacy but also remain practical for data analytics. Moreover, diverse LA scenarios come with varying privacy and utility needs, making the selection of an appropriate synthetic data approach a pressing challenge. To address these gaps, we propose a comprehensive evaluation of synthetic data, which encompasses three dimensions of synthetic data quality, namely resemblance, utility, and privacy. We apply this evaluation to three distinct LA datasets, using three different synthetic data generation methods. Our results show that synthetic data can maintain similar utility (i.e., predictive performance) as real data, while preserving privacy. Furthermore, considering different privacy and data utility requirements in different LA scenarios, we make customized recommendations for synthetic data generation. This paper not only presents a comprehensive evaluation of synthetic data but also illustrates its potential in mitigating privacy concerns within the field of LA, thus contributing to a wider application of synthetic data in LA and promoting a better practice for open science.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {620–631},
numpages = {12},
keywords = {Generative adversarial network, Learning analytics, Privacy Preserving Technologies, Synthetic data generation},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3506860.3506878,
author = {Khalil, Mohammad and Prinsloo, Paul and Slade, Sharon},
title = {A Comparison of Learning Analytics Frameworks: a Systematic Review},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506878},
doi = {10.1145/3506860.3506878},
abstract = {While learning analytics frameworks precede the official launch of learning analytics in 2011, there has been a proliferation of learning analytics frameworks since. This systematic review of learning analytics frameworks between 2011 and 2021 in three databases resulted in an initial corpus of 268 articles and conference proceeding papers based on the occurrence of “learning analytics” and “framework” in titles, keywords and abstracts. The final corpus of 46 frameworks were analysed using a coding scheme derived from purposefully selected learning analytics frameworks. The results found that learning analytics frameworks share a number of elements and characteristics such as source, development and application focus, a form of representation, data sources and types, focus and context. Less than half of the frameworks consider student data privacy and ethics. Finally, while design and process elements of these frameworks may be transferable and scalable to other contexts, users in different contexts will be best-placed to determine their transferability/scalability.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {152–163},
numpages = {12},
keywords = {Learning analytics, comparison, framework, literature review, systematic review},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3636555.3636845,
author = {Bourguet, Marie-Luce},
title = {Demonstrating the impact of study regularity on academic success using learning analytics},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636845},
doi = {10.1145/3636555.3636845},
abstract = {Students can be described as self-regulated learners when they are meta-cognitively, motivationally, and behaviourally active participants in their own learning. Flipping the classroom requires from students good self-regulated learning skills, primarily time management and study regularity, as they must have engaged in learning activities prior to attending live classes. In this short paper, we describe our approach of using learning analytics to demonstrate the impact of study regularity on academic success in a flipped learning environment. Our key contribution is the definition of a measure of study regularity that can uncover various students’ learning profiles during flipped learning. We are showing that the regularity measure correlate strongly with academic success and that it can be used to predict students’ performance. We then discuss how such a measure can also be used to raise student’s awareness about their learning behaviour and lack of appropriate strategy, to nudge the students into modifying their learning behaviour, and to monitor class behaviour, such as detecting a worrying students’ disengagement trend.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {736–741},
numpages = {6},
keywords = {Flipped learning, Grade predictions, Learning pathways, Learning traces, Self-regulated learning, Study regularity},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3706468.3706522,
author = {Acosta, Halim and Hong, Daeun and Lee, Seung and Min, Wookhee and Mott, Bradford and Hmelo-Silver, Cindy and Lester, James},
title = {Collaborative Game-based Learning Analytics: Predicting Learning Outcomes from Game-based Collaborative Problem Solving Behaviors},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706522},
doi = {10.1145/3706468.3706522},
abstract = {Skills in collaborative problem solving (CPS) are essential for the 21st century, enabling students to solve complex problems effectively. As the demand for these skills rises, understanding their development and manifestation becomes increasingly important. To address this need, we present a data-driven framework that identifies behavioral patterns associated with CPS practices and can assess students’ learning outcomes. It provides explainable insights into the relationship between students’ behaviors and learning performance. We employ embedding and clustering techniques to categorize similar trace logs and apply Latent Dirichlet allocation to generate meaningful descriptors. To capture the temporal evolution of student behaviors, we introduce a graph-based representation of transitions between behavior patterns extracted using constraint-based pattern mining. We map behavioral patterns to a CPS ontology by analyzing how action sequences correspond to specific CPS practices. Analysis of semi-structured trace log data from 61 middle school students engaged in collaborative game-based learning reveals that the extracted behavioral patterns significantly predict student learning gains using generalized additive models. Our analysis identifies patterns that provide insights into the relationship between student use of CPS practices and learning outcomes.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {429–438},
numpages = {10},
keywords = {Collaborative Game-Based Learning, Collaborative Problem Solving, Trace Log Analysis, Explainable Machine Learning},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3506860.3506862,
author = {Yan, Lixiang and Zhao, Linxuan and Gasevic, Dragan and Martinez-Maldonado, Roberto},
title = {Scalability, Sustainability, and Ethicality of Multimodal Learning Analytics},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506862},
doi = {10.1145/3506860.3506862},
abstract = {Multimodal Learning Analytics (MMLA) innovations are commonly aimed at supporting learners in physical learning spaces through state-of-the-art sensing technologies and analysis techniques. Although a growing body of MMLA research has demonstrated the potential benefits of sensor-based technologies in education, whether their use can be scalable, sustainable, and ethical remains questionable. Such uncertainty can limit future research and the potential adoption of MMLA by educational stakeholders in authentic learning situations. To address this, we systematically reviewed the methodological, operational, and ethical challenges faced by current MMLA works that can affect the scalability and sustainability of future MMLA innovations. A total of 96 peer-reviewed articles published after 2010 were included. The findings were summarised into three recommendations, including i) improving reporting standards by including sufficient details about sensors, analysis techniques, and the full disclosure of evaluation metrics, ii) fostering interdisciplinary collaborations among experts in learning analytics, software, and hardware engineering to develop affordable sensors and upgrade MMLA innovations that used discontinued technologies, and iii) developing ethical guidelines to address the potential risks of bias, privacy, and equality concerns with using MMLA innovations. Through these future research directions, MMLA can remain relevant and eventually have actual impacts on educational practices.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {13–23},
numpages = {11},
keywords = {ethics, learning analytics, multimodal learning analytics, scalability, sensors, sustainability},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3636555.3636888,
author = {Brezack, Natalie and Chan, Wynnie and Feng, Mingyu},
title = {Student Effort and Progress Learning Analytics Data Inform Teachers’ SEL Discussions in Math Class},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636888},
doi = {10.1145/3636555.3636888},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {338–348},
numpages = {11},
keywords = {learning analytics data, math instruction, online math problem-solving, socioemotional learning},
location = {Kyoto, Japan},
series = {LAK '24}
}

@proceedings{10.1145/3636555,
title = {LAK '24: Proceedings of the 14th Learning Analytics and Knowledge Conference},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Kyoto, Japan}
}

@inproceedings{10.1145/3448139.3448210,
author = {Wise, Alyssa Friend and Sarmiento, Juan Pablo and Boothe Jr., Maurice},
title = {Subversive Learning Analytics},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448210},
doi = {10.1145/3448139.3448210},
abstract = {This paper puts forth the idea of a subversive stance on learning analytics as a theoretically-grounded means of engaging with issues of power and equity in education and the ways in which they interact with the usage of data on learning processes. The concept draws on efforts from fields such as socio-technical systems and critical race studies that have a long history of examining the role of data in issues of race, gender and class. To illustrate the value that such a stance offers the field of learning analytics, we provide examples of how taking a subversive perspective can help us to identify tacit assumptions-in-practice, ask generative questions about our design processes and consider new modes of creation to produce tools that operate differently in the world.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {639–645},
numpages = {7},
keywords = {critical frameworks, educational equity, subversive stance},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3576050.3576096,
author = {Kitto, Kirsty and Manly, Catherine A. and Ferguson, Rebecca and Poquet, Oleksandra},
title = {Towards more replicable content analysis for learning analytics},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576096},
doi = {10.1145/3576050.3576096},
abstract = {Content analysis (CA) is a method frequently used in the learning sciences and so increasingly applied in learning analytics (LA). Despite this ubiquity, CA is a subtle method, with many complexities and decision points affecting the outcomes it generates. Although appearing to be a neutral quantitative approach, coding CA constructs requires an attention to decision making and context that aligns it with a more subjective, qualitative interpretation of data. Despite these challenges, we increasingly see the labels in CA-derived datasets used as training sets for machine learning (ML) methods in LA. However, the scarcity of widely shareable datasets means research groups usually work independently to generate labelled data, with few attempts made to compare practice and results across groups. A risk is emerging that different groups are coding constructs in different ways, leading to results that will not prove replicable. We report on two replication studies using a previously reported construct. A failure to achieve high inter-rater reliability suggests that coding of this scheme is not currently replicable across different research groups. We point to potential dangers in this result for those who would use ML to automate the detection of various educationally relevant constructs in LA.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {303–314},
numpages = {12},
keywords = {content analysis, labelled data, methodology, reproducibility},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3706468.3706484,
author = {Borchers, Conrad and Ooge, Jeroen and Peng, Cindy and Aleven, Vincent},
title = {How Learner Control and Explainable Learning Analytics About Skill Mastery Shape Student Desires to Finish and Avoid Loss in Tutored Practice},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706484},
doi = {10.1145/3706468.3706484},
abstract = {Personalized problem selection enhances student practice in tutoring systems. Prior research has focused on transparent problem selection that supports learner control but rarely engages learners in selecting practice materials. We explored how different levels of control (i.e., full AI control, shared control, and full learner control), combined with showing learning analytics on skill mastery and visual what-if explanations, can support students in practice contexts requiring high degrees of self-regulation, such as homework. Semi-structured interviews with six middle school students revealed three key insights: (1)&nbsp;participants highly valued learner control for an enhanced learning experience and better self-regulation, especially because most wanted to avoid losses in skill mastery; (2)&nbsp;only seeing their skill mastery estimates often made participants base problem selection on their weaknesses; and (3)&nbsp;what-if explanations stimulated participants to focus more on their strengths and improve skills until they were mastered. These findings show how explainable learning analytics could shape students’ selection strategies when they have control over what to practice. They suggest promising avenues for helping students learn to regulate their effort, motivation, and goals during practice with tutoring systems.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {810–816},
numpages = {7},
keywords = {explainable AI, mastery learning, intelligent tutoring systems, design-based research, K-12, self-regulated learning},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3576050.3576055,
author = {Nguyen, Ha},
title = {TikTok as Learning Analytics Data: Framing Climate Change and Data Practices},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576055},
doi = {10.1145/3576050.3576055},
abstract = {Climate change has far-reaching impacts on communities around the world. However, climate change education has more often focused on scientific facts and statistics at a global scale than experiences at personal and local scales. To understand how to frame climate change education, I turn to youth-created videos on TikTok—a video-sharing, social media platform. Semantic network analysis of hashtags related to climate change reveals multifaceted, intertwining discourse around awareness of climate change consequences, call for action to reduce human impacts on natural systems, and environmental activism. I further explore how youth integrate personal, lived experiences data into climate change discussions. A higher usage of second-person perspective ("you"; i.e., addressing the audience), prosocial and agency words, and negative messaging tone are associated with higher odds of a video integrating lived experiences. These findings illustrate the platform’s affordances: In communicating to a broad audience, youth take on agency and pro-social stances and express emotions to relate to viewers and situate their content. Findings suggest the utility of learning analytics to explore youth’s perspectives and provide insights to frame climate change education in ways that elevate lived experiences.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {33–43},
numpages = {11},
keywords = {climate change education, social learning analytics, social media},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3636555.3636913,
author = {Taylor, Megan and Barthakur, Abhinava and Azad, Arslan and Joksimovic, Srecko and Zhang, Xuwei and Siemens, George},
title = {Quantifying Collaborative Complex Problem Solving in Classrooms using Learning Analytics},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636913},
doi = {10.1145/3636555.3636913},
abstract = {Complex problem solving (CPS) is a critical skill with far-reaching implications for personal success and professional development. While CPS research has made extensive progress, additional investigation is needed to explore CPS processes beyond online contexts and performance outcomes. This study, conducted with Year 9 students aged between thirteen and fourteen, focuses on collaborative CPS. It utilises audio and video recordings to capture group communications during a CPS classroom activity. To analyse these interactions, we introduce a novel CPS framework as a dynamic, cognitive and social process involving interrelated main skills, sub-skills, and indicators. Through sequential pattern mining, we identify recurring subskill patterns that reflect CPS processes in an educational environment. Our research underscores the importance of employing diverse patterns before plan execution, particularly building shared knowledge, planning, and negotiation. We uncover patterns related to groups going off-task and highlight the significance of effective communication and maintaining focus for keeping groups on track. Furthermore, we indicate patterns following the detection of emergent issues, recognising the value of cultivating clarity and adaptability among team members. Our CPS framework, combined with our research results, offers practical implications for teaching, learning, and assessment approaches in educational, professional and industry sectors.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {551–562},
numpages = {12},
keywords = {Collaborative learning, Complex problem solving, Sequential pattern mining, problem-solving strategies},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3543873.3587644,
author = {Ekuban, Audrey and Domingue, John},
title = {Towards Decentralised Learning Analytics (Positioning Paper)},
year = {2023},
isbn = {9781450394192},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543873.3587644},
doi = {10.1145/3543873.3587644},
abstract = {When students interact with an online course, the routes they take when navigating through the course can be captured. Learning Analytics is the process of measuring, collecting, recording, and analysing this Student Activity Data. Predictive Learning Analytics, a sub-field of Learning Analytics, can help to identify students who are at risk of dropping out or failing, as well as students who are close to a grade boundary. Course tutors can use the insights provided by the analyses to offer timely assistance to these students. Despite its usefulness, there are privacy and ethical issues with the typically centralised approach to Predictive Learning Analytics. In this positioning paper, it is proposed that the issues associated with Predictive Learning Analytics can be alleviated, in a framework called EMPRESS, by combining 1) self-sovereign data, where data owners control who legitimately has access to data pertaining to them, 2) Federated Learning, where the data remains on the data owner’s device and/or the data is processed by the data owners themselves, and 3) Graph Convolutional Networks for Heterogeneous graphs, which are examples of knowledge graphs.},
booktitle = {Companion Proceedings of the ACM Web Conference 2023},
pages = {1435–1438},
numpages = {4},
keywords = {blockchain, federated learning, heterogeneous knowledge graph, learning analytics},
location = {Austin, TX, USA},
series = {WWW '23 Companion}
}

@inproceedings{10.1145/3576050.3576142,
author = {Kizilcec, Ren\'{e} F. and Viberg, Olga and Jivet, Ioana and Martinez Mones, Alejandra and Oh, Alice and Hrastinski, Stefan and Mutimukwe, Chantal and Scheffel, Maren},
title = {The Role of Gender in Students’ Privacy Concerns about Learning Analytics: Evidence from five countries},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576142},
doi = {10.1145/3576050.3576142},
abstract = {The protection of students’ privacy in learning analytics (LA) applications is critical for cultivating trust and effective implementations of LA in educational environments around the world. However, students’ privacy concerns and how they may vary along demographic dimensions that historically influence these concerns have yet to be studied in higher education. Gender differences, in particular, are known to be associated with people's information privacy concerns, including in educational settings. Building on an empirically validated model and survey instrument for student privacy concerns, their antecedents and their behavioral outcomes, we investigate the presence of gender differences in students’ privacy concerns about LA. We conducted a survey study of students in higher education across five countries (N = 762): Germany, South Korea, Spain, Sweden and the United States. Using multiple regression analysis, across all five countries, we find that female students have stronger trusting beliefs and they are more inclined to engage in self-disclosure behaviors compared to male students. However, at the country level, these gender differences are significant only in the German sample, for Bachelor's degree students, and for students between the ages of 18 and 24. Thus, national context, degree program, and age are important moderating factors for gender differences in student privacy concerns.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {545–551},
numpages = {7},
keywords = {Gender, Learning Analytics, Privacy Concerns, Students},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3587102.3588836,
author = {Caspari, Laura and Greifenstein, Luisa and Heuer, Ute and Fraser, Gordon},
title = {ScratchLog: Live Learning Analytics for Scratch},
year = {2023},
isbn = {9798400701382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587102.3588836},
doi = {10.1145/3587102.3588836},
abstract = {Scratch is a hugely popular block-based programming environment that is often used in educational settings, and has therefore recently also become a focus for research on programming education. Scratch provides dedicated teacher accounts that make it easy and convenient to handle lessons with school classes. However, once learners join a Scratch classroom, it is challenging to keep track of what they are doing: Both teachers and researchers may be interested in learning analytics to help them monitor students or evaluate teaching material. Researchers may also be interested in understanding how programs are created and how learners use Scratch. Neither use case is supported by Scratch itself currently. In this paper, we introduce ScratchLog, a tool that collects data from learners using Scratch. ScratchLog provides custom user management and makes it easy to set up courses and assignments. Starting from a task description and a starter project, learners transparently use Scratch while ScratchLog collects usage data, such as the history of code edits, or statistics about how the Scratch user interface was used. This data can be viewed on the ScratchLog web interface, or exported for further analysis, for example to inspect the functionality of programs using automated tests.},
booktitle = {Proceedings of the 2023 Conference on Innovation and Technology in Computer Science Education V. 1},
pages = {403–409},
numpages = {7},
keywords = {block-based programming, learning analytics, scratch},
location = {Turku, Finland},
series = {ITiCSE 2023}
}

@inproceedings{10.1145/3576050.3576061,
author = {Herodotou, Christothea and Maguire, Claire and Hlosta, Martin and Mulholland, Paul},
title = {Predictive Learning Analytics and University Teachers: Usage and perceptions three years post implementation},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576061},
doi = {10.1145/3576050.3576061},
abstract = {Predictive learning analytics (PLA) dashboards have been used by teachers to identify students at risk of failing their studies and provide proactive support. Yet, very few of them have been deployed at a large scale or had their use studied at a mature level of implementation. In this study, we surveyed 366 distance learning university teachers across four faculties three years after PLA has been made available across university as business as usual. Informed by the Unified Theory of Acceptance and Use of Technology (UTAUT), we present a context-specific version of UTAUT that reflects teachers’ perceptions of PLA in distance learning higher education. The adoption and use of PLA was shown to be positively influenced by less experience in teaching, performance expectancy, self-efficacy, positive attitudes, and low anxiety, while negatively influenced by a lack of facilitating conditions and low effort expectancy, indicating that the type of technology and context within which it is used are significant factors determining our understanding of technology usage and adoption. This study provides significant insights as to how to design, apply and implement PLA with teachers in higher education.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {68–78},
numpages = {11},
keywords = {Predictive learning analytics, Technology Adoption, UTAUT, University teachers},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3643479.3662053,
author = {Le, Lai Hoang and Nguyen, Hoang D. and Crane, Martin and Mai, Tai Tan},
title = {Multimedia learning analytics feedback in simulation-based training: A brief review},
year = {2024},
isbn = {9798400705472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643479.3662053},
doi = {10.1145/3643479.3662053},
abstract = {Learning analytics has gained significant attention in recent years, particularly in the healthcare field. This area of research offers valuable insights to educators, students, and researchers to enhance the quality of education. One area of focus in learning analytics is how stakeholders provide feedback to each other during training in operating theatres. With the availability of diverse multimedia elements, such as text, images, and spoken language, as data, employing effective feedback methods can bring substantial benefits to teachers, students, and researchers. This study synthesizes various approaches that apply multimedia to provide feedback in teaching, comparing and exploring their potential application in simulation-based medical training. The feasibility of input data, the effectiveness of feedback on recipients, and the AI method of generating or synthesizing feedback using existing data efficiency are also discussed in line with ethical standards. Finally, a multimedia feedback framework is proposed, which utilizes diverse multimedia formats and can be effectively implemented in various real-world scenarios.},
booktitle = {Proceedings of the 1st ACM Workshop on AI-Powered Q&amp;A Systems for Multimedia},
pages = {25–30},
numpages = {6},
keywords = {Learning analytics, Multimedia feedback, Simulation-based learning},
location = {Phuket, Thailand},
series = {AIQAM '24}
}

@inproceedings{10.1145/3576050.3576076,
author = {Zhao, Linxuan and Swiecki, Zachari and Gasevic, Dragan and Yan, Lixiang and Dix, Samantha and Jaggard, Hollie and Wotherspoon, Rosie and Osborne, Abra and Li, Xinyu and Alfredo, Riordan and Martinez-Maldonado, Roberto},
title = {METS: Multimodal Learning Analytics of Embodied Teamwork Learning},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576076},
doi = {10.1145/3576050.3576076},
abstract = {Embodied team learning is a form of group learning that occurs in co-located settings where students need to interact with others while actively using resources in the physical learning space to achieve a common goal. In such situations, communication dynamics can be complex as team discourse segments can happen in parallel at different locations of the physical space with varied team member configurations. This can make it hard for teachers to assess the effectiveness of teamwork and for students to reflect on their own experiences. To address this problem, we propose METS (Multimodal Embodied Teamwork Signature), a method to model team dialogue content in combination with spatial and temporal data to generate a signature of embodied teamwork. We present a study in the context of a highly dynamic healthcare team simulation space where students can freely move. We illustrate how signatures of embodied teamwork can help to identify key differences between high and low performing teams: i) across the whole learning session; ii) at different phases of learning sessions; and iii) at particular spaces of interest in the learning space.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {186–196},
numpages = {11},
keywords = {Collaborative learning, Communication, Healthcare simulation, Multimodality, Teamwork},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3636555.3636930,
author = {Fernandez-Nieto, Gloria Milena and Martinez-Maldonado, Roberto and Echeverria, Vanessa and Kitto, Kirsty and Ga\v{s}evi\'{c}, Dragan and Buckingham Shum, Simon},
title = {Data Storytelling Editor: A Teacher-Centred Tool for Customising Learning Analytics Dashboard Narratives},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636930},
doi = {10.1145/3636555.3636930},
abstract = {Dashboards are increasingly used in education to provide teachers and students with insights into learning. Yet, existing dashboards are often criticised for their failure to provide the contextual information or explanations necessary to help students interpret these data. Data Storytelling (DS) is emerging as an alternative way to communicate insights providing guidance and context to facilitate students’ interpretations. However, while data stories have proven effective in prompting students’ reflections, to date, it has been necessary for researchers to craft the stories rather than enabling teachers to do this by themselves. This can make this approach more feasible and scalable while also respecting teachers’ agency. Based on the notion of DS, this paper presents a DS editor for teachers. A study was conducted in two universities to examine whether the editor could enable teachers to create stories adapted to their learning designs. Results showed that teachers appreciated how the tool enabled them to contextualise automated feedback to their teaching needs, generating data stories to support student reflection.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {678–689},
numpages = {12},
keywords = {LA Dashboards, data storytelling, teacher-centered tool},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636885,
author = {Mangaroska, Katerina and Larssen, Kristine and Amundsen, Andreas and Vesin, Boban and Giannakos, Michail},
title = {Understanding engagement through game learning analytics and design elements: Insights from a word game case study},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636885},
doi = {10.1145/3636555.3636885},
abstract = {Educational games have become an efficient and engaging way to enhance learning. Analytics have played a critical role in designing contemporary educational games, with most game design elements leveraging analytics produced during gameplay and learning. The presented study tackles the complex construct of engagement, which has been the central piece behind the success of educational games, by investigating the role of analytics-driven game elements on players’ engagement. To do so, we implemented a casual word game incorporating game design elements relevant to learning and conducted a within-subjects study where 39 participants played the game for two weeks. We found that the frequency of use of different game elements contributed to different dimensions of engagement. Our findings show that five of the eight game elements implemented in the word game engage players on an emotional, motivational, and cognitive level, thus emphasizing the importance of engagement as a multidimensional construct in designing educational casual games that offer highly engaging experiences.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {305–315},
numpages = {11},
keywords = {Cognitive evaluation theory, analytics-driven game design elements},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636884,
author = {Kaliisa, Rogers and Misiejuk, Kamila and L\'{o}pez-Pernas, Sonsoles and Khalil, Mohammad and Saqr, Mohammed},
title = {Have Learning Analytics Dashboards Lived Up to the Hype? A Systematic Review of Impact on Students' Achievement, Motivation, Participation and Attitude},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636884},
doi = {10.1145/3636555.3636884},
abstract = {While learning analytics dashboards (LADs) are the most common form of LA intervention, there is limited evidence regarding their impact on students’ learning outcomes. This systematic review synthesizes the findings of 38 research studies to investigate the impact of LADs on students' learning outcomes, encompassing achievement, participation, motivation, and attitudes. As we currently stand, there is no evidence to support the conclusion that LADs have lived up to the promise of improving academic achievement. Most studies reported negligible or small effects, with limited evidence from well-powered controlled experiments. Many studies merely compared users and non-users of LADs, confounding the dashboard effect with student engagement levels. Similarly, the impact of LADs on motivation and attitudes appeared modest, with only a few exceptions demonstrating significant effects. Small sample sizes in these studies highlight the need for larger-scale investigations to validate these findings. Notably, LADs showed a relatively substantial impact on student participation. Several studies reported medium to large effect sizes, suggesting that LADs can promote engagement and interaction in online learning environments. However, methodological shortcomings, such as reliance on traditional evaluation methods, self-selection bias, the assumption that access equates to usage, and a lack of standardized assessment tools, emerged as recurring issues. To advance the research line for LADs, researchers should use rigorous assessment methods and establish clear standards for evaluating learning constructs. Such efforts will advance our understanding of the potential of LADs to enhance learning outcomes and provide valuable insights for educators and researchers alike.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {295–304},
numpages = {10},
keywords = {Learning analytics dashboards (LADs), impact, learning outcomes, systematic review},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3576050.3576083,
author = {Barthakur, Abhinava and Dawson, Shane and Kovanovic, Vitomir},
title = {Advancing leaner profiles with learning analytics: A scoping review of current trends and challenges},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576083},
doi = {10.1145/3576050.3576083},
abstract = {The term Learner Profile has proliferated over the years, and more recently, with the increased advocacy around personalising learning experiences. Learner profiles are at the center of personalised learning, and the characterisation of diversity in classrooms is made possible by profiling learners based on their strengths and weaknesses, backgrounds and other factors influencing learning. In this paper, we discuss three common approaches of profiling learners based on students’ cognitive knowledge, skills and competencies and behavioral patterns, all latter commonly used within Learning Analytics (LA). Although each approach has its strengths and merits, there are also several disadvantages that have impeded adoption at scale. We propose that the broader adoption of learner profiles can benefit from careful combination of the methods and practices of three primary approaches, allowing for scalable implementation of learner profiles across educational systems. In this regard, LA can leverage from other aligned domains to develop valid and rigorous measures of students' learning and propel learner profiles from education research to more mainstream educational practice. LA could provide the scope for monitoring and reporting beyond an individualised context and allow holistic evaluations of progress. There is promise in LA research to leverage the growing momentum surrounding learner profiles and make a substantial impact on the field's core aim - understanding and optimising learning as it occurs.&nbsp;},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {606–612},
numpages = {7},
keywords = {Competency-based profiling, Learner profile, Learning analytics-based profiling, Open learner models, Personalised learning},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576145,
author = {Li, Xinyu and Yan, Lixiang and Zhao, Linxuan and Martinez-Maldonado, Roberto and Gasevic, Dragan},
title = {CVPE: A Computer Vision Approach for Scalable and Privacy-Preserving Socio-spatial, Multimodal Learning Analytics},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576145},
doi = {10.1145/3576050.3576145},
abstract = {Capturing data on socio-spatial behaviours is essential in obtaining meaningful educational insights into collaborative learning and teamwork in co-located learning contexts. Existing solutions, however, have limitations regarding scalability and practicality since they rely largely on costly location tracking systems, are labour-intensive, or are unsuitable for complex learning environments. To address these limitations, we propose an innovative computer-vision-based approach – Computer Vision for Position Estimation (CVPE) – for collecting socio-spatial data in complex learning settings where sophisticated collaborations occur. CVPE is scalable and practical with a fast processing time and only needs low-cost hardware (e.g., cameras and computers). The built-in privacy protection modules also minimise potential privacy and data security issues by masking individuals’ facial identities and provide options to automatically delete recordings after processing, making CVPE a suitable option for generating continuous multimodal/classroom analytics. The potential of CVPE was evaluated by applying it to analyse video data about teamwork in simulation-based learning. The results showed that CVPE extracted socio-spatial behaviours relatively reliably from video recordings compared to indoor positioning data. These socio-spatial behaviours extracted with CVPE uncovered valuable insights into teamwork when analysed with epistemic network analysis. The limitations of CVPE for effective use in learning analytics are also discussed.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {175–185},
numpages = {11},
keywords = {collaborative learning, computer vision, epistemic network, learning analytics, multimodal},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3636555.3636852,
author = {Barthakur, Abhinava and Jovanovic, Jelena and Zamecnik, Andrew and Kovanovic, Vitomir and Xu, Gongjun and Dawson, Shane},
title = {Towards Comprehensive Monitoring of Graduate Attribute Development: A Learning Analytics Approach in Higher Education},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636852},
doi = {10.1145/3636555.3636852},
abstract = {In response to the evolving demands of the contemporary workplace, higher education (HE) institutions are increasingly emphasising the development of transversal skills and graduate attributes (GAs). The development of GAs, such as effective communication, collaboration, and lifelong learning, are non-linear and follow distinct trajectories for individual learners. The ability to trace and measure the progression of GA remains a significant challenge. While previous studies have focused on empirical methods for measuring GAs in individual courses, a notable gap exists in understanding their longitudinal development within HE programs. To address this research gap, our study focuses on measuring and tracing the development of GAs in an Initial Teacher Education (ITE) undergraduate program at a large public university in Australia. By combining learning analytics (LA) with psychometric models, we analysed students’ assessment grades to measure learners’ GA development in each year of the ITE program. The resulting measurements enabled the identification of distinct profiles of GA attainment, as demonstrated by learners and their distinct pathways. The overall approach allows for a comprehensive representation of a learner's progress throughout the program of study. As such, the developed approach sets the grounds for more personalised learning support, program evaluation, and improvement of students’ GA attainment.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {78–89},
numpages = {12},
keywords = {Higher education, graduate attributes, learner profile, monitoring progression},
location = {Kyoto, Japan},
series = {LAK '24}
}

@proceedings{10.1145/3576050,
title = {LAK2023: LAK23: 13th International Learning Analytics and Knowledge Conference},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Arlington, TX, USA}
}

@inproceedings{10.1145/3576050.3576070,
author = {Farrow, Elaine and Moore, Johanna D. and Gasevic, Dragan},
title = {Names, Nicknames, and Spelling Errors: Protecting Participant Identity in Learning Analytics of Online Discussions},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576070},
doi = {10.1145/3576050.3576070},
abstract = {Messages exchanged between participants in online discussion forums often contain personal names and other details that need to be redacted before the data is used for research purposes in learning analytics. However, removing the names entirely makes it harder to track the exchange of ideas between individuals within a message thread and across threads, and thereby reduces the value of this type of conversational data. In contrast, the consistent use of pseudonyms allows contributions from individuals to be tracked across messages, while also hiding the real identities of the contributors. Several factors can make it difficult to identify all instances of personal names that refer to the same individual, including spelling errors and the use of shortened forms. We developed a semi-automated approach for replacing personal names with consistent pseudonyms. We evaluated our approach on a data set of over 1,700 messages exchanged during a distance-learning course, and compared it to a general-purpose pseudonymisation tool that used deep neural networks to identify names to be redacted. We found that our tailored approach out-performed the general-purpose tool in both precision and recall, correctly identifying all but 31 substitutions out of 2,888.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {145–155},
numpages = {11},
keywords = {anonymisation, de-identification, ethical issues, learning analytics, personal name, privacy, pseudonymisation, redaction},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3502434.3502463,
author = {Abdul Jalil, Nasir and Wong Ei Leen, Mikkay},
title = {Learning Analytics in Higher Education: The Student Expectations of Learning Analytics},
year = {2022},
isbn = {9781450385749},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3502434.3502463},
doi = {10.1145/3502434.3502463},
abstract = {The report's purpose is to discover whether individual differences are connected with student expectations for learning analytics. The aim was to evaluate if the Big Five personality traits are connected to student predictions about learning analytics. Learning analytics is well-positioned to greatly improve student learning. There are many unanswered questions that are intertwined in that belief, which deals with learning analytics as a whole. Students' learning processes can be better supported and understood by using learning analytics. The goal of this investigation is to examine students' ideas about learning analytics tools' attributes and their interest in applying these features for education. In a first-stage investigation, researchers interviewed 364 university students. Smart PLS was used to carry out data analysis. Research that involves exploratory research will benefit from using partial least squares (PLS). The equal engagement of all stakeholders, from external organisations to individual students, is a challenge faced by each university given the global interest in learning analytics. Student expectations of learning analytics elements were discovered, and students are looking for support with planning and organising their learning processes, tools for self-assessment, delivery of adaptive recommendations, and personal evaluations of their learning.},
booktitle = {Proceedings of the 2021 5th International Conference on Education and E-Learning},
pages = {249–254},
numpages = {6},
keywords = {Educational Technology, Higher Education, Learning analytics},
location = {Virtual Event, Japan},
series = {ICEEL '21}
}

@inproceedings{10.1145/3636555.3636894,
author = {Zhou, Qi and Suraworachet, Wannapon and Cukurova, Mutlu},
title = {Harnessing Transparent Learning Analytics for Individualized Support through Auto-detection of Engagement in Face-to-Face Collaborative Learning},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636894},
doi = {10.1145/3636555.3636894},
abstract = {Using learning analytics to investigate and support collaborative learning has been explored for many years. Recently, automated approaches with various artificial intelligence approaches have provided promising results for modelling and predicting student engagement and performance in collaborative learning tasks. However, due to the lack of transparency and interpretability caused by the use of “black box” approaches in learning analytics design and implementation, guidance for teaching and learning practice may become a challenge. On the one hand, the black box created by machine learning algorithms and models prevents users from obtaining educationally meaningful learning and teaching suggestions. On the other hand, focusing on group and cohort level analysis only can make it difficult to provide specific support for individual students working in collaborative groups. This paper proposes a transparent approach to automatically detect student's individual engagement in the process of collaboration. The results show that the proposed approach can reflect student's individual engagement and can be used as an indicator to distinguish students with different collaborative learning challenges (cognitive, behavioural and emotional) and learning outcomes. The potential of the proposed collaboration analytics approach for scaffolding collaborative learning practice in face-to-face contexts is discussed and future research suggestions are provided.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {392–403},
numpages = {12},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3628096.3629067,
author = {Chomunorwa, Silence and Berg, Carolien van den and Janjties, Mmaki},
title = {Critical success factors for human-centred learning analytics systems design},
year = {2024},
isbn = {9798400708879},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3628096.3629067},
doi = {10.1145/3628096.3629067},
abstract = {The benefits of learning analytics have not yet been fully harnessed in developing countries. This is due to low adoption levels, which are caused by various factors, including learning analytics systems design inherent challenges. This study investigated factors considered by stakeholders as critical for success in designing a human-centered learning analytics system that leverages user-centric features, privacy considerations, actionable insights, and collaborative engagement. A case study of a South African university was used in an exploratory study. Data was collected using semi-structured interviews and thematically analysed. Findings reveal that education stakeholders consider co-designing, needs assessment, ethical considerations, usability, and features that include actionable insights as key factors that should be considered when designing a human-centred learning analytics system.},
booktitle = {Proceedings of the 4th African Human Computer Interaction Conference},
pages = {271–273},
numpages = {3},
keywords = {Data-driven decision-making, Educational data, Human-centred design, Insights, Learning analytics},
location = {East London, South Africa},
series = {AfriCHI '23}
}

@inproceedings{10.1145/3576050.3576085,
author = {Bond, Melissa and Viberg, Olga and Bergdahl, Nina},
title = {The current state of using learning analytics to measure and support K-12 student engagement: A scoping review},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576085},
doi = {10.1145/3576050.3576085},
abstract = {Student engagement has been identified as a critical construct for understanding and predicting educational success. However, research has shown that it can be hard to align data-driven insights of engagement with observed and self-reported levels of engagement. Given the emergence and increasing application of learning analytics (LA) within K-12 education, further research is needed to understand how engagement is being conceptualized and measured within LA research. This scoping review identifies and synthesizes literature published between 2011-2022, focused on LA and student engagement in K-12 contexts, and indexed in five international databases. 27 articles and conference papers from 13 different countries were included for review. We found that most of the research was undertaken in middle school years within STEM subjects. The results show that there is a wide discrepancy in researchers’ understanding and operationalization of engagement and little evidence to suggest that LA improves learning outcomes and support. However, the potential to do so remains strong. Guidance is provided for future LA engagement research to better align with these goals.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {240–249},
numpages = {10},
keywords = {K-12, Learning Analytics, Scoping Review, Student Engagement},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3506860.3506910,
author = {Sarmiento, Juan Pablo and Wise, Alyssa Friend},
title = {Participatory and Co-Design of Learning Analytics: An Initial Review of the Literature},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506910},
doi = {10.1145/3506860.3506910},
abstract = {Participatory Design (PD), and Co-design (Co-D), can be effective ways to improve technological innovation and to incorporate users’ needs in the development of learning analytics (LA). However, these methods can be difficult to implement and there has yet to be a synopsis of how its and techniques have been applied to the specific needs of LA. This study reviewed 90 papers that described 52 cases of PD of LA between 2010 and 2020 to address the research question “How is participatory design (PD) being used within LA?”. It focuses on examining which groups of participants are normally included in PD for LA, in what phases of the design process it is used, and what specific tools and techniques have LA designers adapted or developed to co-create with design partners. Findings show that there is a growing number of researchers using these methods in recent years, particularly in higher education and with instructor stakeholders. However, it was also found that often the literature would describe the PD activities only superficially, and that some aspects of PD, such as recruitment, were seldom considered overtly in the descriptions of these processes.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {535–541},
numpages = {7},
keywords = {Co-Design, Learning Analytics, Literature Review, Participatory Design},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3617570.3617867,
author = {Pagano, Alessandro and Angelelli, Mario and Calvano, Miriana and Curci, Antonio and Piccinno, Antonio},
title = {Quantum Computing for Learning Analytics: An Overview of Challenges and Integration Strategies},
year = {2023},
isbn = {9798400703768},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3617570.3617867},
doi = {10.1145/3617570.3617867},
abstract = {Quantum computing has emerged as a promising technology with the potential to revolutionize various fields, including learning analytics. This research paper explores the applications of quantum computing in learning analytics and discusses the suitability of quantum techniques for addressing the challenges posed by large-scale educational datasets. It also investigates the integration of quantum computing with existing learning analytics pipelines, highlighting compatibility issues, data representation and transformation challenges, algorithmic complexity, and evaluation considerations. By understanding the potential benefits, limitations, and integration strategies, researchers can pave the way for the development of innovative tools and approaches to analyze educational data and provide personalized learning experiences.},
booktitle = {Proceedings of the 2nd International Workshop on Quantum Programming for Software Engineering},
pages = {13–16},
numpages = {4},
keywords = {Learning analytics, educational, quantum computing},
location = {San Francisco, CA, USA},
series = {QP4SE 2023}
}

@inproceedings{10.1145/3576050.3576110,
author = {Lang, Charles and Davis, Laura},
title = {Learning Analytics and Stakeholder Inclusion: What do We Mean When We Say "Human-Centered"?},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576110},
doi = {10.1145/3576050.3576110},
abstract = {Given the growth in interest in human-centeredness within the learning analytics community - a workshop at LAK, a special issue in the Journal of Learning Analytics and multiple papers published on the topic - it seems an appropriate time to critically evaluate the popular design approach. Using a corpus of 165 publications that have substantial reference to both learning analytics and human-centeredness, the following paper delineates what is meant by "human-centered" and then discusses what the implications are for this approach. The conclusion reached through this analysis is that when authors refer to human-centeredness in learning analytics they are largely referring to stakeholder inclusion and the means by which this can be achieved (methodologically, politically and logistically). Furthermore, the justification for stakeholder inclusion is often coached in terms of its ability to develop more effective learning analytics applications along several dimensions (efficiency, efficacy, impact). With reference to human-centered design in other fields a discussion follows of the issues with such an approach and a prediction that LA will likely move toward a more neutral stance on stakeholder inclusion, as has occurred in both human-centered design and stakeholder engagement research in the past. A more stakeholder-neutral stance is defined as one in which stakeholder inclusion is one of many tools utilized in developing learning analytics applications.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {411–417},
numpages = {7},
keywords = {Human-centered design, co-design, participatory, user-centered design},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3629296.3629352,
author = {Lusena-Ezera, Inese and Silina-Jasjukevica, Gunta and Liduma, Diana and Kaulens, Oskars},
title = {Do Learning Analytics Promote Schools as Learning Organisations? A Case Study of Latvia: Learning analytics for promoting school as a learning organization},
year = {2024},
isbn = {9798400709111},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3629296.3629352},
doi = {10.1145/3629296.3629352},
abstract = {The aim of this study is to evaluate the current practice of learning analytics in the implementation of the school as a learning organisation (henceforth SLO) in general and vocational education in Latvia. The study identifies what the most common purpose of data analysis in schools is, what data are most often used in schools to analyze students' learning performance and explores the relationship between the learning analytics and the SLO using the eight-dimensional SLO model developed within the framework of the research "A model and tool to support the implementation of the approach school as a learning organisation in educational institutions". The results of the study show that schools mainly analyze data to improve students' learning outcomes and less attention is paid to data analysis to adjust the curriculum to students' interests, and while students' summative assessments (e.g. grades) are regularly used to analyze students' learning performance, student focus group discussions on the learning process at school, the learning experience gained at school and improvements to be made are used less frequently. A positive and significant relationship was found between the learning analytics and the SLO, moreover, it was discovered that the closest relationship was observed particularly between the SLO and the more frequent use of student focus group discussions for the analysis of students' learning performance, and data analysis for adapting the learning content to students' interests as well as for improving students’ collaboration.},
booktitle = {Proceedings of the 15th International Conference on Education Technology and Computers},
pages = {341–347},
numpages = {7},
keywords = {data-driven decision making, general and vocational education, learning analytics, school as a learning organization},
location = {Barcelona, Spain},
series = {ICETC '23}
}

@inproceedings{10.1145/3678957.3688613,
author = {Joshi, Vasundhara},
title = {Enhancing Collaboration and Performance among EMS Students through Multimodal Learning Analytics},
year = {2024},
isbn = {9798400704628},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3678957.3688613},
doi = {10.1145/3678957.3688613},
abstract = {Physiological synchrony plays an important role in measuring collaboration and performance within teams. However, there has been little investigation into awareness of physiological synchrony and its impact on the collaboration and performance. In my dissertation, I am proposing a study to investigate the impact of awareness of near real-time physiological synchrony, through multimodal learning analytic dashboard, on Emergency Medical Services (EMS) students’ perceived collaboration and performance. Also, I plan to investigate the best practices for presenting multimodal data to EMS trainees in collaborative learning environment. The research aims to enhance students’ engagement and reflection on their collaborative interactions and performance.},
booktitle = {Proceedings of the 26th International Conference on Multimodal Interaction},
pages = {607–611},
numpages = {5},
keywords = {Multimodal data visualization, collaboration, electrodermal activity, learning analytics dashboards, physiological synchrony},
location = {San Jose, Costa Rica},
series = {ICMI '24}
}

@inproceedings{10.1145/3576050.3576143,
author = {Chejara, Pankaj and Prieto, Luis P. and Rodriguez-Triana, Maria Jesus and Ruiz-Calleja, Adolfo and Khalil, Mohammad},
title = {Impact of window size on the generalizability of collaboration quality estimation models developed using Multimodal Learning Analytics},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576143},
doi = {10.1145/3576050.3576143},
abstract = {Multimodal Learning Analytics (MMLA) has been applied to collaborative learning, often to estimate collaboration quality with the use of multimodal data, which often have uneven time scales. The difference in time scales is usually handled by dividing and aggregating data using a fixed-size time window. So far, the current MMLA research lacks a systematic exploration of whether and how much window size affects the generalizability of collaboration quality estimation models. In this paper, we investigate the impact of different window sizes (e.g., 30 seconds, 60s, 90s, 120s, 180s, 240s) on the generalizability of classification models for collaboration quality and its underlying dimensions (e.g., argumentation). Our results from an MMLA study involving the use of audio and log data showed that a 60 seconds window size enabled the development of more generalizable models for collaboration quality (AUC 61%) and argumentation (AUC 64%). In contrast, for modeling dimensions focusing on coordination, interpersonal relationship, and joint information processing, a window size of 180 seconds led to better performance in terms of across-context generalizability (on average from 56% AUC to 63% AUC). These findings have implications for the eventual application of MMLA in authentic practice.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {559–565},
numpages = {7},
keywords = {Collaboration Quality, Generalizability, Machine Learning, MultiModal Learning Analytics, Temporal Window},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3678392.3678401,
author = {Nguyen, Viet Anh},
title = {Applying Learning Analytics to Predict the Student's Learning Outcome Based on Online Learning Activities},
year = {2024},
isbn = {9798400717123},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3678392.3678401},
doi = {10.1145/3678392.3678401},
abstract = {Applying machine learning techniques to predict learning outcomes through learning analytics is a research direction of current interest, especially in online courses. In this paper, we present the results from experimenting with machine learning techniques when analyzing interaction data from 209 students in online learning participating in blended classes from September 2022 to December 2023. The research results indicate that Random Forest and Linear Regression classification are practical algorithms for predicting learning outcomes. The results also demonstrate that learner interaction data is reliable information for predicting outcomes to provide early warnings for learners. Furthermore, the study also proposes a process for predicting learning outcomes through online learning activities.},
booktitle = {Proceedings of the 2024 10th International Conference on Frontiers of Educational Technologies},
pages = {140–146},
numpages = {7},
keywords = {e-learning, learning analytics, learning outcome, machine learning},
location = {Malacca, Malaysia},
series = {ICFET '24}
}

@inproceedings{10.1145/3613905.3651111,
author = {Milesi, Mikaela E and Alfredo, Riordan and Echeverria, Vanessa and Yan, Lixiang and Zhao, Linxuan and Tsai, Yi-Shan and Martinez-Maldonado, Roberto},
title = {"It's Really Enjoyable to See Me Solve the Problem like a Hero": GenAI-enhanced Data Comics as a Learning Analytics Tool},
year = {2024},
isbn = {9798400703317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613905.3651111},
doi = {10.1145/3613905.3651111},
abstract = {Data comics are an emergent storytelling format that can enable non-experts to consume salient insights from data. Despite some research investigating the use of comic strips in education, there is limited evidence relating to how students perceive data comics about their own data as a way to reflect on their learning experience. In this paper, we summarise nursing students’ perceptions of the advantages and limitations of data comics by presenting personalised Multimodal Learning Analytics (LA) data in data comics form. We present GenAI-enhanced data comic prototypes created using a combination of the generative artificial intelligence tool, Midjourney, and graphics illustration software. Our findings indicate that while students see the potential of data comics as an engaging and enjoyable visualisation technique, concerns remain regarding the perceived lack of professionalism associated with this format.},
booktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {4},
numpages = {7},
keywords = {data comics, information visualisation, learning analytics},
location = {Honolulu, HI, USA},
series = {CHI EA '24}
}

@inproceedings{10.1145/3506860.3506886,
author = {Pontual Falc\~{a}o, Taciana and Lins Rodrigues, Rodrigo and Cechinel, Cristian and Dermeval, Diego and Harada Teixeira de Oliveira, Elaine and Gasparini, Isabela and D. Ara\'{u}jo, Rafael and Primo, Tiago and Gasevic, Dragan and Ferreira Mello, Rafael},
title = {A Penny for your Thoughts: Students and Instructors’ Expectations about Learning Analytics in Brazil},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506886},
doi = {10.1145/3506860.3506886},
abstract = {Stakeholder engagement is a key aspect for the successful implementation of Learning Analytics (LA) in Higher Education Institutions (HEIs). Studies in Europe and Latin America (LATAM) indicate that, overall, instructors and students have positive views on LA adoption, but there are differences between their ideal expectations and what they consider realistic in the context of their institutions. So far, very little has been found about stakeholders’ views on LA in Brazilian higher education. By replicating the survey conducted in other countries, in seven Brazilian HEIs, we found convergences both with Europe and LATAM, reinforcing the need for local diagnosis and indicating the risk of assuming a ”LATAM identity”. Our findings contribute to building a corpus of knowledge on stakeholders expectations with a contextualised comprehension of the gaps between ideal and predicted scenarios, which can inform institutional policies for LA implementation in Brazil.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {186–196},
numpages = {11},
keywords = {Learning analytics, higher education, institutional adoption, stakeholder perspective},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3375462.3375504,
author = {Verbert, Katrien and Ochoa, Xavier and De Croon, Robin and Dourado, Raphael A. and De Laet, Tinne},
title = {Learning analytics dashboards: the past, the present and the future},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375504},
doi = {10.1145/3375462.3375504},
abstract = {Learning analytics dashboards are at the core of the LAK vision to involve the human into the decision-making process. The key focus of these dashboards is to support better human sense-making and decision-making by visualising data about learners to a variety of stakeholders. Early research on learning analytics dashboards focused on the use of visualisation and prediction techniques and demonstrates the rich potential of dashboards in a variety of learning settings. Present research increasingly uses participatory design methods to tailor dashboards to the needs of stakeholders, employs multimodal data acquisition techniques, and starts to research theoretical underpinnings of dashboards. In this paper, we present these past and present research efforts as well as the results of the VISLA19 workshop on "Visual approaches to Learning Analytics" that was held at LAK19 with experts in the domain to identify and articulate common practices and challenges for the domain. Based on an analysis of the results, we present a research agenda to help shape the future of learning analytics dashboards.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {35–40},
numpages = {6},
keywords = {evaluation, interaction, learning analytics dashboards, visualisation},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375482,
author = {Ruip\'{e}rez-Valiente, Jos\'{e} A. and Jenner, Matt and Staubitz, Thomas and Li, Xitong and Rohloff, Tobias and Halawa, Sherif and Turro, Carlos and Cheng, Yuan and Zhang, Jiayin and Despujol, Ignacio and Reich, Justin},
title = {Macro MOOC learning analytics: exploring trends across global and regional providers},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375482},
doi = {10.1145/3375462.3375482},
abstract = {Massive Open Online Courses (MOOCs) have opened new educational possibilities for learners around the world. Most of the research and spotlight has been concentrated on a handful of global, English-language providers, but there are a growing number of regional providers of MOOCS in languages other than English. In this work, we have partnered with thirteen MOOC providers from around the world. We apply a multi-platform approach generating a joint and comparable analysis with data from millions of learners. This allows us to examine learning analytics trends at a macro level across various MOOC providers, with a goal of understanding which MOOC trends are globally universal and which of them are context-dependent. The analysis reports preliminary results on the differences and similarities of trends based on the country of origin, level of education, gender and age of their learners across global and regional MOOC providers. This study exemplifies the potential of macro learning analytics in MOOCs to understand the ecosystem and inform the whole community, while calling for more large scale studies in learning analytics through partnerships among researchers and institutions.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {518–523},
numpages = {6},
keywords = {MOOCs, cultural factors, large-scale analytics, learning analytics, multi-plaform analytics collaboration},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3506860.3506865,
author = {Ma, Yingbo and Celepkolu, Mehmet and Boyer, Kristy Elizabeth},
title = {Detecting Impasse During Collaborative Problem Solving with Multimodal Learning Analytics},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506865},
doi = {10.1145/3506860.3506865},
abstract = {Collaborative problem solving has numerous benefits for learners, such as improving higher-level reasoning and developing critical thinking. While learners engage in collaborative activities, they often experience impasse, a potentially brief encounter with differing opinions or insufficient ideas to progress. Impasses provide valuable opportunities for learners to critically discuss the problem and re-evaluate their existing knowledge. Yet, despite the increasing research efforts on developing multimodal modeling techniques to analyze collaborative problem solving, there is limited research on detecting impasse in collaboration. This paper investigates multimodal detection of impasse by analyzing 46 middle school learners’ collaborative dialogue—including speech and facial behaviors—during a coding task. We found that the semantics and speaker information in the linguistic modality, the pitch variation in the audio modality, and the facial muscle movements in the video modality are the most significant unimodal indicators of impasse. We also trained several multimodal models and found that combining indicators from these three modalities provided the best impasse detection performance. To the best of our knowledge, this work is the first to explore multimodal modeling of impasse during the collaborative problem solving process. This line of research contributes to the development of real-time adaptive support for collaboration.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {45–55},
numpages = {11},
keywords = {Collaborative Problem Solving, Impasse Detection, Multimodal Learning Analytics, Pair Programming},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3636555.3636854,
author = {Que, Ying and Ng, Jeremy Tzi Dong and Hu, Xiao and Mak, Mitchell Kam Fai and Yip, Peony Tsz Yan},
title = {Using Multimodal Learning Analytics to Examine Learners’ Responses to Different Types of Background Music during Reading Comprehension},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636854},
doi = {10.1145/3636555.3636854},
abstract = {Previous studies have evaluated the affordances and challenges of performing cognitively demanding learning tasks with background music (BGM), yet the effects of various types of BGM on learning still remain an open question. This study aimed to examine the impacts of different music genres and fine-grained music characteristics on learners’ emotional, physiological, and pupillary responses during reading comprehension. Leveraging multimodal learning analytics (MmLA) methods of collecting data in multiple modalities from learners, a user experiment was conducted on 102 participants, with half of them reading with self-selected BGM (i.e., the experimental group), while the other half reading without BGM (i.e., the control group). Results of statistical analyses and interviews revealed significant differences between the two groups in their self-reported emotions and automatically measured physiological responses when the experimental group was exposed to classical, easy-listening, rebellious and rhythmic music. Fine-grained music characteristics (e.g., instrumentation, tempo) could predict learners’ emotions, pupillary, and physiological responses during reading comprehension. The expected contributions of this study include: 1) providing empirical evidence for understanding affective dimensions of learning with BGM, 2) applying MmLA methods for examining the impacts of BGM on learning, and 3) yielding practical implications on how to improve learning with BGM.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {749–756},
numpages = {8},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3506860.3506914,
author = {Khalil, Mohammad and Wong, Jacqueline and Er, Erkan and Heitmann, Martin and Belokrys, Gleb},
title = {Tweetology of Learning Analytics: What does Twitter tell us about the trends and development of the field?},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506914},
doi = {10.1145/3506860.3506914},
abstract = {Twitter is a very popular microblogging platform that has been actively used by scientific communities to exchange scientific information and to promote scholarly discussions. The present study aimed to leverage the tweet data to provide valuable insights into the development of the learning analytics field since its initial days. Descriptive analysis, geocoding analysis, and topic modeling were performed on over 1.6 million tweets related to learning analytics posted between 2010-2021. The descriptive analysis reveals an increasing popularity of the field on the Twittersphere in terms of number of users, twitter posts, and hashtags emergence. The topic modeling analysis uncovers new insights of the major topics in the field of learning analytics. Emergent themes in the field were identified, and the increasing (e.g., Artificial Intelligence) and decreasing&nbsp;(e.g., Education) trends were shared. Finally, the geocoding analysis indicates an increasing participation in the field from more diverse countries all around the world. Further findings are discussed in the paper.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {347–357},
numpages = {11},
keywords = {Twitter, Twitter analysis, geospatial analysis, learning analytics, topic modeling},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3711403.3711432,
author = {Xin, Hua and Yang, Fan},
title = {Research on Visual Monitoring and Feedback of Online Learning for College Students from the Perspective of Learning Analytics},
year = {2025},
isbn = {9798400717468},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711403.3711432},
doi = {10.1145/3711403.3711432},
abstract = {Big data and learning analytics technologies use log data recorded by learning systems, track learning tasks, and provide visual reports to help teachers improve their practices, encourage students to learn and improve their academic performance. This study, grounded in learning analytics technology, delves into the influence of visual monitoring and feedback mechanisms on students' online learning performance and their capacity for self-regulated learning within digital environments. Ultimately, it seeks to offer insightful guidance for the evolution and application of learning analytics and visualization technologies.},
booktitle = {Proceedings of the 2024 7th International Conference on Educational Technology Management},
pages = {44–48},
numpages = {5},
keywords = {Learning analytics, Learning behavior, Online self-regulated learning, Visual monitoring and feedback},
location = {
},
series = {ICETM '24}
}

@inproceedings{10.1145/3657604.3664715,
author = {Li, Qiujie and Zhou, Xuehan and Xu, Di and Baker, Rachel B. and Holton, Amanda J.},
title = {Varying Impacts: The Role of Student Self-Evaluation in Navigating Learning Analytics},
year = {2024},
isbn = {9798400706332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3657604.3664715},
doi = {10.1145/3657604.3664715},
abstract = {The enthusiasm for student-facing analytics as tools for supporting student self-regulation is overshadowed by uncertainties about their actual impact on student outcomes. This study aims to fill the gap in experimental evidence concerning student-facing analytics by implementing a randomized control trial. Specifically, we investigated the effects of data visualizations that display student level of content mastery in comparison to their peers, alongside recommendations for learning strategies. The preliminary results reveal that the intervention impacts student attribution and motivation in varying ways, based on their self-evaluation of their current course performance. Further analysis, including coding students' interpretation of the data visualization, will be conducted to uncover the diverse ways students might interpret the analytics.},
booktitle = {Proceedings of the Eleventh ACM Conference on Learning @ Scale},
pages = {535–538},
numpages = {4},
keywords = {attribution, higher education, learning analytics, online learning},
location = {Atlanta, GA, USA},
series = {L@S '24}
}

@inproceedings{10.1145/3506860.3506861,
author = {Nazaretsky, Tanya and Bar, Carmel and Walter, Michal and Alexandron, Giora},
title = {Empowering Teachers with AI: Co-Designing a Learning Analytics Tool for Personalized Instruction in the Science Classroom},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506861},
doi = {10.1145/3506860.3506861},
abstract = {AI-powered educational technology that is designed to support teachers in providing personalized instruction can enhance their ability to address the needs of individual students, hopefully leading to better learning gains. This paper presents results from a participatory research aimed at co-designing with science teachers a learning analytics tool that will assist them in implementing a personalized pedagogy in blended learning contexts. The development process included three stages. In the first, we interviewed a group of teachers to identify where and how personalized instruction may be integrated into their teaching practices. This yielded a clustering-based personalization strategy. Next, we designed a mock-up of a learning analytics tool that supports this strategy and worked with another group of teachers to define an ‘explainable learning analytics’ scheme that explains each cluster in a way that is both pedagogically meaningful and can be generated automatically. Third, we developed an AI algorithm that supports this ‘explainable clusters’ pedagogy and conducted a controlled experiment that evaluated its contribution to teachers’ ability to plan personalized learning sequences. The planned sequences were evaluated in a blinded fashion by an expert, and the results demonstrated that the experimental group – teachers who received the clusters with the explanations – designed sequences that addressed the difficulties exhibited by different groups of students better than those designed by teachers who received the clusters without explanations. The main contribution of this study is twofold. First, it presents an effective personalization approach that fits blended learning in the science classroom, which combines a real-time clustering algorithm with an explainable-AI scheme that can automatically build pedagogically meaningful explanations from item-level meta-data (Q Matrix). Second, it demonstrates how such an end-to-end learning analytics solution can be built with teachers through a co-design process and highlights the types of knowledge that teachers add to system-provided analytics in order to apply them to their local context. As a practical contribution, this process informed the design of a new learning analytics tool that was integrated into a free online learning platform that is being used by more than 1000 science teachers.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {1–12},
numpages = {12},
keywords = {Blended Learning, Learning Analytics, Participatory Design, Personalized Instruction, Teacher Dashboards},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3448139.3448162,
author = {Gray, Geraldine and Schalk, Ana and Rooney, Pauline and Lang, Charles},
title = {A Stakeholder Informed Professional Development Framework to Support Engagement with Learning Analytics},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448162},
doi = {10.1145/3448139.3448162},
abstract = {This paper reports on a study aimed at identifying training requirements for both staff and students in higher education to enable more widespread use of learning analytics. Opinions of staff and students were captured through ten focus groups (37 students; 40 staff) and two surveys (1,390 students; 160 staff). Participants were predominantly from two higher education institutions in Ireland. Analysis of the results informed a framework for continuous professional development in learning analytics focusing on aspects of using data, legal and ethical considerations, policy, and workload. The framework presented here differentiates between the training needs of students, academic staff and professional services staff.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {237–247},
numpages = {11},
keywords = {Learning analytics, continuous professional development, higher education, stakeholder perspectives},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3706598.3713353,
author = {Zhang, Gefei and Ji, Shenming and Li, Yicao and Tang, Jingwei and Ding, Jihong and Xia, Meng and Sun, Guodao and Liang, Ronghua},
title = {CPVis: Evidence-based Multimodal Learning Analytics for Evaluation in Collaborative Programming},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713353},
doi = {10.1145/3706598.3713353},
abstract = {As programming education becomes more widespread, many college students from non-computer science backgrounds begin learning programming. Collaborative programming emerges as an effective method for instructors to support novice students in developing coding and teamwork abilities. However, due to limited class time and attention, instructors face challenges in monitoring and evaluating the progress and performance of groups or individuals. To address this issue, we collect multimodal data from real-world settings and develop CPVis, an interactive visual analytics system designed to assess student collaboration dynamically. Specifically, CPVis enables instructors to evaluate both group and individual performance efficiently. CPVis employs a novel flower-based visual encoding to represent performance and provides time-based views to capture the evolution of collaborative behaviors. A within-subject experiment (N=22), comparing CPVis with two baseline systems, reveals that users gain more insights, find the visualization more intuitive, and report increased confidence in their assessments of collaboration.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {50},
numpages = {26},
keywords = {Group visualization, education visualization, collaborative programming},
location = {
},
series = {CHI '25}
}

@proceedings{10.1145/3506860,
title = {LAK22: LAK22: 12th International Learning Analytics and Knowledge Conference},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Online, USA}
}

@inproceedings{10.1145/3375462.3375463,
author = {Pel\'{a}nek, Radek},
title = {Learning analytics challenges: trade-offs, methodology, scalability},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375463},
doi = {10.1145/3375462.3375463},
abstract = {Ryan Baker presented in a LAK 2019 keynote a list of six grand challenges for learning analytics research. The challenges are specified as problems with clearly defined success criteria. Education is, however, a domain full of ill-defined problems. I argue that learning analytics research should reflect this nature of the education domain and focus on less clearly defined, but practically essential issues. As an illustration, I discuss three important challenges of this type: addressing inherent trade-offs in learning environments, the clarification of methodological issues, and the scalability of system development.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {554–558},
numpages = {5},
keywords = {methodology, scalability, trade-offs},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@article{10.1145/3622784,
author = {Martinez-Maldonado, Roberto and Echeverria, Vanessa and Fernandez-Nieto, Gloria and Yan, Lixiang and Zhao, Linxuan and Alfredo, Riordan and Li, Xinyu and Dix, Samantha and Jaggard, Hollie and Wotherspoon, Rosie and Osborne, Abra and Shum, Simon Buckingham and Ga\v{s}evi\'{c}, Dragan},
title = {Lessons Learnt from a Multimodal Learning Analytics Deployment In-the-Wild},
year = {2023},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {1},
issn = {1073-0516},
url = {https://doi.org/10.1145/3622784},
doi = {10.1145/3622784},
abstract = {Multimodal Learning Analytics (MMLA) innovations make use of rapidly evolving sensing and artificial intelligence algorithms to collect rich data about learning activities that unfold in physical spaces. The analysis of these data is opening exciting new avenues for both studying and supporting learning. Yet, practical and logistical challenges commonly appear while deploying MMLA innovations “in-the-wild”. These can span from technical issues related to enhancing the learning space with sensing capabilities, to the increased complexity of teachers’ tasks. These practicalities have been rarely investigated. This article addresses this gap by presenting a set of lessons learnt from a 2-year human-centred MMLA in-the-wild study conducted with 399 students and 17 educators in the context of nursing education. The lessons learnt were synthesised into topics related to (i) technological/physical aspects of the deployment; (ii) multimodal data and interfaces; (iii) the design process; (iv) participation, ethics and privacy; and (v) sustainability of the deployment.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = nov,
articleno = {8},
numpages = {41},
keywords = {Learning analytics, sensors, ubiquitous computing, human-centred design, CSCW}
}

@inproceedings{10.1145/3636555.3636932,
author = {Wang, Chao and Hu, Xiao and Hern\'{a}ndez L\'{o}pez, Nora Patricia and Ng, Jeremy Tzi Dong},
title = {Needs Analysis of Learning Analytics Dashboard for College Teacher Online Professional Learning in an International Training Initiative for the Global South},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636932},
doi = {10.1145/3636555.3636932},
abstract = {Online courses enable wide access to educational resources and thus provide a feasible platform for cross-regional teacher professional learning. Learning analytics dashboards (LAD) can support online learners by providing fine-grained feedback generated from learners’ interactions with platforms. Nevertheless, most studies on teacher online professional learning focus on resource-rich and technology-advanced regions, with scarce attention to the Global South. Furthermore, existing studies on LAD design mainly target students’ learning, rather than teachers’ professional learning. Therefore, it is much needed to develop LAD for teacher-learners online professional learning in the Global South. Contextualized in an international online professional training initiative, this study conducted in-depth interviews with 42 teacher-learners from 19 countries in the Global South, aiming to identify their needs for 1) support on their self-regulated learning (SRL), and 2) potential LA components in dashboards. Findings indicated that teacher-learners needed support for self-regulated learning strategies, including motivation maintenance, time management, environment structuring, help-seeking, and self-evaluation. Nine LA features were identified to design the LADs to support SRL preliminarily. This co-designed LAD study with interviewees improved our understanding on the needs of college teachers in the Global South for LA support during their online professional learning, generating practical insights into needs-driven LAD designs.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {915–921},
numpages = {7},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636855,
author = {Zhao, Linxuan and Echeverria, Vanessa and Swiecki, Zachari and Yan, Lixiang and Alfredo, Riordan and Li, Xinyu and Gasevic, Dragan and Martinez-Maldonado, Roberto},
title = {Epistemic Network Analysis for End-users: Closing the Loop in the Context of Multimodal Analytics for Collaborative Team Learning},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636855},
doi = {10.1145/3636555.3636855},
abstract = {Effective collaboration and team communication are critical across many sectors. However, the complex dynamics of collaboration in physical learning spaces, with overlapping dialogue segments and varying participant interactions, pose assessment challenges for educators and self-reflection difficulties for students. Epistemic network analysis (ENA) is a relatively novel technique that has been used in learning analytics (LA) to unpack salient aspects of group communication. Yet, most LA works based on ENA have primarily sought to advance research knowledge rather than directly aid teachers and students by closing the LA loop. We address this gap by conducting a study in which we i) engaged teachers in designing human-centred versions of epistemic networks; ii) formulated an NLP methodology to code physically distributed dialogue segments of students based on multimodal (audio and positioning) data, enabling automatic generation of epistemic networks; and iii) deployed the automatically generated epistemic networks in 28 authentic learning sessions and investigated how they can support teaching. The results indicate the viability of completing the analytics loop through the design of streamlined epistemic network representations that enable teachers to support students’ reflections.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {90–100},
numpages = {11},
keywords = {Collaborative learning, Human-centred, Learning Analytics, Multimodality, Teamwork},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3711403.3711409,
author = {Long, Zhengli and Zhao, Yang and Yang, Liujing},
title = {Study of Teaching Interaction Behaviors in Smart Classrooms from a Perspective of Learning Analytics},
year = {2025},
isbn = {9798400717468},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711403.3711409},
doi = {10.1145/3711403.3711409},
abstract = {With the acceleration of the informatization process, the field of education is experiencing unprecedented changes, and smart education, as a product of the deep integration of information technology and education, is increasingly becoming a key force in promoting the modernization of education and improving the quality of teaching. This study focuses on the in-depth analysis of teaching interactions in a smart classroom, using the coding system of teaching interaction behaviors in a smart classroom and the lag sequence analysis method to analyze a benchmark course in terms of the emotional atmosphere of classroom interactions, the structure of classroom teaching interactions, teacher-student interactions with technology, and the sequence of significant behaviors. The results show that the course in the case study highly integrates information technology, highlights the students' subjective position, promotes harmonious emotional communication between teachers and students, and uses technological innovation classroom activities to promote the interactive effect; it also points out the deficiencies in evaluation methods, data-driven teaching, students' thinking ability development, and technological mastery. Based on this, the study puts forward five suggestions: first, focusing on diversified evaluation methods; second, making full use of data-driven teaching and adjusting teaching in a timely manner; third, optimizing questioning strategies, focusing on innovative thinking training and cultivating in-depth thinking ability; fourth, strengthening teachers' technological mastery and application ability; and fifth, paying attention to the needs of students' physical and mental development.},
booktitle = {Proceedings of the 2024 7th International Conference on Educational Technology Management},
pages = {36–43},
numpages = {8},
keywords = {Instructional Interaction, Lag Sequence Analysis, Learning Analytics, Smart Classroom},
location = {
},
series = {ICETM '24}
}

@inproceedings{10.1145/3626253.3635423,
author = {Snider, Johan Mattias},
title = {Edit, Run, Error, Repeat: Learning Analytics to Find the Most Improved Programming Student},
year = {2024},
isbn = {9798400704246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626253.3635423},
doi = {10.1145/3626253.3635423},
abstract = {In the face of learning to program, students are often divided into two camps: those who excel and those who struggle. Through this challenge, some students manage to persevere from difficulty to ability. This study aims to identify the students who have made the most significant improvements in their programming skills, by leveraging various learning analytics and metrics. The field of learning analytics in programming has often sought to identify struggling students, utilizing metrics such as the error quotient (EQ) and time-on-task. This research distinguishes itself by taking a longitudinal approach to analyzing students' programming data collected over a nine-month period. By framing error quotient through operant conditioning and time-on-task through expectancy-value theory, we aim to root these learning analytics in established learning theory to validate their relevance. These preliminary results illustrate how we can identify students who have improved the most based on these metrics.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1888},
numpages = {1},
keywords = {computer science education, error quotient, learning analytics, time-on-task},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@inproceedings{10.1145/3696230.3696267,
author = {Li, Fanbo and Zhang, Hongfeng and Chen, Bowen and Zhou, Qingyi and Li, Xin},
title = {Visualization of Hotspots and Frontiers in Learning Analytics under Big Data Environment — Based on Citespace Knowledge Map Analysis},
year = {2024},
isbn = {9798400717574},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696230.3696267},
doi = {10.1145/3696230.3696267},
abstract = {In the era of big data, learning analytics has emerged as a core area at the intersection of educational science and data science. This study utilizes the CiteSpace knowledge map tool and the Web of Science database to conduct a comprehensive analysis of learning analytics research literature from 2012 to 2024, in order to explore the evolution trends and frontier topics of academic research. The results indicate that with the development of adaptive learning and multimodal data analysis techniques, the field of learning analytics has shifted from conceptual frameworks to specific implementation strategies, particularly making significant progress in enhancing educational adaptability and learning efficiency. Additionally, through the analysis of international collaboration networks, this study reveals the critical role of global academic collaborations in advancing research in this field. The analysis of keyword co-occurrence and subject co-occurrence further confirms "learning analytics" and "big data" as research focal points, and anticipates the future application of data-driven educational technologies in global educational reforms.},
booktitle = {Proceedings of the 2024 8th International Conference on Digital Technology in Education (ICDTE)},
pages = {221–231},
numpages = {11},
keywords = {Adaptive Learning, Big Data, CiteSpace, Learning Analytics},
location = {
},
series = {ICDTE '24}
}

@inproceedings{10.1145/3375462.3375507,
author = {Michos, Konstantinos and Lang, Charles and Hern\'{a}ndez-Leo, Davinia and Price-Dennis, Detra},
title = {Involving teachers in learning analytics design: lessons learned from two case studies},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375507},
doi = {10.1145/3375462.3375507},
abstract = {Involving teachers in the design of technology-enhanced learning environments is a useful method towards bridging the gap between research and practice. This is especially relevant for learning analytics tools, wherein the presentation of educational data to teachers or students requires meaningful sense-making to effectively support data-driven actions. In this paper, we present two case studies carried out in the context of two research projects in the USA and Spain which aimed to involve teachers in the co-design of learning analytics tools through professional development programs. The results of a cross-case analysis highlight lessons learned around challenges and principles regarding the meaningful involvement of teachers in learning analytics tooling design.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {94–99},
numpages = {6},
keywords = {case studies, co-design, learning analytics, teacher professional development, teachers},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3506860.3506974,
author = {Slibar, Barbara and Gusic Mundjar, Jelena and Rako, Sabina and Simic, Diana},
title = {Co-occurrence patterns of issues and guidelines related to ethics and privacy of learning analytics in higher education—literature review},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506974},
doi = {10.1145/3506860.3506974},
abstract = {Ethics and privacy issues have been recognized as important factors for acceptance and trustworthy implementation of learning analytics. A large number of different issues has been recognized in the literature. Guidelines related to these issues are continuously being developed and discussed in research literature. The aim of this research was to identify patterns of co-occurrence of issues and guidelines in research papers discussing ethics and privacy issues, to gain better understanding of relationships between different ethics and privacy issues arising during implementation of learning analytics in higher education. A total of 93 papers published between 2010 and 2021 were qualitatively analyzed, and nine categories of issues and respective guidelines related to ethics and privacy in learning analytics were identified. Association rules mining Apriori algorithm was applied, where 93 papers represented transactions, and 18 categories of issues or guidelines (nine each) represented items. Two clusters of issues co-occurring in papers were identified, corresponding to deontology ethics (related to rules and duties), and consequentialism ethics (related to consequences of unethical behavior).},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {577–582},
numpages = {6},
keywords = {Apriori algorithm, Ethics, Higher education, Learning analytics},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3529399.3529413,
author = {Alhakbani, Haya A. and Alnassar, Fatema M.},
title = {Open Learning Analytics: A Systematic Review of Benchmark Studies using Open University Learning Analytics Dataset (OULAD)},
year = {2022},
isbn = {9781450395748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3529399.3529413},
doi = {10.1145/3529399.3529413},
abstract = {Virtual learning has gained increased importance because of the recent pandemic situation. A mass shift to virtual means of education delivery has been observed over the past couple of years, forcing the community to develop efficient performance assessment tools. Open University Learning Analytics Dataset (OULAD) is one of the most comprehensive and benchmark datasets in the learning analytics domain. This paper presents the review of benchmark studies performed using OULAD to assess the performance of students in a Virtual Learning Environment (VLE). The presented review aims to highlight the status of technological advancements in this domain and potential future research directions.},
booktitle = {Proceedings of the 2022 7th International Conference on Machine Learning Technologies},
pages = {81–86},
numpages = {6},
keywords = {Open Learning Analytics (OLE), Open University Learning Analytics Dataset (OULAD), Students’ Performance Evaluation., Virtual Learning Environment (VLE)},
location = {Rome, Italy},
series = {ICMLT '22}
}

@inproceedings{10.1145/3576050.3576071,
author = {Haim, Aaron and Shaw, Stacy and Heffernan, Neil},
title = {How to Open Science: A Principle and Reproducibility Review of the Learning Analytics and Knowledge Conference},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576071},
doi = {10.1145/3576050.3576071},
abstract = {Within the field of education technology, learning analytics has increased in popularity over the past decade. Researchers conduct experiments and develop software, building on each other’s work to create more intricate systems. In parallel, open science — which describes a set of practices to make research more open, transparent, and reproducible — has exploded in recent years, resulting in more open data, code, and materials for researchers to use. However, without prior knowledge of open science, many researchers do not make their datasets, code, and materials openly available, and those that are available are often difficult, if not impossible, to reproduce. The purpose of the current study was to take a close look at our field by examining previous papers within the proceedings of the International Conference on Learning Analytics and Knowledge, and document the rate of open science adoption (e.g., preregistration, open data), as well as how well available data and code could be reproduced. Specifically, we examined 133 research papers, allowing ourselves 15 minutes for each paper to identify open science practices and attempt to reproduce the results according to their provided specifications. Our results showed that less than half of the research adopted standard open science principles, with approximately 5% fully meeting some of the defined principles. Further, we were unable to reproduce any of the papers successfully in the given time period. We conclude by providing recommendations on how to improve the reproducibility of our research as a field moving forward. All openly accessible work can be found in an Open Science Foundation project1.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {156–164},
numpages = {9},
keywords = {Open Science, Peer Review, Reproducibility},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3593663.3593686,
author = {Sapsai, Iryna and Valencia Usme, Yeimy Paola and Abke, Joerg},
title = {Learning Analytics Dashboard for Educators: Proposed Project to Design with Pedagogical Background},
year = {2023},
isbn = {9781450399562},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3593663.3593686},
doi = {10.1145/3593663.3593686},
abstract = {In this article, the authors describe a prototype of a Learning Analytics Dashboard (LAD) for educators. It is based on the analysis of pedagogical actions and taking into the process and learning style of students in an online environment based on learning analytics (LA). A description of the Dashboard structure, divided into levels and categories based on available learning analytics, will allow the educator to dive deeper into the online course themselves and explore more. It will also allow them to determine the level of student performance, identify gaps in learning materials, and research student data.The authors have identified further directions for the development of a LAD for a professor, including modeling algorithms for researching student behavior and learning style using Artificial Intelligence and presenting LA in a visualized form. This paper shows the stages of creating a professor's LAD prototype as a functional part of the adaptive learning system in the HASKI-System to analyze visual information obtained from LA and the possibilities to monitor the learning process, learning progress, student activity, and make decisions on careful intervention in the students’ learning process.},
booktitle = {Proceedings of the 5th European Conference on Software Engineering Education},
pages = {38–47},
numpages = {10},
keywords = {Learning Analytics Dashboard, adaptive learning system, distance education pedagogy, information visualization, learning analytics, pedagogical actions, pedagogical knowledge},
location = {Seeon/Bavaria, Germany},
series = {ECSEE '23}
}

@inproceedings{10.1145/3375462.3375536,
author = {Tsai, Yi-Shan and Whitelock-Wainwright, Alexander and Ga\v{s}evi\'{c}, Dragan},
title = {The privacy paradox and its implications for learning analytics},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375536},
doi = {10.1145/3375462.3375536},
abstract = {Learning analytics promises to support adaptive learning in higher education. However, the associated issues around privacy protection, especially their implications for students as data subjects, has been a hurdle to wide-scale adoption. In light of this, we set out to understand student expectations of privacy issues related to learning analytics and to identify gaps between what students desire and what they expect to happen or choose to do in reality when it comes to privacy protection. To this end, an investigation was carried out in a UK higher education institution using a survey (N=674) and six focus groups (26 students). The study highlight a number of key implications for learning analytics research and practice: (1) purpose, access, and anonymity are key benchmarks of ethics and privacy integrity; (2) transparency and communication are key levers for learning analytics adoption; and (3) information asymmetry can impede active participation of students in learning analytics.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {230–239},
numpages = {10},
keywords = {expectations, higher education, learning analytics, privacy, privacy paradox},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3506860.3506871,
author = {Pan, Zilong and Liu, Min},
title = {The effects of learning analytics hint system in supporting students problem-solving},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506871},
doi = {10.1145/3506860.3506871},
abstract = {This mixed-method study examined the impacts of a learning-analytics (LA) hints system on middle school students’ problem-solving performance and self-efficacy (SE). Students in condition A received the LA hint system, students in condition B received a static hint system that contains the same set of hints but without the LA mechanism, condition C was a control group that no hints were provided. The statistical results showed that the problem-solving SE for students who engaged with the LA hint system improved significantly. Student interviews revealed that real-time supports and in-time positive feedback played key roles in supporting their SE growth. Moreover, student-generated quantitative and qualitative log data were collected for interpreting the research outcomes. The quantitative logs provided an in-depth examination of problem-solving strategies across the conditions while the qualitative logs provided another perspective to understand students’ problem-solving status. Implications for future implementation of LA-hint system in virtual PBL environments were provided.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {77–86},
numpages = {10},
keywords = {datasets, gaze detection, neural networks, text tagging},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3576050.3576144,
author = {Chejara, Pankaj and Prieto, Luis P. and Rodriguez-Triana, Maria Jesus and Kasepalu, Reet and Ruiz-Calleja, Adolfo and Shankar, Shashi Kant},
title = {How to Build More Generalizable Models for Collaboration Quality? Lessons Learned from Exploring Multi-Context Audio-Log Datasets using Multimodal Learning Analytics},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576144},
doi = {10.1145/3576050.3576144},
abstract = {Multimodal learning analytics (MMLA) research for building collaboration quality estimation models has shown significant progress. However, the generalizability of such models is seldom addressed. In this paper, we address this gap by systematically evaluating the across-context generalizability of collaboration quality models developed using a typical MMLA pipeline. This paper further presents a methodology to explore modelling pipelines with different configurations to improve the generalizability of the model. We collected 11 multimodal datasets (audio and log data) from face-to-face collaborative learning activities in six different classrooms with five different subject teachers. Our results showed that the models developed using the often-employed MMLA pipeline degraded in terms of Kappa from Fair (.20 &lt; Kappa &lt; .40) to Poor (Kappa &lt; .20) when evaluated across contexts. This degradation in performance was significantly ameliorated with pipelines that emerged as high-performing from our exploration of 32 pipelines. Furthermore, our exploration of pipelines provided statistical evidence that often-overlooked contextual data features improve the generalizability of a collaboration quality model. With these findings, we make recommendations for the modelling pipeline which can potentially help other researchers in achieving better generalizability in their collaboration quality estimation models.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {111–121},
numpages = {11},
keywords = {Collaboration Quality, Generalizability, Machine Learning, MultiModal Learning Analytics},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3506860.3506895,
author = {Fernandez Nieto, Gloria Milena and Kitto, Kirsty and Buckingham Shum, Simon and Martinez-Maldonado, Roberto},
title = {Beyond the Learning Analytics Dashboard: Alternative Ways to Communicate Student Data Insights Combining Visualisation, Narrative and Storytelling},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506895},
doi = {10.1145/3506860.3506895},
abstract = {Learning Analytics (LA) dashboards have become a popular medium for communicating to teachers analytical insights obtained from student data. However, recent research indicates that LA dashboards can be complex to interpret, are often not grounded in educational theory, and frequently provide little or no guidance on how to interpret them. Despite these acknowledged problems, few suggestions have been made as to how we might improve the visual design of LA tools to support richer and alternative ways to communicate student data insights. In this paper, we explore three design alternatives to represent student multimodal data insights by combining data visualisation, narratives and storytelling principles. Based on foundations in data storytelling, three visual-narrative interfaces were designed with teachers: i) visual data slices, ii) a tabular visualisation, and iii) a written report. These were validated as a part of an authentic study where teachers explored activity logs and physiological data from co-located collaborative learning classes in the context of healthcare education. Results suggest that alternatives to LA dashboards can be considered as effective tools to support teachers’ reflection, and that LA designers should identify the representation type that best fits teachers’ needs.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {219–229},
numpages = {11},
keywords = {multimodal data, qualitative analysis, visual learning analytics},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3506860.3506972,
author = {Srivastava, Namrata and Fan, Yizhou and Rakovic, Mladen and Singh, Shaveen and Jovanovic, Jelena and van der Graaf, Joep and Lim, Lyn and Surendrannair, Surya and Kilgour, Jonathan and Molenaar, Inge and Bannert, Maria and Moore, Johanna and Gasevic, Dragan},
title = {Effects of Internal and External Conditions on Strategies of Self-regulated Learning: A Learning Analytics Study},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506972},
doi = {10.1145/3506860.3506972},
abstract = {Self-regulated learning (SRL) skills are essential for successful learning in a technology-enhanced learning environment. Learning Analytics techniques have shown a great potential in identifying and exploring SRL strategies from trace data in various learning environments. However, these strategies have been mainly identified through analysis of sequences of learning actions, and thus interpretation of the strategies is heavily task and context dependent. Further, little research has been done on the association of SRL strategies with different influencing factors or conditions. To address these gaps, we propose an analytic method for detecting SRL strategies from theoretically supported SRL processes and applied the method to a dataset collected from a multi-source writing task. The detected SRL strategies were explored in terms of their association with the learning outcome, internal conditions (prior-knowledge, metacognitive knowledge and motivation) and external conditions (scaffolding). The study results showed our analytic method successfully identified three theoretically meaningful SRL strategies. The study results revealed small effect size in the association between the internal conditions and the identified SRL strategies, but revealed a moderate effect size in the association between external conditions and the SRL strategy use.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {392–403},
numpages = {12},
keywords = {Learning analytics, SRL strategies, Scaffolding, Self-regulated learning},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3636555.3636920,
author = {Li, Lin and Srivastava, Namrata and Rong, Jia and Pianta, Gina and Varanasi, Raju and Ga\v{s}evi\'{c}, Dragan and Chen, Guanliang},
title = {Unveiling Goods and Bads: A Critical Analysis of Machine Learning Predictions of Standardized Test Performance in Early Childhood Education},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636920},
doi = {10.1145/3636555.3636920},
abstract = {Learning analytics (LA) holds a promise to transform education by utilizing data for evidence-based decision-making. Yet, its application in early childhood education (ECE) remains relatively under-explored. ECE plays a crucial role in fostering fundamental numeracy and literacy skills. While standardized tests was intended to be used to monitor student progress, they have been increasingly assumed summative and high-stake due to the substantial impact. The pressures in succeeding in such standardized tests have been well-documented to negatively affect both students and teachers. Attempting to ease such stress and better support students and teachers, the current study delved into the LA potential for predicting standardized test performance using formative assessments. Beyond predictive accuracy, the study addressed ethical considerations related to fairness to uncover potential risks associated with LA adoption. Our findings revealed a promising opportunity to empower teachers and schools with more time and room to help students better prepared based on predictions obtained earlier before standardized tests. Notably, bias can be significantly observed in predictions for students with disabilities even they have same actual competence compared to students without disabilities. In addition, we noticed that inclusion of demographic attribute had no significant impact on the predictive accuracy, and not necessarily exacerbate the overall predictive bias, but may significantly affect the predictions received by certain demographic subgroups (e.g., students with different types of disability).},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {608–619},
numpages = {12},
keywords = {bias, early childhood education, fairness, learning analytics, machine learning, standardized tests},
location = {Kyoto, Japan},
series = {LAK '24}
}

@article{10.1145/3583849.3583851,
author = {Lynch, Grace},
title = {13th International Learning Analytics and Knowledge Conference (LAK23)},
year = {2023},
issue_date = {Winter 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2023},
number = {Winter},
issn = {1931-1745},
url = {https://doi.org/10.1145/3583849.3583851},
doi = {10.1145/3583849.3583851},
abstract = {The Society for Learning Analytics Research (SoLAR.) is an inter-disciplinary network of leading international researchers who are exploring the role and impact of analytics on teaching, learning, training and development. SoLAR has been active in organizing the International Conference on Learning Analytics &amp; Knowledge (LAK) and the Learning Analytics Summer Institute (LASI), launching multiple initiatives to support collaborative and open research around learning analytics, promoting the publication and dissemination of learning analytics research, and advising and consulting with state, provincial, and national governments.},
journal = {SIGWEB Newsl.},
month = mar,
articleno = {2},
numpages = {3}
}

@inproceedings{10.1145/3506860.3506912,
author = {Shibani, Antonette and Knight, Simon and Buckingham Shum, Simon},
title = {Questioning learning analytics? Cultivating critical engagement as student automated feedback literacy},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506912},
doi = {10.1145/3506860.3506912},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {326–335},
numpages = {10},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3486011.3486519,
author = {Conde, Miguel \'{A}ngel and Hern\'{a}ndez-Garc\'{\i}a, \'{A}ngel and Chaparro-Pel\'{a}ez, Julian},
title = {Learning Analytics: Moving on?},
year = {2021},
isbn = {9781450390668},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3486011.3486519},
doi = {10.1145/3486011.3486519},
abstract = {This is the introductory article of the track on Learning Analytics within the 2021 International Conference Technological Ecosystems for Enhancing Multiculturality (TEEM’21). In this article, we lay out the motivation of the main theme of this edition's track, the main topics of interest and the review process followed for submission of contributions. We also present an outline of the accepted contributions and conclude with our own reflections on current issues in research on learning analytics, based on both our opinions and experience as track chairs in previous editions.},
booktitle = {Ninth International Conference on Technological Ecosystems for Enhancing Multiculturality (TEEM'21)},
pages = {583–586},
numpages = {4},
keywords = {Computational thinking, STEAM, STEM, coding in schools, computational thinking skills and curriculum, computer science in K-12, programming, robots},
location = {Barcelona, Spain},
series = {TEEM'21}
}

@inproceedings{10.1145/3448139.3448165,
author = {Hakami, Eyad and Hernandez-Leo, Davinia},
title = {Investigating the Well-being Impacts of Educational Technologies Supported by Learning Analytics: An application of the initial phase of IEEE P7010 recommended practice to a set of cases},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448165},
doi = {10.1145/3448139.3448165},
abstract = {The accelerated adoption of digital technologies by people and communities results in a close relation between, on one hand, the state of individual and societal well-being and, on the other hand, the state of the digital technologies that underpin our life experiences. The ethical concerns and questions about the impact of such technologies on human well-being become more crucial when data analytics and intelligent competences are integrated. To investigate how learning technologies could impact human well-being considering the promising and concerning roles of learning analytics, we apply the initial phase of the recently produced IEEE P7010 Well-being Impact Assessment, a methodology and a set of metrics, to allow the digital well-being of a set of educational technologies to be more comprehensively tackled and evaluated. We posit that the use of IEEE P7010 well-being metrics could help identify where educational technologies supported by learning analytics would increase or decrease well-being, providing new routes to future technological innovation in Learning Analytics research.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {269–279},
numpages = {11},
keywords = {Digital well-being, Ethics, Learning analytics, Values},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3573051.3596186,
author = {Ouhaichi, Hamza and Spikol, Daniel and Vogel, Bahtijar},
title = {Rethinking MMLA: Design Considerations for Multimodal Learning Analytics Systems},
year = {2023},
isbn = {9798400700255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573051.3596186},
doi = {10.1145/3573051.3596186},
abstract = {Designing MMLA systems is a complex task requiring a wide range of considerations. In this paper, we identify key considerations that are essential for designing MMLA systems. These considerations include data management, human factors, sensors and modalities, learning scenarios, privacy and ethics, interpretation and feedback, and data collection. The implications of these considerations are twofold: 1) The need for flexibility in MMLA systems to adapt to different learning contexts and scales, and 2) The need for a researcher-centered approach to designing MMLA systems. Unfortunately, the sheer number of considerations can lead to a state of "analysis paralysis," where deciding where to begin and how to proceed becomes overwhelming. This synthesis paper asks researchers to rethink the design of MMLA systems and aims to provide guidance for developers and practitioners in the field of MMLA.},
booktitle = {Proceedings of the Tenth ACM Conference on Learning @ Scale},
pages = {354–359},
numpages = {6},
keywords = {internet of things, multimodal learning analytics, scalability, system design},
location = {Copenhagen, Denmark},
series = {L@S '23}
}

@inproceedings{10.1145/3303772.3303784,
author = {Dawson, Shane and Joksimovic, Srecko and Poquet, Oleksandra and Siemens, George},
title = {Increasing the Impact of Learning Analytics},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303784},
doi = {10.1145/3303772.3303784},
abstract = {Learning Analytics (LA) studies the learning process in order to optimize learning opportunities for students. Although LA has quickly risen to prominence, there remain questions regarding the impact LA has made to date. To evaluate the extent that LA has impacted our understanding of learning and produced insights that have been translated to mainstream practice or contributed to theory, we reviewed the research published in 2011-2018 LAK conferences and Journal of Learning Analytics. The reviewed studies were coded according to five dimensions: study focus, data types, purpose, institutional setting, and scale of research and implementation. The coding and subsequent epistemic network analysis indicates that while LA research has developed in the areas of focus and sophistication of analyses, the impact on practice, theory and frameworks have been limited. We hypothesize that this finding is due to a continuing predominance of small-scale techno-centric exploratory studies that to date have not fully accounted for the multi-disciplinarity that comprises education. For the field to reach its potential in understanding and optimizing learning and learning environments, there must be a purposeful shift to move from exploratory models to more holistic and integrative systems-level research. This necessitates greater effort applied to understanding the research cycles that emerge when multiple knowledge domains coalesce into new fields of research.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {446–455},
numpages = {10},
keywords = {Learning Analytics, adoption, epistemic network analysis},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3303772.3303798,
author = {Chen, Bodong and Zhu, Haiyi},
title = {Towards Value-Sensitive Learning Analytics Design},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303798},
doi = {10.1145/3303772.3303798},
abstract = {To support ethical considerations and system integrity in learning analytics, this paper introduces two cases of applying the Value Sensitive Design methodology to learning analytics design. The first study applied two methods of Value Sensitive Design, namely stakeholder analysis and value analysis, to a conceptual investigation of an existing learning analytics tool. This investigation uncovered a number of values and value tensions, leading to design trade-offs to be considered in future tool refinements. The second study holistically applied Value Sensitive Design to the design of a recommendation system for the Wikipedia WikiProjects. To proactively consider values among stakeholders, we derived a multi-stage design process that included literature analysis, empirical investigations, prototype development, community engagement, iterative testing and refinement, and continuous evaluation. By reporting on these two cases, this paper responds to a need of practical means to support ethical considerations and human values in learning analytics systems. These two cases demonstrate that Value Sensitive Design could be a viable approach for balancing a wide range of human values, which tend to encompass and surpass ethical issues, in learning analytics design.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {343–352},
numpages = {10},
keywords = {data ethics, learning analytics, social media, value sensitive design, values},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3448139.3448158,
author = {Er, Erkan and Villa-Torrano, Cristina and Dimitriadis, Yannis and Gasevic, Dragan and Bote-Lorenzo, Miguel L. and Asensio-P\'{e}rez, Juan I. and G\'{o}mez-S\'{a}nchez, Eduardo and Mart\'{\i}nez Mon\'{e}s, Alejandra},
title = {Theory-based learning analytics to explore student engagement patterns in a peer review activity},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448158},
doi = {10.1145/3448139.3448158},
abstract = {Peer reviews offer many learning benefits. Understanding students’ engagement in them can help design effective practices. Although learning analytics can be effective in generating such insights, its application in peer reviews is scarce. Theory can provide the necessary foundations to inform the design of learning analytics research and the interpretation of its results. In this paper, we followed a theory-based learning analytics approach to identifying students’ engagement patterns in a peer review activity facilitated via a web-based tool called Synergy. Process mining was applied on temporal learning data, traced by Synergy. The theory about peer review helped determine relevant data points and guided the top-down approach employed for their analysis: moving from the global phases to regulation of learning, and then to micro-level actions. The results suggest that theory and learning analytics should mutually relate with each other. Mainly, theory played a critical role in identifying a priori engagement patterns, which provided an informed perspective when interpreting the results. In return, the results of the learning analytics offered critical insights about student behavior that was not expected by the theory (i.e., low levels of co-regulation). The findings provided important implications for refining the grounding theory and its operationalization in Synergy.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {196–206},
numpages = {11},
keywords = {Peer reviews, learning analytics, process mining, student engagement},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3568739.3568758,
author = {Tran, Tich Phuoc and Jan, Tony and Kew, Si Na},
title = {Learning Analytics for Improved Course Delivery: Applications and Techniques},
year = {2023},
isbn = {9781450398091},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568739.3568758},
doi = {10.1145/3568739.3568758},
abstract = {Learning Analytics (LA) is an emerging research field that harnesses the power of data modelling, data mining and visualization to enhance the understanding of teaching and learning as well as supporting the personalization of education. Typical LA applications include dashboards displaying course progress, intelligence reports tracking the use of educational resources, and systems that predict students' academic performance and identify struggling students. In this article, we review the major elements of LA applications, including typical workflows, data types, and approaches to analytics. In addition to the basic reporting tools available in Learning Management Systems (LMSs), the article provides insights into how Machine Learning (ML) can detect the students at risk of failing. Finally, six educational applications in which data analytics helps improve course delivery are discussed.},
booktitle = {Proceedings of the 6th International Conference on Digital Technology in Education},
pages = {100–106},
numpages = {7},
keywords = {Learning analytics, Machine learning, Student at risk detection, Visual analytics},
location = {Hangzhou, China},
series = {ICDTE '22}
}

@article{10.1145/3589636,
author = {Baker, Ryan S.},
title = {Learning Analytics: An Opportunity for Education},
year = {2023},
issue_date = {Spring 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {3},
issn = {1528-4972},
url = {https://doi.org/10.1145/3589636},
doi = {10.1145/3589636},
abstract = {How analytics is transforming the future for millions of learners.},
journal = {XRDS},
month = apr,
pages = {18–21},
numpages = {4}
}

@inproceedings{10.1145/3549737.3549774,
author = {Tsoni, Rozita and Kalles, Dimitris and Verykios, Vassilios},
title = {A Data Pipeline Approach for Building Learning Analytics Dashboards},
year = {2022},
isbn = {9781450395977},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3549737.3549774},
doi = {10.1145/3549737.3549774},
abstract = {In the era of data abundance, the ability to leverage data to assess the learning process is of great importance. Learning Analytics has been widely used and different approaches of deployment methods have been proposed, aiming to improve teaching and learning. Learning Analytics Dashboards (LAD), as the most dominant method to communicate results to the educational stakeholders, are found to be very effective. However, building a flexible and informative LAD is a complex procedure that incorporates several consecutive steps. The data pipeline framework which is used as a blueprint for generating LADs in this paper offers an important abstraction that helps the non-technical users to appreciate the effectiveness of the approach, as for every insight and report that is generated by a data scientist, there are most probably large such pipelines that implement the underlying functionality. This paper discusses the utility of data pipelines and presents the implementation of a LAD based on a data pipeline in Distance Learning students’ data for summative assessment, along with some preliminary results.},
booktitle = {Proceedings of the 12th Hellenic Conference on Artificial Intelligence},
articleno = {33},
numpages = {6},
keywords = {Data Pipelines, Distance Learning, Learning Analytics Dashboards, Summative Assessment},
location = {Corfu, Greece},
series = {SETN '22}
}

@inproceedings{10.1145/3612783.3612785,
author = {Pel\'{a}ez, Carlos Alberto and Solano, Andr\'{e}s and De La Rosa, Emma Adriana and L\'{o}pez, Jes\'{u}s Alfonso and Parra, Jorge Andrick and Ospina, Johann Alexis and Ram\'{\i}rez, Gabriel Mauricio and Moreira, Fernando},
title = {Learning analytics in the design of interactive multimedia experiences for elementary education: a systematic review},
year = {2024},
isbn = {9798400707902},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3612783.3612785},
doi = {10.1145/3612783.3612785},
abstract = {Interactive multimedia experiences are interesting for students because they stimulate several of their senses, provide assorted styles of interaction, and the user can modify the development of the experience thanks to the interactions with it, among other benefits. Moreover, this interaction produces data that can be used to improve the teaching-learning process of students using Learning Analytics techniques. In this sense, this paper presents a review of the state of the art on the impact of Learning Analytics in the development of multimedia technologies and evolution in the design process in the context of elementary education. A systematic literature review methodology has been applied to carry out this research. The results show the need for a well-defined process and a series of specific guidelines to include Learning Analytics in the solutions for this context.},
booktitle = {Proceedings of the XXIII International Conference on Human Computer Interaction},
articleno = {1},
numpages = {6},
keywords = {Elementary education, Interactive multimedia experience, Learning analytics},
location = {Lleida, Spain},
series = {Interacci\'{o}n '23}
}

@inproceedings{10.1145/3448139.3448186,
author = {Gosch, Nicole and Andrews, David and Barreiros, Carla and Leitner, Philipp and Staudegger, Elisabeth and Ebner, Martin and Lindstaedt, Stefanie},
title = {Learning Analytics as a Service for Empowered Learners: From Data Subjects to Controllers},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448186},
doi = {10.1145/3448139.3448186},
abstract = {As Learning Analytics (LA) in the higher education setting increasingly transitions from a field of research to an implemented matter of fact of the learner's experience, the demand of practical guidelines to support its development is rising. LA Policies bring together different perspectives, like the ethical and legal dimensions, into frameworks to guide the way. Usually the first time learners get in touch with LA is at the act of consenting to the LA tool. Utilising an ethical (TRUESSEC) and a legal framework (GDPR), we question whether sincere consent is possible in the higher education setting. Drawing upon this premise, we then show how it might be possible to recognise the autonomy of the learner by providing LA as a service, rather than an intervention. This could indicate a paradigm shift towards the learner as empowered demander. At last, we show how this might be incorporated within the GDPR by also recognising the demand of the higher education institutions to use the learner's data at the same time. These considerations will in the future influence the development of our own LA policy: a LA criteria catalogue.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {475–481},
numpages = {7},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3303772.3303785,
author = {Shibani, Antonette and Knight, Simon and Buckingham Shum, Simon},
title = {Contextualizable Learning Analytics Design: A Generic Model and Writing Analytics Evaluations},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303785},
doi = {10.1145/3303772.3303785},
abstract = {A major promise of learning analytics is that through the collection of large amounts of data we can derive insights from authentic learning environments, and impact many learners at scale. However, the context in which the learning occurs is important for educational innovations to impact student learning. In particular, for student-facing learning analytics systems like feedback tools to work effectively, they have to be integrated with pedagogical approaches and the learning design. This paper proposes a conceptual model to strike a balance between the concepts of generalizable scalable support and contextualized specific support by clarifying key elements that help to contextualize student-facing learning analytics tools. We demonstrate an implementation of the model using a writing analytics example, where the features, feedback and learning activities around the automated writing feedback tool are tuned for the pedagogical context and the assessment regime in hand, by co-designing them with the subject experts. The model can be employed for learning analytics to move from generalized support to meaningful contextualized support for enhancing learning.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {210–219},
numpages = {10},
keywords = {CLAD, conceptual model, contextualizable learning analytics, learning design, writing analytics},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3612783.3612813,
author = {Becerra, \'{A}Lvaro and Daza, Roberto and Cobos, Ruth and Morales, Aythami and Fierrez, Julian},
title = {User Experience Study using a System for Generating MultiModal Learning Analytics Dashboards},
year = {2024},
isbn = {9798400707902},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3612783.3612813},
doi = {10.1145/3612783.3612813},
abstract = {In the article, we present a Web-based System called M2LADS, which supports the integration and visualization of multimodal data recorded in user experiences (UX) in a Learning Analytics (LA) system in the form of Web-based Dashboards. Based on the edBB platform, the multimodal data gathered contains biometric and behavioral signals including electroencephalogram data to measure learners’ cognitive attention, heart rate for affective measures and visual attention from the video recordings. Additionally, learners’ static background data and their learning performance measures are tracked using LOGGE tool. M2LADS provides opportunities to capture learners’ holistic experience during their interactions with the learning analytic system in order to improve the system and the user experience of the learners.},
booktitle = {Proceedings of the XXIII International Conference on Human Computer Interaction},
articleno = {29},
numpages = {2},
keywords = {Biometrics and Behavior, Dashboard, MOOC, Multimodal Learning Analytics, User Experience (UX), e-Learning},
location = {Lleida, Spain},
series = {Interacci\'{o}n '23}
}

@inproceedings{10.1145/3587103.3594144,
author = {Snider, Johan Mattias},
title = {Edit, Run, Error, Repeat: Learning Analytics to Find Struggling Students in Upper Secondary Programming Classes},
year = {2023},
isbn = {9798400701399},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587103.3594144},
doi = {10.1145/3587103.3594144},
abstract = {This dissertation research explores the potential of using learning analytics to improve programming education. The research goals include replicating previous research through studying heterogeneous groups of students at upper secondary schools over several months. The expected contribution of this dissertation is to provide insights into how learning analytics can identify struggling students.},
booktitle = {Proceedings of the 2023 Conference on Innovation and Technology in Computer Science Education V. 2},
pages = {629–630},
numpages = {2},
keywords = {educational data mining, learning analytics, programming, upper secondary},
location = {Turku, Finland},
series = {ITiCSE 2023}
}

@inproceedings{10.1145/3375462.3375539,
author = {Shabaninejad, Shiva and Khosravi, Hassan and Indulska, Marta and Bakharia, Aneesha and Isaias, Pedro},
title = {Automated insightful drill-down recommendations for learning analytics dashboards},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375539},
doi = {10.1145/3375462.3375539},
abstract = {The big data revolution is an exciting opportunity for universities, which typically have rich and complex digital data on their learners. It has motivated many universities around the world to invest in the development and implementation of learning analytics dashboards (LADs). These dashboards commonly make use of interactive visualisation widgets to assist educators in understanding and making informed decisions about the learning process. A common operation in analytical dashboards is a 'drill-down', which in an educational setting allows users to explore the behaviour of sub-populations of learners by progressively adding filters. Nevertheless, drill-down challenges exist, which hamper the most effective use of the data, especially by users without a formal background in data analysis. Accordingly, in this paper, we address this problem by proposing an approach that recommends insightful drill-downs to LAD users. We present results from an application of our proposed approach using an existing LAD. A set of insightful drill-down criteria from a course with 875 students are explored and discussed.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {41–46},
numpages = {6},
keywords = {decision trees, drill-down analysis, exploratory data analysis, learning analytics dashboards},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@proceedings{10.1145/3448139,
title = {LAK21: LAK21: 11th International Learning Analytics and Knowledge Conference},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Irvine, CA, USA}
}

@inproceedings{10.1145/3362789.3362943,
author = {Conde, Miguel \'{A}. and Hern\'{a}ndez-Garc\'{\i}a, \'{A}ngel},
title = {Learning Analytics: The End of the Beginning},
year = {2019},
isbn = {9781450371919},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3362789.3362943},
doi = {10.1145/3362789.3362943},
abstract = {This document presents the main topics and objectives of the seventh edition of the Track on Learning Analytics within the 2019 International Conference Technological Ecosystems for Enhancing Multiculturality (TEEM'19). The introductory article also details the review process followed for submission of contributions and gives an overview of the accepted submissions. The document reflects about current issues in research on learning analytics, from the authors' views and experience as Track Chairs in previous editions.},
booktitle = {Proceedings of the Seventh International Conference on Technological Ecosystems for Enhancing Multiculturality},
pages = {248–252},
numpages = {5},
keywords = {Educational Data Mining, Educational Research, Ethics, Learning Analytics},
location = {Le\'{o}n, Spain},
series = {TEEM'19}
}

@inproceedings{10.1145/3448139.3448174,
author = {Lim, Lisa-Angelique and Gasevic, Dragan and Matcha, Wannisa and Ahmad Uzir, Nora'Ayu and Dawson, Shane},
title = {Impact of learning analytics feedback on self-regulated learning: Triangulating behavioural logs with students’ recall},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448174},
doi = {10.1145/3448139.3448174},
abstract = {Learning analytics (LA) has been presented as a viable solution for scaling timely and personalised feedback to support students’ self-regulated learning (SRL). Research is emerging that shows some positive associations between personalised feedback with students’ learning tactics and strategies as well as time management strategies, both important aspects of SRL. However, the definitive role of feedback on students’ SRL adaptations is under-researched; this requires an examination of students’ recalled experiences with their personalised feedback. Furthermore, an important consideration in feedback impact is the course context, comprised of the learning design and delivery modality. This mixed-methods study triangulates learner trace data from two different course contexts, with students’ qualitative data collected from focus group discussions, to more fully understand the impact of their personalised feedback and to explicate the role of this feedback on students’ SRL adaptations. The quantitative analysis showed the contextualised impact of the feedback on students’ learning and time management strategies in the different courses, while the qualitative analysis highlighted specific ways in which students used their feedback to adjust these and other SRL processes.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {364–374},
numpages = {11},
keywords = {learning analytics, learning strategies, mixed methods, personalised feedback, self-regulated learning, time management strategies},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448187,
author = {Dourado, Raphael A. and Rodrigues, Rodrigo Lins and Ferreira, Nivan and Mello, Rafael Ferreira and Gomes, Alex Sandro and Verbert, Katrien},
title = {A Teacher-facing Learning Analytics Dashboard for Process-oriented Feedback in Online Learning},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448187},
doi = {10.1145/3448139.3448187},
abstract = {In online learning, teachers need constant feedback about their students’ progress and regulation needs. Learning Analytics Dashboards for process-oriented feedback can be a valuable tool for this purpose. However, few such dashboards have been proposed in literature, and most of them lack empirical validation or grounding in learning theories. We present a teacher-facing dashboard for process-oriented feedback in online learning, co-designed and evaluated through an iterative design process involving teachers and visualization experts. We also reflect on our design process by discussing the challenges, pitfalls, and successful strategies for building this type of dashboard.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {482–489},
numpages = {8},
keywords = {learning analytics dashboards, process-oriented feedback, online learning, visualization},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3654777.3676347,
author = {Tang, Xiaohang and Wong, Sam and Pu, Kevin and Chen, Xi and Yang, Yalong and Chen, Yan},
title = {VizGroup: An AI-assisted Event-driven System for Collaborative Programming Learning Analytics},
year = {2024},
isbn = {9798400706288},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3654777.3676347},
doi = {10.1145/3654777.3676347},
abstract = {Programming instructors often conduct collaborative learning activities, like Peer Instruction, to foster a deeper understanding in students and enhance their engagement with learning. These activities, however, may not always yield productive outcomes due to the diversity of student mental models and their ineffective collaboration. In this work, we introduce VizGroup, an AI-assisted system that enables programming instructors to easily oversee students’ real-time collaborative learning behaviors during large programming courses. VizGroup leverages Large Language Models (LLMs) to recommend event specifications for instructors so that they can simultaneously track and receive alerts about key correlation patterns between various collaboration metrics and ongoing coding tasks. We evaluated VizGroup with 12 instructors in a comparison study using a dataset collected from a Peer Instruction activity that was conducted in a large programming lecture. The results showed that VizGroup helped instructors effectively overview, narrow down, and track nuances throughout students’ behaviors.},
booktitle = {Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology},
articleno = {93},
numpages = {22},
keywords = {Collaborative Learning, Programming Education},
location = {Pittsburgh, PA, USA},
series = {UIST '24}
}

@inproceedings{10.1145/3593663.3593679,
author = {S\"{o}chtig, Philipp and Apel, Sebastian and Windisch, Hans-Michael},
title = {Using Learning Analytics to Identify Student Learning Profiles for Software Development Courses},
year = {2023},
isbn = {9781450399562},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3593663.3593679},
doi = {10.1145/3593663.3593679},
abstract = {Often lecturers encounter the problem of not knowing how students use the course materials during a semester. In our approach we devised a web-based system that presents all learning materials in a digital format, allowing us to record student learning activities. The recorded usage data enabled extensive analyses of student learning behaviour which can support lecturers with improving the materials as well as understanding students’ learning material preferences and learning profiles, which can be composed by combining different usage modes depending on the material used. For the lectures we analysed, a higher success in the exam can be correlated to higher usage of the learning material according to our research data. Furthermore, student preferences regarding the form of presentation (f.e. slides over videos) could also be seen.},
booktitle = {Proceedings of the 5th European Conference on Software Engineering Education},
pages = {31–37},
numpages = {7},
keywords = {Learning Analytics, Learning Management System, Learning Profiles},
location = {Seeon/Bavaria, Germany},
series = {ECSEE '23}
}

@inproceedings{10.1145/3434780.3436712,
author = {Hern\'{a}ndez-Garc\'{\i}a, \'{A}ngel and Conde, Miguel \'{A}ngel and Chaparro-Pel\'{a}ez, Juli\'{a}n},
title = {Learning Analytics: A Time to Shine},
year = {2021},
isbn = {9781450388504},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3434780.3436712},
doi = {10.1145/3434780.3436712},
abstract = {In this article, the chairs of the track on Learning Analytics within the 2020 International Conference Technological Ecosystems for Enhancing Multiculturality (TEEM’20) introduce the main topics and objectives of the track and the session that was celebrated in the online conference. The introductory article also details the review process followed for submission of contributions and gives an overview of the accepted submissions. The document reflects about current issues in research on learning analytics, including both the authors’ views and experience as track chairs in previous editions, as well as those issues and topics raised in the roundtable session.},
booktitle = {Eighth International Conference on Technological Ecosystems for Enhancing Multiculturality},
pages = {713–718},
numpages = {6},
keywords = {COVID-19, Educational Data Mining, Educational Research, Ethics, Learning Analytics},
location = {Salamanca, Spain},
series = {TEEM'20}
}

@inproceedings{10.1145/3375462.3375483,
author = {Viberg, Olga and Khalil, Mohammad and Baars, Martine},
title = {Self-regulated learning and learning analytics in online learning environments: a review of empirical research},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375483},
doi = {10.1145/3375462.3375483},
abstract = {Self-regulated learning (SRL) can predict academic performance. Yet, it is difficult for learners. The ability to self-regulate learning becomes even more important in emerging online learning settings. To support learners in developing their SRL, learning analytics (LA), which can improve learning practice by transforming the ways we support learning, is critical. This scoping review is based on the analysis of 54 papers on LA empirical research for SRL in online learning contexts published between 2011 and 2019. The research question is: What is the current state of the applications of learning analytics to measure and support students' SRL in online learning environments? The focus is on SRL phases, methods, forms of SRL support, evidence for LA and types of online learning settings. Zimmerman's model (2002) was used to examine SRL phases. The evidence about LA was examined in relation to four propositions: whether LA i) improve learning outcomes, ii) improve learning support and teaching, iii) are deployed widely, and iv) used ethically. Results showed most studies focused on SRL parts from the forethought and performance phase but much less focus on reflection. We found little evidence for LA that showed i) improvements in learning outcomes (20%), ii) improvements in learning support and teaching (22%). LA was also found iii) not used widely and iv) few studies (15%) approached research ethically. Overall, the findings show LA research was conducted mainly to measure rather than to support SRL. Thus, there is a critical need to exploit the LA support mechanisms further in order to ultimately use them to foster student SRL in online learning environments.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {524–533},
numpages = {10},
keywords = {learning analytics, literature review, self-regulated learning},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3631700.3665228,
author = {Colling, Leona and Kholin, Mareike and Meurers, Detmar},
title = {A Learning Analytics Dashboard for K-12 English Teachers - Bridging the Gap Between Student Process Data and Teacher Needs},
year = {2024},
isbn = {9798400704666},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3631700.3665228},
doi = {10.1145/3631700.3665228},
abstract = {Educational technologies are being used more and more in secondary school settings. This increases the amount of students’ learning related data produced and stored. To keep up with this rise and to get most out of the collected data, teachers need digital tools that support and facilitate their pedagogical decision-making process. Learning analytics dashboards can be a good source to provide teachers with necessary insights into their students’ learning processes. However, for such tools to be effective and actionable, they have to be aligned with teachers’ needs and thus, provide and visualize data in a concise and structured way. We therefore conducted a survey study with 11 English teachers from K-12 secondary schools in Germany who evaluated the assumed usefulness of possible dashboard features. Based on these findings, we developed a teacher dashboard incorporating the most desired functionalities, such as a quickly accessible summary of strengths, weaknesses and support needs, or an overview of current misconceptions and competencies alongside additional metrics in order to support multiple teaching practices. The implementation and the underlying calculations are described, focusing on the importance of learners’ process data to provide teachers with a detailed and revealing view on their students’ and class learning states. In an evaluation study of the dashboard’s prototype with mock data, teachers (n=6) gave high ratings for the dashboard’s usability.},
booktitle = {Adjunct Proceedings of the 32nd ACM Conference on User Modeling, Adaptation and Personalization},
pages = {538–548},
numpages = {11},
keywords = {computer-assisted language learning, intelligent tutoring systems, learning analytics, teacher dashboard},
location = {Cagliari, Italy},
series = {UMAP Adjunct '24}
}

@inproceedings{10.1145/3700297.3700382,
author = {Chen, Zhiqiang and Chen, Yanru},
title = {Advancing AI-Driven BETC Models for Transformative Educational Data Mining and Learning Analytics},
year = {2024},
isbn = {9798400707100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3700297.3700382},
doi = {10.1145/3700297.3700382},
abstract = {This research paper presents an innovative integration of Artificial Intelligence (AI) with the British Technology Education Council (BETC) method, specifically tailored for civil engineering education. The study introduces an AI-driven BETC model that integrates real-world engineering data, industry standards, and virtual construction technologies to significantly enhance learning efficiency and teaching feedback precision. The model was rigorously evaluated through quantitative metrics, demonstrating a substantial increase in pre-class task completion rates from 30% to 87%, accuracy of achievements from 38% to 82%, and student satisfaction from 56% to 97%. These results not only validate the effectiveness of the AI-BETC model but also highlight the transformative potential of AI in educational data mining and learning analytics within the civil engineering sector. The study's findings underscore the importance of adopting advanced technologies in educational practices to improve outcomes and prepare students for the challenges of the future.},
booktitle = {Proceedings of the 2024 International Symposium on Artificial Intelligence for Education},
pages = {495–500},
numpages = {6},
keywords = {AI, BETC, virtual construction workshop},
location = {
},
series = {ISAIE '24}
}

@inproceedings{10.1145/3027385.3029465,
author = {Jaakonm\"{a}ki, Roope and Drachsler, Hendrik and Kickmeier-Rust, Michael and Dietze, Stefan and Fortenbacher, Albrecht and Marenzi, Ivana},
title = {Cooking with learning analytics recipes},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029465},
doi = {10.1145/3027385.3029465},
abstract = {Learning Analytics is a melting pot for a multitude of research fields and origin of many developments about learning and its environment. There is a serious hype over the concepts of learning analytics, however, concrete solutions and applications are comparably scarce. Of course, data rich environments, such as MOOCs, come with statistical analytics dashboards, although the educational value is often limited. Practical solutions for scenarios in data-lean environments or for small-scale organizations are rarely adopted. The LA4S project is dedicated to gather practical solutions, provide a tool box for practitioners, and publish a cook book with concrete learning analytics recipes for everyone.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {572–573},
numpages = {2},
keywords = {applications, cookbook, learning analytics, recipes, solutions},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3506860.3506900,
author = {Williamson, Kimberly and Kizilcec, Rene},
title = {A Review of Learning Analytics Dashboard Research in Higher Education: Implications for Justice, Equity, Diversity, and Inclusion},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506900},
doi = {10.1145/3506860.3506900},
abstract = {Learning analytics dashboards (LADs) are becoming more prevalent in higher education to help students, faculty, and staff make data-informed decisions. Despite extensive research on the design and implementation of LADs, few studies have investigated their relation to justice, equity, diversity, and inclusion (JEDI). Excluding these issues in LAD research limits the potential benefits of LADs generally and risks reinforcing long-standing inequities in education. We conducted a critical literature review, identifying 45 relevant papers to answer three research questions: how is LAD research improving JEDI, ii. how might it maintain or exacerbate inequitable outcomes, and iii. what opportunities exist in this space to improve JEDI in higher education. Using thematic analysis, we identified four common themes: (1) participant identities and researcher positionality, (2) surveillance concerns, (3) implicit pedagogies, and (4) software development resources. While we found very few studies directly addressing or mentioning JEDI concepts, we used these themes to explore ways researchers could consider JEDI in their studies. Our investigation highlights several opportunities to intentionally incorporate JEDI into LAD research by sharing software resources and conducting cross-border collaborations, better incorporating user needs, and centering considerations of justice in LAD efforts to improve historical inequities.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {260–270},
numpages = {11},
keywords = {Dashboards, Diversity, Equity, Higher Education, Inclusion, Justice, Literature Review},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3375462.3375478,
author = {Falc\~{a}o, Taciana Pontual and Mello, Rafael Ferreira and Rodrigues, Rodrigo Lins and Diniz, Juliana Regueira Basto and Tsai, Yi-Shan and Ga\v{s}evi\'{c}, Dragan},
title = {Perceptions and expectations about learning analytics from a brazilian higher education institution},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375478},
doi = {10.1145/3375462.3375478},
abstract = {Several tools to support learning processes based on educational data have emerged from research on Learning Analytics (LA) in the last few years. These tools aim to support students and instructors in daily activities, and academic managers in making institutional decisions. Although the adoption of LA tools is spreading, the field still needs to deepen the understanding of the contexts where learning takes place, and of the views of the stakeholders involved in implementing and using these tools. In this sense, the SHEILA framework proposes a set of instruments to perform a detailed analysis of the expectations and needs of different stakeholders in higher education institutions, regarding the adoption of LA. Moreover, there is a lacuna in research on stakeholders' expectations from LA outside the Global North. Therefore, this paper reports on the findings of the application of interviews and focus groups, based on the SHEILA framework, with students and teaching staff from a Brazilian public university, to investigate their perceptions of the potential benefits and risks of using LA in higher education in the country. Findings indicate that there is a high interest in using LA for improving the learning experience, in particular, being able to provide personalized feedback, to adapt teaching practices to students' needs, and to make evidence-based pedagogical decisions. From the analysis of these perspectives, we point to opportunities for using LA in Brazilian higher education.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {240–249},
numpages = {10},
keywords = {higher education institutions, human factors, learning analytics, qualitative research},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3448139.3448145,
author = {Ahn, June and Campos, Fabio and Nguyen, Ha and Hays, Maria and Morrison, Jan},
title = {Co-Designing for Privacy, Transparency, and Trust in K-12 Learning Analytics},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448145},
doi = {10.1145/3448139.3448145},
abstract = {The process of using Learning Analytics (LA) to improve teaching works from the assumption that data should be readily shared between stakeholders in an educational organization. However, the design of LA tools often does not account for considerations such as data privacy, transparency and trust among stakeholders. Research in human-centered design of LA does attend to these questions, specifically with a focus on including direct input from K-12 educators. In this paper, we present a series of design studies to articulate and refine conjectures about how privacy and transparency might influence better trust-building and data sharing within four school districts in the United States. By presenting the development of four sequential prototypes, our findings illuminate the tensions between designing for existing norms versus potentially challenging these norms by promoting meaningful discussions around the use of data. We conclude with a discussion about practical and methodological implications of our work to the LA community.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {55–65},
numpages = {11},
keywords = {Co-Design, Dashboards, Ethics, HCI, K-12 Education, Privacy},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3170358.3170379,
author = {Martinez-Maldonado, Roberto and Echeverria, Vanessa and Santos, Olga C. and Santos, Augusto Dias Pereira Dos and Yacef, Kalina},
title = {Physical learning analytics: a multimodal perspective},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170379},
doi = {10.1145/3170358.3170379},
abstract = {The increasing progress in ubiquitous technology makes it easier and cheaper to track students' physical actions unobtrusively, making it possible to consider such data for supporting research, educator interventions, and provision of feedback to students. In this paper, we reflect on the underexplored, yet important area of learning analytics applied to physical/motor learning tasks and to the physicality aspects of `traditional' intellectual tasks that often occur in physical learning spaces. Based on Distributed Cognition theory, the concept of Internet of Things and multimodal learning analytics, this paper introduces a theoretical perspective for bringing learning analytics into physical spaces. We present three prototypes that serve to illustrate the potential of physical analytics for teaching and learning. These studies illustrate advances in proximity, motion and location analytics in collaborative learning, dance education and healthcare training.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {375–379},
numpages = {5},
keywords = {classroom, indoor positioning, internet of things, mobility tracking, motor learning, physical spaces, wearables},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3303772.3303793,
author = {de Quincey, Ed and Briggs, Chris and Kyriacou, Theocharis and Waller, Richard},
title = {Student Centred Design of a Learning Analytics System},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303793},
doi = {10.1145/3303772.3303793},
abstract = {Current Learning Analytics (LA) systems are primarily designed with University staff members as the target audience; very few are aimed at students, with almost none being developed with direct student involvement and undertaking a comprehensive evaluation. This paper describes a HEFCE funded project that has employed a variety of methods to engage students in the design, development and evaluation of a student facing LA dashboard. LA was integrated into the delivery of 4 undergraduate modules with 169 student sign-ups. The design of the dashboard uses a novel approach of trying to understand the reasons why students want to study at university and maps their engagement and predicted outcomes to these motivations, with weekly personalised notifications and feedback. Students are also given the choice of how to visualise the data either via a chart-based view or to be represented as themselves. A mixed-methods evaluation has shown that students' feelings of dependability and trust of the underlying analytics and data is variable. However, students were mostly positive about the usability and interface design of the system and almost all students once signed-up did interact with their LA. The majority of students could see how the LA system could support their learning and said that it would influence their behaviour. In some cases, this has had a direct impact on their levels of engagement. The main contribution of this paper is the transparent documentation of a User Centred Design approach that has produced forms of LA representation, recommendation and interaction design that go beyond those used in current similar systems and have been shown to motivate students and impact their learning behaviour.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {353–362},
numpages = {10},
keywords = {Laddering, Learning Analytics, Usability, User Centred Design, User Experience, Visualisation},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3027385.3027400,
author = {Tsai, Yi-Shan and Gasevic, Dragan},
title = {Learning analytics in higher education --- challenges and policies: a review of eight learning analytics policies},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027400},
doi = {10.1145/3027385.3027400},
abstract = {This paper presents the results of a review of eight policies for learning analytics of relevance for higher education, and discusses how these policies have tried to address prominent challenges in the adoption of learning analytics, as identified in the literature. The results show that more considerations need to be given to establishing communication channels among stakeholders and adopting pedagogy-based approaches to learning analytics. It also reveals the shortage of guidance for developing data literacy among end-users and evaluating the progress and impact of learning analytics. Moreover, the review highlights the need to establish formalised guidelines to monitor the soundness, effectiveness, and legitimacy of learning analytics. As interest in learning analytics among higher education institutions continues to grow, this review will provide insights into policy and strategic planning for the adoption of learning analytics.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {233–242},
numpages = {10},
keywords = {strategy, policy, learning analytics, higher education, code of practice, challenge},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3375462.3375484,
author = {Cukurova, Mutlu and Zhou, Qi and Spikol, Daniel and Landolfi, Lorenzo},
title = {Modelling collaborative problem-solving competence with transparent learning analytics: is video data enough?},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375484},
doi = {10.1145/3375462.3375484},
abstract = {In this study, we describe the results of our research to model collaborative problem-solving (CPS) competence based on analytics generated from video data. We have collected ~500 mins video data from 15 groups of 3 students working to solve design problems collaboratively. Initially, with the help of OpenPose, we automatically generated frequency metrics such as the number of the face-in-the-screen; and distance metrics such as the distance between bodies. Based on these metrics, we built decision trees to predict students' listening, watching, making, and speaking behaviours as well as predicting the students' CPS competence. Our results provide useful decision rules mined from analytics of video data which can be used to inform teacher dashboards. Although, the accuracy and recall values of the models built are inferior to previous machine learning work that utilizes multimodal data, the transparent nature of the decision trees provides opportunities for explainable analytics for teachers and learners. This can lead to more agency of teachers and learners, therefore can lead to easier adoption. We conclude the paper with a discussion on the value and limitations of our approach.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {270–275},
numpages = {6},
keywords = {video analytics, physical learning analytics, multimodal learning analytics, decision trees, collaborative problem-solving},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3170358.3170384,
author = {Gibson, Andrew and Lang, Charles},
title = {The pragmatic maxim as learning analytics research method},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170384},
doi = {10.1145/3170358.3170384},
abstract = {It is arguable that the chief aim of Learning Analytics is to use analytics for meaningful purposes in learning and teaching contexts, and that research in the field should advance this cause. However the field does not present a single clear understanding of what constitutes quality in Learning Analytics research.In this paper we present the Pragmatic Inquiry for Learning Analytics Research (PILAR) method as one approach to conducting Learning Analytics research. Rather than creating a new method, we reintroduce an old method to a new field, drawing on the Pragmatic Maxim, proposed by Charles Sanders Peirce as a principle for making ideas clear. Our instantiation of the Pragmatic Maxim requires the researcher to situate Learning Analytics research within a clearly defined learning context and to consider the analytics in terms of the practical effects on learning. We propose three essential elements and a five step process for addressing them in research.After presenting PILAR we address two potential limitations of the approach, and conclude with some implications for its future use in Learning Analytics research.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {461–465},
numpages = {5},
keywords = {inquiry, learning analytics, pragmatism, research methods},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3636555.3636934,
author = {Rakovi\'{c}, Mladen and Li, Yuheng and Foumani, Navid Mohammadi and Salehi, Mahsa and Kuhlmann, Levin and Mackellar, Geoffrey and Martinez-Maldonado, Roberto and Haffari, Gholamreza and Swiecki, Zachari and Li, Xinyu and Chen, Guanliang and Ga\v{s}evi\'{c}, Dragan},
title = {Measuring Affective and Motivational States as Conditions for Cognitive and Metacognitive Processing in Self-Regulated Learning},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636934},
doi = {10.1145/3636555.3636934},
abstract = {Even though the engagement in self-regulated learning (SRL) has been shown to boost academic performance, SRL skills of many learners remain underdeveloped. They often struggle to productively navigate multiple cognitive, affective, metacognitive and motivational (CAMM) processes in SRL. To provide learners with the required SRL support, it is essential to understand how learners enact CAMM processes as they study. More research is needed to advance the measurement of affective and motivational processes within SRL, and investigate how these processes influence learners’ cognition and metacognition. With this in mind, we conducted a lab study involving 22 university students who worked on a 45-minute reading and writing task in digital learning environment. We used a wearable electroencephalogram device to record learner academic emotional and motivational states, and digital trace data to record learner cognitive and metacognitive processes. We harnessed time series prediction and explainable artificial intelligence methods to examine how learner’s emotional and motivational states influence their choice of cognitive and metacognitive processes. Our results indicate that emotional and motivational states can predict learners’ use of low cognitive, high cognitive and metacognitive processes with considerable classification accuracy (F1 &gt; 0.73), and that higher values of interest, engagement and excitement promote cognitive processing.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {701–712},
numpages = {12},
keywords = {electroencephalography, multi-modal learning analytics, self-regulated learning, time-series classification},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3170358.3170374,
author = {Kovanovi\'{c}, Vitomir and Joksimovi\'{c}, Sre\'{c}ko and Mirriahi, Negin and Blaine, Ellen and Ga\v{s}evi\'{c}, Dragan and Siemens, George and Dawson, Shane},
title = {Understand students' self-reflections through learning analytics},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170374},
doi = {10.1145/3170358.3170374},
abstract = {Reflective writing has been widely recognized as one of the most effective activities for fostering students' reflective and critical thinking. The analysis of students' reflective writings has been the focus of many research studies. However, to date this has been typically a very labor-intensive manual process involving content analysis of student writings. With recent advancements in the field of learning analytics, there have been several attempts to use text analytics to examine student reflective writings. This paper presents the results of a study examining the use of theoretically-sound linguistic indicators of different psychological processes for the development of an analytics system for assessment of reflective writing. More precisely, we developed a random-forest classification system using linguistic indicators provided by the LIWC and Coh-Metrix tools. We also examined what particular indicators are representative of the different types of student reflective writings.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {389–398},
numpages = {10},
keywords = {learning analytics, online learning, self-reflections, text mining},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3170358.3170415,
author = {Taylor, Sarah and Munguia, Pablo},
title = {Towards a data archiving solution for learning analytics},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170415},
doi = {10.1145/3170358.3170415},
abstract = {Data solutions in the teaching and learning space are in need of pro-active innovations in data management, to ensure that systems for learning analytics can scale up to match the size of datasets now available. Here, we illustrate the scale at which a Learning Management System (LMS) accumulates data, and discuss the barriers to using this data for in-depth analyses. We illustrate the exponential growth of our LMS data to represent a single example dataset, and highlight the broader need for taking a pro-active approach to dimensional modelling in learning analytics, anticipating that common learning analytics questions will be computationally expensive, and that the most useful data structures for learning analytics will not necessarily follow those of the source dataset.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {260–264},
numpages = {5},
keywords = {learning management systems, learning analytics, dimensional modelling, data retention, big data, barriers to adoption},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@proceedings{10.1145/3375462,
title = {LAK '20: Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {We are pleased to welcome you to the Tenth International Conference on Learning Analytics and Knowledge (LAK20), organized by the Society for Learning Analytics Research (SoLAR). This year's conference is hosted by the Goethe University in the beautiful city of Frankfurt, Germany on March 23--27, 2020, a place of tremendous importance and rich history of science, art, and philosophy. While this is the first time that LAK is organized in Germany, we want to acknowledge that it is a tremendous collaborative effort by the international community of learning analytics researchers and practitioners. Being the tenth anniversary of the conference, the theme for LAK20 is "Celebrating 10 years of LAK: Shaping the future of the field" and focuses on celebrating the achievements of the learning analytics community in the first ten years as well as on charting a pathway for the next ten years. The LAK20 conference is intended for both researchers and practitioners, and we invite them to come and join a proactive dialogue around the future of learning analytics and its practical adoption. We further extend our invite to educators, leaders, administrators, and government and industry professionals interested in the field of learning analytics and related disciplines.},
location = {Frankfurt, Germany}
}

@inproceedings{10.1145/3511861.3511878,
author = {Ott, Claudia and Liesaputra, Veronica},
title = {Using Affective Learning Analytics in Industry-focused Projects: Experiences and Challenges},
year = {2022},
isbn = {9781450396431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511861.3511878},
doi = {10.1145/3511861.3511878},
abstract = {Project-based learning (PJBL) with real world clients provides students with the skills and knowledge required by industry. Similar to asynchronous online learning environments, PJBL students typically work in self-directed teams at times and places of their choice. Thus, it is difficult for the educators to identify technical and emotional challenges that students experience—especially with large cohorts. To bridge the disconnect between educators and students in such learning situations and to be able to provide students with timely feedback and support, we have experimented with the use of an emotion detection tool to automatically recognise students’ emotional states and the issues that they might face. Our results show that the detected emotions moderately to strongly correlate with students’ issues as observed by the academic coordinators and also with students’ final marks. However, our explorations also highlighted ethical challenges when using emotion-aware learning analytics. This paper describes our experiences and discusses possible ways to address those challenges.},
booktitle = {Proceedings of the 24th Australasian Computing Education Conference},
pages = {153–162},
numpages = {10},
keywords = {project-based learning, mindsets, learning analytics, emotions},
location = {Virtual Event, Australia},
series = {ACE '22}
}

@inproceedings{10.1145/3636555.3636866,
author = {Cheng, Yixin and Lyons, Kayley and Chen, Guanliang and Ga\v{s}evi\'{c}, Dragan and Swiecki, Zachari},
title = {Evidence-centered Assessment for Writing with Generative AI},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636866},
doi = {10.1145/3636555.3636866},
abstract = {We propose a learning analytics-based methodology for assessing the collaborative writing of humans and generative artificial intelligence. Framed by the evidence-centered design, we used elements of knowledge-telling, knowledge transformation, and cognitive presence to identify assessment claims; we used data collected from the CoAuthor writing tool as potential evidence for these claims; and we used epistemic network analysis to make inferences from the data about the claims. Our findings revealed significant differences in the writing processes of different groups of CoAuthor users, suggesting that our method is a plausible approach to assessing human-AI collaborative writing.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {178–188},
numpages = {11},
keywords = {Assessment, Epistemic Network Analysis, Evidence-centered Design, Generative Artificial Intelligence},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3027385.3027389,
author = {Haythornthwaite, Caroline},
title = {An information policy perspective on learning analytics},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027389},
doi = {10.1145/3027385.3027389},
abstract = {Policy for learning analytics joins a stream of initiatives aimed at understanding the expanding world of information collection, storage, processing and dissemination that is being driven by computing technologies. This paper offers a information policy perspective on learning analytics, joining work by others on ethics and privacy in the management of learning analytics data [8], but extending to consider how issues play out across the information lifecycle and in the formation of policy. Drawing on principles from information policy both informs learning analytics and brings learning analytics into the information policy domain. The resulting combination can help inform policy development for educational institutions as they implement and manage learning analytics policy and practices. The paper begins with a brief summary of the information policy perspective, then addresses learning analytics with attention to various categories of consideration for policy development.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {253–256},
numpages = {4},
keywords = {learning analytics, information policy, educational policy},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3029439,
author = {Lang, Charles and Teasley, Stephanie and Stamper, John},
title = {Building the learning analytics curriculum: workshop},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029439},
doi = {10.1145/3027385.3029439},
abstract = {Learning Analytics courses and degree programs both on-and offline have begun to proliferate over the last three years. As a result of this growth in interest from students, university administrators, researchers and instructors we believe it is a good time to review how these educational efforts are impacting the field, how synergy between instructors might be developed to greater serve the field and what kinds of best practices could be developed.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {520–521},
numpages = {2},
keywords = {teaching, learning analytics instruction, curriculum development},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3029436,
author = {Bowe, Megan and Chen, Weiqin and Griffiths, Dai and Hoel, Tore and Lee, Jaeho and Ogata, Hiroaki and Richards, Griff and Yuan, Li and Zhang, Jingjing},
title = {Learning analytics and policy (LAP): international aspirations, achievements and constraints},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029436},
doi = {10.1145/3027385.3029436},
abstract = {The Learning Analytics and Policy (LAP) workshop explores and documents the ways in which policies at national and regional level are shaping the development of learning analytics. It brings together representatives from around the world who report on the circumstances in their own country. The workshop is preceded by an information gathering phase, and followed by the authoring of a report. The aspirations, achievements and constraints in the different countries are contrasted and documented, providing a valuable resource for the future development of learning analytics.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {516–517},
numpages = {2},
keywords = {privacy, policy, open data, learning analytics, data protection},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3362789.3362913,
author = {Pazmi\~{n}o-Maji, Rub\'{e}n and Naranjo-Ordo\~{n}ez, Laura and Conde-Gonz\'{a}lez, Miguel and Garc\'{\i}a-Pe\~{n}alvo, Francisco},
title = {Learning Analytics in Ecuador: An Initial Analysis based in a Mapping Review},
year = {2019},
isbn = {9781450371919},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3362789.3362913},
doi = {10.1145/3362789.3362913},
abstract = {Learning Analytics allows to describe, diagnose, predict and prescribe learning, especially in Higher Education. Two thousand eight hundred fifty-three papers related to Learning Analytics are stored in the Scopus bibliography database from 2014 until the middle of 2019, evidencing the importance and increasing interest in this line of research. This research discovers specific characteristics of investigations in Learning Analytics and answers the general question: What is the actual state of Learning Analytics research in Ecuador? This research uses a systematic mapping to answer four research questions about indexed production in Learning Analytics by Ecuadorian authors. This study has been done from 2014 until June 2019 (11 semesters). Eighty-six articles about Learning Analytics were found in RRAAE, Scopus, WOS and IEEE. Sixty-eight reports were downloaded, arranged and analysed after removing duplicates, applying inclusion, exclusion and quality criteria. The methodology used is replicable by the researchers interested in establishing a baseline in general and in particular in Learning Analytics.},
booktitle = {Proceedings of the Seventh International Conference on Technological Ecosystems for Enhancing Multiculturality},
pages = {304–311},
numpages = {8},
keywords = {Ecuador, Learning Analytics, Literature Mapping Review},
location = {Le\'{o}n, Spain},
series = {TEEM'19}
}

@inproceedings{10.1145/3303772.3303819,
author = {Macarini, Luiz Antonio and Cechinel, Cristian and Santos, Henrique Lemos dos and Ochoa, Xavier and Rod\'{e}s, Virg\'{\i}nia and Alonso, Guillermo Ettlin and Casas, Al\'{e}n P\'{e}rez and D\'{\i}az, Patricia},
title = {Challenges on implementing Learning Analytics over countrywide K-12 data},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303819},
doi = {10.1145/3303772.3303819},
abstract = {The present work describes the challenges faced during the development of a countrywide Learning Analytics tool focused on tracking the trajectories of Uruguayan students during their first three years of secondary education. Due to the large-scale of the project, which covers an entire national educational system, several challenges and constraints (both technical and legal) were faced during its conception and development. This paper presents the design decisions and solutions found to address or mitigate the problems found, with the current state of the project. Early results point out the feasibility of finding meaningful patterns in the available data (using data mining techniques) which can be embedded into a prototype for tracking the students scholar trajectory.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {441–445},
numpages = {5},
keywords = {Academic Trajectory, Early Warning System, Educational Data Mining, Learning Analytics, Primary and Secondary Education},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/2883851.2883864,
author = {Wolff, Annika and Moore, John and Zdrahal, Zdenek and Hlosta, Martin and Kuzilek, Jakub},
title = {Data literacy for learning analytics},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883864},
doi = {10.1145/2883851.2883864},
abstract = {This workshop explores how data literacy impacts on learning analytics both for practitioners and for end users. The term data literacy is used to broadly describe the set of abilities around the use of data as part of everyday thinking and reasoning for solving real-world problems. It is a skill required both by learning analytics practitioners to derive actionable insights from data and by the intended end users, such that it affects their ability to accurately interpret and critique presented analysis of data. The latter is particularly important, since learning analytics outcomes can be targeted at a wide range of end users, some of whom will be young students and many of whom are not data specialists.Whilst data literacy is rarely an end goal of learning analytics projects, this workshop aims to find where issues related to data literacy have impacted on project outcomes and where important insights have been gained. This workshop will further encourage the sharing of knowledge and experience through practical activities with datasets and visualisations. This workshop aims to highlight the need for a greater understanding of data literacy as a field of study, especially with regard to communicating around large, complex, data sets.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {500–501},
numpages = {2},
keywords = {visualization, learning analytics, data literacy, communication, analysis},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/3375462.3375537,
author = {Vezzoli, Yvonne and Mavrikis, Manolis and Vasalou, Asimina},
title = {Inspiration cards workshops with primary teachers in the early co-design stages of learning analytics},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375537},
doi = {10.1145/3375462.3375537},
abstract = {Despite the recognition of the need to include practitioners in the design of learning analytics (LA), especially teacher input tends to come later in the design process rather than in the definition of the initial design agenda. This paper presents a case study of a design project tasked with developing LA tools for a reading game for primary school children. Taking a co-design approach, we use the Inspiration Cards Workshop to promote meaningful teacher involvement even for participants with low background in data literacy or experience in using learning analytics. We discuss opportunities and limitations of using the Inspiration Cards Workshops methodology, and particularly Inspiration Cards as a design tool, to inform future LA design efforts.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {73–82},
numpages = {10},
keywords = {co-design methods, emerging technology, inspiration cards, learning analytics},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3170358.3170372,
author = {Dollinger, Mollie and Lodge, Jason M.},
title = {Co-creation strategies for learning analytics},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170372},
doi = {10.1145/3170358.3170372},
abstract = {In order to further the field of learning analytics (LA), researchers and experts may need to look beyond themselves and their own perspectives and expertise to innovate LA platforms and interventions. We suggest that by co-creating with the users of LA, such as educators and students, researchers and experts can improve the usability, usefulness, and draw greater understanding from LA interventions. Within this article we discuss the current LA issues and barriers and how co-creation strategies can help address many of these challenges. We further outline the considerations, both pre- and during interventions, which support and foster a co-created strategy for learning analytics interventions.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {97–101},
numpages = {5},
keywords = {co-creation, co-design, participatory design},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3170358.3170375,
author = {Dawson, Shane and Poquet, Oleksandra and Colvin, Cassandra and Rogers, Tim and Pardo, Abelardo and Gasevic, Dragan},
title = {Rethinking learning analytics adoption through complexity leadership theory},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170375},
doi = {10.1145/3170358.3170375},
abstract = {Despite strong interest in learning analytics (LA), adoption at a large-scale organizational level continues to be problematic. This may in part be due to the lack of acknowledgement of existing conceptual LA models to operationalize how key adoption dimensions interact to inform the realities of the implementation process. This paper proposes the framing of LA adoption in complexity leadership theory (CLT) to study the overarching system dynamics. The framing is empirically validated in a study analysing interviews with senior staff in Australian universities (n=32). The results were coded for several adoption dimensions including leadership, governance, staff development, and culture. The coded data were then analysed with latent class analysis. The results identified two classes of universities that either i) followed an instrumental approach to adoption - typically top-down leadership, large scale project with high technology focus yet demonstrating limited staff uptake; or ii) were characterized as emergent innovators - bottom up, strong consultation process, but with subsequent challenges in communicating and scaling up innovations. The results suggest there is a need to broaden the focus of research in LA adoption models to move on from small-scale course/program levels to a more holistic and complex organizational level.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {236–244},
numpages = {9},
keywords = {learning analytics adoption, leadership, complexity},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3284179.3284229,
author = {Hern\'{a}ndez-Garc\'{\i}a, \'{A}ngel and Conde, Miguel \'{A}.},
title = {Learning analytics: The good, the bad (and the ugly)},
year = {2018},
isbn = {9781450365185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3284179.3284229},
doi = {10.1145/3284179.3284229},
abstract = {The document describes outlines the theme and main objectives set for the sixth edition of the track on Learning Analytics within the 2018 International Conference Technological Ecosystems for Enhancing Multiculturality (TEEM'18), details the review process, provides an overview of the accepted submissions and reflects about current issues in research on learning analytics, from the authors' views and experience as Track Chairs in previous editions.},
booktitle = {Proceedings of the Sixth International Conference on Technological Ecosystems for Enhancing Multiculturality},
pages = {290–293},
numpages = {4},
keywords = {big data, learning analytics, value, variety, velocity, veracity, volume},
location = {Salamanca, Spain},
series = {TEEM'18}
}

@inproceedings{10.1145/3170358.3170413,
author = {Kitto, Kirsty and Buckingham Shum, Simon and Gibson, Andrew},
title = {Embracing imperfection in learning analytics},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170413},
doi = {10.1145/3170358.3170413},
abstract = {Learning Analytics (LA) sits at the confluence of many contributing disciplines, which brings the risk of hidden assumptions inherited from those fields. Here, we consider a hidden assumption derived from computer science, namely, that improving computational accuracy in classification is always a worthy goal. We demonstrate that this assumption is unlikely to hold in some important educational contexts, and argue that embracing computational "imperfection" can improve outcomes for those scenarios. Specifically, we show that learner-facing approaches aimed at "learning how to learn" require more holistic validation strategies. We consider what information must be provided in order to reasonably evaluate algorithmic tools in LA, to facilitate transparency and realistic performance comparisons.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {451–460},
numpages = {10},
keywords = {validation, performance, pedagogy, hidden assumptions, accuracy},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3576050.3576064,
author = {Lin, Jionghao and Dai, Wei and Lim, Lisa-Angelique and Tsai, Yi-Shan and Mello, Rafael Ferreira and Khosravi, Hassan and Gasevic, Dragan and Chen, Guanliang},
title = {Learner-centred Analytics of Feedback Content in Higher Education},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576064},
doi = {10.1145/3576050.3576064},
abstract = {Feedback is an effective way to assist students in achieving learning goals. The conceptualisation of feedback is gradually moving from feedback as information to feedback as a learner-centred process. To demonstrate feedback effectiveness, feedback as a learner-centred process should be designed to provide quality feedback content and promote student learning outcomes on the subsequent task. However, it remains unclear how instructors adopt the learner-centred feedback framework for feedback provision in the teaching practice. Thus, our study made use of a comprehensive learner-centred feedback framework to analyse feedback content and identify the characteristics of feedback content among student groups with different performance changes. Specifically, we collected the instructors’ feedback on two consecutive assignments offered by an introductory to data science course at the postgraduate level. On the basis of the first assignment, we used the status of student grade changes (i.e., students whose performance increased and those whose performance did not increase on the second assignment) as the proxy of the student learning outcomes. Then, we engineered and extracted features from the feedback content on the first assignment using a learner-centred feedback framework and further examined the differences of these features between different groups of student learning outcomes. Lastly, we used the features to predict student learning outcomes by using widely-used machine learning models and provided the interpretation of predicted results by using the SHapley Additive exPlanations (SHAP) framework. We found that 1) most features from the feedback content presented significant differences between the groups of student learning outcomes, 2) the gradient boost tree model could effectively predict student learning outcomes, and 3) SHAP could transparently interpret the feature importance on predictions.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {100–110},
numpages = {11},
keywords = {Content Analysis, Feedback, Interpretability, Learning Analytics},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3378184.3378229,
author = {Chicaiza, Janneth and Cabrera-Loayza, Ma. Carmen and Elizalde, Rene and Piedra, Nelson},
title = {Application of data anonymization in Learning Analytics},
year = {2020},
isbn = {9781450376303},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3378184.3378229},
doi = {10.1145/3378184.3378229},
abstract = {Thanks to the proliferation of academic services on the Web and the opening of educational content, today, students can access a large number of free learning resources, and interact with value-added services. In this context, Learning Analytics can be carried out on a large scale thanks to the proliferation of open practices that promote the sharing of datasets. However, the opening or sharing of data managed through platforms and educational services, without considering the protection of users' sensitive data, could cause some privacy issues. Data anonymization is a strategy that should be adopted during lifecycle of data processing to reduce security risks. In this research, we try to characterize how much and how the anonymization techniques have been used in learning analytics proposals. From an initial exploration made in the Scopus database, we found that less than 6% of the papers focused on LA have also covered the privacy issue. Finally, through a specific case, we applied data anonymization and learning analytics to demonstrate that both technique can be integrated, in a reliably and effectively way, to support decision making in educational institutions.},
booktitle = {Proceedings of the 3rd International Conference on Applications of Intelligent Systems},
articleno = {33},
numpages = {6},
keywords = {data anonymization, learning analytics, personal data, privacy},
location = {Las Palmas de Gran Canaria, Spain},
series = {APPIS 2020}
}

@inproceedings{10.1145/3706468.3706544,
author = {Yang, Kaixun and Rakovi\'{c}, Mladen and Liang, Zhiping and Yan, Lixiang and Zeng, Zijie and Fan, Yizhou and Ga\v{s}evi\'{c}, Dragan and Chen, Guanliang},
title = {Modifying AI, Enhancing Essays: How Active Engagement with Generative AI Boosts Writing Quality},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706544},
doi = {10.1145/3706468.3706544},
abstract = {Students are increasingly relying on Generative AI (GAI) to support their writing—a key pedagogical practice in education. In GAI-assisted writing, students can delegate core cognitive tasks (e.g., generating ideas and turning them into sentences) to GAI while still producing high-quality essays. This creates new challenges for teachers in assessing and supporting student learning, as they often lack insight into whether students are engaging in meaningful cognitive processes during writing or how much of the essay’s quality can be attributed to those processes. This study aimed to help teachers better assess and support student learning in GAI-assisted writing by examining how different writing behaviors, especially those indicative of meaningful learning versus those that are not, impact essay quality. Using a dataset of 1,445 GAI-assisted writing sessions, we applied the cutting-edge method, X-Learner, to quantify the causal impact of three GAI-assisted writing behavioral patterns (i.e., seeking suggestions but not accepting them, seeking suggestions and accepting them as they are, and seeking suggestions and accepting them with modification) on four measures of essay quality (i.e., lexical sophistication, syntactic complexity, text cohesion, and linguistic bias). Our analysis showed that writers who frequently modified GAI-generated text—suggesting active engagement in higher-order cognitive processes—consistently improved the quality of their essays in terms of lexical sophistication, syntactic complexity, and text cohesion. In contrast, those who often accepted GAI-generated text without changes, primarily engaging in lower-order processes, saw a decrease in essay quality. Additionally, while human writers tend to introduce linguistic bias when writing independently, incorporating GAI-generated text—even without modification—can help mitigate this bias.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {568–578},
numpages = {11},
keywords = {GAI-assisted Writing, Causal Inference, Writing Quality, Linguistic Bias},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3170358.3170409,
author = {Bodily, Robert and Kay, Judy and Aleven, Vincent and Jivet, Ioana and Davis, Dan and Xhakaj, Franceska and Verbert, Katrien},
title = {Open learner models and learning analytics dashboards: a systematic review},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170409},
doi = {10.1145/3170358.3170409},
abstract = {This paper aims to link student facing Learning Analytics Dashboards (LADs) to the corpus of research on Open Learner Models (OLMs), as both have similar goals. We conducted a systematic review of literature on OLMs and compared the results with a previously conducted review of LADs for learners in terms of (i) data use and modelling, (ii) key publication venues, (iii) authors and articles, (iv) key themes, and (v) system evaluation. We highlight the similarities and differences between the research on LADs and OLMs. Our key contribution is a bridge between these two areas as a foundation for building upon the strengths of each. We report the following key results from the review: in reports of new OLMs, almost 60% are based on a single type of data; 33% use behavioral metrics; 39% support input from the user; 37% have complex models; and just 6% involve multiple applications. Key associated themes include intelligent tutoring systems, learning analytics, and self-regulated learning. Notably, compared with LADs, OLM research is more likely to be interactive (81% of papers compared with 31% for LADs), report evaluations (76% versus 59%), use assessment data (100% versus 37%), provide a comparison standard for students (52% versus 38%), but less likely to use behavioral metrics, or resource use data (33% against 75% for LADs). In OLM work, there was a heightened focus on learner control and access to their own data.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {41–50},
numpages = {10},
keywords = {learning analytics dashboards, literature review, open learner models, open student models},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3386527.3406751,
author = {Pan, Zilong and Li, Chenglu and Liu, Min},
title = {Learning Analytics Dashboard for Problem-based Learning},
year = {2020},
isbn = {9781450379519},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3386527.3406751},
doi = {10.1145/3386527.3406751},
abstract = {This study examined two machine learning models for de- signing a learning analytics dashboard to assist teachers in facilitating problem-based learning. Specifically, we used BERT to automatically process a large amount of textual data to understand students' scientific argumentation. We then used Hidden Markov Model (HMM) to find students' cognitive state transition with time-series data. Preliminary results showed the models achieved high accuracy and were coherent with related theories, indicating the models can provide teachers with interpretable information to identify in-need students.},
booktitle = {Proceedings of the Seventh ACM Conference on Learning @ Scale},
pages = {393–396},
numpages = {4},
keywords = {artificial intelligence, learning analytics dashboard, machine learning, problem-based learning (pbl)},
location = {Virtual Event, USA},
series = {L@S '20}
}

@inproceedings{10.1145/3421590.3421662,
author = {Kesselbacher, Max and Wiltschnig, Kevin and Bollin, Andreas},
title = {Block-based learning analytics repository and dashboard: towards an interface between researcher and educator},
year = {2020},
isbn = {9781450387590},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3421590.3421662},
doi = {10.1145/3421590.3421662},
abstract = {The collection of programming session data is a cornerstone of programming learning analytics research. For text-based programming, there are data collection projects, like BlueJ's Blackbox, which provide access to the data and thereby facilitate additional research as well as verification. For block-based programming, only data sets of finished projects but not of programming sessions are available. We introduce a data repository that is extendable by implementing instrumentation plugins for various IDEs. The currently supported features are: data collection of text-based and block-based programming sessions, curated user self-registration and filtered data download. This then enables us to implement an educator dashboard in the future, making use of live programming session data to incorporate educators and students into learning analytics research.},
booktitle = {Proceedings of the 15th Workshop on Primary and Secondary Computing Education},
articleno = {33},
numpages = {2},
keywords = {programming education, learning analytics, data repository},
location = {Virtual Event, Germany},
series = {WiPSCE '20}
}

@inproceedings{10.1145/3448139.3448203,
author = {Hilliger, Isabel and Miranda, Constanza and Schuit, Gregory and Duarte, Fernando and Anselmo, Martin and Parra, Denis},
title = {Evaluating a Learning Analytics Dashboard to Visualize Student Self-Reports of Time-on-task: A Case Study in a Latin American University},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448203},
doi = {10.1145/3448139.3448203},
abstract = {In recent years, instructional design has become even more challenging for teaching staff members in higher education institutions. If instructional design causes student overload, it could lead to superficial learning and decreased student well-being. A strategy to avoid overload is reflecting upon the effectiveness of teaching practices in terms of time-on-task. This article presents a Work-In-Progress conducted to provide teachers with a dashboard to visualize student self-reports of time-on-task regarding subject activities. A questionnaire was applied to 15 instructors during a set trial period to evaluate the perceived usability and usefulness of the dashboard. Preliminary findings reveal that the dashboard helped instructors became aware about the number of hours spent outside of class time. Furthermore, data visualizations of time-on-task evidence enabled them to redesign subject activities. Currently, the dashboard has been adopted by 106 engineering instructors. Future work involves the development of a framework to incorporate user-based improvements.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {592–598},
numpages = {7},
keywords = {Time-on-task, Learning Analytics Dashboards, Instructional Design, Higher Education},
location = {Irvine, CA, USA},
series = {LAK21}
}

@article{10.5555/3606388.3606395,
author = {Chu, Chunbo and Li, Jiang},
title = {Learning Analytics Finds That a Shared Course May Improve Technology Students Retention},
year = {2023},
issue_date = {April 2023},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {38},
number = {6},
issn = {1937-4771},
abstract = {We investigated students' performance data in a Computer Science program by applying Learning Analytics to gain a better understanding of the retention issue. We found that students who have taken a soft introduction course in information systems shared by multiple technology majors tend to transfer to non-CS technology majors instead of transferring to non-technology majors or dropping out. Our research suggests that a holistic strategy on curricula design may improve retention.},
journal = {J. Comput. Sci. Coll.},
month = apr,
pages = {64–71},
numpages = {8}
}

@inproceedings{10.1145/3027385.3027408,
author = {Mouri, Kousuke and Ogata, Hiroaki and Uosaki, Noriko},
title = {Learning analytics in a seamless learning environment},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027408},
doi = {10.1145/3027385.3027408},
abstract = {This paper describes seamless learning analytics methods of VASCORLL (Visualization and Analysis System for COnnecting Relationships of Learning Logs). VASCORLL is a system for visualizing and analyzing the learning logs collected by the seamless learning system, which supports language learning in the real-world. As far, several studies have been made in the seamless learning environments in order to bridge formal learning over informal learning. However, their focus was the implementation of the seamless learning environment in education. This study focuses on visualizing and analyzing learning logs collected in the seamless learning environment. This paper describes how our analytics could contribute to bridging the gap between formal and informal learning. An experiment was conducted to evaluate 1) whether our developed VASCORLL is effective in connecting the words learned in formal learning to the ones learned in informal learning, 2) which social network algorithm is effective to enhance learning in the seamless learning environment. Twenty international students participated in the evaluation experiment, and they were able to increase their learning opportunities by using VASCORLL. In addition, it was found that the betweenness centrality is useful in finding central words bridging formal and informal learning.1},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {348–357},
numpages = {10},
keywords = {ubiquitous learning, seamless learning, learning analytics},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/2723576.2723637,
author = {Ferguson, Rebecca and Cooper, Adam and Drachsler, Hendrik and Kismih\'{o}k, G\'{a}bor and Boyer, Anne and Tammets, Kairit and Mon\'{e}s, Alejandra Mart\'{\i}nez},
title = {Learning analytics: European perspectives},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723637},
doi = {10.1145/2723576.2723637},
abstract = {Since the emergence of learning analytics in North America, researchers and practitioners have worked to develop an international community. The organization of events such as SoLAR Flares and LASI Locals, as well as the move of LAK in 2013 from North America to Europe, has supported this aim. There are now thriving learning analytics groups in North American, Europe and Australia, with smaller pockets of activity emerging on other continents. Nevertheless, much of the work carried out outside these forums, or published in languages other than English, is still inaccessible to most people in the community. This panel, organized by Europe's Learning Analytics Community Exchange (LACE) project, brings together researchers from five European countries to examine the field from European perspectives. In doing so, it will identify the benefits and challenges associated with sharing and developing practice across national boundaries.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {69–72},
numpages = {4},
keywords = {Europe, LACE project, education, international, learning, learning analytics},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/3491140.3528271,
author = {Li, Xiaotian Vivian and Rosson, Mary Beth and Robert, Jenay},
title = {A Scenario-based Exploration of Expected Usefulness, Privacy Concerns, and Adoption Likelihood of Learning Analytics},
year = {2022},
isbn = {9781450391580},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491140.3528271},
doi = {10.1145/3491140.3528271},
abstract = {Learning analytics has become a robust research area in the last decade, as innovative analytic models of learning data have been created with the goal of enhancing teaching and learning. However, barriers to large scale adoption of such technologies in higher education still exist. In recent years, a strand of research has begun to investigate stakeholders' expectations of learning analytics, hoping to find ways to integrate the innovations into everyday teaching practices. For instance, studies have investigated instructors' ideas about how learning analytics might be helpful, as well as concerns about student data privacy. However, most studies have taken a general approach rather than considering instructors' day-to-day experiences. Using survey methods, we presented instructors with hypothetical scenarios of learning analytics in use across disciplines, class sizes, teaching activities, and types of student data. We asked for ratings of both usefulness and privacy concerns for each proposed teaching situation. Our respondents considered scenarios involving learning outcomes-related data (e.g. grades) to be more useful than those that involve student interactions (e.g. language, social activity). In contrast, privacy concerns were lower for outcomes-oriented scenarios than interactions-focused scenarios. An interesting new finding was a negative correlation of usefulness and privacy; we discuss this in the context of instructors' possible cost-benefit reasoning. We reflect on our findings with respect to future efforts in developing and fielding learning analytics tools.},
booktitle = {Proceedings of the Ninth ACM Conference on Learning @ Scale},
pages = {48–59},
numpages = {12},
keywords = {adoption, design, higher education, instructors, learning analytics, privacy, scenarios, survey},
location = {New York City, NY, USA},
series = {L@S '22}
}

@inproceedings{10.1145/2883851.2883881,
author = {Kitto, Kirsty and Bakharia, Aneesha and Lupton, Mandy and Mallet, Dann and Banks, John and Bruza, Peter and Pardo, Abelardo and Buckingham Shum, Simon and Dawson, Shane and Ga\v{s}evi\'{c}, Dragan and Siemens, George and Lynch, Grace},
title = {The connected learning analytics toolkit},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883881},
doi = {10.1145/2883851.2883881},
abstract = {This demonstration introduces the Connected Learning Analytics (CLA) Toolkit. The CLA toolkit harvests data about student participation in specified learning activities across standard social media environments, and presents information about the nature and quality of the learning interactions.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {548–549},
numpages = {2},
keywords = {social learning analytics, sensemaking, dashboards},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/3144826.3145386,
author = {Conde, Miguel \'{A}. and Hern\'{a}ndez-Garc\'{\i}a, \'{A}ngel},
title = {Learning analytics: Expanding the frontier},
year = {2017},
isbn = {9781450353861},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3144826.3145386},
doi = {10.1145/3144826.3145386},
abstract = {1Learning analytics is one of the most active research topics in the field of education and educational technologies. Yet, research on learning analytics still has to address many topics related to the application of learning analytics techniques, such as competence-based learning, learning analytics-based interventions, cross-validation of extant research, standards for learning analytics, or learning analytics and instructional design. In addition, the emergence and growth of new technologies in educational contexts (e.g. smartphones, virtual and augmented reality, social networks) and the ever-changing nature of existing ones bring new challenges and opportunities for research and practice in learning analytics. This piece introduces the fifth edition of the track on Learning Analytics at Technological Ecosystems for Enhancing Multiculturality, an edition aiming to further expand the discipline. The document details the review process and provides an overview of the accepted submissions, and concludes with some reflections about learning analytics and remarks from the authors based on their experience from the past five years as Track Chairs},
booktitle = {Proceedings of the 5th International Conference on Technological Ecosystems for Enhancing Multiculturality},
articleno = {36},
numpages = {5},
keywords = {Predictive systems, Peer Assessment, Opportunities, Needs, Learning Analytics, Data Visualization, Competences, Competence-based Learning},
location = {C\'{a}diz, Spain},
series = {TEEM 2017}
}

@inproceedings{10.1145/3303772.3303803,
author = {Syed, Munira and Anggara, Trunojoyo and Lanski, Alison and Duan, Xiaojing and Ambrose, G. Alex and Chawla, Nitesh V.},
title = {Integrated Closed-loop Learning Analytics Scheme in a First Year Experience Course},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303803},
doi = {10.1145/3303772.3303803},
abstract = {Identifying non-thriving students and intervening to boost them are two processes that recent literature suggests should be more tightly integrated. We perform this integration over six semesters in a First Year Experience (FYE) course with the aim of boosting student success, by using an integrated closed-loop learning analytics scheme that consists of multiple steps broken into three main phases, as follows: Architecting for Collection (steps: design, build, capture), Analyzing for Action (steps: identify, notify, boost), and Assessing for Improvement (steps: evaluate, report). We close the loop by allowing later steps to inform earlier ones in real-time during a semester and iteratively year to year, thereby improving the course from data-driven insights. This process depends on the purposeful design of an integrated learning environment that facilitates data collection, storage, and analysis. Methods for evaluating the effectiveness of our analytics-based student interventions show that our criterion for identifying non-thriving students was satisfactory and that non-thriving students demonstrated more substantial changes from mid-term to final course grades than already-thriving students. Lastly, we make a case for using early performance in the FYE as an indicator of overall performance and retention of first-year students.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {521–530},
numpages = {10},
keywords = {learning analytics, intervention, first year seminars, first year experience, at-risk students, advising},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/2883851.2883925,
author = {Oster, Meghan and Lonn, Steven and Pistilli, Matthew D. and Brown, Michael G.},
title = {The learning analytics readiness instrument},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883925},
doi = {10.1145/2883851.2883925},
abstract = {Little is known about the processes institutions use when discerning their readiness to implement learning analytics. This study aims to address this gap in the literature by using survey data from the beta version of the Learning Analytics Readiness Instrument (LARI) [1]. Twenty-four institutions were surveyed and 560 respondents participated. Five distinct factors were identified from a factor analysis of the results: Culture; Data Management Expertise; Data Analysis Expertise; Communication and Policy Application; and, Training. Data were analyzed using both the role of those completing the survey and the Carnegie classification of the institutions as lenses. Generally, information technology professionals and institutions classified as Research Universities--Very High research activity had significantly different scores on the identified factors. Working within a framework of organizational learning, this paper details the concept of readiness as a reflective process, as well as how the implementation and application of analytics should be done so with ethical considerations in mind. Limitations of the study, as well as next steps for research in this area, are also discussed.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {173–182},
numpages = {10},
keywords = {survey design, reflection, readiness, learning analytics, higher education, ethics},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/3491140.3528292,
author = {Shabaninejad, Shiva and Khosravi, Hassan and Abdi, Solmaz and Indulska, Marta and Sadiq, Shazia},
title = {Incorporating Explainable Learning Analytics to Assist Educators with Identifying Students in Need of Attention},
year = {2022},
isbn = {9781450391580},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491140.3528292},
doi = {10.1145/3491140.3528292},
abstract = {Increased enrolments in higher education, and the shift to online learning that has been escalated by the recent COVID pandemic, have made it challenging for instructors to assist their students with their learning needs. Contributing to the growing literature on instructor-facing systems, this paper reports on the development of a learning analytics (LA) technique called Student Inspection Facilitator (SIF) that provides an explainable interpretation of students learning behaviour to support instructors with the identification of students in need of attention. Unlike many previous predictive systems that automatically label students, our approach provides explainable recommendations to guide data exploration while still reserving judgement about interpreting student learning to instructors. The insights derived from applying SIF in an introductory Information Systems course with 407 enrolled students suggest that SIF can be utilised independent of the context and can provide a meaningful interpretation of students' learning behaviour towards facilitating proactive support of students.},
booktitle = {Proceedings of the Ninth ACM Conference on Learning @ Scale},
pages = {384–388},
numpages = {5},
keywords = {at-risk students, explainable learning analytics, learning analytics dashboards},
location = {New York City, NY, USA},
series = {L@S '22}
}

@inproceedings{10.1145/2883851.2883878,
author = {Ferguson, Rebecca and Clow, Doug},
title = {Learning analytics community exchange: evidence hub},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883878},
doi = {10.1145/2883851.2883878},
abstract = {This poster sets out the background and development of the LACE Evidence Hub, a site that gathers evidence about learning analytics in an accessible form. The poster also describes the functionality of the site, summarises its quantitative and thematic content to date, and assesses the state of evidence. In addition, it encourages people to add to and make use of the Hub.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {520–521},
numpages = {2},
keywords = {teaching, take-up, learning analytics, learning, evidence, ethics},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/3636555.3636926,
author = {Nath, Debarshi and Gasevic, Dragan and Fan, Yizhou and Rajendran, Ramkumar},
title = {CTAM4SRL: A Consolidated Temporal Analytic Method for Analysis of Self-Regulated Learning},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636926},
doi = {10.1145/3636555.3636926},
abstract = {Temporality in Self-Regulated Learning (SRL) has two perspectives: one as a passage of time and the other as an ordered sequence of events. Each of these conceptions is distinct and requires independent considerations. Only a single analytic method is not sufficient in adequately capturing both these facets of temporality. Yet, most research uses a single method in temporally-focused SRL research, and those that use multiple methods do not address both aspects of temporality. We propose CTAM4SRL, a consolidated temporal analytic method which combines advanced data visualisation, network analysis and pattern mining to capture both facets of temporality. We employ CTAM4SRL in a cohort of 36 learners engaged in a reading-writing activity. Using CTAM4SRL, we were able to provide a rich temporal explanation of the interplay of the self-regulatory processes of the learners. We were further able to identify differences in SRL behaviours in high and low performers in terms of their approach to learning comprising deep and surface strategies. High performers were able to more selectively and strategically combine deep and surface learning strategies when compared to low scorers– a behaviour which was only hypothesised in SRL literature previously, but now has empirical support provided by our consolidated analytic method.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {645–655},
numpages = {11},
keywords = {Learning Analytics, Ordered Network Analysis, Pattern Mining, Self-Regulated Learning, Visualisation Techniques},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3170358.3170391,
author = {Milligan, Sandra K.},
title = {Methodological foundations for the measurement of learning in learning analytics},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170391},
doi = {10.1145/3170358.3170391},
abstract = {Learning analysts often claim to measure learning, but their work has attracted growing concern about whether or not the measures are sufficiently accurate, fair, reliable, and valid, with utility for educators and interpretable by them. This paper considers these issues in the light of practices of scholars in more established fields, educational measurement particularly. The focus is on what really matters about methodologies for measuring learning, including foundational assumptions about the nature of learning, what is understood by the term `measured', the criteria applied when assessing quality of data, the standards of proof required to establish validity, reliability, generalizability, utility and interpretability of findings, and assumptions about learners and learning underlying data modeling techniques used to abstract meaning from the data. This paper argues that, for learning analytics to take its place as a fully-fledged member of the learning sciences, it needs seriously to consider how to measure learning. Methodology crafted at the interface of measurement science and learning analytics may be of sufficient interest to create a new subfield of scholarship - dubbed here `metrilytics' - to make a distinctive contribution to the science of learning.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {466–470},
numpages = {5},
keywords = {validation, measurement of learning, learning analytics, IRT},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3641512.3690630,
author = {Geng, Jiaxiang and Tang, Beilong and Zhang, Boyan and Shao, Jiaqi and Luo, Bing},
title = {Demo: FedCampus: A Real-world Privacy-preserving Mobile Application for Smart Campus via Federated Learning &amp; Analytics},
year = {2024},
isbn = {9798400705212},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641512.3690630},
doi = {10.1145/3641512.3690630},
abstract = {In this demo, we introduce FedCampus, a privacy-preserving mobile application for smart campus with federated learning (FL) and federated analytics (FA). FedCampus enables cross-platform on-device FL/FA for both iOS and Android, supporting continuously models and algorithms deployment (MLOps). Our app integrates privacy-preserving processed data via differential privacy (DP) from smartwatches, where the processed parameters are used for FL/FA through the FedCampus backend platform. We distributed 100 smartwatches to volunteers at Duke Kunshan University and have successfully completed a series of smart campus tasks featuring capabilities such as sleep tracking, physical activity monitoring, personalized recommendations, and heavy hitters. Our project is opensourced at https://github.com/FedCampus/FedCampus_Flutter. See the FedCampus video at https://youtu.be/k5iu46IjA38.},
booktitle = {Proceedings of the Twenty-Fifth International Symposium on Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile Computing},
pages = {377–378},
numpages = {2},
keywords = {federated learning, federated analytics, privacy preserving},
location = {Athens, Greece},
series = {MobiHoc '24}
}

@inproceedings{10.1145/3579371.3589113,
author = {Ying, Ziyu and Bhuyan, Sandeepa and Kang, Yan and Zhang, Yingtian and Kandemir, Mahmut T. and Das, Chita R.},
title = {EdgePC: Efficient Deep Learning Analytics for Point Clouds on Edge Devices},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589113},
doi = {10.1145/3579371.3589113},
abstract = {Recently, point cloud (PC) has gained popularity in modeling various 3D objects (including both synthetic and real-life) and has been extensively utilized in a wide range of applications such as AR/VR, 3D reconstruction, and autonomous driving. For such applications, it is critical to analyze/understand the surrounding scenes properly. To achieve this, deep learning based methods (e.g., convolutional neural networks (CNNs)) have been widely employed for higher accuracy. Unlike the deep learning on conventional 2D images/videos, where the feature computation (matrix multiplication) is the major bottleneck, in point cloud-based CNNs, the sample and neighbor search stages are the primary bottlenecks, and collectively contribute to 54% (up to 80%) of the overall execution latency on a typical edge device. While prior efforts have attempted to solve this issue by designing custom ASICs or pipelining the neighbor search with other stages, to our knowledge, none of them has tried to "structurize" the unstructured PC data for improving computational efficiency.In this paper, we first explore the opportunities of structurizing PC data using Morton code (which is originally designed to map data from a high dimensional space to one dimension, while preserving spatial locality) and observe that there is a huge scope to "skip" the sample and neighbor search computation by operating on the "structurized" PC data. Based on this, we propose two approximation techniques for the sampling and neighbor search stages. We implemented our proposals on an NVIDIA Jetson AGX Xavier edge GPU board. The evaluation results collected on six different workloads show that our design can accelerate the sample and neighbor search stages by 3.68\texttimes{} (up to 5.21\texttimes{}) with minimal impact on inference accuracy. This acceleration in turn results in 1.55\texttimes{} speedup in the end-to-end execution latency and saves 33% of energy expenditure.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {78},
numpages = {14},
keywords = {edge device, energy-efficiency, approximation, deep neural network, point cloud},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3303772.3303796,
author = {Slade, Sharon and Prinsloo, Paul and Khalil, Mohammad},
title = {Learning analytics at the intersections of student trust, disclosure and benefit},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303796},
doi = {10.1145/3303772.3303796},
abstract = {Evidence suggests that individuals are often willing to exchange personal data for (real or perceived) benefits. Such an exchange may be impacted by their trust in a particular context and their (real or perceived) control over their data.Students remain concerned about the scope and detail of surveillance of their learning behavior, their privacy, their control over what data are collected, the purpose of the collection, and the implications of any analysis. Questions arise as to the extent to which students are aware of the benefits and risks inherent in the exchange of their data, and whether they are willing to exchange personal data for more effective and supported learning experiences.This study reports on the views of entry level students at the Open University (OU) in 2018. The primary aim is to explore differences between stated attitudes to privacy and their online behaviors, and whether these same attitudes extend to their university's uses of their (personal) data. The analysis indicates, inter alia, that there is no obvious relationship between how often students are online or their awareness of/concerns about privacy issues in online contexts and what they actually do to protect themselves. Significantly though, the findings indicate that students overwhelmingly have an inherent trust in their university to use their data appropriately and ethically.Based on the findings, we outline a number of issues for consideration by higher education institutions, such as the need for transparency (of purpose and scope), the provision of some element of student control, and an acknowledgment of the exchange value of information in the nexus of the privacy calculus.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {235–244},
numpages = {10},
keywords = {surveillance, privacy, informed consent, boundary management, Learning analytics},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3501712.3529742,
author = {Fernandez, Cassia and Freitas, Jo\~{a}o Adriano and Lopes, Roseli de Deus and Blikstein, Paulo},
title = {Using video analysis and learning analytics to understand programming trajectories in data science activities with Scratch},
year = {2022},
isbn = {9781450391979},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3501712.3529742},
doi = {10.1145/3501712.3529742},
abstract = {In this paper, we describe a new automated tool to analyze how students create their projects on Scratch 3.0, with the goal of understanding learning trajectories in a way that considers students’ programming processes and practices, moving beyond the analysis of computational thinking concepts as evidence of learning. Drawing on a combination of qualitative video analysis and temporal learning analytics, we also present preliminary data from a pilot study that illustrates some possibilities afforded by this type of analytical tool. We expect that our tool can help researchers to better understand learning in the context of data visualization activities with block-based programming languages by shedding light on processes that are usually invisible and, thus, better support students in their diverse learning pathways.},
booktitle = {Proceedings of the 21st Annual ACM Interaction Design and Children Conference},
pages = {253–260},
numpages = {8},
keywords = {temporal analysis, learning analytics, data science, computational thinking, Scratch},
location = {Braga, Portugal},
series = {IDC '22}
}

@inproceedings{10.1145/3027385.3027406,
author = {Prinsloo, Paul and Slade, Sharon},
title = {An elephant in the learning analytics room: the obligation to act},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027406},
doi = {10.1145/3027385.3027406},
abstract = {As higher education increasingly moves to online and digital learning spaces, we have access not only to greater volumes of student data, but also to increasingly fine-grained and nuanced data. A significant body of research and existing practice are used to convince key stakeholders within higher education of the potential of the collection, analysis and use of student data to positively impact on student experiences in these environments. Much of the recent focus in learning analytics is around predictive modeling and uses of artificial intelligence to both identify learners at risk, and to personalize interventions to increase the chance of success.In this paper we explore the moral and legal basis for the obligation to act on our analyses of student data. The obligation to act entails not only the protection of student privacy and the ethical collection, analysis and use of student data, but also, the effective allocation of resources to ensure appropriate and effective interventions to increase effective teaching and learning.The obligation to act is, however tempered by a number of factors, including inter and intra-departmental operational fragmentation and the constraints imposed by changing funding regimes. Increasingly higher education institutions allocate resources in areas that promise the greatest return. Choosing (not) to respond to the needs of specific student populations then raises questions regarding the scope and nature of the moral and legal obligation to act. There is also evidence that students who are at risk of failing often do not respond to institutional interventions to assist them.In this paper we build and expand on recent research by, for example, the LACE and EP4LA workshops to conceptually map the obligation to act which flows from both higher education's mandate to ensure effective and appropriate teaching and learning and its fiduciary duty to provide an ethical and enabling environment for students to achieve success. We examine how the collection and analysis of student data links to both the availability of resources and the will to act and also to the obligation to act. Further, we examine how that obligation unfolds in two open distance education providers from the perspective of a key set of stakeholders - those in immediate contact with students and their learning journeys - the tutors or adjunct faculty.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {46–55},
numpages = {10},
keywords = {obligation to act ethics, learning analytics},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3029437,
author = {Spikol, Daniel and Prieto, Luis P. and Rodr\'{\i}guez-Triana, M. J. and Worsley, Marcelo and Ochoa, Xavier and Cukurova, Mutlu and Vogel, Bahtijar and Ruffaldi, Emanuele and Ringtved, Ulla Lunde},
title = {Current and future multimodal learning analytics data challenges},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029437},
doi = {10.1145/3027385.3029437},
abstract = {Multimodal Learning Analytics (MMLA) captures, integrates and analyzes learning traces from different sources in order to obtain a more holistic understanding of the learning process, wherever it happens. MMLA leverages the increasingly widespread availability of diverse sensors, high-frequency data collection technologies and sophisticated machine learning and artificial intelligence techniques. The aim of this workshop is twofold: first, to expose participants to, and develop, different multimodal datasets that reflect how MMLA can bring new insights and opportunities to investigate complex learning processes and environments; second, to collaboratively identify a set of grand challenges for further MMLA research, built upon the foundations of previous workshops on the topic.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {518–519},
numpages = {2},
keywords = {multimodal learning analytics, datasets, challenges},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3303772.3303804,
author = {Lim, Lisa and Dawson, Shane and Joksimovic, Srecko and Ga\v{s}evi\'{c}, Dragan},
title = {Exploring students' sensemaking of learning analytics dashboards: Does frame of reference make a difference?},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303804},
doi = {10.1145/3303772.3303804},
abstract = {Learning Analytics Dashboards (LAD) are becoming an increasingly popular way to provide students with personalised feedback. Despite the number of LADs being developed, significant research gaps exist around the student perspective, especially how students make sense of graphics provided in LADs, and how they intend to act on the feedback provided therein. This study employed a randomized-controlled trial to examine students' sense-making of LADs showing four different frames of reference, and to what extent the impact of LADs was mediated by baseline self-regulation. Using a mix of quantitative and qualitative data analysis, the results revealed rather distinct patterns in students' sense-making across the four LADs. These patterns involved the intersection of visual salience and planned learning actions. However, collectively, across all four LADs a consistent theme emerged around students planned learning actions. This theme was classified as time and study environment management. A key finding of the study is that the use of LADs as a primary feedback process should be personalized and include training and support to aid student sensemaking.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {250–259},
numpages = {10},
keywords = {social comparison, sensemaking, epistemic network analysis, Learning dashboards},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3027385.3029472,
author = {Olivares, Daniel M. and Hundhausen, Christopher D.},
title = {Supporting learning analytics in computing education},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029472},
doi = {10.1145/3027385.3029472},
abstract = {As is the case for many undergraduate STEM degree programs, computing degree programs are plagued by high attrition rates. This is especially true in early computing courses, in which failure and drop-out rates in the 35 to 50 percent range are common. By collecting learning process data as students engage in computer programming assignments, computing educators can place themselves in a position not only to better understand students' struggles, but also to better tailor instructional interventions to students' needs. We have developed OSBLE+, a learning management and analytics environment that interfaces with a computer programming environment to support the automatic collection of learners' programming process and social data as they work on programming assignments, while also providing an interactive environment for the analysis and visualization of those data. In ongoing work, we are using OSBLE+ to explore two possibilities: (a) leveraging learning and social data to strategically deliver automated learning interventions, and (b) presenting learners with visual representations of their learning data in order to prompt them to reflect on and discuss their learning processes.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {584–585},
numpages = {2},
keywords = {visualizations, social learning, learning management system, learning analytics, data collection, computing education research, OSBLE},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/2723576.2723627,
author = {Kitto, Kirsty and Cross, Sebastian and Waters, Zak and Lupton, Mandy},
title = {Learning analytics beyond the LMS: the connected learning analytics toolkit},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723627},
doi = {10.1145/2723576.2723627},
abstract = {We present a Connected Learning Analytics (CLA) toolkit, which enables data to be extracted from social media and imported into a Learning Record Store (LRS), as defined by the new xAPI standard. A number of implementation issues are discussed, and a mapping that will enable the consistent storage and then analysis of xAPI verb/object/activity statements across different social media and online environments is introduced. A set of example learning activities are proposed, each facilitated by the Learning Analytics beyond the LMS that the toolkit enables.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {11–15},
numpages = {5},
keywords = {xAPI, integration, data ownership, connected learning},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/3303772.3303834,
author = {Fiallos, Angel and Ochoa, Xavier},
title = {Semi-Automatic Generation of Intelligent Curricula to Facilitate Learning Analytics},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303834},
doi = {10.1145/3303772.3303834},
abstract = {Several Learning Analytics applications are limited by the cost of generating a computer understandable description of the course domain, what is called an Intelligent Curriculum. The following work contributes a novel approach to (semi-)automatically generate Intelligent Curriculum through ontologies extracted from existing learning materials such as digital books or web content. Through a series of natural language processing steps, the semi-structured information present in existing content is transformed into a concept-graph. This work also evaluates the proposed methodology by applying it to learning content for two different courses and measuring the quality of the extracted ontologies against manually generated ones. The results obtained suggest that the technique can be readily used to provide domain information to other Learning Analytics tools.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {46–50},
numpages = {5},
keywords = {ontologies, intelligent curriculum, NLP},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3027385.3029476,
author = {Fortenbacher, Albrecht and Pinkwart, Niels and Yun, Haeseon},
title = {[LISA] learning analytics for sensor-based adaptive learning},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029476},
doi = {10.1145/3027385.3029476},
abstract = {This paper reports on research conducted in a project named LISA which aims at supporting learners through learner-centered learning analytics using physiological sensor data as well as environmental sensors. We present the concept and a prototypical realization of a mobile sensor device used in LISA.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {592–593},
numpages = {2},
keywords = {sensor based learning, self-regulated learning, learner-centric learning analytics, learner awareness, adaptive learning},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3286689.3286693,
author = {Gilliot, Jean-Marie and Iksal, S\'{e}bastien and Medou, Daniel Magloire and Dabbebi, In\`{e}s},
title = {Participatory design of learning analytics dashboards},
year = {2018},
isbn = {9781450360784},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3286689.3286693},
doi = {10.1145/3286689.3286693},
abstract = {While the field of Learning Analytics is in full development, potential users are just discovering the opportunities offered by these tools. One of the major difficulties consists in proposing a visual data representation which makes sense to the users. After refining our method to identify user needs in terms of visualization, and to define the main dimensions of a learning dashboard, we propose a tool to support participatory design. This tool is based on a canvas and cards to help dashboards' creation using Learning Analytics. It allows users to support creativity around decision making, characterize their context, and draw a dashboard's mockup using existing content.},
booktitle = {Proceedings of the 30th Conference on l'Interaction Homme-Machine},
pages = {119–127},
numpages = {9},
keywords = {participatory design, learning analytics, dashboard, canvas},
location = {Brest, France},
series = {IHM '18}
}

@inproceedings{10.1145/3382507.3421151,
author = {Ciordas-Hertel, George-Petru},
title = {How to Complement Learning Analytics with Smartwatches? Fusing Physical Activities, Environmental Context, and Learning Activities},
year = {2020},
isbn = {9781450375818},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382507.3421151},
doi = {10.1145/3382507.3421151},
abstract = {To obtain a holistic perspective on learning, a multimodal technical infrastructure for Learning Analytics (LA) can be beneficial. Recent studies have investigated various aspects of technical LA infrastructure. However, it has not yet been explored how LA indicators can be complemented with Smartwatch sensor data to detect physical activity and the environmental context. Sensor data, such as the accelerometer, are often used in related work to infer a specific behavior and environmental context, thus triggering interventions on a just-in-time basis. In this dissertation project, we plan to use Smartwatch sensor data to explore further indicators for learning from blended learning sessions conducted in-the-wild, e.g., at home. Such indicators could be used within learning sessions to suggest breaks, or afterward to support learners in reflection processes.We plan to investigate the following three research questions: (RQ1) How can multimodal learning analytics infrastructure be designed to support real-time data acquisition and processing effectively?; (RQ2) how to use smartwatch sensor data to infer environmental context and physical activities to complement learning analytics indicators for blended learning sessions; and (RQ3) how can we align the extracted multimodal indicators with pedagogical interventions.RQ1 was investigated by a structured literature review and by conducting eleven semi-structured interviews with LA infrastructure developers. According to RQ2, we are currently designing and implementing a multimodal learning analytics infrastructure to collect and process sensor and experience data from Smartwatches. Finally, according to RQ3, an exploratory field study will be conducted to extract multimodal learning indicators and examine them with learners and pedagogical experts to develop effective interventions.Researchers, educators, and learners can use and adapt our contributions to gain new insights into learners' time and learning tactics, and physical learning spaces from learning sessions taking place in-the-wild.},
booktitle = {Proceedings of the 2020 International Conference on Multimodal Interaction},
pages = {708–712},
numpages = {5},
keywords = {smartwatch, multimodal learning analytics, mobile sensing, infrastructure, hand activities, environmental context},
location = {Virtual Event, Netherlands},
series = {ICMI '20}
}

@inproceedings{10.1145/3472410.3472442,
author = {Schicchi, Daniele and Marino, Benedetto and Taibi, Davide},
title = {Exploring Learning Analytics on YouTube: a tool to support students’ interactions analysis},
year = {2021},
isbn = {9781450389822},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472410.3472442},
doi = {10.1145/3472410.3472442},
abstract = {YouTube is a free online video-sharing platform that is often used by students for their learning activities. The interactions of the students when using the platform to shape new concepts, are worth to be investigated to better understand and to optimize the learning opportunities that take place in this platform. In this paper, we investigate which types of data are relevant to analyse the interactions of students with content on YouTube, and we introduce a new tool that emulates students’ interactions with the platform in order to provide data to be used in supporting Learning Analytics approaches. Our preliminary study inspects the tool effectiveness in data collection and analyses the effects of the YouTube recommendation system in students’ activities. We also identify methodologies based on statistical indexes and social network analysis that can be adopted to analyse students’ experiences. Results show how the YouTube recommendation system plays a critical role in affecting the student learning trajectory.},
booktitle = {Proceedings of the 22nd International Conference on Computer Systems and Technologies},
pages = {207–211},
numpages = {5},
keywords = {education, dataset, data collection, YouTube, Learning Analytics},
location = {Ruse, Bulgaria},
series = {CompSysTech '21}
}

@inproceedings{10.1145/2883851.2883860,
author = {Ley, Tobias and Klamma, Ralf and Lindstaedt, Stefanie and Wild, Fridolin},
title = {Learning analytics for workplace and professional learning},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883860},
doi = {10.1145/2883851.2883860},
abstract = {Recognizing the need for addressing the rather fragmented character of research in this field, we have held a workshop on learning analytics for workplace and professional learning at the Learning Analytics and Knowledge (LAK) Conference. The workshop has taken a broad perspective, encompassing approaches from a number of previous traditions, such as adaptive learning, professional online communities, workplace learning and performance analytics. Being co-located with the LAK conference has provided an ideal venue for addressing common challenges and for benefiting from the strong research on learning analytics in other sectors that LAK has established. Learning Analytics for Workplace and Professional Learning is now on the research agenda of several ongoing EU projects, and therefore a number of follow-up activities are planned for strengthening integration in this emerging field.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {484–485},
numpages = {2},
keywords = {workplace learning, learning analytics},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883913,
author = {Ochoa, Xavier and Worsley, Marcelo and Weibel, Nadir and Oviatt, Sharon},
title = {Multimodal learning analytics data challenges},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883913},
doi = {10.1145/2883851.2883913},
abstract = {This is a proposal for organizing a Multimodal Learning Analytics (MLA) data challenge as part of the workshop offering of the Learning Analytics and Knowledge (LAK) conference. It explains the motivation of the event, its objectives, target groups, expected format, organization, dissemination strategy and schedule.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {498–499},
numpages = {2},
keywords = {multimodal datasets, multimodal, data challenge},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/3491140.3528324,
author = {Tulha, Carinna Nunes and Carvalho, Marco Antonio G. and de Castro, Leandro N.},
title = {LEDA: A Learning Analytics Based Framework to Analyze Remote Labs Interaction},
year = {2022},
isbn = {9781450391580},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491140.3528324},
doi = {10.1145/3491140.3528324},
abstract = {In order to support the application of digital technologies in the educational context, it is important to understand the relationships between learners and the technologies they use during a practice. Among educational technologies are remote laboratories, tools that provide the manipulation of real experiments through an online platform, available 24/7, overcoming constraints of time and space. To extract information of the large amount of data generated during interactions, it is necessary to use technology-supported representations, in order to apply techniques capable of analyzing and extracting information from the data generated from interactions with technologies and, finally, enabling learning interventions. Learning Analytics (LA) consist of on measuring, collecting, analyzing and reporting student data during practices. LA combines data mining techniques to extract information and pedagogical intervention. This project proposes to develop an educational data mining framework based on Learning Analytics interventions, called LEDA (Laboratory Experimentation Data Analysis). The LEDA framework aims to extract information of interaction data with remote laboratories to relate students' interaction behavior with their learning progress. Our approach will apply association rules and clustering techniques using learning data, including clicks, number of controlled components, and time spent during the activity.},
booktitle = {Proceedings of the Ninth ACM Conference on Learning @ Scale},
pages = {379–383},
numpages = {5},
keywords = {association rules, clustering, data mining, educational datasets},
location = {New York City, NY, USA},
series = {L@S '22}
}

@inproceedings{10.1145/3027385.3027403,
author = {Bodily, Robert and Verbert, Katrien},
title = {Trends and issues in student-facing learning analytics reporting systems research},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027403},
doi = {10.1145/3027385.3027403},
abstract = {We conducted a literature review on systems that track learning analytics data (e.g., resource use, time spent, assessment data, etc.) and provide a report back to students in the form of visualizations, feedback, or recommendations. This review included a rigorous article search process; 945 articles were identified in the initial search. After filtering out articles that did not meet the inclusion criteria, 94 articles were included in the final analysis. Articles were coded on five categories chosen based on previous work done in this area: functionality, data sources, design analysis, perceived effects, and actual effects. The purpose of this review is to identify trends in the current student-facing learning analytics reporting system literature and provide recommendations for learning analytics researchers and practitioners for future work.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {309–318},
numpages = {10},
keywords = {student-facing systems, literature review, learning analytics dashboards, learning analytics, educational recommender systems},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/2883851.2883874,
author = {van Leeuwen, Anouschka},
title = {Learning analytics in a flipped university course},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883874},
doi = {10.1145/2883851.2883874},
abstract = {In this poster, we describe the design of a university course with a blended learning character. Learning analytics were used both within the course to facilitate effective teacher-student interaction, as well as after the course to examine patterns between students' activities during the course and their performance on the test and the group assignment at the end of the course.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {514–515},
numpages = {2},
keywords = {web lectures, learning analytics, higher education, formative assessment, blended learning},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/3568739.3568775,
author = {Che Hussin, Sabariha and Kew, Si Na and Tran, Tich Phuoc and Tasir, Zaidatun and Koh, Tieng Wei},
title = {Improving English Communication Skills in Online Learning Through the Utilization of Feedback Provision and Learning Analytics: A Conceptual Framework},
year = {2023},
isbn = {9781450398091},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568739.3568775},
doi = {10.1145/3568739.3568775},
abstract = {The emergence of educational technological tools and applications have changed the ways of teaching and learning among students and teachers in online learning, especially with the sudden outbreak of COVID-19 all over the place, forcing the educational sector to completely be shifted from traditional classroom to online learning. However, despite the rise of the educational technologies in online learning setting, the problem of English communication skills especially in written form faced by students is being emphasized as one of the urgent matters that requests more attentions and solutions. The problem also includes the students lost their motivation to communicate in English language, in online learning setting particularly. Through literature, it is found that the implementation of feedback provision and the utilization of learning analytics are regarded as one of the effective ways in improving students’ written communication skills and motivation to communicate in online learning setting. However, very limited studies focusing on such area are found in language education. As a result, it is necessary to propose a conceptual framework of implementation of both feedback provision and learning analytics in language education context to improve English communication skills in online learning. Therefore, this study is intended to provide a conceptual framework on improving English communication skills in online learning through the utilization of feedback provision and learning analytics. This could serve as a roadmap for future research into this topic. Furthermore, the ways to implement this framework and the implication of the framework are also discussed in this paper that can contribute for literatures.},
booktitle = {Proceedings of the 6th International Conference on Digital Technology in Education},
pages = {213–218},
numpages = {6},
keywords = {online learning, motivation, learning analytics, feedback provision, communication skills},
location = {Hangzhou, China},
series = {ICDTE '22}
}

@inproceedings{10.1145/3284179.3284231,
author = {Forment, Marc Alier and Filv\`{a}, Daniel Amo and Garc\'{\i}a-Pe\~{n}alvo, Francisco Jos\'{e} and Escudero, David Fonseca and Casa\~{n}, Mar\'{\i}a Jos\'{e}},
title = {Learning Analytics' Privacy on the Blockchain},
year = {2018},
isbn = {9781450365185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3284179.3284231},
doi = {10.1145/3284179.3284231},
abstract = {Learning Analytics collect sensitive data from students. In some cases, the ethics behind the use or access by third parties are not clear. This situation raises adverse reactions and feelings of fear that generates a negative perception towards the use of Learning Analytics. In consequence, it is questioned whether privacy and security can be preserved when collecting educational data. As a result, some policies and good practices are set to frame how student data should be used in the application of this analytical approach. Its objective is to increase confidence in the application of Learning Analytics. However, some of these legal actions, which are limited to the areas in which they are originated, are the result of allegations of data leakage. Hence, these initiatives can do little to insure the use of sensitive data of students in unknown situations. To ensure continuity and an increase of confidence in the application of Learning Analytics is necessary to bind to legality a new approach to safeguard privacy of students' data in its current and future uses. Considering the above, it is possible to add a technological layer above these policies that ensures its viability. Some emerging technologies such as blockchain and smart contracts are strong candidates to ensure privacy and secure sensible data of students. The use of smart contracts allows the automation of legal actions so that they are executed as soon as irregularities in the use or data collection are detected. In this work, we propose a series of actions to preserve the identity of students and secure their data with emerging technologies such as blockchain.},
booktitle = {Proceedings of the Sixth International Conference on Technological Ecosystems for Enhancing Multiculturality},
pages = {294–298},
numpages = {5},
keywords = {digital identity, data security management, data privacy, blockchain, Learning analytics},
location = {Salamanca, Spain},
series = {TEEM'18}
}

@inproceedings{10.1145/3436756.3437032,
author = {Islam, Shareeful and Mahmud, Hasan},
title = {An Intelligence Learner Management System using Learning Analytics and Machine learning},
year = {2021},
isbn = {9781450388276},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3436756.3437032},
doi = {10.1145/3436756.3437032},
abstract = {Learner Management System (LMS) facilitates educational institutions to offer e-learning through web-based applications. LMS provides many benefits from cost saving to flexible learning opportunities independent of any location with cloud-based deployment. Hence, LMS organizes learning data and learners detail in a central repository, helps to improve resource allocation, and facilitates access to the learning resources. These benefits drive the LMS market growth at a rapid rate and it is now deployed across the industry of any size. Despite of these significant benefits of using LMS, the traditional LMS system cannot fully support with modern learning needs in terms of learners' progression, retention rate, prediction of assessment outcomes to improve overall teaching and learning experience. Learning analytics can effectively support for a better learning experience by analyzing and correlating learner data to predicate the future needs. This work as a part of the Knowledge Transfer Project (KTP) develops an intelligence Learner Management System(iLMS) that integrates learning analytics into the learner management system. We use Machine Learning(ML) techniques for descriptive, predictive, and prescriptive analytics of learners’ data. This paper presents the key iLMS features including user interfaces, reports and learning analytics.},
booktitle = {Proceedings of the 12th International Conference on Education Technology and Computers},
pages = {120–125},
numpages = {6},
keywords = {Predictive Analytics, Machine Learning, Learning Management System, Learning Analytics, Decision Tree},
location = {London, United Kingdom},
series = {ICETC '20}
}

@inproceedings{10.1145/3027385.3027407,
author = {Fu, Xinyu and Shimada, Atsushi and Ogata, Hiroaki and Taniguchi, Yuta and Suehiro, Daiki},
title = {Real-time learning analytics for C programming language courses},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027407},
doi = {10.1145/3027385.3027407},
abstract = {Many universities choose the C programming language (C) as the first one they teach their students, early on in their program. However, students often consider programming courses difficult, and these courses often have among the highest dropout rates of computer science courses offered. It is therefore critical to provide more effective instruction to help students understand the syntax of C and prevent them losing interest in programming. In addition, homework and paper-based exams are still the main assessment methods in the majority of classrooms. It is difficult for teachers to grasp students' learning situation due to the large amount of evaluation work. To facilitate teaching and learning of C, in this article we propose a system---LAPLE (Learning Analytics in Programming Language Education)---that provides a learning dashboard to capture the behavior of students in the classroom and identify the different difficulties faced by different students looking at different knowledge. With LAPLE, teachers may better grasp students' learning situation in real time and better improve educational materials using analysis results. For their part, novice undergraduate programmers may use LAPLE to locate syntax errors in C and get recommendations from educational materials on how to fix them.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {280–288},
numpages = {9},
keywords = {programming education, learning dashboard, learning analytics, information visualization, C programming},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3012430.3012533,
author = {Hern\'{a}ndez-Garc\'{\i}a, \'{a}ngel and Conde, Miguel \'{A}.},
title = {Learning analytics: needs and opportunities},
year = {2016},
isbn = {9781450347471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3012430.3012533},
doi = {10.1145/3012430.3012533},
abstract = {Learning analytics is not a trend anymore. It is a must in education. Whenever we hear about the use of information technologies in education, the term learning analytics is mentioned. However, learning usually comes more as an afterthought than a core part of educational processes involving the use of IT. Think about how many times have you heard the sentence "... and, of course, we provide learning analytics with it" in the past couple of years. Right?Yet, despite the increasing amount of theoretical and applied research on learning analytics, there are still needs that are practically left unattended in most recent research. For example, the role of learning analytics in the current paradigm of student-centred and competence-based learning, information about successful learning analytics-based interventions, cross-validation of prior research, standards for learning analytics, or learning analytics and instructional design. In addition, the emergence and growth of new technologies in educational contexts (e.g. smartphones, virtual and augmented reality, social networks), and the ever changing nature of existing ones, bring new challenges and opportunities for research and practice in learning analytics.},
booktitle = {Proceedings of the Fourth International Conference on Technological Ecosystems for Enhancing Multiculturality},
pages = {309–312},
numpages = {4},
keywords = {predictive systems, peer assessment, opportunities, needs, learning analytics, data visualization, competences, competence-based learning},
location = {Salamanca, Spain},
series = {TEEM '16}
}

@inproceedings{10.1145/3170358.3170385,
author = {Tempelaar, Dirk and Rienties, Bart and Nguyen, Quan},
title = {Investigating learning strategies in a dispositional learning analytics context: the case of worked examples},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170385},
doi = {10.1145/3170358.3170385},
abstract = {This study aims to contribute to recent developments in empirical studies on students' learning strategies, whereby the use of trace data is combined with self-report data to distinguish profiles of learning strategy use [3--5]. We do so in the context of an application of dispositional learning analytics in a large introductory course mathematics and statistics, based on blended learning. Building on our previous work which showed marked differences in how students used worked examples as a learning strategy [7, 11], this study compares different profiles of learning strategies with learning approaches, learning outcomes, and learning dispositions. One of our key findings is that deep learners were less dependent on worked examples as a resource for learning, and that students who only sporadically used worked examples achieved higher test scores.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {201–205},
numpages = {5},
keywords = {technology enhanced learning, prediction models, learning strategies, dispositional learning analytics, blended learning},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3170358.3170387,
author = {Fiorini, Stefano and Sewell, Adrienne and Bumbalough, Mathew and Chauhan, Pallavi and Shepard, Linda and Rehrey, George and Groth, Dennis},
title = {An application of participatory action research in advising-focused learning analytics},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170387},
doi = {10.1145/3170358.3170387},
abstract = {Advisors assist students in developing successful course pathways through the curriculum. The purpose of this project is to augment advisor institutional and tacit knowledge with knowledge from predictive algorithms (i.e., Matrix Factorization and Classifiers) specifically developed to identify risk. We use a participatory action research approach that directly involves key members from both advising and research communities in the assessment and provisioning of information from the predictive analytics. The knowledge gained from predictive algorithms is evaluated using a mixed method approach. We first compare the predictive evaluations with advisors evaluations of student performance in courses and actual outcomes in those courses We next expose and classify advisor knowledge of student risk and identify ways to enhance the value of the prediction model. The results highlight the contribution that this collaborative approach can give to the constructive integration of Learning Analytics in higher education settings.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {89–96},
numpages = {8},
keywords = {participatory approaches to learning analytics, intervention evaluation, advising},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3397453.3397456,
author = {Jiang, Xuefeng and Liu, Wenbo and Liu, Junrui},
title = {Learning Analytics in a Blended Computer Education Course},
year = {2020},
isbn = {9781450376235},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397453.3397456},
doi = {10.1145/3397453.3397456},
abstract = {Massive open online courses (MOOCs) have been widely used and many institutions have invested considerable effort in developing, promoting and delivering such courses. Online educational pattern, both inside and outside of the campus community, have been become more popular. In recent years, a new research agenda has emerged, focusing on predicting and explaining dropouts and MOOCs with low completion rates. However, due to different problem specifications and evaluation metrics, performing learning analytics (LA) of online education is a challenging task. The online learner has a variety of purposes, such as complete learning courses, some knowledge points in work-based learning courses, selective learning for reviewing exams, certificate-based learning, and watching videos for interest. According to users'online behavior data, it is difficult to study why so many people drop out of MOOCs. Because users have many reasons, such as not understanding, bad network connection, other things to do, and so on. This paper study was performed to analyze data of students' online activity in a blended computer education course in Northwestern Polytechnical University (NPU) to identify quantitative markers that correlate with students' performance and might be used as early warning signs for possible data-driven measures. We applied the research methodology to three online courses offered generously by the Chinese University MOOC (CUMOOC) to evaluate the effectiveness.},
booktitle = {Proceedings of the International Workshop on Artificial Intelligence and Education},
pages = {6–12},
numpages = {7},
keywords = {Prediction, MOOC, Learning Management System, Learning Analytics},
location = {Singapore, Singapore},
series = {WAIE 2019}
}

@inproceedings{10.1145/3501709.3544299,
author = {Smith, Julie M.},
title = {Using Learning Analytics to Interrogate Learning Theories: An Exploration of How Students Learn to Program},
year = {2022},
isbn = {9781450391955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3501709.3544299},
doi = {10.1145/3501709.3544299},
abstract = {A variety of approaches have been used in computer science education research; one relatively new addition is the use of very large data sets. This dissertation will use one of these data sets in order to investigate the process by which novice students learn to program. Specifically, an iterative approach will be used to determine which learning theories are (or are not) supported by the data. The focus will be on which patterns of behavior do (or do not) lead a student to learning a programming concept.},
booktitle = {Proceedings of the 2022 ACM Conference on International Computing Education Research - Volume 2},
pages = {17–18},
numpages = {2},
keywords = {novice programmers, learning theory, learning analytics, CS1},
location = {Lugano and Virtual Event, Switzerland},
series = {ICER '22}
}

@inproceedings{10.1145/3506860.3506904,
author = {Lin, Jionghao and Rakovic, Mladen and Lang, David and Gasevic, Dragan and Chen, Guanliang},
title = {Exploring the Politeness of Instructional Strategies from Human-Human Online Tutoring Dialogues},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506904},
doi = {10.1145/3506860.3506904},
abstract = {Existing research indicates that students prefer to work with tutors who express politely in online human-human tutoring, but excessive polite expressions might lower tutoring efficacy. However, there is a shortage of understanding about the use of politeness in online tutoring and the extent to which the politeness of instructional strategies can contribute to students’ achievement. To address these gaps, we conducted a study on a large-scale dataset (5,165 students and 116 qualified tutors in 18,203 online tutoring sessions) of both effective and ineffective human-human online tutorial dialogues. The study made use of a well-known dialogue act coding scheme to identify instructional strategies, relied on the linguistic politeness theory to analyse the politeness levels of the tutors’ instructional strategies, and utilised Gradient Tree Boosting to evaluate the predictive power of these politeness levels in revealing students’ problem-solving performance. The results demonstrated that human tutors used both polite and non-polite expressions in the instructional strategies. Tutors were inclined to express politely in the strategy of providing positive feedback but less politely while providing negative feedback and asking questions to evaluate students’ understanding. Compared to the students with prior progress, tutors provided more polite open questions to the students without prior progress but less polite corrective feedback. Importantly, we showed that, compared to previous research, the accuracy of predicting student problem-solving performance can be improved by incorporating politeness levels of instructional strategies with other documented predictors (e.g., the sentiment of the utterances).},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {282–293},
numpages = {12},
keywords = {Student Performance, Prediction, Politeness, Learning Analytics, Educational Dialogue Analysis},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3576050.3576119,
author = {Li, Lin and Sha, Lele and Li, Yuheng and Rakovi\'{c}, Mladen and Rong, Jia and Joksimovic, Srecko and Selwyn, Neil and Ga\v{s}evi\'{c}, Dragan and Chen, Guanliang},
title = {Moral Machines or Tyranny of the Majority? A Systematic Review on Predictive Bias in Education},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576119},
doi = {10.1145/3576050.3576119},
abstract = {Machine Learning (ML) techniques have been increasingly adopted to support various activities in education, including being applied in important contexts such as college admission and scholarship allocation. In addition to being accurate, the application of these techniques has to be fair, i.e., displaying no discrimination towards any group of stakeholders in education (mainly students and instructors) based on their protective attributes (e.g., gender and age). The past few years have witnessed an explosion of attention given to the predictive bias of ML techniques in education. Though certain endeavors have been made to detect and alleviate predictive bias in learning analytics, it is still hard for newcomers to penetrate. To address this, we systematically reviewed existing studies on predictive bias in education, and a total of 49 peer-reviewed empirical papers published after 2010 were included in this study. In particular, these papers were reviewed and summarized from the following three perspectives: (i) protective attributes, (ii) fairness measures and their applications in various educational tasks, and (iii) strategies for enhancing predictive fairness. These findings were summarized into recommendations to guide future endeavors in this strand of research, e.g., collecting and sharing more quality data containing protective attributes, developing fairness-enhancing approaches which do not require the explicit use of protective attributes, validating the effectiveness of fairness-enhancing on students and instructors in real-world settings.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {499–508},
numpages = {10},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3027385.3029427,
author = {Bergner, Yoav and Lang, Charles and Gray, Geraldine},
title = {Workshop on methodology in learning analytics (MLA)},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029427},
doi = {10.1145/3027385.3029427},
abstract = {Learning analytics is an interdisciplinary and inclusive field, a fact which makes the establishment of methodological norms both challenging and important. This community-building workshop intends to convene methodology-focused researchers to discuss new and established approaches, comment on the state of current practice, author pedagogical manuscripts, and co-develop guidelines to help move the field forward with quality and rigor.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {500–501},
numpages = {2},
keywords = {statistics, models, methodology, measurement, evaluation},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3027448,
author = {Corrin, Linda and de Barba, Paula G. and Bakharia, Aneesha},
title = {Using learning analytics to explore help-seeking learner profiles in MOOCs},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027448},
doi = {10.1145/3027385.3027448},
abstract = {In online learning environments, learners are often required to be more autonomous in their approach to learning. In scaled online learning environments, like Massive Open Online Courses (MOOCs), there are differences in the ability of learners to access teachers and peers to get help with their study than in more traditional educational environments. This exploratory study examines the help-seeking behaviour of learners across several MOOCs with different audiences and designs. Learning analytics techniques (e.g., dimension reduction with t-sne and clustering with affinity propagation) were applied to identify clusters and determine profiles of learners on the basis of their help-seeking behaviours. Five help-seeking learner profiles were identified which provide an insight into how learners' help-seeking behaviour relates to performance. The development of a more in-depth understanding of how learners seek help in large online learning environments is important to inform the way support for learners can be incorporated into the design and facilitation of online courses delivered at scale.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {424–428},
numpages = {5},
keywords = {learning design, learning analytics, help-seeking, MOOCs},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3573051.3596165,
author = {Ochoa, Xavier and Echeverria, Vanessa and Carrillo, Gladys and Heredia, Vanessa and Chiluiza, Katherine},
title = {Supporting Online Collaborative Work at Scale: A Mixed-Methods Study of a Learning Analytics Tool},
year = {2023},
isbn = {9798400700255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573051.3596165},
doi = {10.1145/3573051.3596165},
abstract = {Collaborative Learning Analytics (CLA) tools have recently emerged as a potential solution to address the onerous process of monitoring and providing timely feedback on collaboration skills in higher education students. However, prior studies on the efficacy of such tools have mainly been carried out in small, controlled settings. This study aims to measure the impact of a specific CLA tool that can be easily deployed on a larger scale with minimal instructor effort in real-world online group work activities. Additionally, this research examines the potential influence that the characteristics of the collaborative activity may have on the tool's effectiveness. The CLA tool under investigation displays speaking participation time and peer evaluation scores from students engaged in online collaborative activities as part of their regular courses. The tool was evaluated with five instructors and 156 students over the course of one semester. The effects of the tool on students' speaking participation and peer evaluation scores were quantitatively measured and tested. A qualitative analysis of reflections from both students and instructors provided supplementary information on the quantitative results. The main finding of this study indicates that the tool has an overall small positive impact. The effectiveness of the CLA tool is primarily modulated by the synchronous or asynchronous presence of the instructor, as students tend to interact more naturally and feel less scrutinized in the absence of instructor evaluation. Based on the discussion of the findings, this research suggests design insights to enhance future CLA tools at scale for the purpose of supporting the development of online collaboration skills.},
booktitle = {Proceedings of the Tenth ACM Conference on Learning @ Scale},
pages = {237–247},
numpages = {11},
keywords = {collaboration skills, online collaboration, participation time},
location = {Copenhagen, Denmark},
series = {L@S '23}
}

@inproceedings{10.1145/2567574.2567633,
author = {Gasevic, Dragan and Rose, Carolyn and Siemens, George and Wolff, Annika and Zdrahal, Zdenek},
title = {Learning analytics and machine learning},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567633},
doi = {10.1145/2567574.2567633},
abstract = {Learning analytics (LA) as a field remains in its infancy. Many of the techniques now prominent from practitioners have been drawn from various fields, including HCI, statistics, computer science, and learning sciences. In order for LA to grow and advance as a discipline, two significant challenges must be met: 1) development of analytics methods and techniques that are native to the LA discipline, and 2) practitioners in LA to develop algorithms and models that reflect the social and computational dimensions of analytics. This workshop introduces researchers in learning analytics to machine learning (ML) and the opportunities that ML can provide in building next generation analysis models.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {287–288},
numpages = {2},
keywords = {theory, machine learning, learning analytics, collaboration},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/3027385.3029426,
author = {Macfadyen, Leah P. and Groth, Dennis and Rehrey, George and Shepard, Linda and Greer, Jim and Ward, Douglas and Bennett, Caroline and Kaupp, Jake and Molinaro, Marco and Steinwachs, Matt},
title = {Developing institutional learning analytics 'communities of transformation' to support student success},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029426},
doi = {10.1145/3027385.3029426},
abstract = {Institutional implementation of learning analytics calls for thoughtful management of cultural change. This interactive halfday workshop responds to the LA literature describing the benefits and challenges of institutional LA implementation by offering participants an opportunity to learn about and begin planning for a program to actively engage faculty as leaders of data exploration around the theme of 'student success'. This session will share experiences from five institutions actively engaged in fostering Learning Analytics Communities (LAC) by identifying key issues, sharing lessons learned, and considering structural frameworks that are transferable to other institutional contexts. Structured discussion and activities will engage participants in developing an action plan for establishing an LAC on their own campus.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {498–499},
numpages = {2},
keywords = {student success, learning analytics fellows program, institutional learning analytics, faculty engagement, communities of transformation, change management},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3029430,
author = {Wang, Yuan and Davis, Dan and Chen, Guanliang and Paquette, Luc},
title = {Workshop on integrated learning analytics of MOOC post-course development},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029430},
doi = {10.1145/3027385.3029430},
abstract = {MOOC research is typically limited to evaluations of learner behavior in the context of the learning environment. However, some research has begun to recognize that the impact of MOOCs may extend beyond the confines of the course platform or conclusion of the course time limit. This workshop aims to encourage our community of learning analytics researchers to examine the relationship between performance and engagement within the course and learner behavior and development beyond the course. This workshop intends to build awareness in the community regarding the importance of research measuring multi-platform activity and long-term success after taking a MOOC. We hope to build the community's understanding of what it takes to operationalize MOOC learner success in a novel context by employing data traces across the social web.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {506–507},
numpages = {2},
keywords = {massive online open courses, long-term learning development, learning outcomes, learning analytics, career development},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3386527.3405919,
author = {Shen, Hao and Liang, Leming and Law, Nancy and Hemberg, Erik and O'Reilly, Una-May},
title = {Understanding Learner Behavior Through Learning Design Informed Learning Analytics},
year = {2020},
isbn = {9781450379519},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3386527.3405919},
doi = {10.1145/3386527.3405919},
abstract = {A goal of learning analytics is to inform and improve learning design. Previous studies have attempted to interpret learners' clickstream data based on learning science theories. Many of these interpretations are made without reference to the specific learning designs of the courses being analyzed. Here, we report on a learning design informed analytics exploration of an introductory MOOC on Computer Science and Python programming. The learning resources (videos) and practice resources (short exercises and problem sets) are analyzed according to the knowledge types and cognitive process levels respectively, both based on a revised Bloom's Taxonomy. A heat map visualization of the access intensity on a learner resource access transition matrix and social network analysis are used to analyze learners' behavior with respect to the different resource categories. The results show distinctively different patterns of access between groups of students with different course performance and different academic backgrounds.},
booktitle = {Proceedings of the Seventh ACM Conference on Learning @ Scale},
pages = {135–145},
numpages = {11},
keywords = {social network analysis, moocs, learning trajectory, learning design informed learning analytics, learning behavior, learner resource access transition matrix},
location = {Virtual Event, USA},
series = {L@S '20}
}

@inproceedings{10.1145/3027385.3029474,
author = {Healion, Donal and Russell, Sam and Cukurova, Mutlu and Spikol, Daniel},
title = {Tracing physical movement during practice-based learning through multimodal learning analytics},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029474},
doi = {10.1145/3027385.3029474},
abstract = {In this paper, we pose the question, can the tracking and analysis of the physical movements of students and teachers within a Practice-Based Learning (PBL) environment reveal information about the learning process that is relevant and informative to Learning Analytics (LA) implementations? Using the example of trials conducted in the design of a LA system, we aim to show how the analysis of physical movement from a macro level can help to enrich our understanding of what is happening in the classroom. The results suggest that Multimodal Learning Analytics (MMLA) could be used to generate valuable information about the human factors of the collaborative learning process and we propose how this information could assist in the provision of relevant supports for small group work. More research is needed to confirm the initial findings with larger sample sizes and refine the data capture and analysis methodology to allow automation.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {588–589},
numpages = {2},
keywords = {practice-based learning, movement, learning analytics, collaborative problem solving, collaborative learning environment},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3401861.3401862,
author = {Islam, Shareeful and Mahmud, Hasan},
title = {Integration of Learning Analytics into Learner Management System using Machine Learning},
year = {2020},
isbn = {9781450377423},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3401861.3401862},
doi = {10.1145/3401861.3401862},
abstract = {The demand of e-learning is constantly increasing at a rapid rate for the educational institutions. Web-based Learning Management System (LMS) is one of the main components for the e-learning system. There are multiple benefits of using LMS including cost reduction, content management, flexibility, and many more. Despite of theses significant benefits of using LMS, traditional LMS system cannot supports with modern learning needs. In particular, extracting useful information from the huge educational data and analysis and interpretation that information is challenging. Learning analytics can effectively support these needs in terms of predict learner performance, engagement and potential problems. This research presents learning analytics using machine learning techniques and considers its integration into LMS. The approach considers various indicators such as assessment score, gender and age for the prediction. This certainly supports organization to undertake actionable decisions as preventive measures for the overall teaching and learning support. We have considered a widely used learner data sets to demonstrate the applicability of our approach. The result shows that Decision Tree shows the highest accuracy among the chosen three ML algorithms. We have observed that average grade for a given course acts as an important indicator to predict over all outcome.},
booktitle = {Proceedings of the 2020 2nd International Conference on Modern Educational Technology},
pages = {1–4},
numpages = {4},
keywords = {Machine Learning, Learning Analytics, Learner Management System, Decision Tree},
location = {Singapore, Singapore},
series = {ICMET '20}
}

@inproceedings{10.1145/2883851.2883921,
author = {Muslim, Arham and Chatti, Mohamed Amine and Mahapatra, Tanmaya and Schroeder, Ulrik},
title = {A rule-based indicator definition tool for personalized learning analytics},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883921},
doi = {10.1145/2883851.2883921},
abstract = {In the last few years, there has been a growing interest in learning analytics (LA) in technology-enhanced learning (TEL). Generally, LA deals with the development of methods that harness educational data sets to support the learning process. Recently, the concept of open learning analytics (OLA) has received a great deal of attention from LA community, due to the growing demand for self-organized, networked, and lifelong learning opportunities. A key challenge in OLA is to follow a personalized and goal-oriented LA model that tailors the LA task to the needs and goals of multiple stakeholders. Current implementations of LA rely on a predefined set of questions and indicators. There is, however, a need to adopt a personalized LA approach that engages end users in the indicator definition process by supporting them in setting goals, posing questions, and self-defining the indicators that help them achieve their goals. In this paper, we address the challenge of personalized LA and present the conceptual, design, and implementation details of a rule-based indicator definition tool to support flexible definition and dynamic generation of indicators to meet the needs of different stakeholders with diverse goals and questions in the LA exercise.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {264–273},
numpages = {10},
keywords = {personalized learning analytics, open learning analytics, learning analytics, indicator},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2460296.2460316,
author = {Blikstein, Paulo},
title = {Multimodal learning analytics},
year = {2013},
isbn = {9781450317856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2460296.2460316},
doi = {10.1145/2460296.2460316},
abstract = {New high-frequency data collection technologies and machine learning analysis techniques could offer new insights into learning, especially in tasks in which students have ample space to generate unique, personalized artifacts, such as a computer program, a robot, or a solution to an engineering challenge. To date most of the work on learning analytics and educational data mining has focused on online courses or cognitive tutors, in which the tasks are more structured and the entirety of interaction happens in front of a computer. In this paper, I argue that multimodal learning analytics could offer new insights into students' learning trajectories, and present several examples of this work and its educational application.},
booktitle = {Proceedings of the Third International Conference on Learning Analytics and Knowledge},
pages = {102–106},
numpages = {5},
keywords = {multimodal interaction, learning analytics, constructivism, constructionism, assessment},
location = {Leuven, Belgium},
series = {LAK '13}
}

@inproceedings{10.1145/3498765.3498768,
author = {Maraza-Quispe, Benjamin and Damian Valderrama-Chauca, Enrique and Henry Cari-Mogrovejo, Lenin and Milton Apaza-Huanca, Jorge},
title = {Predictive Model of Student Academic Performance from LMS data based on Learning Analytics},
year = {2022},
isbn = {9781450385114},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498765.3498768},
doi = {10.1145/3498765.3498768},
abstract = {The present research aims to implement a predictive model in the KNIME platform to analyze and compare the prediction of academic performance using data from a Learning Management System (LMS), identifying students at academic risk in order to generate timely and timely interventions. The CRISP-DM methodology was used, structured in six phases: Problem analysis, data analysis, data understanding, data preparation, modeling, evaluation and implementation. Based on the analysis of online learning behavior through 22 behavioral indicators observed in the LMS of the Faculty of Educational Sciences of the National University of San Agustin. These indicators are distributed in five dimensions: Academic Performance, Access, Homework, Social Aspects and Quizzes. The model has been implemented in the KNIME platform using the Simple Regression Tree Learner training algorithm. The total population consists of 30,000 student records from which a sample of 1,000 records has been taken by simple random sampling. The accuracy of the model for early prediction of students' academic performance is evaluated, the 22 observed behavioral indicators are compared with the means of academic performance in three courses. The prediction results of the implemented model are satisfactory where the mean absolute error compared to the mean of the first course was 3. 813 and with an accuracy of 89.7%, the mean absolute error compared to the mean of the second course was 2.809 with an accuracy of 94.2% and the mean absolute error compared to the mean of the third course was 2.779 with an accuracy of 93.8%. These results demonstrate that the proposed model can be used to predict students' future academic performance from an LMS data set.},
booktitle = {Proceedings of the 13th International Conference on Education Technology and Computers},
pages = {13–19},
numpages = {7},
keywords = {virtual, prediction, performance, learning analytics, learning, environments, academic, Model, KNIME},
location = {Wuhan, China},
series = {ICETC '21}
}

@inproceedings{10.1145/3582580.3582646,
author = {Deng, Jia Hong and Zhao, Yiming},
title = {A Literature Review of Data-Driven Multimodal Learning Analytics in Education Based on Citespace},
year = {2023},
isbn = {9781450398015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582580.3582646},
doi = {10.1145/3582580.3582646},
abstract = {Multimodal learning analysis offers the possibility to comprehensively study students' learning processes, promote effective learning and explain teaching patterns in complex environments. In this paper, Citespace software is used to review the literature related to multimodal learning analysis techniques in education in WoS database and CNKI. The start of research in this field and the research intensity can be shown by the distribution of the time of publication of the literature. The domestic research situation, the research situation in each country and its influence can be shown by analyzing the country and institutional distribution of the literature. And the analysis of the co-occurrence of keywords shows the hot research content in this field. After the econometric and qualitative analysis of the literature, this paper discusses the future research trends of multimodal learning analytics in education, mainly including: taking students' personalized learning and emotional experience as the value orientation, carrying out in-depth empirical research on multimodal fusion analytics on the basis of protecting students' data and privacy security, and promoting the construction of intelligent learning service system.},
booktitle = {Proceedings of the 2022 5th International Conference on Education Technology Management},
pages = {390–397},
numpages = {8},
keywords = {Keyword co-occurrence analysis, Multimodal data, Multimodal learning analysis},
location = {Lincoln, United Kingdom},
series = {ICETM '22}
}

@inproceedings{10.1145/2883851.2883944,
author = {Bakharia, Aneesha and Corrin, Linda and de Barba, Paula and Kennedy, Gregor and Ga\v{s}evi\'{c}, Dragan and Mulder, Raoul and Williams, David and Dawson, Shane and Lockyer, Lori},
title = {A conceptual framework linking learning design with learning analytics},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883944},
doi = {10.1145/2883851.2883944},
abstract = {In this paper we present a learning analytics conceptual framework that supports enquiry-based evaluation of learning designs. The dimensions of the proposed framework emerged from a review of existing analytics tools, the analysis of interviews with teachers, and user scenarios to understand what types of analytics would be useful in evaluating a learning activity in relation to pedagogical intent. The proposed framework incorporates various types of analytics, with the teacher playing a key role in bringing context to the analysis and making decisions on the feedback provided to students as well as the scaffolding and adaptation of the learning design. The framework consists of five dimensions: temporal analytics, tool-specific analytics, cohort dynamics, comparative analytics and contingency. Specific metrics and visualisations are defined for each dimension of the conceptual framework. Finally the development of a tool that partially implements the conceptual framework is discussed.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {329–338},
numpages = {10},
keywords = {learning design, learning analytics, intervention design},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/3462204.3481799,
author = {Villagr\'{a}n, Ignacio},
title = {Exploring the Effects of Applying Learning Analytics for Teaching Procedural Skills in Health Sciences Education},
year = {2021},
isbn = {9781450384797},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3462204.3481799},
doi = {10.1145/3462204.3481799},
abstract = {In Health Science Education (HSE), students must demonstrate technical skills in many procedures. However, traditional teaching methodologies limit the possibilities for personalized feedback from an instructor and generally do not allow students to achieve the required proficiency levels. Concerning this, technology has been installed as a learning resource with numerous benefits that allow new multidisciplinary lines of research, such as learning analytics (LA) and educational data mining (EDM). Both LA and EDM seek to improve educational practice based on the intensive use of educational data, such as analyzing online learning patterns, creating performance predictions, and incorporating artificial intelligence techniques. There is a range of possibilities in applying LA and EDM in teaching procedural skills that have not yet been explored. For this reason, this study aims to answer (1) How the educational data explain the development of procedural skills in virtual environments to support the teaching-learning process in Health Science Education?; and (2) How automatic feedback and adaptive personalization affect the performance and instructional design of the procedural skills learning process of health science students? We expect that this study contributes to the field of technical skills learning mediated by technology in higher education by using the data provided by the interaction of students with virtual resources to support educational decision-making and optimize the teaching and learning processes in HSE.},
booktitle = {Companion Publication of the 2021 Conference on Computer Supported Cooperative Work and Social Computing},
pages = {299–302},
numpages = {4},
keywords = {procedural skills, learning analytics, health science education, educational data mining, adaptative learning},
location = {Virtual Event, USA},
series = {CSCW '21 Companion}
}

@inproceedings{10.1145/3144826.3145397,
author = {Mart\'{\i}nez-Mon\'{e}s, Alejandra and Reffay, Christophe and Tor\'{\i}o, Javier Hoyos and Crist\'{o}bal, Juan A. Mu\~{n}oz},
title = {Learning Analytics with Google Classroom: Exploring the possibilities},
year = {2017},
isbn = {9781450353861},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3144826.3145397},
doi = {10.1145/3144826.3145397},
abstract = {Google Classroom (GC) is gaining momentum in the educational milieu, but its functionalities are limited. Learning analytics applications integrated with GC can help to face these limitations, but to reach this aim, developers need access to the data generated by GC's users. This paper reports on the results of an analysis of the existing alternatives to collect data from GC. The study is based on the analysis of the documentation provided by the involved tools. The analysis shows that GC's API is a potential source of data about the activity of the users in GC-enabled settings, but that the information it provides is limited. Further work is needed to explore if Chrome OS synchronization functions can deliver more detailed information about GC usage, thus enabling for more advanced learning analytics applications.},
booktitle = {Proceedings of the 5th International Conference on Technological Ecosystems for Enhancing Multiculturality},
articleno = {47},
numpages = {6},
keywords = {Teachers' adoption, Learning Analytics, Google Classroom},
location = {C\'{a}diz, Spain},
series = {TEEM 2017}
}

@inproceedings{10.1145/3234698.3234721,
author = {Panchoo, Shireen},
title = {Learning Analytics: Requirements for Enhanced Learner-Content Interactions in an Online Environment},
year = {2018},
isbn = {9781450363921},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3234698.3234721},
doi = {10.1145/3234698.3234721},
abstract = {During face to face classroom interactions, learning is facilitated through the dissemination of course materials by subject specialists, in a pedagogical manner, suiting the pace of the learners. It is known that proper interactions with contents, tutors and peers encourage learning to take place. However, the learner-content process is known to be challenging for online learners who struggle alone to overcome learning difficulties, and which may lead them to drop out from the course. In absence of face to face interactions, the content, being the prime source of knowledge for distant learners, has to be up to date and comprehensive while catering for different learning styles of students. Therefore, this paper aims at investigating whether learners meet the cognitive level as required by the course's learning objectives. Through a questionnaire, 18 online learners were requested to identify their learning activities done in both asynchronous and synchronous modes as part of their learning process. Furthermore, content analysis, using both Activity Theory and the Jaillet &amp; Panchoo grid, was done on four online tutorials to identify if learners study their course materials as desired and if their interactions are of cognitive level. A total of 761 lines of logged textual interactions was coded twice. Results show that learners go through different learning activities as imposed by the module but less emphasis is put on ensuring that learners study the course content before embarking on the assignment. Acquiring the relevant knowledge from the course materials prior to online discussions, will encourage better interactions and learning to take place in this socio-constructivism set up.},
booktitle = {Proceedings of the Fourth International Conference on Engineering &amp; MIS 2018},
articleno = {23},
numpages = {6},
keywords = {online tutoring, learning analytics, content-learning interactions, content analysis, chat, activity theory},
location = {Istanbul, Turkey},
series = {ICEMIS '18}
}

@inproceedings{10.1145/3706468.3706565,
author = {Divjak, Bla\v{z}enka and Barthakur, Abhinava and Kovanovi\'{c}, Vitomir and Svetec, Barbi},
title = {The Impact of Learning Design on the Mastery of Learning Outcomes in Higher Education},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706565},
doi = {10.1145/3706468.3706565},
abstract = {Ensuring constructive alignment between learning outcomes (LOs) and assessment design is crucial to effective learning design (LD). While previous research has explored the alignment of LOs with assessments, there is a lack of empirical studies on how assessment design influences LO mastery, particularly the relationship between formative and summative assessments. To address this gap, we conducted an empirical study within an undergraduate mathematics course. First, we evaluated the course's learning design to identify potential gaps in constructive alignment. Then, using a sample of 169 students, we analysed their assessment results to explore how LO mastery is demonstrated through formative and summative assessments. This study provides a novel learning analytics (LA) methodology by combining cognitive diagnostic models, epistemic network analysis, and social network analysis to examine LO mastery and interdependencies. Our findings reveal a strong connection between the mastery of LOs through formative and summative assessments, underscoring the importance of well-constructed LD. The practical implications suggest that LA can serve as a critical tool for quality assurance by guiding the revision of LOs and optimising LD to foster deeper student engagement and mastery of critical concepts. These insights offer actionable pathways for more targeted, student-centered teaching practices.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {726–737},
numpages = {12},
keywords = {Assessment design, Assessment validity, Learning analytics, Learning design, Learning outcomes},
location = {
},
series = {LAK '25}
}

@article{10.1145/3105759,
author = {Hundhausen, C. D. and Olivares, D. M. and Carter, A. S.},
title = {IDE-Based Learning Analytics for Computing Education: A Process Model, Critical Review, and Research Agenda},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
url = {https://doi.org/10.1145/3105759},
doi = {10.1145/3105759},
abstract = {In recent years, learning process data have become increasingly easy to collect through computer-based learning environments. This has led to increased interest in the field of learning analytics, which is concerned with leveraging learning process data in order to better understand, and ultimately to improve, teaching and learning. In computing education, the logical place to collect learning process data is through integrated development environments (IDEs), where computing students typically spend large amounts of time working on programming assignments. While the primary purpose of IDEs is to support computer programming, they might also be used as a mechanism for delivering learning interventions designed to enhance student learning. The possibility of using IDEs both to collect learning process data, and to strategically intervene in the learning process, suggests an exciting design space for computing education research: that of IDE-based learning analytics. In order to facilitate the systematic exploration of this design space, we present an IDE-based data analytics process model with four primary activities: (1) Collect data, (2) Analyze data, (3) Design intervention, and (4) Deliver intervention. For each activity, we identify key design dimensions and review relevant computing education literature. To provide guidance on designing effective interventions, we describe four relevant learning theories, and consider their implications for design. Based on our review, we present a call-to-action for future research into IDE-based learning analytics.},
journal = {ACM Trans. Comput. Educ.},
month = aug,
articleno = {11},
numpages = {26},
keywords = {learning process data, learning interventions, Learning analytics}
}

@inproceedings{10.1145/2723576.2723635,
author = {Gibson, Andrew and Kitto, Kirsty},
title = {Analysing reflective text for learning analytics: an approach using anomaly recontextualisation},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723635},
doi = {10.1145/2723576.2723635},
abstract = {Reflective writing is an important learning task to help foster reflective practice, but even when assessed it is rarely analysed or critically reviewed due to its subjective and affective nature. We propose a process for capturing subjective and affective analytics based on the identification and recontextualisation of anomalous features within reflective text. We evaluate 2 human supervised trials of the process, and so demonstrate the potential for an automated Anomaly Recontextualisation process for Learning Analytics.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {275–279},
numpages = {5},
keywords = {reflective text, learning analytics, affective computing},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2723576.2723649,
author = {Rogers, Tim and Colvin, Cassandra and West, Deborah and Dawson, Shane},
title = {Learning analytics in Oz: what's happening now, what's planned, and where could it (and should it) go?},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723649},
doi = {10.1145/2723576.2723649},
abstract = {This poster outlines the process and purpose of two related Australian Office for Learning and Teaching (OLT) commissioned grants to investigate the current usage and future potential of learning analytics in Australian Higher Education, with a view to developing resources to guide Australian universities in their adoption of learning analytics. The commissioned grants run from February 2014 to June 2015. Preliminary results will be available for LAK 15.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {432–433},
numpages = {2},
keywords = {student retention, maturity model, learning analytics, institutional preparedness},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@proceedings{10.1145/3303772,
title = {LAK19: Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Tempe, AZ, USA}
}

@inproceedings{10.1145/3375462.3375476,
author = {Alvarez, Carlos Prieto and Martinez-Maldonado, Roberto and Buckingham Shum, Simon},
title = {LA-DECK: a card-based learning analytics co-design tool},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375476},
doi = {10.1145/3375462.3375476},
abstract = {Human-centred software design gives all stakeholders an active voice in the design of the systems that they are expected to use. However, this is not yet commonplace in Learning Analytics (LA). Co-design techniques from other domains therefore have much to offer to LA, in principle, but there are few detailed accounts of exactly how such sessions unfold. This paper presents the rationale driving a card-based co-design tool specifically tuned for LA, called LA-DECK. In the context of a pilot study with students, educators, LA researchers and developers, we provide qualitative and quantitative accounts of how participants used the cards. Using three different forms of analysis (transcript-centric design vignettes, card-graphs and time-on-topic), we characterise in what ways the sessions were "participatory" in nature, and argue that the cards succeeded in playing very similar roles to those documented in the literature on successful card-based design tools.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {63–72},
numpages = {10},
keywords = {participatory design, learning analytics, co-design, cards},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3706468.3706557,
author = {Hicks, Ben and Kitto, Kirsty},
title = {Game Theoretic Models of Intangible Learning Data},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706557},
doi = {10.1145/3706468.3706557},
abstract = {Learning Analytics is full of situations where features essential to understanding the learning process cannot be measured. The cognitive processes of students, their decisions to cooperate or cheat on an assessment, their interactions with class environments can all be critical contextual features of an educational system that are impossible to measure. This leaves an empty space where essential data is missing from our analysis. This paper proposes the use of Game Theoretic models as a way to explore that empty space and potentially even to generate synthetic data for our models. Cooperating or free-riding on the provisioning of feedback in a class activity is used as a case study. We show how our initially simple model can gradually be built up to help understand potential educator responses as new situations arise, using the emergence of GenAI in the classroom as a case in point.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {970–976},
numpages = {7},
keywords = {game theory, learning analytics, missing data, learning theory},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3377290.3377319,
author = {Yarygina, Olga},
title = {Learning analytics of CS0 students programming errors: the case of data science minor},
year = {2020},
isbn = {9781450377744},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377290.3377319},
doi = {10.1145/3377290.3377319},
abstract = {In this work, I report the preliminary results and further research design of the study on coding errors in the first year of Data Science Minor course. The larger aim of the project is the analysis of changes in the amount, type, and complexity of errors students get and the connection between the patterns of these changes, mathematical and CS background, as well as self-regulation, motivation, and previous academic successes.},
booktitle = {Proceedings of the 23rd International Conference on Academic Mindtrek},
pages = {149–152},
numpages = {4},
keywords = {logs, learning analytics, data science, blended learning},
location = {Tampere, Finland},
series = {AcademicMindtrek '20}
}

@inproceedings{10.1145/2883851.2883892,
author = {Molenaar, Inge and van Campen, Carolien Knoop},
title = {Learning analytics in practice: the effects of adaptive educational technology Snappet on students' arithmetic skills},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883892},
doi = {10.1145/2883851.2883892},
abstract = {Even though the recent influx of tablets in primary education goes together with the vision that educational technology empowered with learning analytics will revolutionize education, empirical results supporting this claim are scares. Adaptive educational technology Snappet combines extracted and embedded learning analytics daily in classrooms. While students make exercises on the tablet this technology displays real-time data of learner performance in a teacher dashboard (extracted analytics). At the same time, learner performance is used to adaptively adjust exercises to students' progress (embedded analytics). This quasiexperimental study compares the development of students' arithmetic skills over one schoolyear (grade 2 and 4) in a traditional paper based setting to learning with the adaptive educational technology Snappet. The results indicate that students in the Snappet condition make significantly more progress on arithmetic skills in grade 4. Moreover, in this grade students with a high ability level, benefit the most from working with this adaptive educational technology. Overall the development pattern of students with different abilities was more divergent in the AET condition compared to the control condition. These results indicate that adaptive educational technologies combining extracted and embedded learning analytics are indeed creating new education scenarios that contribute to personalized learning in primary education.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {538–539},
numpages = {2},
keywords = {primary education, educational technologies, arithmetic's, ability levels},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2567574.2567613,
author = {Swenson, Jenni},
title = {Establishing an ethical literacy for learning analytics},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567613},
doi = {10.1145/2567574.2567613},
abstract = {This paper borrows multiple frameworks from the field of technical communication in order to review theory, research, practice, and ethics of the Learning Analytics and Knowledge (LAK) discipline. These frameworks also guide discussion on the ethics of learning analytics "artifacts" (data visualizations, dashboards, and methodology), and the ethical consequences of using learning analytics (classification, social power moves, and absence of voice). Finally, the author suggests a literacy for learning analytics that includes an ethical viewpoint.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {246–250},
numpages = {5},
keywords = {literacy, learning analytics, higher education, ethics},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/2883851.2883933,
author = {Drachsler, Hendrik and Hoel, Tore and Cooper, Adam and Kismih\'{o}k, G\'{a}bor and Berg, Alan and Scheffel, Maren and Chen, Weiqin and Ferguson, Rebecca},
title = {Ethical and privacy issues in the design of learning analytics applications},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883933},
doi = {10.1145/2883851.2883933},
abstract = {Issues related to Ethics and Privacy have become a major stumbling block in application of Learning Analytics technologies on a large scale. Recently, the learning analytics community at large has more actively addressed the EP4LA issues, and we are now starting to see learning analytics solutions that are designed not only as an afterthought, but also with these issues in mind. The 2nd EP4LA@LAK16 workshop will bring the discussion on ethics and privacy for learning analytics to a the next level, helping to build an agenda for organizational and technical design of LA solutions, addressing the different processes of a learning analytics workflow.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {492–493},
numpages = {2},
keywords = {surveillance, privacy, legal rights, learning analytics, ethics, data ownership},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/3027385.3027397,
author = {Herodotou, Christothea and Rienties, Bart and Boroowa, Avinash and Zdrahal, Zdenek and Hlosta, Martin and Naydenova, Galina},
title = {Implementing predictive learning analytics on a large scale: the teacher's perspective},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027397},
doi = {10.1145/3027385.3027397},
abstract = {In this paper, we describe a large-scale study about the use of predictive learning analytics data with 240 teachers in 10 modules at a distance learning higher education institution. The aim of the study was to illuminate teachers' uses and practices of predictive data, in particular identify how predictive data was used to support students at risk of not completing or failing a module. Data were collected from statistical analysis of 17,033 students' performance by the end of the intervention, teacher usage statistics, and five individual semi-structured interviews with teachers. Findings revealed that teachers endorse the use of predictive data to support their practice yet in diverse ways and raised the need for devising appropriate intervention strategies to support students at risk.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {267–271},
numpages = {5},
keywords = {teachers, retention, predictive analytics, perceptions, higher education},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3303772.3303841,
author = {Rodriguez, Fernando and Yu, Renzhe and Park, Jihyun and Rivas, Mariela Janet and Warschauer, Mark and Sato, Brian K.},
title = {Utilizing Learning Analytics to Map Students' Self-Reported Study Strategies to Click Behaviors in STEM Courses},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303841},
doi = {10.1145/3303772.3303841},
abstract = {Informed by cognitive theories of learning, this work examined how students' self-reported study patterns (spacing vs. cramming) corresponded to their engagement with the Learning Management System (LMS) across two years in a large biology course. We specifically focused on how students accessed non-mandatory resources (lecture videos, lecture slides) and considered whether this pattern differed by underrepresented minority (URM) status. Overall, students who self-reported utilizing spacing strategies throughout the course had higher grades than students who reported cramming throughout the course. When examining LMS engagement, only a small percentage of students accessed the lecture videos and lecture slides. Applying a negative binomial regression model to daily counts of click activities, we also found that students who utilized spacing strategies accessed LMS resources more often but not earlier before major deadlines. Moreover, this finding was not different for underrepresented students. Our results provide some initial evidence showing how spacing behaviors correspond to accessing learning resources. However, given the lack of general engagement with LMS resources, our results underscore the value of encouraging students to utilize these resources when studying course material.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {456–460},
numpages = {5},
keywords = {Underrepresented Students, Study Skills, Spacing Effect, STEM Education, Learning Analytics, Higher Education},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@proceedings{10.1145/3170358,
title = {LAK '18: Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The aim of the conference is to provide a forum for presentation, exchange and discussion of research and practices related to the transdisciplinary field of learning analytics.},
location = {Sydney, New South Wales, Australia}
}

@inproceedings{10.1145/3027385.3027427,
author = {Spann, Catherine A. and Schaeffer, James and Siemens, George},
title = {Expanding the scope of learning analytics data: preliminary findings on attention and self-regulation using wearable technology},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027427},
doi = {10.1145/3027385.3027427},
abstract = {The ability to pay attention and self-regulate is a fundamental skill required of learners of all ages. Learning analytics researchers have to date relied on data generated by a computing system (such as a learning management system, click stream or log data) to examine learners' self-regulatory abilities. The development of wearable computing through fitness trackers, watches, heart rate monitors, and clinical grade devices such as Empatica's E4 wristband now provides researchers with access to biometric data as students interact with learning content or software systems. This level of data collection promises to provide valuable insight into cognitive and affective experiences of individuals, especially when combined with traditional learning analytics data sources. Our study details the use of wearable technologies to assess the relationship between heart rate variability and the self-regulatory abilities of an individual. This is relevant for the field of learning analytics as methods become more complex and the assessment of learner performance becomes more nuanced and attentive to the affective factors that contribute to learner success.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {203–207},
numpages = {5},
keywords = {wearable technology, self-regulation, psychophysiology, heart-rate variability, attention},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1109/CHASE.2019.00009,
author = {R\'{\i}os, Julio C\'{e}sar Cort\'{e}s and Kopec-Harding, Kamilla and Eraslan, Sukru and Page, Christopher and Haines, Robert and Jay, Caroline and Embury, Suzanne M.},
title = {A methodology for using GitLab for software engineering learning analytics},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CHASE.2019.00009},
doi = {10.1109/CHASE.2019.00009},
abstract = {To bridge the digital skills gap, we need to train more people in Software Engineering techniques. This paper reports on a project exploring the way students solve tasks using collaborative development platforms and version control systems, such as GitLab, to find patterns and evaluation metrics that can be used to improve the course content and reflect on the most common issues the students are facing. In this paper, we explore Learning Analytics approaches that can be used with GitLab and similar tools, and discuss the challenges raised when applying those approaches in Software Engineering Education, with the objective of building a pipeline that supports the full Learning Analytics cycle, from data extraction to data analysis. We focus in particular on the data anonymisation step of the proposed pipeline to explore the available alternatives to satisfy the data protection requirements when handling personal information in academic environments for research purposes.},
booktitle = {Proceedings of the 12th International Workshop on Cooperative and Human Aspects of Software Engineering},
pages = {3–6},
numpages = {4},
keywords = {software engineering, learning analytics, git, data extraction, data anonymisation},
location = {Montreal, Quebec, Canada},
series = {CHASE '19}
}

@inproceedings{10.1145/3301551.3301582,
author = {Ruangvanich, Supparang and Nilsook, Prachyanun},
title = {Personality Learning Analytics System in Intelligent Virtual Learning Environment},
year = {2018},
isbn = {9781450366298},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3301551.3301582},
doi = {10.1145/3301551.3301582},
abstract = {In this paper, the researchers propose a conceptual for system architecture of learning analytics process in the intelligent learning environment. Within this concept, today's competitive business environment need for businesses in order to implement the monitor and analyze the user-generated data on their own and their competitors. The achievement of competitive advantage is often necessary to listen to and understand what customers are saying about competitors' products and services. Not only personality analytics but also the conceptual description can capture an intelligent learning environment, and it is the analytic tools that are used to improve learning and education. The researchers also discuss how learning analytics is developed in different fields. It closely tied to, a series of other fields of study including business intelligence, web analytics, academic analytics, educational data mining, and action analytics. The researchers believe that conceptual of personality analytics in the intelligent learning environment can play an essential role in managing and analyzing personality and contribute to the concept of personality analytics in the intelligent learning environment. The results of this research could be summarized as follows: learning analytics process should be used as measuring and collecting data about learners and learning with the aim of improving teaching and learning practice through analysis of the data. By achieving this process, it should collect data to report or analyze the happening about the learner. Then, instructors monitor learning what is happening now, while as learning analytics should get what is going to happen in the future for learners. Finally, instructors take action to feedback learners.},
booktitle = {Proceedings of the 6th International Conference on Information Technology: IoT and Smart City},
pages = {245–250},
numpages = {6},
keywords = {Virtual Learning Environment, System Architecture, Personal Analytics, Learning Analytics, Intelligent Environment},
location = {Hong Kong, Hong Kong},
series = {ICIT '18}
}

@inproceedings{10.1145/3706468.3706508,
author = {Herodotou, Christothea and Carr, Jessica and Shrestha, Sagun and Comfort, Catherine and Bayer, Vaclav and Maguire, Claire and Lee, John and Mulholland, Paul and Fernandez, Miriam},
title = {Prescriptive analytics motivating distance learning students to take remedial action: A case study of a student-facing dashboard},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706508},
doi = {10.1145/3706468.3706508},
abstract = {Student-facing learning analytics dashboards aim to help students to monitor their study progress, achieve learning goals and develop self-regulation skills. Only few of them present personalised data visualisations and aim to develop agentic students who take remedial action to improve their study habits, learning and performance. In this paper, a student-facing dashboard, designed following principles of participatory research, was tested with 30 undergraduate students, who engaged with it over a period of 4 to 15 weeks and while studying an online course. This is one of the few dashboards available that presents all different types of analytics to students: descriptive, predictive and prescriptive. A mixed methods approach was used to assess its usefulness and impact on motivation to study and take remedial action to support learning. Data analysis showcased that such a dashboard can be “a roadmap to success” by motivating students to study more and improve their performance, in addition to helping with monitoring, planning and reflection. While all dashboard features were perceived as being useful, special value was placed on prescriptive elements, in particular material recommendations and contacting tutors and university support teams, emphasizing the significance of making explicit on a dashboard the actions students should take to improve their performance. Implications for future studies are discussed.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {306–316},
numpages = {11},
keywords = {learning analytics dashboards, student-facing dashboards, prescriptive analytics},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3027385.3027414,
author = {Hoel, Tore and Griffiths, Dai and Chen, Weiqin},
title = {The influence of data protection and privacy frameworks on the design of learning analytics systems},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027414},
doi = {10.1145/3027385.3027414},
abstract = {Learning analytics open up a complex landscape of privacy and policy issues, which, in turn, influence how learning analytics systems and practices are designed. Research and development is governed by regulations for data storage and management, and by research ethics. Consequently, when moving solutions out the research labs implementers meet constraints defined in national laws and justified in privacy frameworks. This paper explores how the OECD, APEC and EU privacy frameworks seek to regulate data privacy, with significant implications for the discourse of learning, and ultimately, an impact on the design of tools, architectures and practices that now are on the drawing board. A detailed list of requirements for learning analytics systems is developed, based on the new legal requirements defined in the European General Data Protection Regulation, which from 2018 will be enforced as European law. The paper also gives an initial account of how the privacy discourse in Europe, Japan, South-Korea and China is developing and reflects upon the possible impact of the different privacy frameworks on the design of LA privacy solutions in these countries. This research contributes to knowledge of how concerns about privacy and data protection related to educational data can drive a discourse on new approaches to privacy engineering based on the principles of Privacy by Design. For the LAK community, this study represents the first attempt to conceptualise the issues of privacy and learning analytics in a cross-cultural context. The paper concludes with a plan to follow up this research on privacy policies and learning analytics systems development with a new international study.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {243–252},
numpages = {10},
keywords = {privacy frameworks, privacy by design, personal information, learning analytics systems design, learning analytics process requirements, learning analytics, data protection by design, data protection by default, data protection},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3706468.3706485,
author = {\v{S}v\'{a}bensk\'{y}, Valdemar and Borchers, Conrad and Cloude, Elizabeth B. and Shimada, Atsushi},
title = {Evaluating the Impact of Data Augmentation on Predictive Model Performance},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706485},
doi = {10.1145/3706468.3706485},
abstract = {In supervised machine learning (SML) research, large training datasets are essential for valid results. However, obtaining primary data in learning analytics (LA) is challenging. Data augmentation can address this by expanding and diversifying data, though its use in LA remains underexplored. This paper systematically compares data augmentation techniques and their impact on prediction performance in a typical LA task: prediction of academic outcomes. Augmentation is demonstrated on four SML models, which we successfully replicated from a previous LAK study based on AUC values. Among 21 augmentation techniques, SMOTE-ENN sampling performed the best, improving the average AUC by 0.01 and approximately halving the training time compared to the baseline models. In addition, we compared 99 combinations of chaining 21 techniques, and found minor, although statistically significant, improvements across models when adding noise to SMOTE-ENN (+0.014). Notably, some augmentation techniques significantly lowered predictive performance or increased performance fluctuation related to random chance. This paper’s contribution is twofold. Primarily, our empirical findings show that sampling techniques provide the most statistically reliable performance improvements for LA applications of SML, and are computationally more efficient than deep generation methods with complex hyperparameter settings. Second, the LA community may benefit from validating a recent study through independent replication.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {126–136},
numpages = {11},
keywords = {learning analytics, prediction, supervised learning, data generation, synthetic data, replication},
location = {
},
series = {LAK '25}
}

@proceedings{10.1145/3027385,
title = {LAK '17: Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The theme for LAK'17 purposely focused on the transdisciplinary nature of research in learning analytics. This theme extends the work of prior conferences that sought to bring together the diversity of disciplinary fields that now comprise learning analytics. The great diversity of papers submitted for LAK'17 demonstrates that LA research has very much embraced the benefits that can be leveraged from a truly transdisciplinary model of research. While there are inherent complexities in such an approach, the research presented for LAK'17 brings much excitement and promise to the field through the application of novel methods, cutting-edge learning technologies, and actual impact on the learning process. Following this theme, the aim of the conference is to provide a forum for presentation, exchange and discussion of research and practices regarding the transdisciplinary field of Learning Analytics.},
location = {Vancouver, British Columbia, Canada}
}

@inproceedings{10.1145/3184558.3193130,
author = {Cardoso, Elsa},
title = {The Past, Present, and Future of Learning Analytics},
year = {2018},
isbn = {9781450356404},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3184558.3193130},
doi = {10.1145/3184558.3193130},
abstract = {Learning Analytics (LA) is a recent research field, in which Business Intelligence and Analytics techniques are applied to learners and their contexts, with the purpose of acquiring a greater insight about the entire learning process (including outcomes). In this talk, we explore the LA landscape, delving into the definitions, techniques, challenges, and lessons learned.},
booktitle = {Companion Proceedings of the The Web Conference 2018},
pages = {1021},
numpages = {1},
keywords = {business intelligence, data management, higher education, learning analytics},
location = {Lyon, France},
series = {WWW '18}
}

@inproceedings{10.1145/2883851.2883873,
author = {Martinez-Maldonado, Roberto and Schneider, Bertrand and Charleer, Sven and Buckingham Shum, Simon and Klerkx, Joris and Duval, Erik},
title = {Interactive surfaces and learning analytics: data, orchestration aspects, pedagogical uses and challenges},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883873},
doi = {10.1145/2883851.2883873},
abstract = {The proliferation of varied types of multi-user interactive surfaces (such as digital whiteboards, tabletops and tangible interfaces) is opening a new range of applications in face-to-face (f2f) contexts. They offer unique opportunities for Learning Analytics (LA) by facilitating multi-user sensemaking of automatically captured digital footprints of students' f2f interactions. This paper presents an analysis of current research exploring learning analytics associated with the use of surface devices. We use a framework to analyse our first-hand experiences, and the small number of related deployments according to four dimensions: the orchestration aspects involved; the phases of the pedagogical practice that are supported; the target actors; and the levels of iteration of the LA process. The contribution of the paper is twofold: 1) a synthesis of conclusions that identify the degree of maturity, challenges and pedagogical opportunities of the existing applications of learning analytics and interactive surfaces; and 2) an analysis framework that can be used to characterise the design space of similar areas and LA applications.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {124–133},
numpages = {10},
keywords = {visualizations, studies in the wild, groupware, face-to-face, design, dashboard, awareness},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/3355402.3355407,
author = {Rueangprathum, Atchara and Witosurapot, Suntorn},
title = {Design of Modular Learning Analytics Framework for Early Childhood Education: A Case Study},
year = {2019},
isbn = {9781450372282},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3355402.3355407},
doi = {10.1145/3355402.3355407},
abstract = {On the provision of early childhood education, education-oriented software for enabling parent and teacher collaboration becomes popular. However, by providing merely on the asynchronous communication support, this sort of software overlooks a feature allowing parents (or teachers) to trace their child's physical development or learning progress. In this paper, it is argued that the missing feature is indeed necessary, and should be realized through the use of learning analytics technology. In this regard, a service-oriented design framework for rapid application development is suggested by a means of web mashup of open source software. Based on the minimum viable product, it is clearly the graph visualization of learning analytics can potentially empower parents on their early childhood education so that actions can be taken in time accordingly if necessary.},
booktitle = {Proceedings of the 2019 International Conference on Information Technology and Computer Communications},
pages = {24–28},
numpages = {5},
keywords = {Service-Oriented Architectures, Learning Analytics, Headless CMS, Early Childhood Education},
location = {Singapore, Singapore},
series = {ITCC '19}
}

@inproceedings{10.1145/3027385.3029463,
author = {Bannert, Maria and Molenaar, Inge and Azevedo, Roger and J\"{a}rvel\"{a}, Sanna and Ga\v{s}evi\'{c}, Dragan},
title = {Relevance of learning analytics to measure and support students' learning in adaptive educational technologies},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029463},
doi = {10.1145/3027385.3029463},
abstract = {In this poster, we describe the aim and current activities of the EARLI-Centre for Innovative Research (E-CIR) "Measuring and Supporting Student's Self-Regulated Learning in Adaptive Educational Technologies" which is funded by the European Association for Research on Learning and Instruction (EARLI) from 2015 to 2019. The aim is to develop our understanding of multimodal data that unobtrusively capture cognitive, meta-cognitive, affective and motivational states of learners over time. This demands for a concerted interdisciplinary dialogue combining findings from psychology and educational sciences with advances in computer sciences and artificial intelligence. The participants in this E-CIR are leading international researchers who have articulated different emerging perspectives and methodologies to measure cognition, metacognition, motivation, and emotions during learning. The participants recognize the need for intensive collaboration to accelerate progress with new interdisciplinary methods including learning analytics to develop more powerful adaptive educational technologies.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {568–569},
numpages = {2},
keywords = {self-regulated learning, multimodal data, learning analytics, educational data mining, adaptive educational technologies},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/2723576.2723584,
author = {Chen, Bodong and Chen, Xin and Xing, Wanli},
title = {"Twitter Archeology" of learning analytics and knowledge conferences},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723584},
doi = {10.1145/2723576.2723584},
abstract = {The goal of the present study was to uncover new insights about the learning analytics community by analyzing Twitter archives from the past four Learning Analytics and Knowledge (LAK) conferences. Through descriptive analysis, interaction network analysis, hashtag analysis, and topic modeling, we found: extended coverage of the community over the years; increasing interactions among its members regardless of peripheral and in-persistent participation; increasingly dense, connected and balanced social networks; and more and more diverse research topics. Detailed inspection of semantic topics uncovered insights complementary to the analysis of LAK publications in previous research.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {340–349},
numpages = {10},
keywords = {topic modeling, social network, learning analytics, hashtag analysis, Twitter analytics, Twitter},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2330601.2330616,
author = {Ferguson, Rebecca and Buckingham Shum, Simon},
title = {Social learning analytics: five approaches},
year = {2012},
isbn = {9781450311113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2330601.2330616},
doi = {10.1145/2330601.2330616},
abstract = {This paper proposes that Social Learning Analytics (SLA) can be usefully thought of as a subset of learning analytics approaches. SLA focuses on how learners build knowledge together in their cultural and social settings. In the context of online social learning, it takes into account both formal and informal educational environments, including networks and communities. The paper introduces the broad rationale for SLA by reviewing some of the key drivers that make social learning so important today. Five forms of SLA are identified, including those which are inherently social, and others which have social dimensions. The paper goes on to describe early work towards implementing these analytics on SocialLearn, an online learning space in use at the UK's Open University, and the challenges that this is raising. This work takes an iterative approach to analytics, encouraging learners to respond to and help to shape not only the analytics but also their associated recommendations.},
booktitle = {Proceedings of the 2nd International Conference on Learning Analytics and Knowledge},
pages = {23–33},
numpages = {11},
keywords = {transferable skills, social learning analytics, social learning, learning how to learn, learning analytics, educational assessment, discourse analytics, SocialLearn, 21st century skills},
location = {Vancouver, British Columbia, Canada},
series = {LAK '12}
}

@inproceedings{10.1145/2460296.2460337,
author = {Tempelaar, Dirk T. and Heck, Andr\'{e} and Cuypers, Hans and van der Kooij, Henk and van de Vrie, Evert},
title = {Formative assessment and learning analytics},
year = {2013},
isbn = {9781450317856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2460296.2460337},
doi = {10.1145/2460296.2460337},
abstract = {Learning analytics seeks to enhance the learning process through systematic measurements of learning related data, and informing learners and teachers of the results of these measurements, so as to support the control of the learning process. Learning analytics has various sources of information, two main types being intentional and learner activity related metadata [1]. This contribution aims to provide a practical application of Shum and Crick's theoretical framework [1] of a learning analytics infrastructure that combines learning dispositions data with data extracted from computer-based, formative assessments. The latter data component is derived from one of the educational projects of ONBETWIST, part of the SURF program 'Testing and Test Driven Learning'.},
booktitle = {Proceedings of the Third International Conference on Learning Analytics and Knowledge},
pages = {205–209},
numpages = {5},
keywords = {test directed learning, student profiles, learning dispositions, learning analytics, formative assessment, blended learning},
location = {Leuven, Belgium},
series = {LAK '13}
}

@inproceedings{10.1145/2567574.2567610,
author = {Gibson, Andrew and Kitto, Kirsty and Willis, Jill},
title = {A cognitive processing framework for learning analytics},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567610},
doi = {10.1145/2567574.2567610},
abstract = {Incorporating a learner's level of cognitive processing into Learning Analytics presents opportunities for obtaining rich data on the learning process. We propose a framework called COPA that provides a basis for mapping levels of cognitive operation into a learning analytics system. We utilise Bloom's taxonomy, a theoretically respected conceptualisation of cognitive processing, and apply it in a flexible structure that can be implemented incrementally and with varying degree of complexity within an educational organisation. We outline how the framework is applied, and its key benefits and limitations. Finally, we apply COPA to a University undergraduate unit, and demonstrate its utility in identifying key missing elements in the structure of the course.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {212–216},
numpages = {5},
keywords = {learning analytics, curriculum design, cognitive processing, Bloom's revised taxonomy},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/2460296.2460340,
author = {Dyckhoff, A. L. and Lukarov, V. and Muslim, A. and Chatti, M. A. and Schroeder, U.},
title = {Supporting action research with learning analytics},
year = {2013},
isbn = {9781450317856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2460296.2460340},
doi = {10.1145/2460296.2460340},
abstract = {Learning analytics tools should be useful, i.e., they should be usable and provide the functionality for reaching the goals attributed to learning analytics. This paper seeks to unite learning analytics and action research. Based on this, we investigate how the multitude of questions that arise during technology-enhanced teaching and learning systematically can be mapped to sets of indicators. We examine, which questions are not yet supported and propose concepts of indicators that have a high potential of positively influencing teachers' didactical considerations. Our investigation shows that many questions of teachers cannot be answered with currently available research tools. Furthermore, few learning analytics studies report about measuring impact. We describe which effects learning analytics should have on teaching and discuss how this could be evaluated.},
booktitle = {Proceedings of the Third International Conference on Learning Analytics and Knowledge},
pages = {220–229},
numpages = {10},
keywords = {learning analytics, indicators, impact analysis, action research},
location = {Leuven, Belgium},
series = {LAK '13}
}

@inproceedings{10.1145/2883851.2883904,
author = {Beheshitha, Sanam Shirazi and Hatala, Marek and Ga\v{s}evi\'{c}, Dragan and Joksimovi\'{c}, Sre\'{c}ko},
title = {The role of achievement goal orientations when studying effect of learning analytics visualizations},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883904},
doi = {10.1145/2883851.2883904},
abstract = {When designing learning analytics tools for use by learners we have an opportunity to provide tools that consider a particular learner's situation and the learner herself. To afford actual impact on learning, such tools have to be informed by theories of education. Particularly, educational research shows that individual differences play a significant role in explaining students' learning process. However, limited empirical research in learning analytics has investigated the role of theoretical constructs, such as motivational factors, that are underlying the observed differences between individuals. In this work, we conducted a field experiment to examine the effect of three designed learning analytics visualizations on students' participation in online discussions in authentic course settings. Using hierarchical linear mixed models, our results revealed that effects of visualizations on the quantity and quality of messages posted by students with differences in achievement goal orientations could either be positive or negative. Our findings highlight the methodological importance of considering individual differences and pose important implications for future design and research of learning analytics visualizations.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {54–63},
numpages = {10},
keywords = {visualizations, online discussions, learning analytics, dashboards, achievement goal orientation},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/3284179.3284236,
author = {Gon\c{c}alves, Alexandre L. and Alves, Gustavo R. and Carlos, Lucas M. and da Silva, Juarez B. and Alves, Jo\~{a}o B. da M.},
title = {Remote Experimentation supported by Learning Analytics and Recommender Systems},
year = {2018},
isbn = {9781450365185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3284179.3284236},
doi = {10.1145/3284179.3284236},
abstract = {This paper proposes a process based on learning analytics and recommender systems targeted at making suggestions to students about their remote laboratories activities and providing insights to all stakeholders taking part in the learning process. To apply the process, a log with requests and responses of remote experiments from the VISIR project were analyzed. A request is the setup of the experiment including the assembled circuits and the configurations of the measuring equipment. In turn, a response is a message provided by the measurement server indicating measures or an error when it is not possible to execute the experiment. Along the two phases of analysis, the log was analyzed and summarized in order to provide insights about students' experiments. In addition, there is a recommendation service responsible for analyzing the requests thus returning, in case of error, precise information about the assembly of circuits or configurations. The evaluation of the process is consistent in what regards its ability to afford recommendations to the students as they carry out the experiments. Moreover, the summarized information intends to offer teachers means to better understand and develop strategies to scaffold students' learning.},
booktitle = {Proceedings of the Sixth International Conference on Technological Ecosystems for Enhancing Multiculturality},
pages = {313–319},
numpages = {7},
keywords = {remote experimentation, recommender systems, learning analytics},
location = {Salamanca, Spain},
series = {TEEM'18}
}

@inproceedings{10.1145/2460296.2460298,
author = {Suthers, Dan and Verbert, Katrien},
title = {Learning analytics as a "middle space"},
year = {2013},
isbn = {9781450317856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2460296.2460298},
doi = {10.1145/2460296.2460298},
abstract = {Learning Analytics, an emerging field concerned with analyzing the vast data "given off" by learners in technology supported settings to inform educational theory and practice, has from its inception taken a multidisciplinary approach that integrates studies of learning with technological capabilities. In this introduction to the Proceedings of the Third International Learning Analytics &amp; Knowledge Conference, we discuss how Learning Analytics must function in the "middle space" where learning and analytic concerns meet. Dialogue in this middle space involves diverse stakeholders from multiple disciplines with various conceptions of the agency and nature of learning. We hold that a singularly unified field is not possible nor even desirable if we are to leverage the potential of this diversity, but progress is possible if we support "productive multivocality" between the diverse voices involved, facilitated by appropriate use of boundary objects. We summarize the submitted papers and contents of these Proceedings to characterize the voices and topics involved in the multivocal discourse of Learning Analytics.},
booktitle = {Proceedings of the Third International Conference on Learning Analytics and Knowledge},
pages = {1–4},
numpages = {4},
keywords = {productive multivocality, multidisciplinarity, learning analytics, boundary objects},
location = {Leuven, Belgium},
series = {LAK '13}
}

@inproceedings{10.1145/2567574.2576773,
author = {Gruzd, Anatoliy and Haythornthwaite, Caroline and Paulin, Drew and Absar, Rafa and Huggett, Michael},
title = {Learning analytics for the social media age},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2576773},
doi = {10.1145/2567574.2576773},
abstract = {In just a short period of time, social media have altered many aspects of our daily lives, from how we form and maintain social relationships to how we discover, access and share information online. Now social media are also beginning to affect how we teach and learn in this increasingly interconnected and information-rich world. The panelists will discuss their ongoing work that seeks to understand the affordances and potential roles of social media in learning, as well as to determine and provide methods that can help researchers and educators evaluate the use of social media for teaching and learning based on automated analyses of social media texts and networks. The panel will focus on the first phase of this five-year research initiative "Learning Analytics for the Social Media Age" funded by the Social Science and Humanites Research Council of Canada (2013--2018).},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {254–256},
numpages = {3},
keywords = {social media, online communities, learning networks, learning analytics},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/3481056.3481077,
author = {Yuang, Wei and Jining, Xu and Zehua, Zhang and Zhijun, Li},
title = {A Review of the Research on the Prediction of Learning Outcomes in the Field of Learning Analytics},
year = {2021},
isbn = {9781450390224},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3481056.3481077},
doi = {10.1145/3481056.3481077},
abstract = {Learning analytics is a new research field that seeks beneficial results from various education data. Recently, the prediction of students’ learning outcomes is becoming one of the hottest research issues. Despite large amounts of&nbsp;multilevel and multidimensional theoretical and practical research that has been performed by domestic and foreign scholars, comprehensive summaries are still lacking. Through systematic literature retrieval, this paper sorts out the current research hotspots, research limitations and main directions in the field of predictive analysis of learning results. In order to clearly describe and analyze the literature results, this paper uses the teaching goal classification theory, the individualized learning theory and the social cognitive theory as tools to classify the research, and conducts a systematic analysis of the empirical research results at different levels. The results show that in the current study of learning outcome prediction, most of them are based on knowledge space and made through students' learning behaviors, while there are few evaluations and predictions of students' potential traits from the level of ability and thinking. However, in recent years, the number of research results in this direction has been on the rise. Finally, this paper raises questions for future research and practice in the field may appear and potential research directions.},
booktitle = {Proceedings of the 5th International Conference on Education and Multimedia Technology},
pages = {154–162},
numpages = {9},
keywords = {Predictive index, Literature review, Learning outcome prediction, Learning behavior analysis, Learning analytics},
location = {Kyoto, Japan},
series = {ICEMT '21}
}

@inproceedings{10.1145/3231644.3231704,
author = {Gruzd, Anatoliy and Conroy, Nadia},
title = {Designing a learning analytics dashboard for twitter-facilitated teaching},
year = {2018},
isbn = {9781450358866},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3231644.3231704},
doi = {10.1145/3231644.3231704},
abstract = {Social media sites are increasingly being adopted to support teaching practice in higher education. Learning Analytics (LA) dashboards can be used to reveal how students engage with course material and others in the class. However, research on the best practices of designing, developing, and evaluating such dashboards to support teaching and learning with social media has been limited. Considering the increasing use of Twitter for both formal and informal learning processes, this paper presents our design process and a LA prototype dashboard developed based on a comprehensive literature review and an online survey among 54 higher education instructors who have used Twitter in their teaching.},
booktitle = {Proceedings of the Fifth Annual ACM Conference on Learning at Scale},
articleno = {46},
numpages = {4},
keywords = {teaching, survey, learning analytics, dashboards},
location = {London, United Kingdom},
series = {L@S '18}
}

@inproceedings{10.1145/3706468.3706513,
author = {Saqr, Mohammed and L\'{o}pez-Pernas, Sonsoles and T\"{o}rm\"{a}nen, Tiina and Kaliisa, Rogers and Misiejuk, Kamila and Tikka, Santtu},
title = {Transition Network Analysis: A Novel Framework for Modeling, Visualizing, and Identifying the Temporal Patterns of Learners and Learning Processes},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706513},
doi = {10.1145/3706468.3706513},
abstract = {This paper presents a novel learning analytics method: Transition Network Analysis (TNA), a method that integrates Stochastic Process Mining and probabilistic graph representation to model, visualize, and identify transition patterns in the learning process data. Combining the relational and temporal aspects into a single lens offers capabilities beyond either framework, including centralities to capture important learning events, community detection to identify behavior patterns, and clustering to reveal temporal patterns. Furthermore, TNA introduces several significance tests that go beyond either method and add rigor to the analysis. Here, we introduce the theoretical and mathematical foundations of TNA and we demonstrate the functionalities of TNA with a case study where students (n=191) engaged in small-group collaboration to map patterns of group dynamics using the theories of co-regulation and socially-shared regulated learning. The analysis revealed that TNA can map the regulatory processes as well as identify important events, patterns, and clusters. Bootstrap validation established the significant transitions and eliminated spurious transitions. As such, TNA can capture learning dynamics and provide a robust framework for investigating the temporal evolution of learning processes. Future directions include —inter alia— expanding estimation methods, reliability assessment, and building longitudinal TNA.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {351–361},
numpages = {11},
keywords = {Transition Network Analysis, Process Mining, Social Network Analysis, Learning process, Learning analytics},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/2723576.2723602,
author = {Xing, Wanli and Goggins, Sean},
title = {Learning analytics in outer space: a Hidden Na\"{\i}ve Bayes model for automatic student off-task behavior detection},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723602},
doi = {10.1145/2723576.2723602},
abstract = {Learning analytics (LA) has invested much effort in the investigation of students' behavior and performance within learning systems. This paper expands the influence of LA to students' behavior outside of learning systems and describes a novel machine learning model which automatically detects students' off-task behavior as students interact with a learning system, ASSISTments, based solely on log file data. We first operationalize social cognitive theory to introduce two new variables, affect states and problem set, both of which can be automatically derived from the logs, and can be considered to have a major influence on students' behavior. These two variables further work as the feature vector data for a K-means clustering algorithm in order to quantify students' different behavioral characteristics. This quantified variable representing student behavior type expands the feature space and contributes to the improvement of the various model performance compared with only time- and performance-related features. In addition, an advanced Hidden Na\"{\i}ve Bayes (HNB) algorithm is coded for off-task behavior detection and to show the best performance compared with traditional modeling techniques. Implications of the study are then discussed.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {176–183},
numpages = {8},
keywords = {social cognitive theory, off-task behavior, learning analytics, assessment, ITS, Hidden Na\"{\i}ve Bayes},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/3375462.3375500,
author = {Chen, Guanliang and Rolim, Vitor and Mello, Rafael Ferreira and Ga\v{s}evi\'{c}, Dragan},
title = {Let's shine together! a comparative study between learning analytics and educational data mining},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375500},
doi = {10.1145/3375462.3375500},
abstract = {Learning Analytics and Knowledge (LAK) and Educational Data Mining (EDM) are two of the most popular venues for researchers and practitioners to report and disseminate discoveries in data-intensive research on technology-enhanced education. After the development of about a decade, it is time to scrutinize and compare these two venues. By doing this, we expected to inform relevant stakeholders of a better understanding of the past development of LAK and EDM and provide suggestions for their future development. Specifically, we conducted an extensive comparison analysis between LAK and EDM from four perspectives, including (i) the topics investigated; (ii) community development; (iii) community diversity; and (iv) research impact. Furthermore, we applied one of the most widely-used language modeling techniques (Word2Vec) to capture words used frequently by researchers to describe future works that can be pursued by building upon suggestions made in the published papers to shed light on potential directions for future research.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {544–553},
numpages = {10},
keywords = {learning analytics, language modeling, hierarchical topic detection, educational data mining},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375519,
author = {Papamitsiou, Zacharoula and Giannakos, Michail N. and Ochoa, Xavier},
title = {From childhood to maturity: Are we there yet? Mapping the intellectual progress in learning analytics during the past decade},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375519},
doi = {10.1145/3375462.3375519},
abstract = {This study aims to identify the conceptual structure and the thematic progress in Learning Analytics (evolution) and to elaborate on backbone/emerging topics in the field (maturity) from 2011 to September 2019. To address this objective, this paper employs hierarchical clustering, strategic diagrams and network analysis to construct the intellectual map of the Learning Analytics community and to visualize the thematic landscape in this field, using co-word analysis. Overall, a total of 459 papers from the proceedings of the Learning Analytics and Knowledge (LAK) conference and 168 articles published in the Journal of Learning Analytics (JLA), and the respective 3092 author-assigned keywords and 4051 machine-extracted key-phrases, were included in the analyses. The results indicate that the community has significantly focused in areas like Massive Open Online Courses and visualizations; Learning Management Systems, assessment and self-regulated learning are also basic topics, yet topics like natural language processing and orchestration are emerging. The analysis highlights the shift of the research interest throughout the past decade, and the rise of new topics, comprising evidence that the field is expanding. Limitations of the approach and future work plans conclude the paper.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {559–568},
numpages = {10},
keywords = {learning analytics, conceptual evolution, co-word analysis, bibliometrics},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3170358.3170396,
author = {Lang, Charles and Macfadyen, Leah P. and Slade, Sharon and Prinsloo, Paul and Sclater, Niall},
title = {The complexities of developing a personal code of ethics for learning analytics practitioners: implications for institutions and the field},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170396},
doi = {10.1145/3170358.3170396},
abstract = {In this paper we explore the potential role, value and utility of a personal code of ethics (COE) for learning analytics practitioners, and in particular we consider whether such a COE might usefully mediate individual actions and choices in relation to a more abstract institutional COE. While several institutional COEs now exist, little attention has been paid to detailing the ethical responsibilities of individual practitioners. To investigate the problems associated with developing and implementing a personal COE, we drafted an LA Practitioner COE based on other professional codes, and invited feedback from a range of learning analytics stakeholders and practitioners: ethicists, students, researchers and technology executives. Three main themes emerged from their reflections: 1. A need to balance real world demands with abstract principles, 2. The limits to individual accountability within the learning analytics space, and 3. The continuing value of debate around an aspirational code of ethics within the field of learning analytics.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {436–440},
numpages = {5},
keywords = {professionalization, professional responsibility, code of ethics},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3386527.3406732,
author = {Haynes, Carl C.},
title = {The Role of Self-Regulated Learning in the Design, Implementation, and Evaluation of Learning Analytics Dashboards},
year = {2020},
isbn = {9781450379519},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3386527.3406732},
doi = {10.1145/3386527.3406732},
abstract = {Learning technologies are generating a vast quantity of data every day. This data is often presented to students through learning analytics dashboards (LADs) with a goal of improving learners' self-regulated learning. However, are students actually using these dashboards, and do they perceive that using dashboards lead to any changes in their behavior? In this paper we report on the development and implementation of several dashboard views, which we call My Learning Analytics (MyLA). This study found that students thought using the dashboard would have more of an effect on the way they planned their course activity at pre-use (after a demo) than post use. Low self-regulated learners believed so significantly less post-use and used the grade distribution view the least. Students made several suggestions for ways to improve the grade distribution view and rated MyLA's usability more positively at pre- than post-use. Given the low use and low perceived impact of the current dashboard, we suggest that researchers use participatory design to illicit students' needs and better incorporate student suggestions.},
booktitle = {Proceedings of the Seventh ACM Conference on Learning @ Scale},
pages = {297–300},
numpages = {4},
keywords = {self-regulated learning, learning analytics, dashboards},
location = {Virtual Event, USA},
series = {L@S '20}
}

@inproceedings{10.1145/3636555.3636939,
author = {Song, Yige and Oliveira, Eduardo and Kirley, Michael and Thompson, Pauline},
title = {A Case Study on University Student Online Learning Patterns Across Multidisciplinary Subjects},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636939},
doi = {10.1145/3636555.3636939},
abstract = {This case study explores the online learning patterns of a cohort of first-year university students in two subjects: a compulsory science subject and an introductory programming subject, by analysing trace data from the Learning Management Systems (LMS). The methodology extends existing learning analytics techniques to incorporate temporal aspects of students’ learning, such as session duration and weekly online behaviours. By examining over 82,000 learning actions, the research unveils significant variations in students’ online learning strategies between subjects, offering deeper insights into these differences and their associated challenges. The study seeks to initiate broader discussions in learning analytics, emphasising the need to comprehend students’ diverse online learning experiences and encouraging further exploration in future research.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {936–942},
numpages = {7},
keywords = {Blended learning, Learning analytics, Learning strategy, Learning tactic, Trace data},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3027385.3027429,
author = {Andrade, Alejandro},
title = {Understanding student learning trajectories using multimodal learning analytics within an embodied-interaction learning environment},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027429},
doi = {10.1145/3027385.3027429},
abstract = {The aim of this paper is to show how multimodal learning analytics (MMLA) can help understand how elementary students explore the concept of feedback loops while controlling an embodied simulation of a predator-prey ecosystem using hand movements as an interface with the computer simulation. We represent student motion patterns from fine-grained logs of hands and gaze data, and then map these observed motion patterns against levels of student performance to make inferences about how embodiment plays a role in the learning process. Results show five distinct motion sequences in students' embodied interactions, and these motion patterns are statistically associated with initial and post-tutorial levels of students' understanding of feedback loops. Analysis of student gaze also shows distinctive patterns as to how low- and high-performing students attended to information presented in the simulation. Using MMLA, we show how students' explanations of feedback loops look differently according to cluster membership, which provides evidence that embodiment interacts with conceptual understanding.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {70–79},
numpages = {10},
keywords = {sensing technologies, science education, multimodal learning analytics, learning environments, embodiment, embodied cognition},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3029482,
author = {Quigley, David and McNamara, Conor and Sumner, Tamara},
title = {Using learning analytics in iterative design of a digital modeling tool},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029482},
doi = {10.1145/3027385.3029482},
abstract = {Iterative design is a powerful method for developing digital classroom tools and curricula. We explore how infusing learning analytics into this process has influenced our development of EcoSurvey, a digital modeling tool for mapping the organisms and interactions in the local ecosystem. We have found that analytic techniques can help us discover areas in which students struggle to engage with scientific modeling, and we can iteratively use learning analytics to demonstrate the impact of design changes.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {602–603},
numpages = {2},
keywords = {scientific modeling, iterative design, collaborative modeling},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/2460296.2460312,
author = {Knight, Simon and Buckingham Shum, Simon and Littleton, Karen},
title = {Epistemology, pedagogy, assessment and learning analytics},
year = {2013},
isbn = {9781450317856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2460296.2460312},
doi = {10.1145/2460296.2460312},
abstract = {There is a well-established literature examining the relationships between epistemology (the nature of knowledge), pedagogy (the nature of learning and teaching), and assessment. Learning Analytics (LA) is a new assessment technology and should engage with this literature since it has implications for when and why different LA tools might be deployed. This paper discusses these issues, relating them to an example construct, epistemic beliefs -- beliefs about the nature of knowledge -- for which analytics grounded in pragmatic, sociocultural theory might be well placed to explore. This example is particularly interesting given the role of epistemic beliefs in the everyday knowledge judgements students make in their information processing. Traditional psychological approaches to measuring epistemic beliefs have parallels with high stakes testing regimes; this paper outlines an alternative LA for epistemic beliefs which might be readily applied to other areas of interest. Such sociocultural approaches afford opportunity for engaging LA directly in high quality pedagogy.},
booktitle = {Proceedings of the Third International Conference on Learning Analytics and Knowledge},
pages = {75–84},
numpages = {10},
keywords = {social learning analytics, pedagogy, learning analytics, epistemology, educational assessment, discourse analytics},
location = {Leuven, Belgium},
series = {LAK '13}
}

@inproceedings{10.1145/2567574.2567592,
author = {Ferguson, Rebecca and Clow, Doug and Macfadyen, Leah and Essa, Alfred and Dawson, Shane and Alexander, Shirley},
title = {Setting learning analytics in context: overcoming the barriers to large-scale adoption},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567592},
doi = {10.1145/2567574.2567592},
abstract = {Once learning analytics have been successfully developed and tested, the next step is to implement them at a larger scale -- across a faculty, an institution or an educational system. This introduces a new set of challenges, because education is a stable system, resistant to change. Implementing learning analytics at scale involves working with the entire technological complex that exists around technology-enhanced learning (TEL). This includes the different groups of people involved -- learners, educators, administrators and support staff -- the practices of those groups, their understandings of how teaching and learning take place, the technologies they use and the specific environments within which they operate. Each element of the TEL Complex requires explicit and careful consideration during the process of implementation, in order to avoid failure and maximise the chances of success. In order for learning analytics to be implemented successfully at scale, it is crucial to provide not only the analytics and their associated tools but also appropriate forms of support, training and community building.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {251–253},
numpages = {3},
keywords = {technology-enhanced learning, teaching, learning analytics, learning, implementation, higher education, education, change management, change, administration, TEL complex},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/2330601.2330605,
author = {Siemens, George},
title = {Learning analytics: envisioning a research discipline and a domain of practice},
year = {2012},
isbn = {9781450311113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2330601.2330605},
doi = {10.1145/2330601.2330605},
abstract = {Learning analytics are rapidly being implemented in different educational settings, often without the guidance of a research base. Vendors incorporate analytics practices, models, and algorithms from datamining, business intelligence, and the emerging "big data" fields. Researchers, in contrast, have built up a substantial base of techniques for analyzing discourse, social networks, sentiments, predictive models, and in semantic content (i.e., "intelligent" curriculum). In spite of the currently limited knowledge exchange and dialogue between researchers, vendors, and practitioners, existing learning analytics implementations indicate significant potential for generating novel insight into learning and vital educational practices. This paper presents an integrated and holistic vision for advancing learning analytics as a research discipline and a domain of practices. Potential areas of collaboration and overlap are presented with the intent of increasing the impact of analytics on teaching, learning, and the education system.},
booktitle = {Proceedings of the 2nd International Conference on Learning Analytics and Knowledge},
pages = {4–8},
numpages = {5},
keywords = {theory, research, practice, learning analytics, ethics, data integration, collaboration},
location = {Vancouver, British Columbia, Canada},
series = {LAK '12}
}

@inproceedings{10.1145/3375462.3375540,
author = {Wiley, Korah J. and Dimitriadis, Yannis and Bradford, Allison and Linn, Marica C.},
title = {From theory to action: developing and evaluating learning analytics for learning design},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375540},
doi = {10.1145/3375462.3375540},
abstract = {The effectiveness of using learning analytics for learning design primarily depends upon two concepts: grounding and alignment. This is the primary conjecture for the study described in this paper. In our design-based research study, we design, test, and evaluate teacher-facing learning analytics for an online inquiry science unit on global climate change. We design our learning analytics in accordance with a socioconstructivism-based pedagogical framework, called Knowledge Integration, and the principles of learning analytics Implementation Design. Our methodology for the design process draws upon the principle of the Orchestrating for Learning Analytics framework to engage stakeholders (i.e. teachers, researchers, and developers). The resulting learning analytics were aligned to unit activities that engaged students in key aspects of the knowledge integration process. They provided teachers with actionable insight into their students' understanding at critical junctures in the learning process. We demonstrate the efficacy of the learning analytics in supporting the optimization of the unit's learning design. We conclude by synthesizing the principles that guided our design process into a framework for developing and evaluating learning analytics for learning design.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {569–578},
numpages = {10},
keywords = {theory, learning design, learning analytics, design-based research, TEL environment},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3636555.3636880,
author = {Valle Torre, Manuel and Oertel, Catharine and Specht, Marcus},
title = {The Sequence Matters in Learning - A Systematic Literature Review},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636880},
doi = {10.1145/3636555.3636880},
abstract = {Describing and analysing learner behaviour using sequential data and analysis is becoming more and more popular in Learning Analytics. Nevertheless, we found a variety of definitions of learning sequences, as well as choices regarding data aggregation and the methods implemented for analysis. Furthermore, sequences are used to study different educational settings and serve as a base for various interventions. In this literature review, the authors aim to generate an overview of these aspects to describe the current state of using sequence analysis in educational support and learning analytics. The 74 included articles were selected based on the criteria that they conduct empirical research on an educational environment using sequences of learning actions as the main focus of their analysis. The results enable us to highlight different learning tasks where sequences are analysed, identify data mapping strategies for different types of sequence actions, differentiate techniques based on purpose and scope, and identify educational interventions based on the outcomes of sequence analysis.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {263–272},
numpages = {10},
keywords = {Learning Analytics, Learning Sequences, Literature Review, Sequence Analysis},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/2567574.2567588,
author = {Wise, Alyssa Friend},
title = {Designing pedagogical interventions to support student use of learning analytics},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567588},
doi = {10.1145/2567574.2567588},
abstract = {This article addresses a relatively unexplored area in the emerging field of learning analytics, the design of learning analytics interventions. A learning analytics intervention is defined as the surrounding frame of activity through which analytic tools, data, and reports are taken up and used. It is a soft technology that involves the orchestration of the human process of engaging with the analytics as part of the larger teaching and learning activity. This paper first makes the case for the overall importance of intervention design, situating it within the larger landscape of the learning analytics field, and then considers the specific issues of intervention design for student use of learning analytics. Four principles of pedagogical learning analytics intervention design that can be used by teachers and course developers to support the productive use of learning analytics by students are introduced: Integration, Agency, Reference Frame and Dialogue. In addition three core processes in which to engage students are described: Grounding, Goal-Setting and Reflection. These principles and processes are united in a preliminary model of pedagogical learning analytics intervention design for students, presented as a starting point for further inquiry.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {203–211},
numpages = {9},
keywords = {student participation, learning analytics, intervention design},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/2330601.2330635,
author = {Prinsloo, Paul and Slade, Sharon and Galpin, Fenella},
title = {Learning analytics: challenges, paradoxes and opportunities for mega open distance learning institutions},
year = {2012},
isbn = {9781450311113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2330601.2330635},
doi = {10.1145/2330601.2330635},
abstract = {Despite all the research on student retention and success since the first conceptual mappings of student success e.g. Spady [12], there have not been equal impacts on the rates of both student success and retention. To realise the potential of learning analytics to impact on student retention and success, mega open distance learning (ODL) institutions face a number of challenges, paradoxes and opportunities.For the purpose of this paper we critique a 'closed' view of learning analytics as focusing only on data produced by students' interactions with institutions of higher learning. Students are not the only actors in their learning journeys and it would seem crucial that learning analytics also includes the impacts of all stakeholders on students' learning journeys in order to increase the success of students' learning. As such the notion of 'Thirdspace' as used by cultural, postmodern and identity theorists provide a useful heuristic to map the challenges and opportunities, but also the paradoxes of learning analytics and its potential impact on student success and retention.This paper explores some of these challenges, paradoxes and opportunities with reference to two mega ODL institutions namely the Open University in the UK (OU) and the University of South Africa (Unisa). Although these two institutions share a number of characteristics, there are also some major and important differences between them. We explore some of the shared challenges, paradoxes and opportunities learning analytics offer in the context of these two institutions.},
booktitle = {Proceedings of the 2nd International Conference on Learning Analytics and Knowledge},
pages = {130–133},
numpages = {4},
keywords = {student walk, open university (OU), learning analytics, University of South Africa (Unisa), Thirdspace},
location = {Vancouver, British Columbia, Canada},
series = {LAK '12}
}

@inproceedings{10.1145/3430895.3460160,
author = {Williamson, Kimberly and Kizilcec, Ren\'{e} F.},
title = {Learning Analytics Dashboard Research Has Neglected Diversity, Equity and Inclusion},
year = {2021},
isbn = {9781450382151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3430895.3460160},
doi = {10.1145/3430895.3460160},
abstract = {Learning analytic dashboards (LADs) have become more prevalent in higher education to help students, faculty, and staff make data-informed decisions. Despite extensive research on the design and usability of LADs, few studies have examined them in relation to issues of diversity, equity, and inclusion. We conducted a critical literature review to address three research questions: How does LAD research contribute to improving diversity, equity, and inclusion? How might LADs contribute to maintaining or exacerbating inequitable outcomes? And what future opportunities exist in this research space? Our review showed little use of LADs to address or improve issues of diversity, equity, and inclusion in the literature thus far. We argue that excluding these issues from LAD research is not an isolated oversight and it risks reinforcing existing inequities within the higher education system. We argue that LADs can be designed, researched, and deployed intentionally to advance equitable outcomes and help dismantle inequities in education. We highlight opportunities for future LAD research to address issues of diversity, equity, and inclusion.},
booktitle = {Proceedings of the Eighth ACM Conference on Learning @ Scale},
pages = {287–290},
numpages = {4},
keywords = {literature review, learning analytic dashboards, inclusion, equity, diversity},
location = {Virtual Event, Germany},
series = {L@S '21}
}

@inproceedings{10.1145/3284179.3284233,
author = {Villama\~{n}e, Mikel and \'{A}lvarez, Ainhoa and Larra\~{n}aga, Mikel and Caballero, Jessica and Hern\'{a}ndez-Rivas, Oscar},
title = {Using Visual Learning Analytics to Support Competence-based Learning},
year = {2018},
isbn = {9781450365185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3284179.3284233},
doi = {10.1145/3284179.3284233},
abstract = {Competence-based Learning has become more and more popular in the last few years and some researchers have claimed that formative assessment should be followed to improve the learning processes. In order to achieve the mastery of competencies, many lecturers have been required to adopt various and diverse applications in their courses to work on those competences. The use of several applications brings new challenges, as the information of the learning processes is distributed among these systems and getting a global picture of the evolution becomes much harder. Therefore, systems that enable gathering and unifying formative assessment information coming from various sources in order to extract significant information about the global learning process are needed. In this paper, we present a system called COBLE (Competence-Based Learning Environment) that supports Competence-based Learning and combines Visual Learning Analytics and recommendation aspects in order to promote the students' and lecturers' self-reflection about the learning and teaching processes. COBLE supports data from different sources to be integrated, providing the users with the complete report of what is going on in their courses.},
booktitle = {Proceedings of the Sixth International Conference on Technological Ecosystems for Enhancing Multiculturality},
pages = {333–338},
numpages = {6},
keywords = {Visual learning analytics, Feedback, Competency-based Learning},
location = {Salamanca, Spain},
series = {TEEM'18}
}

@inproceedings{10.1145/3345120.3345160,
author = {Chen, Xueliang and Miyazaki, Yoshinori},
title = {Using Mobile Application for Word Reordering Problems to Enhance Learning Analytics},
year = {2019},
isbn = {9781450372107},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3345120.3345160},
doi = {10.1145/3345120.3345160},
abstract = {The authors have developed a mobile application for word reordering problems (WRPs) and its learning management system (LMS) based on the WRP web application in order to collect learners' study logs. Various parameters such as "total distance of movements" and "answer time" are generated by learners' trajectories to detect their hesitations in answering WRPs. The main objective of this research is to exploit the practical use of new data types provided by physical sensors embedded in mobile devices, with the belief that specific new data types possibly benefit learning analytics. The authors used acceleration data and force data to detect abnormal trajectories - a series of trajectories caused by unstable environments resulting in low accuracy of detecting hesitations. The authors conducted an experiment to determine the most suitable pair of thresholds (acceleration and force parameter values). By setting this pair, f-measure to identify abnormal trajectories has reached approximately 0.727. This means that we are able to detect abnormal trajectories at a certain rate and cut some of them off to get more accurate results to detect hesitations using mobile devices.},
booktitle = {Proceedings of the 3rd International Conference on Education and Multimedia Technology},
pages = {316–320},
numpages = {5},
keywords = {Word Reordering Problems, Mobile Device, Learning Management System, Learning Analytics, Hesitation},
location = {Nagoya, Japan},
series = {ICEMT '19}
}

@inproceedings{10.1145/3085585.3085586,
author = {Hui, Bowen and Farvolden, Shannon},
title = {How Can Learning Analytics Improve a Course?},
year = {2017},
isbn = {9781450350662},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3085585.3085586},
doi = {10.1145/3085585.3085586},
abstract = {Despite much excitement with learning analytics, there is still a lack of adoption in the classrooms. Possible reasons may include not having enough time to incorporate the use of analytics, not being familiar enough with specific techniques to readily apply them, or not knowing how data can help shape a curriculum or the classroom experience altogether. Learning analytics is a problem-driven research field, where the domain problem -- the people involved, the subject matter, and the learning environment -- drives the techniques and the solutions that are used. From this perspective, we propose a new framework with a suite of pedagogical questions that can be addressed using data to support decisions made about the curriculum or classroom structure. In addition, we present a case study with 69 participants in a CS1 course as a way to demonstrate how some of these questions are addressed. Our ultimate goal is to improve the quality of the students' learning experience using an evidence-based approach.},
booktitle = {Proceedings of the 22nd Western Canadian Conference on Computing Education},
articleno = {1},
numpages = {6},
keywords = {needs analysis, learning analytics, evidence-based course design, CS1},
location = {Abbotsford, BC, Canada},
series = {WCCCE '17}
}

@inproceedings{10.1145/2567574.2567630,
author = {Drachsler, Hendrik and Dietze, Stefan and Herder, Eelco and d'Aquin, Mathieu and Taibi, Davide},
title = {The learning analytics &amp; knowledge (LAK) data challenge 2014},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567630},
doi = {10.1145/2567574.2567630},
abstract = {The LAK Data Challenge 2014 continues the research efforts of the second edition by stimulating research on the evolving fields Learning Analytics (LA) and Educational Data Mining (EDM). Building on a series of activities of the LinkedUp project, the challenge aims to generate new insights and analysis on the LA &amp; EDM disciplines and is supported through the LAK Dataset - a unique corpus of LA &amp; EDM literature, exposed in structured and machine-readable formats.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {289–290},
numpages = {2},
keywords = {visualization, linked data, learning analytics, data mining},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/3706468.3706520,
author = {Albuquerque, Josmario and Rienties, Bart and Divjak, Bla\v{z}enka},
title = {Decoding Learning Design Decisions: A Cluster Analysis of 12,749 Teaching and Learning Activities},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706520},
doi = {10.1145/3706468.3706520},
abstract = {Substantial progress has been made in how educators can be supported to implement effective learning design (LD) with learning analytics (LA). However, how educators make micro-decisions about designing individual teaching and learning activities (TLAs) and how these are related to wider pedagogical approaches has received limited empirical support. This study explored how 165 educators designed and integrated 12,749 TLA in 218 LDs using clustering, pattern-mining, and correlational analysis. The findings suggest most educators use a combination of four common LD TLAs (i.e., Collaboration, Generating independent learning, Assessment, and Traditional classroom activities). The four common TLAs could be used to develop LA and Generative Artificial Intelligence (Gen-AI) approaches to support educators in making more informed and evidence-based design decisions for effective learning and teaching.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {407–417},
numpages = {11},
keywords = {Learning Design, Learning Analytics, Cluster Analysis, Teaching and Learning Activities, Artificial Intelligence},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/2330601.2330636,
author = {Clow, Doug},
title = {The learning analytics cycle: closing the loop effectively},
year = {2012},
isbn = {9781450311113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2330601.2330636},
doi = {10.1145/2330601.2330636},
abstract = {This paper develops Campbell and Oblinger's [4] five-step model of learning analytics (Capture, Report, Predict, Act, Refine) and other theorisations of the field, and draws on broader educational theory (including Kolb and Sch\"{o}n) to articulate an incrementally more developed, explicit and theoretically-grounded Learning Analytics Cycle.This cycle conceptualises successful learning analytics work as four linked steps: learners (1) generating data (2) that is used to produce metrics, analytics or visualisations (3). The key step is 'closing the loop' by feeding back this product to learners through one or more interventions (4).This paper seeks to begin to place learning analytics practice on a base of established learning theory, and draws several implications from this theory for the improvement of learning analytics projects. These include speeding up or shortening the cycle so feedback happens more quickly, and widening the audience for feedback (in particular, considering learners and teachers as audiences for analytics) so that it can have a larger impact.},
booktitle = {Proceedings of the 2nd International Conference on Learning Analytics and Knowledge},
pages = {134–138},
numpages = {5},
keywords = {policy, learning analytics, feedback, analytics, academic analytics},
location = {Vancouver, British Columbia, Canada},
series = {LAK '12}
}

@inproceedings{10.1145/2567574.2567609,
author = {Papamitsiou, Zacharoula K. and Terzis, Vasileios and Economides, Anastasios A.},
title = {Temporal learning analytics for computer based testing},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567609},
doi = {10.1145/2567574.2567609},
abstract = {Predicting student's performance is a challenging, yet complicated task for institutions, instructors and learners. Accurate predictions of performance could lead to improved learning outcomes and increased goal achievement. In this paper we explore the predictive capabilities of student's time-spent on answering (in-)correctly each question of a multiple-choice assessment quiz, along with student's final quiz-score, in the context of computer-based testing. We also explore the correlation between the time-spent factor (as defined here) and goal-expectancy. We present a case study and investigate the value of using this parameter as a learning analytics factor for improving prediction of performance during computer-based testing. Our initial results are encouraging and indicate that the temporal dimension of learning analytics should be further explored.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {31–35},
numpages = {5},
keywords = {temporal learning analytics, prediction of performance, prediction, goal-expectancy, educational data mining, computer-based testing, computer based assessment},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/2883851.2883912,
author = {Jo, Yohan and Tomar, Gaurav and Ferschke, Oliver and Ros\'{e}, Carolyn Penstein and Ga\v{s}evi\'{c}, Dragan},
title = {Pipeline for expediting learning analytics and student support from data in social learning},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883912},
doi = {10.1145/2883851.2883912},
abstract = {An important research problem in learning analytics is to expedite the cycle of data leading to the analysis of student progress and the improvement of student support. For this goal in the context of social learning, we propose a pipeline that includes data infrastructure, learning analytics, and intervention, along with computational models for individual components. Next, we describe an example of applying this pipeline to real data in a case study, whose goal is to investigate the positive effects that goal-setting students have on their peers, which suggests ways in which we might foster these social benefits through intervention.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {542–543},
numpages = {2},
keywords = {social learning, learning analytics},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/3369199.3369227,
author = {Lo, Chung Kwan and Chen, Gaowei},
title = {The Use of Classroom Visual Learning Analytics in Professional Development: Preliminary Findings of Mathematics Teachers' Instructional Changes},
year = {2020},
isbn = {9781450372206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3369199.3369227},
doi = {10.1145/3369199.3369227},
abstract = {The use of digital technology has become increasingly widespread in the education sector. In this article, we describe how we used visual learning analytics of classroom videos in our year-long professional development program for secondary school mathematics teachers in Shanghai, China. The program introduced the knowledge and skills of classroom talk, aiming to change the teacher-dominated classroom culture. We used our classroom discourse analyzer to facilitate teacher reflection on their classroom practice. Using this kind of digital technology, the complex data of classroom videos became visual learning analytics and comprehensible for a review. This article focuses on the instructional changes of a novice teacher and an experienced teacher. After attending our program, the teachers changed their practice to some extent. Nevertheless, the novice teacher had a greater improvement compared with the experienced teacher in terms of the percentage of students' word contributions and the average number of words per turn in lessons. This article presents and discusses preliminary findings of our lesson analyses and teacher perceptions of our professional development program.},
booktitle = {Proceedings of the 3rd International Conference on Digital Technology in Education},
pages = {185–189},
numpages = {5},
keywords = {professional development, mathematics education, classroom talk, Visual learning analytics},
location = {Yamanashi, Japan},
series = {ICDTE '19}
}

@inproceedings{10.1145/2883851.2883902,
author = {Sharma, Kshitij and Alavi, Hamed S. and Jermann, Patrick and Dillenbourg, Pierre},
title = {A gaze-based learning analytics model: in-video visual feedback to improve learner's attention in MOOCs},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883902},
doi = {10.1145/2883851.2883902},
abstract = {In the context of MOOCs, "With-me-ness" refers to the extent to which the learner succeeds in following the teacher, specifically in terms of looking at the area in the video that the teacher is explaining. In our previous works, we employed eye-tracking methods to quantify learners' With-me-ness and showed that it is positively correlated with their learning gains. In this contribution, we describe a tool that is designed to improve With-me-ness by providing a visual-aid superimposed on the video. The position of the visual-aid is suggested by the teachers' dialogue and deixis, and it is displayed when the learner's With-me-ness is under the average value, which is computed from the other students' gaze behavior. We report on a user-study that examines the effectiveness of the proposed tool. The results show that it significantly improves the learning gain and it significantly increases the extent to which the students follow the teacher. Finally, we demonstrate how With-me-ness can create a complete theoretical framework for conducting gaze-based learning analytics in the context of MOOCs.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {417–421},
numpages = {5},
keywords = {video based learning, student attention, eye-tracking, MOOCs},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/3290605.3300824,
author = {Sun, Kaiwen and Mhaidli, Abraham H. and Watel, Sonakshi and Brooks, Christopher A. and Schaub, Florian},
title = {It's My Data! Tensions Among Stakeholders of a Learning Analytics Dashboard},
year = {2019},
isbn = {9781450359702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290605.3300824},
doi = {10.1145/3290605.3300824},
abstract = {Early warning dashboards in higher education analyze student data to enable early identification of underperforming students, allowing timely interventions by faculty and staff. To understand perceptions regarding the ethics and impact of such learning analytics applications, we conducted a multi-stakeholder analysis of an early-warning dashboard deployed at the University of Michigan through semi-structured interviews with the system's developers, academic advisors (the primary users), and students. We identify multiple tensions among and within the stakeholder groups, especially with regard to awareness, understanding, access and use of the system. Furthermore, ambiguity in data provenance and data quality result in differing levels of reliance and concerns about the system among academic advisors and students. While students see the system's benefits, they argue for more involvement, control, and informed consent regarding the use of student data. We discuss our findings' implications for the ethical design and deployment of learning analytics applications in higher education. Early warning dashboards in higher education analyze student data to enable early identification of underperforming students, allowing timely interventions by faculty and staff. To understand perceptions regarding the ethics and impact of such learning analytics applications, we conducted a multi-stakeholder analysis of an early-warning dashboard deployed at the University of Michigan through semi-structured interviews with the system's developers, academic advisors (the primary users), and students. We identify multiple tensions among and within the stakeholder groups, especially with regard to awareness, understanding, access, and use of the system. Furthermore, ambiguity in data provenance and data quality result in differing levels of reliance and concerns about the system among academic advisors and students. While students see the system's benefits, they argue for more involvement, control, and informed consent regarding the use of student data. We discuss our findings' implications for the ethical design and deployment of learning analytics applications in higher education.},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–14},
numpages = {14},
keywords = {student data, privacy, learning analytics, higher education, ethics, early warning dashboards},
location = {Glasgow, Scotland Uk},
series = {CHI '19}
}

@inproceedings{10.1145/3382507.3418890,
author = {Emerson, Andrew and Henderson, Nathan and Rowe, Jonathan and Min, Wookhee and Lee, Seung and Minogue, James and Lester, James},
title = {Early Prediction of Visitor Engagement in Science Museums with Multimodal Learning Analytics},
year = {2020},
isbn = {9781450375818},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382507.3418890},
doi = {10.1145/3382507.3418890},
abstract = {Modeling visitor engagement is a key challenge in informal learning environments, such as museums and science centers. Devising predictive models of visitor engagement that accurately forecast salient features of visitor behavior, such as dwell time, holds significant potential for enabling adaptive learning environments and visitor analytics for museums and science centers. In this paper, we introduce a multimodal early prediction approach to modeling visitor engagement with interactive science museum exhibits. We utilize multimodal sensor data including eye gaze, facial expression, posture, and interaction log data captured during visitor interactions with an interactive museum exhibit for environmental science education, to induce predictive models of visitor dwell time. We investigate machine learning techniques (random forest, support vector machine, Lasso regression, gradient boosting trees, and multi-layer perceptron) to induce multimodal predictive models of visitor engagement with data from 85 museum visitors. Results from a series of ablation experiments suggest that incorporating additional modalities into predictive models of visitor engagement improves model accuracy. In addition, the models show improved predictive performance over time, demonstrating that increasingly accurate predictions of visitor dwell time can be achieved as more evidence becomes available from visitor interactions with interactive science museum exhibits. These findings highlight the efficacy of multimodal data for modeling museum exhibit visitor engagement.},
booktitle = {Proceedings of the 2020 International Conference on Multimodal Interaction},
pages = {107–116},
numpages = {10},
keywords = {visitor modeling, museum-based learning, multimodal learning analytics, early prediction},
location = {Virtual Event, Netherlands},
series = {ICMI '20}
}

@inproceedings{10.1145/3284179.3284354,
author = {Filv\`{a}, Daniel Amo and Garc\'{\i}a-Pe\~{n}alvo, Francisco Jos\'{e} and Forment, Marc Alier and Escudero, David Fonseca and Casa\~{n}, Maria Jos\'{e}},
title = {Privacy and identity management in Learning Analytics processes with Blockchain},
year = {2018},
isbn = {9781450365185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3284179.3284354},
doi = {10.1145/3284179.3284354},
abstract = {The collection of students' sensible data raises adverse reactions against Learning Analytics that decreases the confidence in its adoption. The laws and policies that surround the use of educational data are not enough to ensure privacy, security, validity, integrity and reliability of students' data. This problem has been detected through literature review and can be solved if a technological layer of automated checking rules is added above these policies. The aim of this thesis is to research about an emerging technology such as blockchain to preserve the identity of students and secure their data. In a first stage a systematic literature review will be conducted in order to set the context of the research. Afterwards, and through the scientific method, we will develop a blockchain based solution to automate rules and constraints with the aim to let students the governance of their data and to ensure data privacy and security.},
booktitle = {Proceedings of the Sixth International Conference on Technological Ecosystems for Enhancing Multiculturality},
pages = {997–1003},
numpages = {7},
keywords = {digital identity, data security management, data privacy, blockchain, Learning analytics},
location = {Salamanca, Spain},
series = {TEEM'18}
}

@article{10.1145/3632365,
author = {Modak, Masooda M. and Gharpure, Prachi and M, Sasikumar},
title = {Adaptive Learning and Correlative Assessment of Differential Usage Patterns for Students with-or-without Learning Disabilities via Learning Analytics},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {12},
issn = {2375-4699},
url = {https://doi.org/10.1145/3632365},
doi = {10.1145/3632365},
abstract = {Learning Disabilities (LD) can be categorized into logical, analytical, grammatical, vocabulary, sequential, and inference disabilities. Analysis of such disabilities assists students to identify and strengthen their weak areas. A wide variety of analysis models is proposed by researchers to perform such tasks, but most of these models are highly complex and cannot be scaled for multimodal parameter sets. To overcome these issues, this text proposes a model for correlative assessment of differential usage patterns in students with-or-without learning disabilities via multimodal analysis. The proposed model initially collects real-time inference sets for students with Learning Disabilities (LD) and without LDs. These sets consist of question-specific recorded responses for “Addition,” “Carry Propagation,” “Basic to Advanced Grammar,” “Direct, Inference and Vocabulary Comprehension,” “Finding odd-man-out,” “Sequencing,” and “Pseudo and Sight Spelling” for different question sets. Answers to these questions and their metadata were processed via a correlative engine that assisted in evaluation of correctness, time needed per question per category, number of skips, number of revisits, and unanswered ratio for different students. This evaluation was combined with temporal analysis to identify per-category progress of students. Based on this progress, students were either upgraded to next level or given lower-level questions, which assisted them to incrementally improve their grades. The model proved that the performance of LD students is 55% less than the non-LD students and an average of 18 LD students have achieved an average of 33% of improvement after having multiple attempts of the adaptive lessons. The model uses a correlation function, which enables to identify answering patterns of LD and non-LD students with 98.4% accuracy, thus can be used for clinical scenarios.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = dec,
articleno = {258},
numpages = {25},
keywords = {multimodal analysis, multiple attempt analysis, correlation analysis, Learning disability}
}

@inproceedings{10.1145/3027385.3027444,
author = {Aguerrebere, Cecilia and Cobo, Crist\'{o}bal and Gomez, Marcela and Mateu, Mat\'{\i}as},
title = {Strategies for data and learning analytics informed national education policies: the case of Uruguay},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027444},
doi = {10.1145/3027385.3027444},
abstract = {This work provides an overview of an education and technology monitoring system developed at Plan Ceibal, a nationwide initiative created to enable technology enhanced learning in Uruguay. Plan Ceibal currently offers one-to-one access to technology and connectivity to every student and teacher (from primary and secondary education) as well as a comprehensive set of educational software platforms. All these resources generate massive amounts of data about the progress and style of students learning. This work introduces the conceptual framework, design and preliminary results of the Big Data Center for learning analytics currently being developed at Plan Ceibal. This initiative is focused on exploiting these datasets and conducting advanced analytics to support the educational system. To this aim, a 360 degrees profile will be built including information characterizing the user's online behavior as well as a set of technology enhanced learning factors. These profiles will be studied both at user (e.g., student or teacher) and larger scale levels (e.g., per school or school system), addressing both the need of understanding how technology is being used for learning as well as to provide accurate feedback to support evidence based educational policies.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {449–453},
numpages = {5},
keywords = {technology enhanced learning, plan ceibal, education policies, big data},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3434780.3436601,
author = {Conde, Miguel \'{A}. and Rodr\'{\i}guez-Sedano, Francisco J. and Fern\'{a}ndez, Camino and Guti\'{e}rrez-Fern\'{a}ndez, Alexis and Fern\'{a}ndez-Robles, Laura and Castej\'{o}n Limas, Manuel},
title = {A Learning Analytics tool for the analysis of students’ Telegram messages in the context of teamwork virtual activities},
year = {2021},
isbn = {9781450388504},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3434780.3436601},
doi = {10.1145/3434780.3436601},
abstract = {In the current COVID-19 pandemic situation, online education has been the only approach of most educational institutions. It is necessary to have tools to assess students in online education, even when they carry out activities that are most common in face to face contexts such as teamwork. In order to do so, different methodologies and learning analytics tools can be employed. However, in a complete online context the students not only interact with the asynchronous tools that educational platforms provide but they also use instant messaging tools. This paper describes a Learning Analytics tool that facilitates teachers the evaluation of students’ interactions in Telegram Instant Messaging Tool. It has been employed in the context of the evaluation of the individual acquisition of teamwork competence. The tool has been tested in a computer science course. It had associated an improvement on students’ grades and they show their preference in using instant messaging tools because by using them conversations are more natural.},
booktitle = {Eighth International Conference on Technological Ecosystems for Enhancing Multiculturality},
pages = {719–724},
numpages = {6},
keywords = {teamwork, students’ Interaction, instant messaging, collaboration, Telegram, Learning analytics},
location = {Salamanca, Spain},
series = {TEEM'20}
}

@inproceedings{10.1145/2883851.2883899,
author = {Greer, Jim and Molinaro, Marco and Ochoa, Xavier and McKay, Timothy},
title = {Learning analytics for curriculum and program quality improvement (PCLA 2016)},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883899},
doi = {10.1145/2883851.2883899},
abstract = {This workshop on Learning Analytics for Curriculum and Program Quality Improvement investigates how LAK can drive improvements in teaching practices, instructional and curricular design, and academic program delivery. This workshop brings forward research and examples of how LAK can help build the case for instructional, curricular, or programmatic change and further how LAK can be used to foster acceptance of change processes by teachers, administrators, and other stakeholders in the educational enterprise.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {494–495},
numpages = {2},
keywords = {text tagging, LATEX, ACM proceedings},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/3334480.3383060,
author = {Pastushenko, Olena and Oliveira, Wilk and Isotani, Seiji and Hru\v{s}ka, Tom\'{a}\v{s}},
title = {A Methodology for Multimodal Learning Analytics and Flow Experience Identification within Gamified Assignments},
year = {2020},
isbn = {9781450368193},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3334480.3383060},
doi = {10.1145/3334480.3383060},
abstract = {Much research has sought to provide a flow experience for students in gamified educational systems to increase motivation and engagement. However, there is still a lack of quantitative research for evaluating the influence of the flow state on learning outcomes. One of the issues related to flow experience identification is that used techniques are often invasive or not suitable for massive applications. The current paper suggests a way to deal with this challenge. We describe a methodology based on multimodal learning analytics, aimed to provide automatic students' flow experience identification in the gamified assignments and measuring its influence on the learning outcomes. The application of the developed methodology showed that there are correlations between learning outcomes and flow state, but they depend on the initial level of the user. This finding suggests adding dynamic difficulty adjustment and other flow experience dimension to the gamified assignment.},
booktitle = {Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–9},
numpages = {9},
keywords = {automatic identification, educational systems, flow theory, gamification, multimodal learning analytics},
location = {Honolulu, HI, USA},
series = {CHI EA '20}
}

@inproceedings{10.1145/3369255.3369288,
author = {Cahyani, Andharini D. and Marshall, Lindsay and Forshaw, Matthew},
title = {Students' Perception on Data Sources from Outside Virtual Learning Environment for Learning Analytics},
year = {2020},
isbn = {9781450372541},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3369255.3369288},
doi = {10.1145/3369255.3369288},
abstract = {Every time a student interacts during their learning, they leave behind a digital footprint. The process of using this data to improve learning and teaching is called as Learning Analytics. Researches in this field grow and are more popular, specifically that usage of data outside the Virtual Learning Environment. Although often proposed data in previous research use students' personal data, their perception of the usage of those data is still underexplored. This study investigates higher education students' understanding of how useful the proposed data might be helpful as their input. Our study reveals that each degree-level student response differently regarding the usefulness each data sources. Therefore, we need to consider students' perception when we design personal learning analytics for students, so the app can fit to their preference and needs.},
booktitle = {Proceedings of the 11th International Conference on Education Technology and Computers},
pages = {165–170},
numpages = {6},
keywords = {usefulness, students' perception, Learning Analytics data sources},
location = {Amsterdam, Netherlands},
series = {ICETC '19}
}

@inproceedings{10.1145/3439147.3439161,
author = {WANG, YUEHUA and LU, SHULAN and HARTER, DEREK},
title = {Eye Tracking and Learning Analytics for Promoting Proactive Teaching and Learning in Classroom: A Survey},
year = {2021},
isbn = {9781450388795},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3439147.3439161},
doi = {10.1145/3439147.3439161},
abstract = {While an increasing amount of teaching methodologies, technologies, and equipment has been commonly used in learning, it has not bridged the gap to efficiently promote proactive teaching and learning beyond just learning and knowing how to use them. This paper is intended to aid the researchers and educators to discover valuable insights into student learning and learning trajectories for better decision making based on available resource including hardware, software, data, and methods. To this end, we conduct a systematic literature review on eye tracking and multimodal learning analytics for promoting active teaching and learning in classrooms. Based on review of literature, we then describe technical barriers, opportunities and challenges in the literature and suggest future research directions so as to promote proactive teaching and learning and learning environment evolution.},
booktitle = {Proceedings of the 2020 4th International Conference on Education and E-Learning},
pages = {156–160},
numpages = {5},
keywords = {Teaching and learning, Multimodal learning analytics, Eye tracking, Data mining, Classroom},
location = {Yamanashi, Japan},
series = {ICEEL '20}
}

@inproceedings{10.1145/2723576.2723642,
author = {Drachsler, Hendrik and Hoel, Tore and Scheffel, Maren and Kismih\'{o}k, G\'{a}bor and Berg, Alan and Ferguson, Rebecca and Chen, Weiqin and Cooper, Adam and Manderveld, Jocelyn},
title = {Ethical and privacy issues in the application of learning analytics},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723642},
doi = {10.1145/2723576.2723642},
abstract = {The large-scale production, collection, aggregation, and processing of information from various learning platforms and online environments have led to ethical and privacy concerns regarding potential harm to individuals and society. In the past, these types of concern have impacted on areas as diverse as computer science, legal studies and surveillance studies. Within a European consortium that brings together the EU project LACE, the SURF SIG Learning Analytics, the Apereo Foundation and the EATEL SIG dataTEL, we aim to understand the issues with greater clarity, and to find ways of overcoming the issues and research challenges related to ethical and privacy aspects of learning analytics practice. This interactive workshop aims to raise awareness of major ethics and privacy issues. It will also be used to develop practical solutions to advance the application of learning analytics technologies.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {390–391},
numpages = {2},
keywords = {surveillance, privacy, legal rights, learning analytics, ethics, data ownership},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/3027385.3029438,
author = {Hu, Xiao and Cheong, Christy W. L. and Ding, Wenwen and Woo, Michelle},
title = {A systematic review of studies on predicting student learning outcomes using learning analytics},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029438},
doi = {10.1145/3027385.3029438},
abstract = {Predicting student learning outcomes is one of the prominent themes in Learning Analytics research. These studies varied to a significant extent in terms of the techniques being used, the contexts in which they were situated, and the consequent effectiveness of the prediction. This paper presented the preliminary results of a systematic review of studies in predictive learning analytics. With the goal to find out what methodologies work for what circumstances, this study will be able to facilitate future research in this area, contributing to relevant system developments that are of pedagogic values.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {528–529},
numpages = {2},
keywords = {systematic review, prediction, performances, methods, learning outcomes, learning context},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/2723576.2723588,
author = {Ezen-Can, Aysu and Grafsgaard, Joseph F. and Lester, James C. and Boyer, Kristy Elizabeth},
title = {Classifying student dialogue acts with multimodal learning analytics},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723588},
doi = {10.1145/2723576.2723588},
abstract = {Supporting learning with rich natural language dialogue has been the focus of increasing attention in recent years. Many adaptive learning environments model students' natural language input, and there is growing recognition that these systems can be improved by leveraging multimodal cues to understand learners better. This paper investigates multimodal features related to posture and gesture for the task of classifying students' dialogue acts within tutorial dialogue. In order to accelerate the modeling process by eliminating the manual annotation bottleneck, a fully unsupervised machine learning approach is utilized for this task. The results indicate that these unsupervised models are significantly improved with the addition of automatically extracted posture and gesture information. Further, even in the absence of any linguistic features, a model that utilizes posture and gesture features alone performed significantly better than a majority class baseline. This work represents a step toward achieving better understanding of student utterances by incorporating multimodal features within adaptive learning environments. Additionally, the technique presented here is scalable to very large student datasets.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {280–289},
numpages = {10},
keywords = {tutorial dialogue, text-based learning analytics, multimodal learning analytics, dialogue act modeling},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/3486011.3486520,
author = {Amo, Daniel and Alier, Marc and Sansaloni, Rogelio and Geli, Javier and Fonseca, David and Garc\'{\i}a-Pe\~{n}alvo, Francisco Jos\'{e} and Casa\~{n}, Mar\'{\i}a Jos\'{e}},
title = {Learning Analytics Icons for analytics' transparency, information, and easy comprehension of data treatment of students},
year = {2021},
isbn = {9781450390668},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3486011.3486520},
doi = {10.1145/3486011.3486520},
abstract = {The principles defined in the General Data Protection Regulation (GDPR) of fair and transparent processing require that 1) the data subject be informed of the existence of the processing operation and its purposes, 2) the data subject should be informed of the existence of profiling and the consequences of such profiling, and 3) that information may be provided in combination with standardized icons to give in an easily visible, intelligible, and legible manner, a meaningful overview of the intended processing. These 3 points apply to education. However, processes mediated by Learning Analytics treat sensitive educational data, generate profiles, and have direct consequences for students, but no icons exist that represent this data processing accurately. Our study aims to provide icons that report the processing of data in Learning Analytics processes. The main objective is to facilitate educational institutions to be transparent and enhance information and easy comprehension of data treatment of students to students themselves. The methodology of the study consists of designs of icons and surveys to the students in an iterative execution. Our results expose a series of initial icons despite the strong dispersion in responses among students.},
booktitle = {Ninth International Conference on Technological Ecosystems for Enhancing Multiculturality (TEEM'21)},
pages = {587–593},
numpages = {7},
location = {Barcelona, Spain},
series = {TEEM'21}
}

@inproceedings{10.1145/2723576.2723631,
author = {Rogers, Tim},
title = {Critical realism and learning analytics research: epistemological implications of an ontological foundation},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723631},
doi = {10.1145/2723576.2723631},
abstract = {Learning analytics is a broad church that incorporates a range of topics and methodologies. As the field has developed some tension has emerged regarding a perceived contradiction between the implied constructivist ethos of the field and prevalent empirical practices that have been characterised as 'behaviourist' and 'positivist'. This paper argues that this tension is a sign of deeper metatheoretical faultlines that have plagued the social sciences more broadly. Critical realism is advanced as a philosophy of science that can help reconcile the apparent contradictions between the constructivist aims and the empirical practices of learning analytics and simultaneously can justify learning analytics' current methodological tolerance. The paper concludes that learning analytics, arrayed in realist terms, is essentially longitudinal and multimethodological, concerned with the socio-technical systems of learning and the problems of implementation, and has the potential to be emancipatory. Some methodological implications for learning analytics practice are discussed.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {223–230},
numpages = {8},
keywords = {theory, philosophy of science, critical realism},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2567574.2567594,
author = {Yu, Taeho and Jo, Il-Hyun},
title = {Educational technology approach toward learning analytics: relationship between student online behavior and learning performance in higher education},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567594},
doi = {10.1145/2567574.2567594},
abstract = {The aim of this study is to suggest more meaningful components for learning analytics in order to help learners improving their learning achievement continuously through an educational technology approach. Multiple linear regression analysis is conducted to determine which factors influence student's academic achievement. 84 undergraduate students in a women's university in South Korea participated in this study. The six-predictor model was able to account for 33.5% of the variance in final grade, F(6, 77) = 6.457, p &lt; .001, R2 = .335. Total studying time in LMS, interaction with peers, regularity of learning interval in LMS, and number of downloads were determined to be significant factors for students' academic achievement in online learning environment. These four controllable variables not only predict learning outcomes significantly but also can be changed if learners put more effort to improve their academic performance. The results provide a rationale for the treatment for student time management effort.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {269–270},
numpages = {2},
keywords = {learning analytics, higher education, educational technology, e-learning},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inbook{10.1145/3107990.3108003,
author = {Oviatt, Sharon and Grafsgaard, Joseph and Chen, Lei and Ochoa, Xavier},
title = {Multimodal learning analytics: assessing learners' mental state during the process of learning},
year = {2018},
isbn = {9781970001716},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/3107990.3108003},
booktitle = {The Handbook of Multimodal-Multisensor Interfaces: Signal Processing, Architectures, and Detection of Emotion and Cognition - Volume 2},
pages = {331–374},
numpages = {44}
}

@inproceedings{10.1145/2330601.2330613,
author = {Baker, Ryan S. J. d. and Duval, Erik and Stamper, John and Wiley, David and Buckingham Shum, Simon},
title = {Educational data mining meets learning analytics},
year = {2012},
isbn = {9781450311113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2330601.2330613},
doi = {10.1145/2330601.2330613},
abstract = {W This panel is proposed as a means of promoting mutual learning and continued dialogue between the Educational Data Mining and Learning Analytics communities. EDM has been developing as a community for longer than the LAK conference, so what if anything makes the LAK community different, and where is the common ground?},
booktitle = {Proceedings of the 2nd International Conference on Learning Analytics and Knowledge},
pages = {20},
numpages = {1},
keywords = {learning analytics, education, data mining},
location = {Vancouver, British Columbia, Canada},
series = {LAK '12}
}

@proceedings{10.1145/2883851,
title = {LAK '16: Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {We are pleased to welcome you to the 6th International Conference on Learning Analytics and Knowledge (LAK16). This year's conference is held in the beautiful city of Edinburgh, Scotland, April 25-29. For the first time, the international Learning Analytics and Knowledge conference is co-located with ACM Learning @ Scale 2016. The LAK16 conference is organized by the Society for Learning Analytics Research (SoLAR), and this year is hosted by the University of Edinburgh, a university with a long and rich history of innovation and research in teaching, learning and technologies. Building on the momentum generated from previous LAK conferences, we have extend invitations to practitioners, researchers, administrators, government and industry groups alike, interested in the field of learning analytics and related disciplines. This annual conference provides a multidisciplinary forum for addressing the critical issues and challenges confronting the education sector today. A particular emphasis of this year's program is enhancing our impact through synergistic connections with other related research communities.The field of learning analytics is rapidly growing in all facets of its research, application into practice and theoretical contributions. The theme for the 6th International Learning Analytics and Knowledge (LAK16) conference aims to explore the multidisciplinary connections that effectively illustrate how learning analytics can provide critical insights into the individual and collective learning process. This year's theme particularly highlights the multidisciplinary nature of the field and embraces the convergence of these disciplines to provide theoretical and practical insights that will further advance the field - through research, adoption and implementation and ultimately provide a foundation for informing government and institutional policy. We invite research and practice papers that address the "convergence of communities" in LAK and bring a novel perspective and approach for reflecting on the field. This theme is reflected in the workshops, papers, posters, panels, and especially our keynote talks. The conference will culminate with a leadership panel featuring leaders from a spectrum of research societies dedicated to advancing technology in service of education.},
location = {Edinburgh, United Kingdom}
}

@inproceedings{10.1145/3287324.3293816,
author = {\v{S}v\'{a}bensk\'{y}, Valdemar and Vykopal, Jan and Celeda, Pavel},
title = {Towards Learning Analytics in Cybersecurity Capture the Flag Games},
year = {2019},
isbn = {9781450358903},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287324.3293816},
doi = {10.1145/3287324.3293816},
abstract = {Capture the Flag games are software applications designed to exercise cybersecurity concepts, practice using security tools, and understand cyber attacks and defense. We develop and employ these games at our university for training purposes, unlike in the traditional competitive setting. During the gameplay, it is possible to collect data about players' in-game actions, such as typed commands or solution attempts, including the timing of these actions. Although such data was previously employed in computer security research, to the best of our knowledge, there were few attempts to use this data primarily to improve education. In particular, we see an open and challenging research problem in creating an artificial intelligence assistant that would facilitate the learning of each player. Our goal is to propose, apply, and experimentally evaluate data analysis and machine learning techniques to derive information about the players' interactions from the in-game data. We want to use this information to automatically provide each player with a personalized formative assessment. Such assessment will help the players identify their mastered concepts and areas for improvement, along with suggestions and actionable steps to take. Furthermore, we want to identify high- or low-performing players during the game, and subsequently, offer them game tasks more suitable to their skill level. These interventions would supplement or even replace feedback from instructors, which would significantly increase the learning impact of the games, enable more students to learn cybersecurity skills at an individual pace, and lower the costs.},
booktitle = {Proceedings of the 50th ACM Technical Symposium on Computer Science Education},
pages = {1255},
numpages = {1},
keywords = {learning analytics, cybersecurity games, capture the flag},
location = {Minneapolis, MN, USA},
series = {SIGCSE '19}
}

@inproceedings{10.1145/3027385.3029488,
author = {Molenaar, Inge and Knoop-van Campen, Carolien A. N. and Hasselman, Fred},
title = {The effects of a learning analytics empowered technology on students' arithmetic skill development},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029488},
doi = {10.1145/3027385.3029488},
abstract = {Learning analytics empowered educational technologies (LA-ET) in primary classrooms allow for blended learning scenarios with teacher-lead instructions, class-paced and individually-paced practice. This quasi-experimental study investigates the effects of a LA-ET on the development of students' arithmetic skills over one schoolyear. Children learning in a traditional paper &amp; pencil condition were compared to learners using a LA-ET on tablet computers in grade 4. The educational technology combined teacher dashboards (extracted analytics) and class and individually paced assignments (embedded analytics). The results indicated that children in the LA-ET condition made significantly more progress on arithmetic skills in one schoolyear compared to children in the paper &amp; pencil condition.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {614–615},
numpages = {2},
keywords = {primary education, educational technologies, arithmetic's, ability levels},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/2460296.2460354,
author = {Baer, Linda L. and Duin, Ann Hill and Norris, Donald and Brodnick, Robert},
title = {Crafting transformative strategies for personalized learning/analytics},
year = {2013},
isbn = {9781450317856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2460296.2460354},
doi = {10.1145/2460296.2460354},
abstract = {Personalized learning environments and learning analytics hold the promise to transform learning experiences, enhance and accelerate student success, and "open up" student learning to resources and experiences from outside individual institutions. To achieve their potential, personalized learning projects must move beyond individual, stand-alone projects or innovations to reshaping the institutional experience.Learning science must connect with learning pedagogy and design. Learners and institutions must have access to tools and resources that assist in customizing student progress and supplemental learning needs. Teachers and faculty must be empowered to provide teaching and learning environments that allow individual students to thrive. All this will require unique partnerships and collaborations within and across institutions, incorporating the best learning science findings and bridging with public and private entities developing the learning and analytic tools to support personalized learning.Crafting a strategy to embrace and sustain the transformative power of personalized learning systems will require strong leadership and clear planning models to align with institutional planning and future investments.},
booktitle = {Proceedings of the Third International Conference on Learning Analytics and Knowledge},
pages = {275–277},
numpages = {3},
keywords = {strategic planning, personalized learning, learning analytics, collaboration, academic analytics},
location = {Leuven, Belgium},
series = {LAK '13}
}

@article{10.1145/3243140,
author = {Korhonen, Ari and Grover, Shuchi},
title = {Second Special Issue on Learning Analytics in Computing Education},
year = {2018},
issue_date = {December 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {4},
url = {https://doi.org/10.1145/3243140},
doi = {10.1145/3243140},
journal = {ACM Trans. Comput. Educ.},
month = oct,
articleno = {16},
numpages = {2}
}

@inproceedings{10.1145/3206157.3206177,
author = {Yanhui, Wu},
title = {Language E-learning based on Learning Analytics in Big Data Era},
year = {2018},
isbn = {9781450363587},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206157.3206177},
doi = {10.1145/3206157.3206177},
abstract = {Language E-learning, an online language learning mode, completely transforms the traditional learning and teaching mode. However, with the coming of Big Data era, it not only enjoys some benefits, but also is confronted with great challenges. The article first concludes the reforms Big Data brought to the world, and then introduces the definition, key elements, the applying model, main analyzing methods and tools of learning analytics. Finally, the article fully shows the implementation of learning analytics to the study of language E-learning. Through the study of learning analytics, the designer of Language E-learning can learn the learners' learning behaviors and provide the efficient learning material, tools and systems.},
booktitle = {Proceedings of the 2018 International Conference on Big Data and Education},
pages = {106–111},
numpages = {6},
keywords = {learning analytics, language E-learning, Big Data},
location = {Honolulu, HI, USA},
series = {ICBDE '18}
}

@inproceedings{10.1145/2567574.2567617,
author = {Drachsler, Hendrik and Stoyanov, Slavi and Specht, Marcus},
title = {The impact of learning analytics on the dutch education system},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567617},
doi = {10.1145/2567574.2567617},
abstract = {The article reports the findings of a Group Concept Mapping study that was conducted within the framework of the Learning Analytics Summer Institute (LASI) in the Netherlands. Learning Analytics are expected to be beneficial for students and teacher empowerment, personalization, research on learning design, and feedback for performance. The study depicted some management and economics issues and identified some possible treats. No differences were found between novices and experts on how important and feasible are changes in education triggered by Learning Analytics.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {158–162},
numpages = {5},
keywords = {learning analytics, group concept mapping, focus group, community building},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/2090116.2090140,
author = {Lockyer, Lori and Dawson, Shane},
title = {Learning designs and learning analytics},
year = {2011},
isbn = {9781450309448},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2090116.2090140},
doi = {10.1145/2090116.2090140},
abstract = {Government and institutionally-driven reforms focused on quality teaching and learning in universities emphasize the importance of developing replicable, scalable teaching approaches that can be evaluated. In this context, learning design and learning analytics are two fields of research that may help university teachers design quality learning experiences for their students, evaluate how students are learning within that intended learning context and support personalized learning experiences for students. Learning Designs are ways of describing an educational experience such that it can be applied across a range of disciplinary contexts. Learning analytics offers new approaches to investigating the data associated with a learner's experience. This paper explores the relationship between learning designs and learning analytics.},
booktitle = {Proceedings of the 1st International Conference on Learning Analytics and Knowledge},
pages = {153–156},
numpages = {4},
keywords = {university teaching, social network analysis, pedagogical models, learning design, learning analytics},
location = {Banff, Alberta, Canada},
series = {LAK '11}
}

@inproceedings{10.1145/2883851.2883920,
author = {Spikol, Daniel and Avramides, Katerina and Cukurova, Mutlu and Vogel, Bahtijar and Luckin, Rose and Ruffaldi, Emanuele and Mavrikis, Manolis},
title = {Exploring the interplay between human and machine annotated multimodal learning analytics in hands-on STEM activities},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883920},
doi = {10.1145/2883851.2883920},
abstract = {This poster explores how to develop a working framework for STEM education that uses both human annotated and machine data across a purpose-built learning environment. Our dual approach is to develop a robust framework for analysis and investigate how to design a learning analytics system to support hands-on engineering design tasks. Data from the first user tests are presented along with the framework for discussion.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {522–523},
numpages = {2},
keywords = {mobile, learning analytics, CSCL},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/3041021.3054164,
author = {Daud, Ali and Aljohani, Naif Radi and Abbasi, Rabeeh Ayaz and Lytras, Miltiadis D. and Abbas, Farhat and Alowibdi, Jalal S.},
title = {Predicting Student Performance using Advanced Learning Analytics},
year = {2017},
isbn = {9781450349147},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3041021.3054164},
doi = {10.1145/3041021.3054164},
abstract = {Educational Data Mining (EDM) and Learning Analytics (LA) research have emerged as interesting areas of research, which are unfolding useful knowledge from educational databases for many purposes such as predicting students' success. The ability to predict a student's performance can be beneficial for actions in modern educational systems. Existing methods have used features which are mostly related to academic performance, family income and family assets; while features belonging to family expenditures and students' personal information are usually ignored. In this paper, an effort is made to investigate aforementioned feature sets by collecting the scholarship holding students' data from different universities of Pakistan. Learning analytics, discriminative and generative classification models are applied to predict whether a student will be able to complete his degree or not. Experimental results show that proposed method significantly outperforms existing methods due to exploitation of family expenditures and students' personal information feature sets. Outcomes of this EDM/LA research can serve as policy improvement method in higher education.},
booktitle = {Proceedings of the 26th International Conference on World Wide Web Companion},
pages = {415–421},
numpages = {7},
keywords = {students personal information, student performance prediction, learning analytics (la), family expenditures, educational data mining (edm)},
location = {Perth, Australia},
series = {WWW '17 Companion}
}

@inproceedings{10.1145/2723576.2723629,
author = {Scheffel, Maren and Drachsler, Hendrik and Specht, Marcus},
title = {Developing an evaluation framework of quality indicators for learning analytics},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723629},
doi = {10.1145/2723576.2723629},
abstract = {This paper presents results from the continuous process of developing an evaluation framework of quality indicators for learning analytics (LA). Building on a previous study, a group concept mapping approach that uses multidimensional scaling and hierarchical clustering, the study presented here applies the framework to a collection of LA tools in order to evaluate the framework. Using the quantitative and qualitative results of this study, the first version of the framework was revisited so as to allow work towards an improved version of the evaluation framework of quality indicators for LA.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {16–20},
numpages = {5},
keywords = {quality indicators, group concept mapping, evaluation framework, assessment of learning analytics tools},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/3706468.3706511,
author = {Song, Yige and Oliveira, Eduardo and de Barba, Paula and Kirley, Michael and Thompson, Pauline},
title = {Investigating Validity and Generalisability in Trace-Based Measurement of Self-Regulated Learning: A Multidisciplinary Study},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706511},
doi = {10.1145/3706468.3706511},
abstract = {Self-regulated learning (SRL) skills are critical for effective learning and academic success. With the growing availability of trace data from students’ online learning activities, researchers are increasingly leveraging this data to infer SRL processes. However, challenges remain regarding the validity of these inferences and their generalisability across diverse learning contexts. This study presents a structured approach to investigate these challenges by examining SRL behaviours in a multidisciplinary university cohort. The dataset includes 76 baseline survey responses, over 300 daily SRL survey submissions, and more than 6,000 sequences of recorded learning actions as trace data. Using mixed linear models and sequence mining, the analysis is grounded in SRL theory and evaluated through machine learning performance metrics. Our findings indicate consistent within-person patterns of SRL and online learning behaviours, supporting the concept of transferable, holistic skill development. Additionally, the results validate the trace-based detection of SRL engagement but highlight limitations in accurately detecting planning and reflection phases. These findings underscore the potential of automating SRL engagement detection while emphasising the need for multi-modal approaches to capture the full spectrum of SRL processes comprehensively.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {339–350},
numpages = {12},
keywords = {Learning Analytics, Blended Learning, Trace data, Self-Regulated Learning, Trace-SRL, Mixed Linear Model, Sequence Mining},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3012430.3012540,
author = {Pazmi\~{n}o-Maji, Rub\'{e}n A. and Garc\'{\i}a-Pe\~{n}alvo, Francisco J. and Conde-Gonz\'{a}lez, Miguel A.},
title = {Approximation of statistical implicative analysis to learning analytics: a systematic review},
year = {2016},
isbn = {9781450347471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3012430.3012540},
doi = {10.1145/3012430.3012540},
abstract = {The learning Analytics has been and is still an emerging technology, the amount of research on learning analysis are increasing every day. The integration of new tools, methods and theories is necessary. The aim of this paper is to study the approximation of Statistical Implicative Analysis theory (SIA) to Learning Analytics (LA). To this end, we have created an approximation framework based on the definition, stages, and methods used in LA. In total, three criteria approach and thirty-six sub-themes were compared. We use systematic review in the literature published in the last 66 months in bibliographic database ACM, EBSCO, Google Scholar, IEEE, ProQuest, Scopus and WOS. We started with 319 papers and finally 24 met all quality criteria. This document provides the themes by which SIA approximates to LA, also provides the percentages by category approach and identifies a number of future researches.},
booktitle = {Proceedings of the Fourth International Conference on Technological Ecosystems for Enhancing Multiculturality},
pages = {355–376},
numpages = {22},
keywords = {systematic review, statistical implicative analysis, learning analytics stages, learning analytics methods, learning analytics definition, learning analytics, approximation},
location = {Salamanca, Spain},
series = {TEEM '16}
}

@inproceedings{10.1145/3636555.3636872,
author = {Li, Zaibei and Jensen, Martin Thoft and Nolte, Alexander and Spikol, Daniel},
title = {Field report for Platform mBox: Designing an Open MMLA Platform},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636872},
doi = {10.1145/3636555.3636872},
abstract = {Multimodal Learning Analytics (MMLA) is an evolving sector within learning analytics that has become increasingly useful for examining complex learning and collaboration dynamics for group work across all educational levels. The availability of low-cost sensors and affordable computational power allows researchers to investigate different modes of group work. However, the field faces challenges stemming from the complexity and specialization of the systems required for capturing diverse interaction modalities, with commercial systems often being expensive or narrow in scope and researcher-developed systems needing to be more specialized and difficult to deploy. Therefore, more user-friendly, adaptable, affordable, open-source, and easy-to-deploy systems are needed to advance research and application in the MMLA field. The paper presents a field report on the design of mBox that aims to support group work across different contexts. We share the progress of mBox, a low-cost, easy-to-use platform grounded on learning theories to investigate collaborative learning settings. Our approach has been guided by iterative design processes that let us rapidly prototype different solutions for these settings.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {785–791},
numpages = {7},
keywords = {Multimodal Learning Analytics, Prototyping, Sociometric Wearable Devices},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/2883851.2883965,
author = {Tan, Jennifer Pei-Ling and Yang, Simon and Koh, Elizabeth and Jonathan, Christin},
title = {Fostering 21st century literacies through a collaborative critical reading and learning analytics environment: user-perceived benefits and problematics},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883965},
doi = {10.1145/2883851.2883965},
abstract = {The affordances of learning analytics (LA) are being increasingly harnessed to enhance 21st century (21C) pedagogy and learning. Relatively rare, however, are use cases and empirically based understandings of students' actual experiences with LA tools and environments at fostering 21C literacies, especially in secondary schooling and Asian education contexts. This paper addresses this knowledge gap by 1) presenting a first iteration design of a computer-supported collaborative critical reading and LA environment and its 16-week implementation in a Singapore high school; and 2) foregrounding students' quantitative and qualitative accounts of the benefits and problematics associated with this learning innovation. We focus the analytic lens on the LA dashboard components that provided visualizations of students' reading achievement, 21C learning dispositions, critical literacy competencies and social learning network positioning within the class. The paper aims to provide insights into the potentialities, paradoxes and pathways forward for designing LA that take into consideration the voices of learners as critical stakeholders.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {430–434},
numpages = {5},
keywords = {learning analytics, critical literacy, CSCL, 21st century skills},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/3706468.3706559,
author = {Li, Tongguang and Nath, Debarshi and Cheng, Yixin and Fan, Yizhou and Li, Xinyu and Rakovi\'{c}, Mladen and Khosravi, Hassan and Swiecki, Zachari and Tsai, Yi-Shan and Ga\v{s}evi\'{c}, Dragan},
title = {Turning Real-Time Analytics into Adaptive Scaffolds for Self-Regulated Learning Using Generative Artificial Intelligence},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706559},
doi = {10.1145/3706468.3706559},
abstract = {In computer-based learning environments (CBLEs), adopting effective self-regulated learning (SRL) strategies requires sophisticated coordination of multiple SRL processes. While various studies have proposed adaptive SRL scaffolds (i.e. real-time advice on adopting effective SRL processes) and embedded them in CBLEs to facilitate learners’ effective use of SRL strategies, two key research gaps remain. First, there is a lack of research on SRL scaffolds that are based on continuous assessment of both learners’ SRL processes and learning conditions (e.g., awareness of learning resources) to provide adaptive support. Second, current analytics-based scaffolding mechanisms lack the scalability needed to effectively address multiple learning conditions. Integration of analytics of SRL with generative artificial intelligence (GenAI) can provide scalable scaffolding for real-time SRL processes and evolving conditions. Yet, empirical studies implementing and evaluating effects of this integration remain scarce. To address these limitations, we conducted a randomized control trial, assigning participants to three groups (control, process only, and process with condition groups) to investigate the effects of using GenAI to turn insights from real-time analytics about students’ SRL processes and conditions into adaptive scaffolds. The results demonstrate that integrating real-time analytics with GenAI in adaptive SRL scaffolds – addressing both SRL processes and dynamic conditions – promotes more metacognitive learning patterns compared to the control and process-only groups. In addition, the learners showed varying levels of compliance with analytics-based GenAI scaffolds, and this was also reflected in how the learners coordinated their SRL processes, particularly in the performance phase of SRL. This study contributes to the literature by designing, implementing, and evaluating the impact of adaptive scaffolds on learners’ SRL processes using real-time analytics with GenAI.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {667–679},
numpages = {13},
keywords = {self-regulated learning, scaffolding compliance, GenAI, scaffolding, learning analytics},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/2567574.2567593,
author = {Arnold, Kimberly E. and Lynch, Grace and Huston, Daniel and Wong, Lorna and Jorn, Linda and Olsen, Christopher W.},
title = {Building institutional capacities and competencies for systemic learning analytics initiatives},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567593},
doi = {10.1145/2567574.2567593},
abstract = {The last five years have brought an explosion of research in the learning analytics field. However, much of what has emerged has been small scale or tool-centric. While these efforts are vitally important to the development of the field, in order to truly transform education, learning analytics must scale and become institutionalized at multiple levels throughout an educational system. Many institutions are currently undertaking this grand challenge and this panel will highlight cases from: the University of Wisconsin System, the Society for Learning Analytics Research, the University of New England, and Rio Salado College.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {257–260},
numpages = {4},
keywords = {systemic application, sustainability, learning analytics, leadership, higher education, cultural change, capacity building},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/2330601.2330654,
author = {McNely, Brian J. and Gestwicki, Paul and Hill, J. Holden and Parli-Horne, Philip and Johnson, Erika},
title = {Learning analytics for collaborative writing: a prototype and case study},
year = {2012},
isbn = {9781450311113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2330601.2330654},
doi = {10.1145/2330601.2330654},
abstract = {This paper explores the ways in which participants in writing intensive environments might use learning analytics to make productive interventions during, rather than after, the collaborative construction of written artifacts. Specifically, our work considered how university students learning in a knowledge work model---one that is collaborative, project-based, and that relies on consistent peer-to-peer interaction and feedback---might leverage learning analytics as formative assessment to foster metacognition and improve final deliverables. We describe Uatu, a system designed to visualize the real time contribution and edit history of collaboratively written documents. After briefly describing the technical details of this system, we offer initial findings from a fifteen week qualitative case study of 8 computer science students who used Uatu in conjunction with Google Docs while collaborating on a variety of writing and programming tasks. These findings indicate both the challenges and promise of delivering useful metrics for collaborative writing scenarios in academe and industry.},
booktitle = {Proceedings of the 2nd International Conference on Learning Analytics and Knowledge},
pages = {222–225},
numpages = {4},
keywords = {writing, programming, metacognition, learning analytics, knowledge work, distributed work, collaboration},
location = {Vancouver, British Columbia, Canada},
series = {LAK '12}
}

@inproceedings{10.1145/2330601.2330609,
author = {Lockyer, Lori and Dawson, Shane},
title = {Where learning analytics meets learning design},
year = {2012},
isbn = {9781450311113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2330601.2330609},
doi = {10.1145/2330601.2330609},
abstract = {The wealth of data available through student management systems and eLearning systems has the potential to provide faculty with important, just-in-time information that may allow them to positively intervene with struggling students and/or enhance the learning experience during the delivery of a course. This information might also facilitate post-delivery review and reflection for faculty who wish to revise course design and content. But to be effective, this data needs to be appropriate to the context or pedagogical intent of the course -- this is where learning analytics meets learning design.},
booktitle = {Proceedings of the 2nd International Conference on Learning Analytics and Knowledge},
pages = {14–15},
numpages = {2},
keywords = {university teaching, social network analysis, pedagogical models, learning design, learning analytics},
location = {Vancouver, British Columbia, Canada},
series = {LAK '12}
}

@inproceedings{10.1145/2330601.2330614,
author = {Graf, Sabine and Ives, Cindy and Lockyer, Lori and Hobson, Paul and Clow, Doug},
title = {Building a data governance model for learning analytics},
year = {2012},
isbn = {9781450311113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2330601.2330614},
doi = {10.1145/2330601.2330614},
abstract = {This international panel presentation aims to explore and discuss the issues that emerge when an educational institution decides to develop learning analytics initiatives. While learning analytics may provide data that lead to improvements in the quality of teaching and learning design, and therefore has the potential to enhance the overall quality of education, the successful development and implementation of tools and processes for learning analytics are complex and problematic. In this panel, data governance considerations will be discussed from organizational, ethical, learning design, and technical points of view.},
booktitle = {Proceedings of the 2nd International Conference on Learning Analytics and Knowledge},
pages = {21–22},
numpages = {2},
keywords = {learning analytics, ethics},
location = {Vancouver, British Columbia, Canada},
series = {LAK '12}
}

@inproceedings{10.1145/3706468.3706523,
author = {Khalil, Mohammad and Vadiee, Farhad and Shakya, Ronas and Liu, Qinyi},
title = {Creating Artificial Students that Never Existed: Leveraging Large Language Models and CTGANs for Synthetic Data Generation},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706523},
doi = {10.1145/3706468.3706523},
abstract = {In this study, we explore the growing potential of AI and deep learning technologies, particularly Generative Adversarial Networks (GANs) and Large Language Models (LLMs), for generating synthetic tabular data. Access to quality students’ data is critical for advancing learning analytics, but privacy concerns and stricter data protection regulations worldwide limit their availability and usage. Synthetic data offers a promising alternative. We investigate whether synthetic data can be leveraged to create artificial students for serving learning analytics models. Using the popular GAN model- CTGAN and three LLMs- GPT2, DistilGPT2, and DialoGPT, we generate synthetic tabular student data. Our results demonstrate the strong potential of these methods to produce high-quality synthetic datasets that resemble real students’ data. To validate our findings, we apply a comprehensive set of utility evaluation metrics to assess the statistical and predictive performance of the synthetic data and compare the different generator models used, specially the performance of LLMs. Our study aims to provide the learning analytics community with valuable insights into the use of synthetic data, laying the groundwork for expanding the field’s methodological toolbox with new innovative approaches for learning analytics data generation.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {439–450},
numpages = {12},
keywords = {Synthetic Data Generation; Artificial data; Learning Analytics (LA); Artificial Intelligence for Education (AIED); Large Language Models (LLMs); Conditional Tabular GAN (CTGAN); Deep Learning},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3330430.3333651,
author = {Nacu, Denise and Upadhyay, Pooja and Skorepa, Evan and Everette, Tre and Flores, Evelyn and Jackson, Mighel and Pinkard, Nichole},
title = {Implementing Learning Analytics to Foster a STEM Learning Ecosystem at the City-Level: Emerging Research and Design Challenges},
year = {2019},
isbn = {9781450368049},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3330430.3333651},
doi = {10.1145/3330430.3333651},
abstract = {To address the goal of increasing and broadening participation of youth in STEM fields, a learning ecosystem approach is a promising strategy. Learning analytics can play an important role in such efforts which aim to build learning supports across the diverse spaces in which learning and development occurs, including informal, formal, and online contexts. This paper introduces a city-level learning analytics implementation effort in a developing STEM ecosystem in one mid-sized city. We describe aspects of our design and research approach and challenges that emerge by taking a learning ecosystem perspective of learning and development.},
booktitle = {Proceedings of the Sixth (2019) ACM Conference on Learning @ Scale},
articleno = {38},
numpages = {4},
keywords = {learning ecosystems, learning analytics, human-centered design, broadening participation, STEM learning},
location = {Chicago, IL, USA},
series = {L@S '19}
}

@inproceedings{10.1145/3242969.3243010,
author = {Oviatt, Sharon},
title = {Ten Opportunities and Challenges for Advancing Student-Centered Multimodal Learning Analytics},
year = {2018},
isbn = {9781450356923},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3242969.3243010},
doi = {10.1145/3242969.3243010},
abstract = {This paper presents a summary and critical reflection on ten major opportunities and challenges for advancing the field of multimodal learning analytics (MLA). It identifies emerging technology trends likely to disrupt learning analytics, challenges involved in forging viable participatory design partnerships, and impending issues associated with the control of data and privacy. Trends in health care analytics provide one attractive model for how new infrastructure can enable the collection of largerscale and more diverse datasets, and how end-user analytics can be designed to empower individuals and expand market adoption.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimodal Interaction},
pages = {87–94},
numpages = {8},
keywords = {privacy, prediction of cognitive state, participatory design, multimodal learning analytics, end-user analytics, data infrastructure},
location = {Boulder, CO, USA},
series = {ICMI '18}
}

@inproceedings{10.1145/3209281.3209342,
author = {Tambouris, Efthimios and Hermans, Paul and Tarrant, David and Zotou, Maria and Tarabanis, Konstantinos},
title = {Using problem-based learning and learning analytics in open data education},
year = {2018},
isbn = {9781450365260},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209281.3209342},
doi = {10.1145/3209281.3209342},
abstract = {Open Data initiatives worldwide are boosting with an aim to increase transparency and contribute to economic growth. With a global annual economic potential value estimated to $3 trillion, this boost seems justified. Current progress however is not satisfactory. We believe a main reason is the lack of relevant skills and competencies. Current education and training activities are scarce and do not exploit practice-oriented learning methods such as Problem Based Learning (PBL) or novel assessment opportunities such as Learning Analytics (LA). As a result, stakeholders are missing skills related to publishing and reusing Open Data. The aim of this tutorial is to introduce to the audience Open Data education and training based on the PBL strategy and by exploiting LA for performance assessment. We will present a curriculum structure and learning content on Open Data for academia, business and the public sector that can be reused by all interested stakeholders. We will also demonstrate a university and a VET course model that have been designed in order to facilitate innovative and data-driven Open Data education and training. Finally, we will share lessons learnt from the practical application of the above in universities and businesses across multiple European countries.},
booktitle = {Proceedings of the 19th Annual International Conference on Digital Government Research: Governance in the Data Age},
articleno = {129},
numpages = {2},
keywords = {problem based learning, open data, learning analytics},
location = {Delft, The Netherlands},
series = {dg.o '18}
}

@inproceedings{10.1145/2460296.2460299,
author = {Balacheff, Nicolas and Lund, Kristine},
title = {Multidisciplinarity vs. Multivocality, the case of "learning analytics"},
year = {2013},
isbn = {9781450317856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2460296.2460299},
doi = {10.1145/2460296.2460299},
abstract = {In this paper, we consider an analysis of the TeLearn archive, of the Grand Challenges from the STELLAR Network of Excellence, of two Alpine Rendez-Vous 2011 workshops and research conducted in the Productive Multivocality initiative in order to discuss the notions of multidisciplinarity, multivocality and interidisciplinarity. We use this discussion as a springboard for addressing the term "Learning Analytics" and its relation to "Educational Data Mining". Our goal is to launch a debate pertaining to what extent the different disciplines involved in the TEL community can be integrated on methodological and theoretical levels.},
booktitle = {Proceedings of the Third International Conference on Learning Analytics and Knowledge},
pages = {5–13},
numpages = {9},
keywords = {technology enhanced learning, multivocality, multidiscipinarity, learning analytics, educational data mining},
location = {Leuven, Belgium},
series = {LAK '13}
}

@inproceedings{10.1145/3706468.3706476,
author = {Yan, Lixiang and Gasevic, Dragan and Echeverria, Vanessa and Jin, Yueqiao and Zhao, Linxuan and Martinez-Maldonado, Roberto},
title = {From Complexity to Parsimony: Integrating Latent Class Analysis to Uncover Multimodal Learning Patterns in Collaborative Learning},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706476},
doi = {10.1145/3706468.3706476},
abstract = {Multimodal Learning Analytics (MMLA) leverages advanced sensing technologies and artificial intelligence to capture complex learning processes, but integrating diverse data sources into cohesive insights remains challenging. This study introduces a novel methodology for integrating latent class analysis (LCA) within MMLA to map monomodal behavioural indicators into parsimonious multimodal ones. Using a high-fidelity healthcare simulation context, we collected positional, audio, and physiological data, deriving 17 monomodal indicators. LCA identified four distinct latent classes: Collaborative Communication, Embodied Collaboration, Distant Interaction, and Solitary Engagement, each capturing unique monomodal patterns. Epistemic network analysis compared these multimodal indicators with the original monomodal indicators and found that the multimodal approach was more parsimonious while offering higher explanatory power regarding students’ task and collaboration performances. The findings highlight the potential of LCA in simplifying the analysis of complex multimodal data while capturing nuanced, cross-modality behaviours, offering actionable insights for educators and enhancing the design of collaborative learning interventions. This study proposes a pathway for advancing MMLA, making it more parsimonious and manageable, and aligning with the principles of learner-centred education.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {70–81},
numpages = {12},
keywords = {multimodal learning analytics, collaborative learning, healthcare simulation, latent class analysis},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706547,
author = {Park, Seehee and Nixon, Nia and D'Mello, Sidney and Shariff, Danielle and Choi, Jaeyoon},
title = {Understanding Collaborative Learning Processes and Outcomes Through Student Discourse Dynamics},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706547},
doi = {10.1145/3706468.3706547},
abstract = {This study explores the relation between students’ discourse dynamics and performance during collaborative problem-solving activities utilizing Linguistic Inquiry Word Count (LIWC). We analyzed linguistic variables from students’ communications to explore social and cognitive behavior. Participants include 279 undergraduate students from two U.S. universities engaged in a controlled lab setting using the physics related educational game named Physics Playground. Findings highlight the relationship between social and cognitive linguistic variables and student’s physics performance outcome in a virtual collaborative learning context. This study contributes to a deeper understanding of how these discourse dynamics are related to learning outcomes in collaborative learning. It provides insights for optimizing educational strategies in collaborative remote learning environments. We further discuss the potential for conducting computational linguistic modeling on learner discourse and the role of natural language processing in deriving insights on learning behavior to support collaborative learning.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {938–943},
numpages = {6},
keywords = {Learning Analytics, Discourse Analytics, Natural Language Processing, Collaborative Problem Solving, Virtual Learning Environment},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/2723576.2723624,
author = {Worsley, Marcelo and Blikstein, Paulo},
title = {Leveraging multimodal learning analytics to differentiate student learning strategies},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723624},
doi = {10.1145/2723576.2723624},
abstract = {Multimodal analysis has had demonstrated effectiveness in studying and modeling several human-human and human-computer interactions. In this paper, we explore the role of multimodal analysis in the service of studying complex learning environments. We compare uni-modal and multimodal; manual and semi-automated methods for examining how students learn in a hands-on, engineering design context. Specifically, we compare human annotations, speech, gesture and electro-dermal activation data from a study (N=20) where student participating in two different experimental conditions. The experimental conditions have already been shown to be associated with differences in learning gains and design quality. Hence, one objective of this paper is to identify the behavioral practices that differed between the two experimental conditions, as this may help us better understand how the learning interventions work. An additional objective is to provide examples of how to conduct learning analytics research in complex environments and compare how the same algorithm, when used with different forms of data can provide complementary results.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {360–367},
numpages = {8},
keywords = {learning sciences, data mining, constructionist, computational},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/3144826.3145387,
author = {Moreira, Fernando and Gon\c{c}alves, Ramiro and Martins, Jos\'{e} and Branco, Frederico and Au-Yong-Oliveira, Manuel},
title = {Learning Analytics as a Core Component for Higher Education Disruption: Governance Stakeholder},
year = {2017},
isbn = {9781450353861},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3144826.3145387},
doi = {10.1145/3144826.3145387},
abstract = {Higher education institutions are at this stage, on the one hand, faced with challenges never seen before and, on the other hand, their action is moving very rapidly into digital learning spaces. These challenges are increasingly complex because of the global competition for resources, students and teachers. In addition, the amount of data produced inside and outside higher education institutions has grown exponentially, so more and more institutions are exploring the potential of Big Data to meet these challenges. In this context, higher education institutions and key stakeholders (students, teachers, and governance) can derive multiple benefits from learning analytics using different data analysis strategies to produce summative, real-time and predictive information and recommendations. However, it may be questioned whether institutions, academic administrative staff as well as including those with responsibility for governance, are prepared for learning analytics? As a response to the question raised in this paper is presented an extension of a disruptive conceptual approach to higher education, using information gathered by IoT and based on Big Data &amp; Cloud Computing and Learning Analytics analysis tools, with the main focus on the stakeholder governance.},
booktitle = {Proceedings of the 5th International Conference on Technological Ecosystems for Enhancing Multiculturality},
articleno = {37},
numpages = {8},
keywords = {Learning Analytics, Higher Education Institutions, Governance, Disruption},
location = {C\'{a}diz, Spain},
series = {TEEM 2017}
}

@inproceedings{10.1145/3323771.3323825,
author = {Gong, Lilin and Liu, Yang},
title = {Design and Application of Intervention Model based on Learning Analytics under Blended Learning Environment},
year = {2019},
isbn = {9781450366397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3323771.3323825},
doi = {10.1145/3323771.3323825},
abstract = {The arrival of big data and AI era promote educational reforms, which make personalized learning become normalization. Teaching intervention is an indispensable bridge between big data and students' personalized learning. This study proposes an intervention model based on learning analytics from four iteration modules: data collection, data processing, intervention implementation and effect evaluation, and applies it to blended learning environment. Through one-group pretest-posttest experiment design, the effect of the intervention model were measured by learning engagement and learning achievement. The results show that the intervention model can effectively improve students' behavioral engagement and cognitive engagement as well as learning achievement, especially for risky students.},
booktitle = {Proceedings of the 2019 7th International Conference on Information and Education Technology},
pages = {225–229},
numpages = {5},
keywords = {Learning engagement, Learning analytics, Intervention model, Blended learning},
location = {Aizu-Wakamatsu, Japan},
series = {ICIET 2019}
}

@inproceedings{10.1145/3706468.3706527,
author = {Se\ss{}ler, Kathrin and F\"{u}rstenberg, Maurice and B\"{u}hler, Babette and Kasneci, Enkelejda},
title = {Can AI grade your essays? A comparative analysis of large language models and teacher ratings in multidimensional essay scoring},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706527},
doi = {10.1145/3706468.3706527},
abstract = {The manual assessment and grading of student writing is a time-consuming yet critical task for teachers. Recent developments in generative AI offer potential solutions to facilitate essay-scoring tasks for teachers. In our study, we evaluate the performance (e.g. alignment and reliability) of both open-source and closed-source LLMs in assessing German student essays, comparing their evaluations to those of 37 teachers across 10 pre-defined criteria (i.e., plot logic, expression). A corpus of 20 real-world essays from Year 7 and 8 students was analyzed using five LLMs: GPT-3.5, GPT-4, o1-preview, LLaMA 3-70B, and Mixtral 8x7B, aiming to provide in-depth insights into LLMs’ scoring capabilities. Closed-source GPT models outperform open-source models in both internal consistency and alignment with human ratings, particularly excelling in language-related criteria. The o1 model outperforms all other LLMs, achieving Spearman’s r =.74 with human assessments in the Overall score, and an internal consistency of ICC =.80, though biased towards higher scores. These findings indicate that LLM-based assessment can be a useful tool to reduce teacher workload by supporting the evaluation of essays, especially with regard to language-related criteria. However, due to their tendency to overrate and their remaining issues to capture the content quality, the models require further refinement.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {462–472},
numpages = {11},
keywords = {Large Language Models, Automated Essay Scoring, Learning Analytics, Education},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3170358.3170421,
author = {Jivet, Ioana and Scheffel, Maren and Specht, Marcus and Drachsler, Hendrik},
title = {License to evaluate: preparing learning analytics dashboards for educational practice},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170421},
doi = {10.1145/3170358.3170421},
abstract = {Learning analytics can bridge the gap between learning sciences and data analytics, leveraging the expertise of both fields in exploring the vast amount of data generated in online learning environments. A typical learning analytics intervention is the learning dashboard, a visualisation tool built with the purpose of empowering teachers and learners to make informed decisions about the learning process. Related work has investigated learning dashboards, yet none have explored the theoretical foundation that should inform the design and evaluation of such interventions. In this systematic literature review, we analyse the extent to which theories and models from learning sciences have been integrated into the development of learning dashboards aimed at learners. Our analysis revealed that very few dashboard evaluations take into account the educational concepts that were used as a theoretical foundation for their design. Furthermore, we report findings suggesting that comparison with peers, a common reference frame for contextualising information on learning analytics dashboards, was not perceived positively by all learners. We summarise the insights gathered through our literature review in a set of recommendations for the design and evaluation of learning analytics dashboards for learners.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {31–40},
numpages = {10},
keywords = {systematic review, social comparison, learning theory, learning science, learning dashboards, learning analytics, evaluation, competition},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3636555.3636922,
author = {Samadi, Mohammad Amin and Jaquay, Spencer and Lin, Yiwen and Tajik, Elham and Park, Seehee and Nixon, Nia},
title = {Minds and Machines Unite: Deciphering Social and Cognitive Dynamics in Collaborative Problem Solving with AI},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636922},
doi = {10.1145/3636555.3636922},
abstract = {We investigated the feasibility of automating the modeling of collaborative problem-solving skills encompassing both social and cognitive aspects. Leveraging a diverse array of cutting-edge techniques, including machine learning, deep learning, and large language models, we embarked on the classification of qualitatively coded interactions within groups. These groups were composed of four undergraduate students, each randomly assigned to tackle a decision-making challenge. Our dataset comprises contributions from 514 participants distributed across 129 groups. Employing a suite of prominent machine learning methods such as Random Forest, Support Vector Machines, Naive Bayes, Recurrent and Convolutional Neural Networks, BERT, and GPT-2 language models, we undertook the intricate task of classifying peer interactions. Notably, we introduced a novel task-based train-test split methodology, allowing us to assess classification performance independently of task-related context. This research carries significant implications for the learning analytics field by demonstrating the potential for automated modeling of collaborative problem-solving skills, offering new avenues for understanding and enhancing group learning dynamics.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {885–891},
numpages = {7},
keywords = {Artificial Intelligence, CPS, Learning Analytics, Machine Learning, NLP},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/2090116.2090120,
author = {De Liddo, Anna and Buckingham Shum, Simon and Quinto, Ivana and Bachler, Michelle and Cannavacciuolo, Lorella},
title = {Discourse-centric learning analytics},
year = {2011},
isbn = {9781450309448},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2090116.2090120},
doi = {10.1145/2090116.2090120},
abstract = {Drawing on sociocultural discourse analysis and argumentation theory, we motivate a focus on learners' discourse as a promising site for identifying patterns of activity which correspond to meaningful learning and knowledge construction. However, software platforms must gain access to qualitative information about the rhetorical dimensions to discourse contributions to enable such analytics. This is difficult to extract from naturally occurring text, but the emergence of more-structured annotation and deliberation platforms for learning makes such information available. Using the Cohere web application as a research vehicle, we present examples of analytics at the level of individual learners and groups, showing conceptual and social network patterns, which we propose as indicators of meaningful learning.},
booktitle = {Proceedings of the 1st International Conference on Learning Analytics and Knowledge},
pages = {23–33},
numpages = {11},
keywords = {web semantics, social network analysis, sensemaking, learning analytics, discourse analytics, discourse analysis, argumentation},
location = {Banff, Alberta, Canada},
series = {LAK '11}
}

@inproceedings{10.1145/2567574.2567616,
author = {Jo, Il-Hyun and Kim, Dongho and Yoon, Meehyun},
title = {Analyzing the log patterns of adult learners in LMS using learning analytics},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567616},
doi = {10.1145/2567574.2567616},
abstract = {In this paper, we describe a process of constructing proxy variables that represent adult learners' time management strategies in an online course. Based upon previous research, three values were selected from a data set. According to the result of empirical validation, an (ir)regularity of the learning interval was proven to be correlative with and predict learning performance. As indicated in previous research, regularity of learning is a strong indicator to explain learners' consistent endeavors. This study demonstrates the possibility of using learning analytics to address a learner's specific competence on the basis of a theoretical background. Implications for the learning analytics field seeking a pedagogical theory-driven approach are discussed.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {183–187},
numpages = {5},
keywords = {time management strategy, log data, learning analytics, big-data mining, adult education},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/2460296.2460308,
author = {Wise, Alyssa Friend and Zhao, Yuting and Hausknecht, Simone Nicole},
title = {Learning analytics for online discussions: a pedagogical model for intervention with embedded and extracted analytics},
year = {2013},
isbn = {9781450317856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2460296.2460308},
doi = {10.1145/2460296.2460308},
abstract = {This paper describes an application of learning analytics that builds on an existing research program investigating how students contribute and attend to the messages of others in online discussions. A pedagogical model that translates the concepts and findings of the research program into guidelines for practice and analytics with which students and instructors can assess their discussion participation are presented. The analytics are both embedded in the learning environment and extracted from it, allowing for integrated and reflective metacognitive activity. The pedagogical intervention is based on the principles of (1) Integration (2) Diversity (of Metrics) (3) Agency (4) Reflection (5) Parity and (6) Dialogue. Details of an initial implementation of this approach and preliminary findings are described. Initial results strongly support the value of student-teacher dialogue around the analytics. In contrast, instructor parity in analytics use did not seem as important to students as was expected. Analytics were reported as useful in validating invisible discussion activity, but at times triggered emotionally-charged responses.},
booktitle = {Proceedings of the Third International Conference on Learning Analytics and Knowledge},
pages = {48–56},
numpages = {9},
keywords = {student participation, online learning, learning analytics, computer mediated communication, asynchronous discussion groups},
location = {Leuven, Belgium},
series = {LAK '13}
}

@inproceedings{10.1145/2330601.2330610,
author = {Slade, Sharon and Galpin, Fenella},
title = {Learning analytics and higher education: ethical perspectives},
year = {2012},
isbn = {9781450311113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2330601.2330610},
doi = {10.1145/2330601.2330610},
abstract = {Take two students who were enrolled on the same higher education course, both of whom were identified as likely to benefit from additional support and tailoring of their learning experience. Three years later, one student has gone on to gain a good degree and is now making great progress in her career. The other student, whose background and learning needs appeared similar, scraped through the experience, has recently been eased out of her organization and is unemployed. To what extent were decisions taken by their tutors and institution about the design of their learning experiences, responsible for these two very different outcomes?},
booktitle = {Proceedings of the 2nd International Conference on Learning Analytics and Knowledge},
pages = {16–17},
numpages = {2},
keywords = {student walk, open university (OU), learning analytics},
location = {Vancouver, British Columbia, Canada},
series = {LAK '12}
}

@inproceedings{10.1145/3486011.3486539,
author = {Garc\'{\i}a Garc\'{\i}a, Domingo and Garc\'{\i}a-Pe\~{n}alvo, Francisco Jos\'{e} and Amo Filv\'{a}, Daniel},
title = {Multimodal Learning Analytics in Students with Learning Difficulties: How the Environment Can Affect their Learning Positively or Negatively},
year = {2021},
isbn = {9781450390668},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3486011.3486539},
doi = {10.1145/3486011.3486539},
abstract = {The present paper summarizes the 3-year research plan that will be conducted with the support of the University of Salamanca within the Doctoral Programme Education in the Knowledge Society. During this period, and through the analysis of multi-modal data, specially from the physical environment, we will study how the environment affects the learning of students with special education needs, which factors are the ones that make a positive impact in them and how to enhance the learning of these pupils.},
booktitle = {Ninth International Conference on Technological Ecosystems for Enhancing Multiculturality (TEEM'21)},
pages = {662–664},
numpages = {3},
keywords = {Special Education Needs, Multimodal Learning Analytics, Learning Environment, Learning Difficulties, Enhanced Learning},
location = {Barcelona, Spain},
series = {TEEM'21}
}

@inproceedings{10.1145/3636555.3636915,
author = {Osakwe, Ikenna and Chen, Guanliang and Fan, Yizhou and Rakovic, Mladen and Singh, Shaveen and Molenaar, Inge and Ga\v{s}evi\'{c}, Dragan},
title = {Measurement of Self-regulated Learning: Strategies for mapping trace data to learning processes and downstream analysis implications},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636915},
doi = {10.1145/3636555.3636915},
abstract = {Trace data provides opportunities to study self-regulated learning (SRL) processes as they unfold. However, raw trace data must be translated into meaningful SRL constructs to enable analysis. This typically involves developing a pattern dictionary that maps trace sequences to SRL processes, and a trace parser to implement the mappings. While much attention focuses on the pattern dictionary, trace parsing methodology remains under-investigated. This study explores how trace parsers affect extracted processes and downstream analysis. Four methods were compared: Disconnected, Connected, Lookahead, and Synonym Matching. Statistical analysis of medians and process mining networks showed parsing choices significantly impacted SRL process identification and sequencing. Disconnected parsing isolated metacognitive processes while Connected approaches showed greater connectivity between meta-cognitive and cognitive events. Furthermore, Connected methods provided process maps more aligned with cyclical theoretical models of SRL. The results demonstrate trace parser design critically affects the validity of extracted SRL processes, with implications for SRL measurement in learning analytics.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {563–575},
numpages = {13},
keywords = {learning analytics, learning strategies, self-regulated learning},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/2090116.2090136,
author = {Vatrapu, Ravi},
title = {Cultural considerations in learning analytics},
year = {2011},
isbn = {9781450309448},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2090116.2090136},
doi = {10.1145/2090116.2090136},
abstract = {This paper discusses empirical findings demonstrating cultural influences in social behavior, communication, cognition, technology enhanced learning and draws implications for learning analytics.},
booktitle = {Proceedings of the 1st International Conference on Learning Analytics and Knowledge},
pages = {127–133},
numpages = {7},
keywords = {online learning culture, learning analytics, human-computer interaction (HCI), general terms, computers, communication, cognition, behavior},
location = {Banff, Alberta, Canada},
series = {LAK '11}
}

@inproceedings{10.1145/3551708.3556217,
author = {Shen, Wenyao and Zhan, Zehui and Li, Chen and Chen, Han and Shen, Ranhao},
title = {Constructing Behavioral Representation of Computational Thinking based on Event Graph: A new approach for learning analytics},
year = {2022},
isbn = {9781450396455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551708.3556217},
doi = {10.1145/3551708.3556217},
abstract = {At present, the mainstream means of measuring and assessing computational thinking are test papers and scales, which enable us to measure the level of students' computational thinking as a whole, but fail to represent the inner laws and thinking processes, thus cause difficulty on providing appropriate guidance. In this study, data embedding technology was adopted to collect data on students' operational behavior in answering questions, and learning analytic techniques (i.e., semantic analysis, lagged sequence analysis, etc.) were used to evaluate computational thinking. Students with higher- and lower-level of computational thinking were compared on their behavioral patterns in a seriel events in the computational thinking training process. An event graph was constructed to represent the computational thinking behaviors based on logical event relationships. This study provides insights on a novel approach of analyzing computational thinking from the procedural perspective, which could be helpful for understanding how the thinking path is structured during problem-solving process.},
booktitle = {Proceedings of the 6th International Conference on Education and Multimedia Technology},
pages = {45–52},
numpages = {8},
location = {Guangzhou, China},
series = {ICEMT '22}
}

@inproceedings{10.1145/3636555.3636877,
author = {Chejara, Pankaj and Kasepalu, Reet and Prieto, Luis and Rodr\'{\i}guez-Triana, Mar\'{\i}a Jes\'{u}s and Ruiz-Calleja, Adolfo},
title = {Bringing Collaborative Analytics using Multimodal Data to Masses: Evaluation and Design Guidelines for Developing a MMLA System for Research and Teaching Practices in CSCL},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636877},
doi = {10.1145/3636555.3636877},
abstract = {The Multimodal Learning Analytics (MMLA) research community has significantly grown in the past few years. Researchers in this field have harnessed diverse data collection devices such as eye-trackers, motion sensors, and microphones to capture rich multimodal data about learning. This data, when analyzed, has been proven highly valuable for understanding learning processes across a variety of educational settings. Notwithstanding this progress, an ubiquitous use of MMLA in education is still limited by challenges such as technological complexity, high costs, etc. In this paper, we introduce CoTrack, a MMLA system for capturing the multimodality of a group’s interaction in terms of audio, video, and writing logs in online and co-located collaborative learning settings. The system offers a user-friendly interface, designed to cater to the needs of teachers and students without specialized technical expertise. Our usability evaluation with 2 researchers, 2 teachers and 24 students has yielded promising results regarding the system’s ease of use. Furthermore, this paper offers design guidelines for the development of more user-friendly MMLA systems. These guidelines have significant implications for the broader aim of making MMLA tools accessible to a wider audience, particularly for non-expert MMLA users.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {800–806},
numpages = {7},
keywords = {CSCL, MMLA, Multimodal Learning Analytics},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3241748.3241760,
author = {Gong, Lilin and Liu, Yazhao and Zhao, Wei},
title = {Using Learning Analytics to Promote Student Engagement and Achievement in Blended Learning: An Empirical Study},
year = {2018},
isbn = {9781450364812},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3241748.3241760},
doi = {10.1145/3241748.3241760},
abstract = {The emergence of blended learning has huge impact on traditional learning. Blended learning has its own unique characteristics combining the advantages of traditional learning and online learning. However, some problems of blended learning have also been found in practice such as low student engagement and lack of autonomy in learning. This study aims to examine the student engagement and achievement in blended learning environment and try to improve student engagement and achievement by using interventions based on learning analytics. This study takes 31 undergraduates of Northeast Normal University as the research object. Quantitative self-report (student engagement questionnaire) and quantitative observation measures (number of viewing records and posts to a discussion board) were being used to measure student engagement. Classroom test score was being used to measure student achievement. The results of this study show that: a) intervention based on learning analytics can improve student engagement in blended learning mode. b) in the blended learning mode, whether the intervention was used or not, it has not been found yet that students' achievement was related to students' engagement. In sum, this study indicates that educators can use some strategic methods to promote student engagement and achievement when doing teaching practices in blended learning environment, such as intervention based on learning analytics.},
booktitle = {Proceedings of the 2018 2nd International Conference on E-Education, E-Business and E-Technology},
pages = {19–24},
numpages = {6},
keywords = {student engagement, student achievement, learning analytics, blended learning},
location = {Beijing, China},
series = {ICEBT '18}
}

@inproceedings{10.1145/2656719.2656729,
author = {Shoukry, Laila and G\"{o}bel, Stefan and Steinmetz, Ralf},
title = {Learning Analytics and Serious Games: Trends and Considerations},
year = {2014},
isbn = {9781450331210},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2656719.2656729},
doi = {10.1145/2656719.2656729},
abstract = {This paper reviews the current status of Learning Analytics with special focus on their application in Serious Games. After presenting the advantages of incorporating Learning Analytics into game-based learning applications, different aspects regarding the integration process including modeling, tracing, aggregation, visualisation, analysis and employment of gameplay data are discussed. Associated challenges in this field as well as examples of best practices are also examined.},
booktitle = {Proceedings of the 2014 ACM International Workshop on Serious Games},
pages = {21–26},
numpages = {6},
keywords = {serious games, learning analytics, game-based learning},
location = {Orlando, Florida, USA},
series = {SeriousGames '14}
}

@inproceedings{10.1145/3706468.3706571,
author = {Deininger, Hannah and Parrisius, Cora and Lavelle-Hill, Rosa and Meurers, Detmar and Trautwein, Ulrich and Nagengast, Benjamin and Kasneci, Gjergji},
title = {Who Did What to Succeed? Individual Differences in Which Learning Behaviors Are Linked to Achievement},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706571},
doi = {10.1145/3706468.3706571},
abstract = {It is commonly assumed that digital learning environments such as intelligent tutoring systems facilitate learning and positively impact achievement. This study explores how different groups of students exhibit distinct relationships between learning behaviors and academic achievement in an intelligent tutoring system for English as a foreign language. We examined whether these differences are linked to students’ prior knowledge, personality traits, and motivation. We collected behavioral trace data from 507 German seventh-grade students during the 2021/22 school year and applied machine learning models to predict English performance based on learning behaviors (best-performing model’s R2 =.41). To understand the impact of specific behaviors, we applied the explainable AI method SHAP and identified three student clusters with distinct learning behavior patterns. Subsequent analyses revealed that these clusters also varied in prior knowledge and motivation: one with high prior knowledge and average motivation, another with low prior knowledge and average motivation, and a third with both low prior knowledge and low motivation. Our findings suggest that learning behaviors are linked differently to academic success across students and are closely tied to their prior knowledge and motivation. This hints towards the importance of personalizing learning systems to support individual learning needs better.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {771–782},
numpages = {12},
keywords = {Learning Analytics, Behavioral Trace Data, Academic Performance, Interindividual Differences},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3027385.3029487,
author = {Alabi, Halimat and Hatala, Marek},
title = {Best intentions: learner feedback on learning analytics visualization design},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029487},
doi = {10.1145/3027385.3029487},
abstract = {A mixed methods approach was undertaken in this exploratory study to better understand how learners perceive and utilize learning analytics visualizations during online discussions activities. Internal conditions such as goal orientation and numeracy were measured alongside the external conditions created by the discussion structure and learning analytics. Our results emphasize key factors that should be considered when designing learning analytics tools.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {612–613},
numpages = {2},
keywords = {learning analytics, judgments of learning, information visualization, evaluation},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3170358.3170367,
author = {Tsai, Yi-Shan and Moreno-Marcos, Pedro Manuel and Tammets, Kairit and Kollom, Kaire and Ga\v{s}evi\'{c}, Dragan},
title = {SHEILA policy framework: informing institutional strategies and policy processes of learning analytics},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170367},
doi = {10.1145/3170358.3170367},
abstract = {This paper introduces a learning analytics policy development framework developed by a cross-European research project team - SHEILA (Supporting Higher Education to Integrate Learning Analytics), based on interviews with 78 senior managers from 51 European higher education institutions across 16 countries. The framework was developed using the RAPID Outcome Mapping Approach (ROMA), which is designed to develop effective strategies and evidence-based policy in complex environments. This paper presents three case studies to illustrate the development process of the SHEILA policy framework, which can be used to inform strategic planning and policy processes in real world environments, particularly for large-scale implementation in higher education contexts.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {320–329},
numpages = {10},
keywords = {strategy, policy, learning analytics, higher education, ROMA model},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3027385.3029432,
author = {Martinez-Maldonado, Roberto and Hernandez-Leo, Davinia and Pardo, Abelardo and Ogata, Hiroaki},
title = {2nd cross-LAK: learning analytics across physical and digital spaces},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029432},
doi = {10.1145/3027385.3029432},
abstract = {Student's learning happens where the learner is, rather than being constrained to a single physical or digital environment. It is of high relevance for the LAK community to provide analytics support in blended learning scenarios where students can interact at diverse learning spaces and with a variety of educational tools. This workshop aims to gather the sub-community of LAK researchers, learning scientists and researchers in other areas, interested in the intersection between ubiquitous, mobile and/or classroom learning analytics. The underlying concern is how to integrate and coordinate learning analytics seeking to understand the particular pedagogical needs and context constraints to provide learning analytics support across digital and physical spaces. The goals of the workshop are to consolidate the Cross-LAK sub-community and provide a forum for idea generation that can build up further collaborations. The workshop will also serve to disseminate current work in the area by both producing proceedings of research papers and working towards a journal special issue.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {510–511},
numpages = {2},
keywords = {seamless learning, monitoring, learning analytics, integration},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3313831.3376148,
author = {Martinez-Maldonado, Roberto and Echeverria, Vanessa and Fernandez Nieto, Gloria and Buckingham Shum, Simon},
title = {From Data to Insights: A Layered Storytelling Approach for Multimodal Learning Analytics},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376148},
doi = {10.1145/3313831.3376148},
abstract = {Significant progress to integrate and analyse multimodal data has been carried out in the last years. Yet, little research has tackled the challenge of visualising and supporting the sensemaking of multimodal data to inform teaching and learning. It is na\"{\i}ve to expect that simply by rendering multiple data streams visually, a teacher or learner will be able to make sense of them. This paper introduces an approach to unravel the complexity of multimodal data by organising it into meaningful layers that explain critical insights to teachers and students. The approach is illustrated through the design of two data storytelling prototypes in the context of nursing simulation. Two authentic studies with educators and students identified the potential of the approach to create learning analytics interfaces that communicate insights on team performance, as well as concerns in terms of accountability and automated insights discovery.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–15},
numpages = {15},
keywords = {CSCW, data storytelling, teamwork, visualization},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/2787622.2787746,
author = {Olivares, Daniel},
title = {Exploring Learning Analytics for Computing Education},
year = {2015},
isbn = {9781450336307},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2787622.2787746},
doi = {10.1145/2787622.2787746},
abstract = {Student retention in STEM disciplines is a growing problem. The number of students receiving undergraduate STEM degrees will need to increase by about 34% annually in order to meet projected needs [6]. One way to address this problem is by leveraging the emerging field of learning analytics, a data-driven approach to designing learning interventions based on continuously-updated data on learning processes and outcomes. Through an iterative, user-centered, design approach, we propose to develop a learning dashboard tailored for computing courses. The dashboard will collect, analyze, and present learning process and outcome data to instructors and students, thus providing an empirical basis for automated, teacher-initiated, and learner-initiated interventions to positively influence learning outcomes and retention. Through a series of mixed-method empirical studies, we will determine what data should be made available to instructors, how that data can be best displayed, how effective teaching interventions can be fashioned from the data, and how such interventions affect student grades and persistence in introductory computing science courses.},
booktitle = {Proceedings of the Eleventh Annual International Conference on International Computing Education Research},
pages = {271–272},
numpages = {2},
keywords = {social programming, social learning theory, learning dashboard, learning analytics, computing education},
location = {Omaha, Nebraska, USA},
series = {ICER '15}
}

@inproceedings{10.1145/3170358.3170365,
author = {Ocheja, Patrick and Flanagan, Brendan and Ogata, Hiroaki},
title = {Connecting decentralized learning records: a blockchain based learning analytics platform},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170365},
doi = {10.1145/3170358.3170365},
abstract = {As Learners move from one learning environment to another, there is a key necessity of taking with them a proof of previous learning achievements or experiences. In most cases, this is either expressed in terms of receipt of scores or a certificate of completion. While this may be sufficient for enrollment and other administrative decisions, it poses some limitations to the depth of learning analytics and consequently a slow onboarding process. Also, with different institutions having their learning data isolated from each other, it becomes more difficult to easily access a learner's learning history for all learning activities on other systems. In this paper, we propose a blockchain based approach for connecting learning data across different Learning Management Systems (LMS), Learning Record Stores (LRS), institutions and organizations. Leveraging on unique properties of blockchain technology, we also propose solutions to ensuring learning data consistency, availability, immutability, security, privacy and access control.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {265–269},
numpages = {5},
keywords = {smart contracts, privacy, learning record store, learning management systems, learning data, learning analytics, blockchain},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/2883851.2883886,
author = {Rienties, Bart and Boroowa, Avinash and Cross, Simon and Farrington-Flint, Lee and Herodotou, Christothea and Prescott, Lynda and Mayles, Kevin and Olney, Tom and Toetenel, Lisette and Woodthorpe, John},
title = {Reviewing three case-studies of learning analytics interventions at the open university UK},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883886},
doi = {10.1145/2883851.2883886},
abstract = {This study provides a conceptual framework how organizations may adopt evidence-based interventions at scale, and how institutions may evaluate the costs and benefits of such interventions. Building on a new conceptual model developed by the Open University UK (OU), we will analyse three case-studies of evidence-based interventions. By working with 90+ large-scale modules for a period of two years across the five faculties and disciplines within the OU, Analytics4Action provides a bottom-up-approach for working together with key stakeholders within their respective contexts. Using principles of embedded case-study approaches by Yin [1], by comparing the learning behavior, satisfaction and performance of 11079 learners the findings indicated that each of the three learning designs led to satisfied students and average to good student retention. In the second part we highlighted that the three module teams made in-presentation interventions based upon real-time analytics, whereby initial user data indicated VLE behaviour in line with expectations. In 2-5 years, we hope that a rich, robust evidence-base will be presented to show how learning analytics can help teachers to make informed, timely and successful interventions that will help learners to achieve their learning outcomes.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {534–535},
numpages = {2},
keywords = {online learning settings, distance learning, collaborative learning},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@article{10.1145/3122773,
author = {Grover, Shuchi and Korhonen, Ari},
title = {Unlocking the Potential of Learning Analytics in Computing Education},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
url = {https://doi.org/10.1145/3122773},
doi = {10.1145/3122773},
journal = {ACM Trans. Comput. Educ.},
month = aug,
articleno = {11e},
numpages = {4}
}

@inproceedings{10.1145/2883851.2883914,
author = {Koh, Elizabeth and Shibani, Antonette and Tan, Jennifer Pei-Ling and Hong, Helen},
title = {A pedagogical framework for learning analytics in collaborative inquiry tasks: an example from a teamwork competency awareness program},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883914},
doi = {10.1145/2883851.2883914},
abstract = {Many pedagogical models in the field of learning analytics are implicit and do not overtly direct learner behavior. While this allows flexibility of use, this could also result in misaligned practice, and there are calls for more explicit pedagogical models in learning analytics. This paper presents an explicit pedagogical model, the Team and Self Diagnostic Learning (TSDL) framework, in the context of collaborative inquiry tasks. Key informing theories include experiential learning, collaborative learning, and the learning analytics process model. The framework was trialed through a teamwork competency awareness program for 14 year old students. A total of 272 students participated in the program. This paper foregrounds students' and teachers' evaluative accounts of the program. Findings reveal positive perceptions of the stages of the TSDL framework, despite identified challenges, which points to its potential usefulness for teaching and learning. The TSDL framework aims to provide theoretical clarity of the learning process, and foster alignment between learning analytics and the learning design. The current work provides trial outcomes of a teamwork competency awareness program that used dispositional analytics, and further efforts are underway to develop the discourse layer of the analytic engine. Future work will also be dedicated to application and refinement of the framework for other contexts and participants, both learners and teachers alike.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {74–83},
numpages = {10},
keywords = {twenty-first century skills, teamwork competency, teamwork, pedagogical model, learning design, evaluation, dispositional analytics, collaboration, assessment},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/3706468.3706533,
author = {Ortega-Arranz, Alejandro and Topali, Paraskevi and Molenaar, Inge},
title = {Configuring and Monitoring Students' Interactions with Generative AI Tools: Supporting Teacher Autonomy},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706533},
doi = {10.1145/3706468.3706533},
abstract = {The widespread use of Generative Artificial Intelligence (GenAI) tools, such as ChatGPT, has come along with multiple benefits in education (e.g., 24h teacher, augmenting student monitoring). However, at the same time, these tools hinder teachers’ autonomy, limiting the capacity and freedom to exert control over students’ actions and their learning process. Additionally, the generic character of the GenAI output usually lacks contextualization (e.g., course curriculum, students’ age), thus hampering the successful attainment of the course goals. To address these issues, this paper proposes the development of a system mediating between the GenAI interfaces and their back-ends. This system allows teachers to monitor the students’ interactions and align the given answers with the course learning objectives and teaching methods. This research follows the Systems Development Research methodology, and within the first iteration, we developed a system prototype that was evaluated with 8 secondary-school teachers. Results showed a high perceived usefulness of the system for monitoring students’ interactions; for alerting the teachers to take specific actions (e.g., suspicious copy-paste behaviours), and for having control over the GenAI outputs. Additionally, while most teachers perceived a higher autonomy level within the given scenarios, some teachers did not. The evaluation also served to collect further requirements and usability features to keep improving the tool in the next methodological iterations.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {895–902},
numpages = {8},
keywords = {Generative AI, Learning Analytics, GenAI Analytics, Human-Centred Design, Teachers},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/2090116.2090133,
author = {Atkisson, Michael and Wiley, David},
title = {Learning analytics as interpretive practice: applying Westerman to educational intervention},
year = {2011},
isbn = {9781450309448},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2090116.2090133},
doi = {10.1145/2090116.2090133},
abstract = {In Westerman's [12] disruptive article, "Quantitative research as an interpretive enterprise: The mostly unacknowledged role of interpretation in research efforts and suggestions for explicitly interpretive quantitative investigations," he invited qualitative researchers in psychology to adopt quantitative methods into interpretive inquiry, given that they were as capable as qualitative measures in producing meaning-laden results. The objective of this article is to identify Westerman's [12] key arguments and apply them to the practice of Learning Analytics in educational interventions. The primary implication for Learning Analytics practitioners is the need to interpret quantitative analysis procedures at every phase from philosophy to conclusions. Furthermore, Learning Analytics practitioners and consumers must critically examine any assumption that suggests quantitative methodologies in Learning Analytics are inherently objective or that Learning Analytics algorithms may replace judgment rather than aid it. Lastly we propose a method for making observational data in virtual environments concrete through nested models.},
booktitle = {Proceedings of the 1st International Conference on Learning Analytics and Knowledge},
pages = {117–121},
numpages = {5},
keywords = {quantitative inquiry, positivism, operationalism, learning analytics, interpretive inquiry, hermeneutics, educational intervention},
location = {Banff, Alberta, Canada},
series = {LAK '11}
}

@inproceedings{10.1145/3706468.3706562,
author = {Kim, Jinwon and Li, Qiujie and Jiang, Zilu and Xu, Di},
title = {Not ALL Delay is Procrastination: Analyzing Subpatterns of Academic Delayers in Online Learning},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706562},
doi = {10.1145/3706468.3706562},
abstract = {In prior literature on using clickstream data to capture student behavior in virtual learning environments, procrastination is typically measured by the extent to which students delay their coursework. However, students may delay coursework under personal and environmental contexts and not all delays should be considered procrastination. Thus, this study aims to identify different types of delayers and examine how they differ in academic engagement and performance. We utilized learning management system (LMS) data from three online undergraduate courses. Specifically, using data from the first three weeks of the course, we classified delayers into three subgroups – high-achieving, low-achieving, and sporadic delayers – based on the timing of their coursework access and submission, the consistency of these behaviors, and their short-term course performance. Our findings reveal that the subgroups significantly differ in course engagement and long-term performance. Low-achieving delayers exhibited the lowest levels of engagement and performance. While sporadic delayers and high-achieving delayers demonstrated comparable levels of engagement, the latter received higher course grades. These findings challenge commonly used LMS measures for procrastination, highlight the complexity of academic delays, and reveal nuanced patterns of student behavior. The results contribute to discussions on future interventions and research related to distinct forms of delays.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {977–983},
numpages = {7},
keywords = {Academic Delays, Learning Management Systems, Procrastination, Learning Analytics, Student Performance, Online Learning, Higher Education},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/2330601.2330661,
author = {Siemens, George and Baker, Ryan S. J. d.},
title = {Learning analytics and educational data mining: towards communication and collaboration},
year = {2012},
isbn = {9781450311113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2330601.2330661},
doi = {10.1145/2330601.2330661},
abstract = {Growing interest in data and analytics in education, teaching, and learning raises the priority for increased, high-quality research into the models, methods, technologies, and impact of analytics. Two research communities -- Educational Data Mining (EDM) and Learning Analytics and Knowledge (LAK) have developed separately to address this need. This paper argues for increased and formal communication and collaboration between these communities in order to share research, methods, and tools for data mining and analysis in the service of developing both LAK and EDM fields.},
booktitle = {Proceedings of the 2nd International Conference on Learning Analytics and Knowledge},
pages = {252–254},
numpages = {3},
keywords = {learning analytics and knowledge, educational data mining, collaboration},
location = {Vancouver, British Columbia, Canada},
series = {LAK '12}
}

@inproceedings{10.1145/3706468.3706563,
author = {Ferguson, Rebecca and Gopalan, Yuveena and Buckingham Shum, Simon},
title = {What's the Value of a Doctoral Consortium? Analysing a Decade of LAK DCs as a Community of Practice},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706563},
doi = {10.1145/3706468.3706563},
abstract = {Since 2013, the Learning Analytics and Knowledge (LAK) conference has included a Doctoral Consortium (DC), supported by the Society for Learning Analytics Research (SoLAR). Given the LAK25 conference theme of ‘expanding the horizons of learning analytics’, it is timely to reflect on how well the LAK DC is meeting its objectives of building capacity in the field and developing the next generation. We frame the DC as a structured entry into the LAK community of practice (CoP), familiarising students with the domain of learning analytics, understanding its practices, and building connections with other members. CoPs generate five types of value for their members: immediate, potential, applied, realised and reframing. This study used a survey of the 92 DC students from the first decade (2013–22), supplemented with scientometric analysis of LAK publications, to address the questions: What value do students gain from attending the LAK doctoral consortium? and Do students gain the same value from face-to-face and virtual doctoral consortia? Thematic analysis of responses (N=37, a 40% response rate) showed that students gained a wide range of immediate and potential value from the DC, which in many cases also prompted changes in practice, performance improvement or redefinition of success. However, the value reported by a third of respondents who had attended virtually was more limited. We note that, despite the value already offered, the DC could provide clearer routes both to future community engagement and to extending goals that expand the horizons of learning analytics. This paper's contributions are (i) the first systematic documentation of student perceptions of LAK DCs, (ii) identification of ways in which doctoral consortia can be developed in the future, and (iii) specific attention to how virtual DCs can offer greater value for both participants and the host community of practice. The findings related to future development and value do not apply only to LAK but can be generalised to other doctoral consortia.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {702–712},
numpages = {11},
keywords = {applied value, communities of practice, doctoral consortium, immediate value, potential value, qualitative research, realized value, reframing value, thematic analysis},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706516,
author = {Venugopalan, Devika and Yan, Ziwen and Borchers, Conrad and Lin, Jionghao and Aleven, Vincent},
title = {Combining Large Language Models with Tutoring System Intelligence: A Case Study in Caregiver Homework Support},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706516},
doi = {10.1145/3706468.3706516},
abstract = {Caregivers (i.e., parents and members of a child’s caring community) are underappreciated stakeholders in learning analytics. Although caregiver involvement can enhance student academic outcomes, many obstacles hinder involvement, most notably knowledge gaps with respect to modern school curricula. An emerging topic of interest in learning analytics is hybrid tutoring, which includes instructional and motivational support. Caregivers assert similar roles in homework, yet it is unknown how learning analytics can support them. Our past work with caregivers suggested that conversational support is a promising method of providing caregivers with the guidance needed to effectively support student learning. We developed a system that provides instructional support to caregivers through conversational recommendations generated by a Large Language Model (LLM). Addressing known instructional limitations of LLMs, we use instructional intelligence from tutoring systems while conducting prompt engineering experiments with the open-source Llama 3 LLM. This LLM generated message recommendations for caregivers supporting their child’s math practice via chat. Few-shot prompting and combining real-time problem-solving context from tutoring systems with examples of tutoring practices yielded desirable message recommendations. These recommendations were evaluated with ten middle school caregivers, who valued recommendations facilitating content-level support and student metacognition through self-explanation. We contribute insights into how tutoring systems can best be merged with LLMs to support hybrid tutoring settings through conversational assistance, facilitating effective caregiver involvement in tutoring systems.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {373–383},
numpages = {11},
keywords = {large language models, tutoring systems, hybrid tutoring, K-12, mathematics education, caregivers},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3636555.3636912,
author = {Snyder, Caitlin and Hutchins, Nicole M and Cohn, Clayton and Fonteles, Joyce Horn and Biswas, Gautam},
title = {Analyzing Students Collaborative Problem-Solving Behaviors in Synergistic STEM+C Learning},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636912},
doi = {10.1145/3636555.3636912},
abstract = {This study introduces a methodology to investigate students’ collaborative behaviors as they work in pairs to build computational models of scientific processes. We expand the Self-Regulated Learning (SRL) framework—specifically, Planning, Enacting, and Reflection—proposed in the literature, applying it to examine students’ collaborative problem-solving (CPS) behaviors in a computational modeling task. We analyze these behaviors by employing a Markov Chain (MC) modeling approach that scrutinizes students’ model construction and model debugging behaviors during CPS. This involves interpreting their actions in the system collected through computer logs and analyzing their conversations using a Large Language Model (LLM) as they progress through their modeling task in segments. Our analytical framework assesses the behaviors of high- and low-performing students by evaluating their proficiency in completing the specified computational model for a kinematics problem. We employ a mixed-methods approach, combining Markov Chain analysis of student problem-solving transitions with qualitative interpretations of their conversation segments. The results highlight distinct differences in behaviors between high- and low-performing groups, suggesting potential for developing adaptive scaffolds in future work to enhance support for students in collaborative problem-solving.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {540–550},
numpages = {11},
keywords = {SRL, STEM, collaboration, learning analytics},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3706468.3706534,
author = {Zhao, Linxuan and Rakovi\'{c}, Mladen and B. Cloude, Elizabeth and Li, Xinyu and Ga\v{s}evi\'{c}, Dragan and Bardach, Lisa},
title = {The Effect of Sequential Transition of Self-Regulated Learning Processes on Performance: Insights from Ordered Network Analysis},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706534},
doi = {10.1145/3706468.3706534},
abstract = {Productively engaging in SRL is challenging for learners since it involves coordinating multiple motivational, affective, cognitive, and metacognitive processes. Researchers have investigated methods to adaptively scaffold learners’ productive engagement using SRL processes automatically captured by SRL detectors. However, most previous studies relied solely on the frequency of SRL processes to drive adaptive scaffolds (e.g., feedback, hints), possibly missing the sequential characteristics inherent to self-regulation, a crucial dimension of productive SRL. To address this gap, this study analysed the impact of sequential transitions between multiple SRL processes on learners’ performance on a reading-writing task with a hypermedia environment called Flora. A sample of 66 secondary-school learners completed the task and trace data were collected. Grounded in the COPES model of SRL, a rule-based SRL detector was employed to capture SRL processes from collected trace data. We employed a method combining logistic regression with ordered network analysis (ONA) to analyse the transitions between the detected SRL processes. This exploratory study revealed several influential transitions to learners’ performance in different temporal learning blocks of self-regulation. The implications suggest the potential of using COPES SRL process transitions to drive adaptive scaffolds to facilitate engagement in productive SRL, benefiting performance outcomes in hypermedia environments.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {516–526},
numpages = {11},
keywords = {Self-regulated learning, Learning analytics, Ordered network analysis, Learning strategies},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/2460296.2460304,
author = {Koulocheri, Eleni and Xenos, Michalis},
title = {Considering formal assessment in learning analytics within a PLE: the HOU2LEARN case},
year = {2013},
isbn = {9781450317856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2460296.2460304},
doi = {10.1145/2460296.2460304},
abstract = {Personal Learning Environments are used more and more by the academic community. They can coexist with formal courses as a communication and collaboration channel. In this paper, an application of learning analytics into HOU2LEARN, a Personal Learning Environment set by Hellenic Open University is discussed. The present part of research focuses on the social network analysis as a branch of learning analytics, along with formal grading system. Since it is an ongoing research, this paper presents the preliminary results of the study of the correlation between the social network metrics and the formal grades, through a test case course, the PLH42.},
booktitle = {Proceedings of the Third International Conference on Learning Analytics and Knowledge},
pages = {28–32},
numpages = {5},
keywords = {social network analysis, outdegree centrality, metrics, learning analytics, indegree centrality, graphs, grades, centrality, betweeness centrality},
location = {Leuven, Belgium},
series = {LAK '13}
}

@inproceedings{10.1145/3027385.3029478,
author = {Jeremic, Zoran and Kumar, Vive and Graf, Sabine},
title = {MORPH: supporting the integration of learning analytics at institutional level},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029478},
doi = {10.1145/3027385.3029478},
abstract = {While there is high potential in using learning analytics to provide educational institutions as well as teachers and learners with actionable information and improve learning experiences, currently only very few learning analytics tools are actually used in educational institutions. In this paper, we introduce MORPH, a platform that facilitates the integration of learning analytics modules and tools into institutional learning systems. MORPH provides a robust distributed architecture which combines batch, stream and real-time data processing using a parallel processing model to enable and support efficient processing of large amounts of data. Furthermore, it provides common management and administration features that enable the seamless integration of learning analytics research modules and tools into existing institutional learning systems.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {596–597},
numpages = {2},
keywords = {real-time processing, learning analytics, institutional learning environments, data streaming, dashboards, batch processing},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3636555.3636892,
author = {Borchers, Conrad and Wang, Yeyu and Karumbaiah, Shamya and Ashiq, Muhammad and Shaffer, David Williamson and Aleven, Vincent},
title = {Revealing Networks: Understanding Effective Teacher Practices in AI-Supported Classrooms using Transmodal Ordered Network Analysis},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636892},
doi = {10.1145/3636555.3636892},
abstract = {Learning analytics research increasingly studies classroom learning with AI-based systems through rich contextual data from outside these systems, especially student-teacher interactions. One key challenge in leveraging such data is generating meaningful insights into effective teacher practices. Quantitative ethnography bears the potential to close this gap by combining multimodal data streams into networks of co-occurring behavior that drive insight into favorable learning conditions. The present study uses transmodal ordered network analysis to understand effective teacher practices in relationship to traditional metrics of in-system learning in a mathematics classroom working with AI tutors. Incorporating teacher practices captured by position tracking and human observation codes into modeling significantly improved the inference of how efficiently students improved in the AI tutor beyond a model with tutor log data features only. Comparing teacher practices by student learning rates, we find that students with low learning rates exhibited more hint use after monitoring. However, after an extended visit, students with low learning rates showed learning behavior similar to their high learning rate peers, achieving repeated correct attempts in the tutor. Observation notes suggest conceptual and procedural support differences can help explain visit effectiveness. Taken together, offering early conceptual support to students with low learning rates could make classroom practice with AI tutors more effective. This study advances the scientific understanding of effective teacher practice in classrooms learning with AI tutors and methodologies to make such practices visible.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {371–381},
numpages = {11},
keywords = {AI-supported classrooms, multimodal learning analytics, quantitative ethnography, teacher practices},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3706468.3706496,
author = {Ocumpaugh, Jaclyn and Liu, Xiner and Zambrano, Andres Felipe},
title = {Language Models and Dialect Differences},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706496},
doi = {10.1145/3706468.3706496},
abstract = {The advancements in automatic language processing being ushered in by Large Language Models suggest enormous potential for better personalization during student learning. However, this potential can be best exploited if we know that LLMs are equally capable of interacting with students who speak or write in a range of different dialects. This case study uses systematically manipulated student essays, previously evaluated by human raters, to examine how ChatGPT responds to and addresses specific dialect differences. Results point to important concerns about the potential biases and limitations of both LLMs and humans when evaluating and providing feedback to students who use minoritized dialects. Addressing these concerns is critical for the field of learning analytics, as it seeks to ensure equity and asset-based approaches to learning analytics.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {204–215},
numpages = {12},
keywords = {African American Language, automatic writing assessment, equity, large language models (LLMs)},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/2460296.2460335,
author = {Dimopoulos, Ioannis and Petropoulou, Ourania and Retalis, Symeon},
title = {Assessing students' performance using the learning analytics enriched rubrics},
year = {2013},
isbn = {9781450317856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2460296.2460335},
doi = {10.1145/2460296.2460335},
abstract = {The assessment of students' performance in e-learning environments is a challenging and demanding task for the teachers. Focusing on this challenge, a new assessment tool, called Learning Analytics Enriched Rubric (LAe-R) is presented in this paper. LAe-R is based on the concept of assessment rubrics which is a very popular assessment technique in education. LAe-R contains "enriched" criteria and grading levels that are associated to data extracted from the analysis of learners' interaction and learning behavior in an e-learning environment. LAe-R has been developed as a plug-in for the Moodle learning management system. Via an example, we will show how LAe-R can be used by teachers and students.},
booktitle = {Proceedings of the Third International Conference on Learning Analytics and Knowledge},
pages = {195–199},
numpages = {5},
keywords = {students' assessment performance, learning analytics, interaction analysis indicators, enriched assessment rubrics},
location = {Leuven, Belgium},
series = {LAK '13}
}

@inproceedings{10.1145/3576050.3576074,
author = {Dawson, Shane and Pardo, Abelardo and Salehian Kia, Fatemeh and Panadero, Ernesto},
title = {An Integrated Model of Feedback and Assessment: From fine grained to holistic programmatic review},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576074},
doi = {10.1145/3576050.3576074},
abstract = {Abstract: Research in learning analytics (LA) has long held a strong interest in improving student self-regulated learning and measuring the impact of feedback on student outcomes. Despite more than a decade of work in this space very little is known around the contextual factors that influence the topics and diversity of feedback and assessment a student encounters during their full program of study. This paper presents research investigating the institutional adoption of a personalized feedback tool. The reported findings illustrate an association between the topics of feedback, student performance, year level of the course and discipline. The results highlight the need for LA research to capture feedback, assessment and learning outcomes over an entire program of study. Herein we propose a more integrated model drawing on contemporary understandings of feedback with current research findings. The goal is to push LA towards addressing more complex teaching and learning processes from a systems lens. The model posed in this paper begins to illustrate where and how LA can address noted deficits in education practice to better understand how feedback and assessment are enacted by instructors and interpreted by students.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {579–584},
numpages = {6},
keywords = {Assessment, Feedback, Learning Analytics, Personalized Learning},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576054,
author = {Yan, Lixiang and Martinez-Maldonado, Roberto and Zhao, Linxuan and Li, Xinyu and Gasevic, Dragan},
title = {SeNA: Modelling Socio-spatial Analytics on Homophily by Integrating Social and Epistemic Network Analysis},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576054},
doi = {10.1145/3576050.3576054},
abstract = {Homophily is a fundamental sociological theory that describes the tendency of individuals to interact with others who share similar attributes. This theory has shown evident relevance for studying collaborative learning and classroom orchestration in learning analytics research from a social constructivist perspective. Emerging advancements in multimodal learning analytics have shown promising results in capturing interaction data and generating socio-spatial analytics in physical learning spaces through computer vision and wearable positioning technologies. Yet, there are limited ways for analysing homophily (e.g., social network analysis; SNA), especially for unpacking the temporal connections between different homophilic behaviours. This paper presents a novel analytic approach, Social-epistemic Network Analysis (SeNA), for analysing homophily by combining social network analysis with epistemic network analysis to infuse socio-spatial analytics with temporal insights. The additional insights SeNA may offer over traditional approaches (e.g., SNA) were illustrated through analysing the homophily of 98 students in open learning spaces. The findings showed that SeNA could reveal significant behavioural differences in homophily between comparison groups across different learning designs, which were not accessible to SNA alone. The implications and limitations of SeNA in supporting future learning analytics research regarding homophily in physical learning spaces are also discussed.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {22–32},
numpages = {11},
keywords = {collaborative learning, epistemic network, homophily, learning analytics, social network},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/2330601.2330607,
author = {Drachsler, Hendrik and Dietze, Stefan and Greller, Wolfgang and D'Aquin, Mathieu and Jovanovic, Jelena and Pardo, Abelardo and Reinhardt, Wolfgang and Verbert, Katrien},
title = {1st International Workshop on Learning Analytics and Linked Data},
year = {2012},
isbn = {9781450311113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2330601.2330607},
doi = {10.1145/2330601.2330607},
abstract = {The main objective of the 1st International Workshop on Learning Analytics and Linked Data (#LALD2012) is to connect the research efforts on Linked Data and Learning Analytics in order to create visionary ideas and foster synergies between the two young research fields. Therefore, the workshop will collect, explore, and present datasets, technologies and applications for Technology Enhanced Learning (TEL) to discuss Learning Analytics approaches that make use of educational data or Linked Data sources. During the workshop, an overview of available educational datasets and related initiatives will be given. The participants will have the opportunity to present their own research with respect to educational datasets, technologies and applications and discuss major challenges to collect, reuse, and share these datasets.},
booktitle = {Proceedings of the 2nd International Conference on Learning Analytics and Knowledge},
pages = {9–10},
numpages = {2},
keywords = {privacy, linked data, learning analytics, ethics, educational datasets},
location = {Vancouver, British Columbia, Canada},
series = {LAK '12}
}

@inproceedings{10.1145/2090116.2090146,
author = {Bader-Natal, Ari and Lotze, Thomas},
title = {Evolving a learning analytics platform},
year = {2011},
isbn = {9781450309448},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2090116.2090146},
doi = {10.1145/2090116.2090146},
abstract = {Web-based learning systems offer researchers the ability to collect and analyze fine-grained educational data on the performance and activity of students, as a basis for better understanding and supporting learning among those students. The availability of this data enables stakeholders to pose a variety of interesting questions, often specifically focused on some subset of students. As a system matures, the number of stakeholders, the number of interesting questions, and the number of relevant sub-populations of students also grow, adding complexity to the data analysis task. In this work, we describe an internal analytics system designed and developed to address this challenge, adding flexibility and scalability. Here we present several examples of typical examples of analysis, discuss a few uncommon but powerful use-cases, and share lessons learned from the first two years of iteratively developing the platform.},
booktitle = {Proceedings of the 1st International Conference on Learning Analytics and Knowledge},
pages = {180–185},
numpages = {6},
keywords = {web-based learning, learning analytics platform, collaborative learning},
location = {Banff, Alberta, Canada},
series = {LAK '11}
}

@inproceedings{10.1145/2839509.2850492,
author = {Hundhausen, Christopher D. and Carter, Adam S.},
title = {Exploring Learning Analytics for Computing Education (Abstract Only)},
year = {2016},
isbn = {9781450336857},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2839509.2850492},
doi = {10.1145/2839509.2850492},
abstract = {Computing educators have become increasingly interested in learning analytics, which involves collecting and analyzing data on students' learning processes and outcomes for the purpose of improving learning and instructional practices. A variety of computer programming environments enable the automated collection of log data on students' programming processes. In addition, log data on students' online social behavior can be easily collected. All of these data can be analyzed alongside data on students' learning outcomes in order to identify correlations between learning processes and outcomes, and ultimately to better tailor instruction to students' needs. This BOF will provide a platform for discussing the emerging field of learning analytics within the context of computing education. The following questions will serve as a starting point for our discussions: (1) What types of data should we be collecting on computing students' (2) How can we best analyze these data in order to gain meaningful insights into students' learning processes? (3) How can we design effective instructional interventions based on the data we collect and analyze?},
booktitle = {Proceedings of the 47th ACM Technical Symposium on Computing Science Education},
pages = {707},
numpages = {1},
keywords = {learning management systems, learning analytics, computer science education},
location = {Memphis, Tennessee, USA},
series = {SIGCSE '16}
}

@inproceedings{10.1145/3706468.3706540,
author = {Choi, Jaeyoon and Karumbaiah, Shamya and Matayoshi, Jeffrey},
title = {Bias or Insufficient Sample Size? Improving Reliable Estimation of Algorithmic Bias for Minority Groups},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706540},
doi = {10.1145/3706468.3706540},
abstract = {Despite the prevalent use of predictive models in learning analytics, several studies have demonstrated that these models can show disparate performance across different demographic groups of students. The first step to audit for and mitigate bias is to accurately estimate it. However, the current practice of identifying and measuring group bias faces reliability issues. In this paper, we use simulations and real-world data analysis to explore statistical factors that impact the reliability of bias estimation and suggest approaches to account for it. Our analysis revealed that small group sizes lead to high variability in group bias estimation due to sampling error – an issue that is more likely to impact students from historically marginalized communities. We then suggest statistical approaches, such as bootstrapping, to construct confidence intervals for a more reliable estimation of group bias. Based on our findings, we encourage future learning analytics research to ensure sufficiently large group sizes, construct confidence intervals, use at least two metrics, and move beyond the dichotomy of the presence or absence of bias for a more comprehensive evaluation of group bias.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {547–557},
numpages = {11},
keywords = {Algorithmic bias, Fairness, Group bias, Estimation of bias, Reliability, Predictive models},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/2460296.2460343,
author = {Lonn, Steven and Aguilar, Stephen and Teasley, Stephanie D.},
title = {Issues, challenges, and lessons learned when scaling up a learning analytics intervention},
year = {2013},
isbn = {9781450317856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2460296.2460343},
doi = {10.1145/2460296.2460343},
abstract = {This paper describes an intra-institutional partnership between a research team and a technology service group that was established to facilitate the scaling up of a learning analytics intervention. Our discussion focuses on the benefits and challenges that arose from this partnership in order to provide useful information for similar partnerships developed to support scaling up learning analytics interventions.},
booktitle = {Proceedings of the Third International Conference on Learning Analytics and Knowledge},
pages = {235–239},
numpages = {5},
keywords = {scale, learning analytics, higher education, design-research},
location = {Leuven, Belgium},
series = {LAK '13}
}

@inproceedings{10.1145/2460296.2460344,
author = {Prinsloo, Paul and Slade, Sharon},
title = {An evaluation of policy frameworks for addressing ethical considerations in learning analytics},
year = {2013},
isbn = {9781450317856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2460296.2460344},
doi = {10.1145/2460296.2460344},
abstract = {Higher education institutions have collected and analysed student data for years, with their focus largely on reporting and management needs. A range of institutional policies exist which broadly set out the purposes for which data will be used and how data will be protected. The growing advent of learning analytics has seen the uses to which student data is put expanding rapidly. Generally though the policies setting out institutional use of student data have not kept pace with this change.Institutional policy frameworks should provide not only an enabling environment for the optimal and ethical harvesting and use of data, but also clarify: who benefits and under what conditions, establish conditions for consent and the de-identification of data, and address issues of vulnerability and harm. A directed content analysis of the policy frameworks of two large distance education institutions shows that current policy frameworks do not facilitate the provision of an enabling environment for learning analytics to fulfil its promise.},
booktitle = {Proceedings of the Third International Conference on Learning Analytics and Knowledge},
pages = {240–244},
numpages = {5},
keywords = {policy, learning analytics, ethics, distance learning},
location = {Leuven, Belgium},
series = {LAK '13}
}

@inproceedings{10.1145/3636555.3636935,
author = {Wang, Zuo and Ng, Jeremy Tzi Dong and Que, Ying and Hu, Xiao},
title = {Unveiling Synchrony of Learners’ Multimodal Data in Collaborative Maker Activities},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636935},
doi = {10.1145/3636555.3636935},
abstract = {While current evaluation of maker activities has rarely explored students’ learning processes, the multi-perspective and multi-level nature of collaboration adds complexity to learning processes of collaborative maker activities. In terms of group dynamics as an important indicator of collaboration quality, extant studies have shown the benefits of synchrony between learners’ actions during collaborative learning processes. However, synchrony of learners’ cognitive processes and visual attention in collaborative maker activities remains under-explored. Leveraging the multimodal learning analytics (MMLA) approach, this pilot study examines learners’ synchrony patterns from multiple modalities of data in the collaborative maker activity of virtual reality (VR) content creation. We conducted a user experiment with five pairs of students, and collected and analyzed their electroencephalography (EEG) signals, eye movement and system log data. Results showed that the five pairs of collaborators demonstrated diverse synchrony patterns. We also discovered that, while some groups exhibited synchrony in one modality of data before becoming not synchronized in another modality, other groups started with a lack of synchrony followed by maintaining synchrony. This study is expected to make methodological and practical contributions to MMLA research and assessment of collaborative maker activities.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {922–928},
numpages = {7},
keywords = {Computer-supported collaborative learning, Maker activities, Multimodal learning analytics},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/2567574.2567587,
author = {Xing, Wanli and Wadholm, Bob and Goggins, Sean},
title = {Learning analytics in CSCL with a focus on assessment: an exploratory study of activity theory-informed cluster analysis},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567587},
doi = {10.1145/2567574.2567587},
abstract = {In this paper we propose an automated strategy to assess participation in a multi-mode math discourse environment called Virtual Math Teams with Geogrebra (VMTwG). A holistic participation clustering algorithm is applied through the lens of activity theory. Our activity theory-informed algorithm is a step toward accelerating heuristic approaches to assessing collaborative work in synchronous technology mediated environments like VMTwG. Our Exploratory findings provide an example of a novel, time-efficient, valid, and reliable participatory learning assessment tool for teachers in computer mediated learning environments. Scaling online learning with a combination of computation and theory is the overall goal of the work this paper is situated within.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {59–67},
numpages = {9},
keywords = {learning analytics, educational assessment, assessment, activity theory, CSCL},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/3706468.3706512,
author = {Shah, Mamta and Tan, Yuanru and Eagan, Brendan and Chabalowski, Brittny and Chen, Yahan},
title = {A Dual-Method Examination of Nursing Students’ Teamwork in Simulation-Based Learning: Combining CORDTRA and Ordered Network Analysis to Reveal Patterns and Dynamics},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706512},
doi = {10.1145/3706468.3706512},
abstract = {This study examines nursing students’ teamwork during a simulated pediatric scenario by combining Chronologically Ordered Representations of Discourse and Tool-Related Activity (CORDTRA) with Ordered Network Analysis (ONA). CORDTRA revealed each dyad's progression and critical moments during the scenario, while ONA illustrated how roles were divided. Our findings show that patient and parent interactions, education, and assessments were typically shared between students, whereas technical tasks such as dosage calculations were led by one student with support from the other. These findings highlight the nuanced ways in which manikin-based simulations foster essential teamwork skills, such as communication, task delegation, and problem-solving. This study highlights the methodological benefit of integrating CORDTRA and ONA to capture both temporal and relational dynamics, along with the practical implication that targeted feedback and debriefing informed by these approaches can enhance nursing students’ individual and team performance, and by extension their practice readiness.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {858–864},
numpages = {7},
keywords = {CORDTRA, Learning Analytics, Nursing Education, Ordered Network Analysis, Simulations, Visualization Techniques},
location = {
},
series = {LAK '25}
}

@proceedings{10.1145/2723576,
title = {LAK '15: Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to Learning Analytics &amp; Knowledge 2015 (LAK15), the fifth edition of this international conference. This year LAK takes place at Marist College in Poughkeepsie, a city located on the banks of the majestic Hudson River, midway between Albany, the state capital, and New York City. The theme of LAK15, Scaling Up: Big Data to Big Impact, reflects our growing community of researchers, practitioners, and learners and our success in leveraging the power of "big data" to create substantial impact within higher education and learning at increasingly larger scales, while simultaneously reflecting the need for analytics to be effective at the level of individual learning (n of 1). Building on the momentum generated in earlier conferences and in recognition of our growth and of our mission 'inventing effective means to improve the way students learn', we have introduced a practitioner track this year. We hope that addressing and discussing the learning from two points of view; that of the researcher/learner and the practitioner, will provide each of us some impulse to think diversely about the many stakeholders in learning analytics and to open new perspectives on the intersection between research and the practice.},
location = {Poughkeepsie, New York}
}

@inproceedings{10.1145/2674683.2674699,
author = {Suero Montero, Calkin and Suhonen, Jarkko},
title = {Emotion analysis meets learning analytics: online learner profiling beyond numerical data},
year = {2014},
isbn = {9781450330657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2674683.2674699},
doi = {10.1145/2674683.2674699},
abstract = {Learning analytics is an emerging field of research, which deals with collecting and analysing data about learners and their learning context, as well as developing solutions that utilise the analysed data. Traditionally, learning analytics methods focus on the analysis of learners' digital trails or numerical big data, e.g., online material access, digital learners' records, grades, and length of interaction with the learning environment. However, profiling a learner without taking into account the emotional aspects that may hinder the learner's progress, can only offer an incomplete view of the learning experience. Hence, in this paper, we elaborate on the fusion of emotional aspects (i.e., emotion data) and learning analytics, specifically in online learning settings. We bring an open discussion to the educational technology community regarding the potential of analysing learner's emotions from pedagogical texts (i.e., non-structured text data) generated during an online course. We also discuss the role of negative emotions during learning, the ethical issues with the use of emotion data and the technology acceptance and reliability.},
booktitle = {Proceedings of the 14th Koli Calling International Conference on Computing Education Research},
pages = {165–169},
numpages = {5},
keywords = {online learning, learning analytics, learner's profile, emotional awareness, emotion analysis, big data},
location = {Koli, Finland},
series = {Koli Calling '14}
}

@inproceedings{10.1145/3144826.3145399,
author = {Pazmi\~{n}o-Maji, Rub\'{e}n A. and Garc\'{\i}a-Pe\~{n}alvo, Francisco J. and Conde-Gonz\'{a}lez, Miguel A.},
title = {Comparing Hierarchical Trees in Statistical Implicative Analysis &amp; Hierarchical Cluster in Learning Analytics},
year = {2017},
isbn = {9781450353861},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3144826.3145399},
doi = {10.1145/3144826.3145399},
abstract = {Learning Analytics1 has been and is still an emerging technology in education; the amount of research on learning analysis is increasing every year. The integration of new open source tools, analysis methods, and other calculation options are important. This paper aims to compare hierarchical trees in Statistical Implicative Analysis (SIA) and some hierarchical clusters in Learning Analytics. To this end, we must use a quasi-experimental design with random binary data. A comparison is about the time it takes to evaluate the function for execute the four cluster algorithms: cohesion tree (ASI), similarity tree (ASI), agnes (cluster R package) and hclust (R base function). This paper provides a alternative hierarchical cluster used in Statistical Implicative Analysis that is possible to use in Learning Analytics (LA). Also, provides a comparative R-program used and identifies future research about software performance.},
booktitle = {Proceedings of the 5th International Conference on Technological Ecosystems for Enhancing Multiculturality},
articleno = {49},
numpages = {7},
keywords = {statistical implicative analysis, similarity tree, hierarchical cluster, Software performance, Open source software, Learning analytics, Clustering},
location = {C\'{a}diz, Spain},
series = {TEEM 2017}
}

@inproceedings{10.1145/2883851.2883852,
author = {Bull, S. and Ginon, B. and Kay, J. and Kickmeier-Rust, M. and Johnson, M. D.},
title = {LAL workshop: learning analytics for learners},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883852},
doi = {10.1145/2883851.2883852},
abstract = {With the arrival of 'big data; in education, the potential was recognised for learning analytics to track students' learning, to reveal patterns in their learning, or to identify at-risk students, in addition to guiding reform and supporting educators in improving teaching and learning processes [1]. Learning Analytics dashboards have been used at all levels, including institutional, regional and national level [2]. In classroom use, while learning visualisations are often based on counts of activity data or interaction patterns, there is increasing recognition that learning analytics relate to learning, and should therefore provide pedagogically useful information [3]. While increasing numbers of technology-enhanced learning applications are embracing the potential of learning analytics at the classroom level, often these are aimed at teachers. However, learners can also benefit from learning analytics data (e.g. [4][5]).},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {496–497},
numpages = {2},
keywords = {visual learning analytics, open learner models, learning data for learners, learning analytics for learners, dashboards},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/3313991.3314017,
author = {Jeon, Inseong and Song, Ki-Sang},
title = {The Effect of Learning Analytics System towards Learner's Computational Thinking Capabilities},
year = {2019},
isbn = {9781450362870},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313991.3314017},
doi = {10.1145/3313991.3314017},
abstract = {The purpose of this paper is to show the effect of learning analytics system based teaching that instructor depends on a system which monitors learner's programming activities and grasps his/her achievement level in real-time under block-based programming environment. Based on the analyzed information, teachers might provide timely help and lead students to engage in coding education. Also, we found that there exits statistically significant difference in decomposition, abstraction, and algorithm components among several components of Computational thinking skills.},
booktitle = {Proceedings of the 2019 11th International Conference on Computer and Automation Engineering},
pages = {12–16},
numpages = {5},
keywords = {Software education, Intellectual Tutoring System, Computational thinking, Coding education},
location = {Perth, WN, Australia},
series = {ICCAE 2019}
}

@inproceedings{10.1145/3706468.3706498,
author = {Borchers, Conrad and Baker, Ryan S.},
title = {ABROCA Distributions For Algorithmic Bias Assessment: Considerations Around Interpretation},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706498},
doi = {10.1145/3706468.3706498},
abstract = {Algorithmic bias continues to be a key concern of learning analytics. We study the statistical properties of the Absolute Between-ROC Area (ABROCA) metric. This fairness measure quantifies group-level differences in classifier performance through the absolute difference in ROC curves. ABROCA is particularly useful for detecting nuanced performance differences even when overall Area Under the ROC Curve (AUC) values are similar. We sample ABROCA under various conditions, including varying AUC differences and class distributions. We find that ABROCA distributions exhibit high skewness dependent on sample sizes, AUC differences, and class imbalance. When assessing whether a classifier is biased, this skewness inflates ABROCA values by chance, even when data is drawn (by simulation) from populations with equivalent ROC curves. These findings suggest that ABROCA requires careful interpretation given its distributional properties, especially when used to assess the degree of bias and when classes are imbalanced.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {837–843},
numpages = {7},
keywords = {algorithmic bias, algorithmic fairness, ABROCA, AUC ROC, simulation, classification, prediction},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3242969.3264969,
author = {Thomas, Chinchu},
title = {Multimodal Teaching and Learning Analytics for Classroom and Online Educational Settings},
year = {2018},
isbn = {9781450356923},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3242969.3264969},
doi = {10.1145/3242969.3264969},
abstract = {Automatic analysis of teacher student interactions is an interesting research problem in social computing. Such interactions happen in both online and class room settings. While teaching effectiveness is the goal in both settings, the mechanism to achieve the same could differ in different settings. In order to characterize these interactions multimodal behavioral signals and language use need to be measured, and a model to predict effectiveness needs to be learnt. These would help characterize the teaching skill of the teacher and level of engagement of students. Also, there could be multiple styles of teaching which can be effective.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimodal Interaction},
pages = {542–545},
numpages = {4},
keywords = {teaching analytics, multimodal learning analytics, educational data mining},
location = {Boulder, CO, USA},
series = {ICMI '18}
}

@inproceedings{10.1145/2729094.2742613,
author = {Tarmazdi, Hamid and Vivian, Rebecca and Szabo, Claudia and Falkner, Katrina and Falkner, Nickolas},
title = {Using Learning Analytics to Visualise Computer Science Teamwork},
year = {2015},
isbn = {9781450334402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2729094.2742613},
doi = {10.1145/2729094.2742613},
abstract = {Industry has called upon academia to better prepare Computer Science graduates for teamwork, especially in developing the soft skills necessary for collaborative work. However, the teaching and assessment of teamwork is not easy, with instructors being pressed for time and a lack of tools available to efficiently analyse student teamwork, where large cohorts are involved. We have developed a teamwork dashboard, founded on learning analytics, learning theory and teamwork models that analyses students' online teamwork discussion data and visualises the team mood, role distribution and emotional climate. This tool allows educators to easily monitor teams in real-time. Educators may use the tool to provide students with feedback about team interactions as well as to identify problematic teams. We present a case study, trialing the dashboard on one university Computer Science course and include reflections from the course lecturer to determine its utility in monitoring online student teamwork.},
booktitle = {Proceedings of the 2015 ACM Conference on Innovation and Technology in Computer Science Education},
pages = {165–170},
numpages = {6},
keywords = {learning analytics, computer science education, collaboration},
location = {Vilnius, Lithuania},
series = {ITiCSE '15}
}

@inproceedings{10.1145/2723576.2723658,
author = {Monroy, Carlos and Rangel, Virginia Snodgrass and Bell, Elizabeth R. and Whitaker, Reid},
title = {A learning analytics approach to characterize and analyze inquiry-based pedagogical processes},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723658},
doi = {10.1145/2723576.2723658},
abstract = {Here we describe the use of learning analytics (LA) for investigating inquiry-based science instruction. We define several variables that quantify curriculum usage and leverage tools from process mining to examine inquiry-based pedagogical processes. These are initial steps toward measuring and modeling fidelity of implementation of a science curriculum. We use data from one school district's use of an online science curriculum (N=1,021 teachers and nearly 330,000 page views).},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {398–399},
numpages = {2},
keywords = {process mining, learninformatics, inquiry-based pedagogy},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2330601.2330620,
author = {Bieke, Schreurs and Maarten, De Laat},
title = {Network awareness tool - learning analytics in the workplace: detecting and analyzing informal workplace learning},
year = {2012},
isbn = {9781450311113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2330601.2330620},
doi = {10.1145/2330601.2330620},
abstract = {This paper aims to contribute to the understanding of informal workplace learning in contemporary face-to-face and virtual environments. Informal learning is an important driver for professional development and workplace learning. However powerful informal learning may be, there is a problem when it comes to making it a real asset within organizations: Informal learning activities are mostly invisible to others, sometimes the learners themselves might not even be aware of the learning that occurs. As a consequence informal learning in organizations goes undetected, remains off the radar of HR departments and is therefore hard to asses, manage and value [1]. This problem poses an interesting challenge for the field of Learning Analytics, namely finding ways to capture and analyse traces of (social) informal learning in every day life and work networks. Therefore empirical research and tools are needed that can raise awareness about informal learning activities to make it surface the radar, amplify the benefits of it and strengthen the social relations through which it occurs. In this paper we introduce a tool that aims to facilitate exactly this and we hope to stimulate to widen the discussion on Learning Analytics by expanding the field from a predominantly educational focus to informal and workplace learning. In this paper we will discuss methodologies that Learning Analytics can draw upon to make informal learning more explicit and accessible to analyse and to share amongst professionals.},
booktitle = {Proceedings of the 2nd International Conference on Learning Analytics and Knowledge},
pages = {59–64},
numpages = {6},
keywords = {workplace learning, social network analysis, networked learning visualizations, learning analytics, informal learning networks},
location = {Vancouver, British Columbia, Canada},
series = {LAK '12}
}

@inproceedings{10.1145/3636555.3636900,
author = {Li, Tongguang and Fan, Yizhou and Srivastava, Namrata and Zeng, Zijie and Li, Xinyu and Khosravi, Hassan and Tsai, Yi-Shan and Swiecki, Zachari and Ga\v{s}evi\'{c}, Dragan},
title = {Analytics of Planning Behaviours in Self-Regulated Learning: Links with Strategy Use and Prior Knowledge},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636900},
doi = {10.1145/3636555.3636900},
abstract = {A sophisticated grasp of self-regulated learning (SRL) skills has become essential for learners in computer-based learning environment (CBLE). One aspect of SRL is the plan-making process, which, although emphasized in many SRL theoretical frameworks, has attracted little research attention. Few studies have investigated the extent to which learners complied with their planned strategies, and whether making a strategic plan is associated with actual strategy use. Limited studies have examined the role of prior knowledge in predicting planned and actual strategy use. In this study, we developed a CBLE to collect trace data, which were analyzed to investigate learners’ plan-making process and its association with planned and actual strategy use. Analysis of prior knowledge and trace data of 202 participants indicated that 1) learners tended to adopt strategies that significantly deviated from their planned strategies, 2) the level of prior knowledge was associated with planned strategies, and 3) neither the act of plan-making nor prior knowledge predicted actual strategy use. These insights bear implications for educators and educational technologists to recognise the dynamic nature of strategy adoption and to devise approaches that inspire students to continually revise and adjust their plans, thereby strengthening SRL.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {438–449},
numpages = {12},
keywords = {learning analytics, learning strategies, self-regulated learning, strategic planning},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/2556325.2567855,
author = {Chorianopoulos, Konstantinos and Giannakos, Michail N. and Chrisochoides, Nikos},
title = {Open system for video learning analytics},
year = {2014},
isbn = {9781450326698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2556325.2567855},
doi = {10.1145/2556325.2567855},
abstract = {Video lectures are nowadays widely used by growing numbers of learners all over the world. Nevertheless, learners' interactions with the videos are not readily available, because online video platforms do not share them. In this paper, we present an open-source video learning analytics system, which is also available as a free service to researchers. Our system facilitates the analysis of video learning behavior by capturing learners' interactions with the video player (e.g, seek/scrub, play, pause). In an empirical user study, we captured hundreds of user interactions with the video player by analyzing the interactions as a learner activity time series. We found that learners employed the replaying activity to retrieve the video segments that contained the answers to the survey questions. The above findings indicate the potential of video analytics to represent learner behavior. Further research, should be able to elaborate on learner behavior by collecting large-scale data. In this way, the producers of online video pedagogy will be able to understand the use of this emerging medium and proceed with the appropriate amendments to the current video-based learning systems and practices.},
booktitle = {Proceedings of the First ACM Conference on Learning @ Scale Conference},
pages = {153–154},
numpages = {2},
keywords = {video, user interactions, learning analytics, education},
location = {Atlanta, Georgia, USA},
series = {L@S '14}
}

@inproceedings{10.1145/3012430.3012542,
author = {Menchaca, Iratxe and Guenaga, Mariluz and Solabarrieta, Josu},
title = {Using learning analytics to assess project management skills on engineering degree courses},
year = {2016},
isbn = {9781450347471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3012430.3012542},
doi = {10.1145/3012430.3012542},
abstract = {Learning analytics is a field of study that has been evolving since the outset in attempting to meet various needs. The use of learning analytics techniques has helped us ascertain the level of students' participation and their degree of satisfaction in order to learn how they use resources or identify students at risk. Research currently focuses on applying these techniques to find out how the student learns and to improve teaching/learning processes. A key aspect in improving these processes is the assessment of general competences, which constitutes key learning in engineering students and has thus been identified as a need that can be met by learning analytics. An experiment was conducted on 93 students from different engineering groups at the University of Deusto with a view to assessing the extent to which students have developed the project management competence, using learning analytics techniques. The model designed for analysis is described in this paper, in addition to the methodology and research carried out. Results have shown that by combining an automatic analysis and exploratory learning analytics techniques, conclusions can effectively be drawn about the extent to which a given student has developed a competence based on data obtained via use of a technological tool.},
booktitle = {Proceedings of the Fourth International Conference on Technological Ecosystems for Enhancing Multiculturality},
pages = {369–376},
numpages = {8},
keywords = {project management, learning analytics, formative assessment, engineering education},
location = {Salamanca, Spain},
series = {TEEM '16}
}

@inproceedings{10.1145/3027063.3053256,
author = {Dazo, Suzanne L. and Stepanek, Nicholas R. and Chauhan, Aarjav and Dorn, Brian},
title = {Examining Instructor Use of Learning Analytics},
year = {2017},
isbn = {9781450346566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027063.3053256},
doi = {10.1145/3027063.3053256},
abstract = {This study takes an instructor-centric approach to Learning Analytic (LA) research by analyzing instructor use of the LA within an educational streaming video platform called TrACE. The goal of this study is to understand how instructors naturally interact with analytic dashboards through an empirical analysis. To accomplish this, data of 14 instructors from three institutions that used TrACE from Spring 2015 to Spring 2016 was collected. Data was analyzed to identify frequency of analytic visits, duration of analytic use, differences in analytic use, and differences in use between semesters. Instructors demonstrated preferences for some analytics over others, but the majority of teachers generate short sessions that may not allow for in-depth exploration in analytics. Finally, instructor activity is not always consistent between semesters. Focus groups were conducted to explore motivations behind these findings and future work includes developing LA that address discovered issues.},
booktitle = {Proceedings of the 2017 CHI Conference Extended Abstracts on Human Factors in Computing Systems},
pages = {2504–2510},
numpages = {7},
keywords = {instructor support, analytics usage trends},
location = {Denver, Colorado, USA},
series = {CHI EA '17}
}

@inproceedings{10.1145/3170358.3170364,
author = {Rodr\'{\i}guez-Triana, Mar\'{\i}a Jes\"{u}s and Prieto, Luis P. and Mart\'{\i}nez-Mon\'{e}s, Alejandra and Asensio-P\'{e}rez, Juan I. and Dimitriadis, Yannis},
title = {The teacher in the loop: customizing multimodal learning analytics for blended learning},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170364},
doi = {10.1145/3170358.3170364},
abstract = {In blended learning scenarios, evidence needs to be gathered from digital and physical spaces to obtain a more complete view of the teaching and learning processes. However, these scenarios are highly heterogeneous, and the varying data sources available in each particular context can condition the accuracy, relevance, interpretability and actionability of the Learning Analytics (LA) solutions, affecting also the user's sense of agency and trust in such solutions. To aid stakeholders in making use of learning analytics, we propose a process to involve teachers in customizing multimodal LA (MMLA) solutions, adapting them to their particular blended learning situation (e.g., identifying relevant data sources and metrics). Since measuring the added value of adopting an LA solution is not straightforward, we also propose a concrete method for doing so. The results obtained from two case studies in authentic, blended computer-supported collaborative learning settings show an improvement in the sensitivity and F1 scores of the customized MMLA solution. Aside from these quantitative improvements, participant teachers reported both an increment in the effort involved, but also increased relevance, understanding and actionability of the results.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {417–426},
numpages = {10},
keywords = {personalization, multimodal learning analytics, customization, blended learning},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3448139.3448172,
author = {Li, Qiujie and Jung, Yeonji and Friend Wise, Alyssa},
title = {Beyond First Encounters with Analytics: Questions, Techniques and Challenges in Instructors’ Sensemaking},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448172},
doi = {10.1145/3448139.3448172},
abstract = {Despite growing implementation of teacher-facing analytics in higher education, relatively little is known about the detailed processes through which instructors make sense of analytics in their teaching practices beyond their initial encounters with tools. This study unpacked the sensemaking process of thirteen instructors with analytic experience, using interviews that included walkthroughs of their analytics use. Qualitative inductive analysis was used to identify themes related to (1) the questions they asked of the analytics, (2) the techniques they used to interpret them, and (3) the challenges they encountered. Findings indicated that instructors went beyond a general curiosity to develop three types of questions of the analytics (goal-oriented, problem-oriented, and instruction modification questions). Instructors also used specific techniques to read and explain data by (a) developing expectations about the answers the analytics would provide, and (b) making comparisons to reveal student diversity, identify effects of instructional revision and diagnose issues. The study found instructors faced an initial learning curve when seeking and making use of relevant information, but also continued to revisit these challenges when they were not able to develop a routine of analytics use. These findings both contribute to a conceptual understanding of instructor analytic sensemaking and have practical implications for its systematic support.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {344–353},
numpages = {10},
keywords = {Instructional dashboards, Human-centered analytics, Data-informed instruction, Analytic sensemaking},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3027385.3027405,
author = {Dawson, Shane and Jovanovic, Jelena and Ga\v{s}evi\'{c}, Dragan and Pardo, Abelardo},
title = {From prediction to impact: evaluation of a learning analytics retention program},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027405},
doi = {10.1145/3027385.3027405},
abstract = {Learning analytics research has often been touted as a means to address concerns regarding student retention outcomes. However, few research studies to date, have examined the impact of the implemented intervention strategies designed to address such retention challenges. Moreover, the methodological rigor of some of the existing studies has been challenged. This study evaluates the impact of a pilot retention program. The study contrasts the findings obtained by the use of different methods for analysis of the effect of the intervention. The pilot study was undertaken between 2012 and 2014 resulting in a combined enrolment of 11,160 students. A model to predict attrition was developed, drawing on data from student information system, learning management system interactions, and assessment. The predictive model identified some 1868 students as academically at-risk. Early interventions were implemented involving learning and remediation support. Common statistical methods demonstrated a positive association between the intervention and student retention. However, the effect size was low. The use of more advanced statistical methods, specifically mixed-effect methods explained higher variability in the data (over 99%), yet found the intervention had no effect on the retention outcomes. The study demonstrates that more data about individual differences is required to not only explain retention but to also develop more effective intervention approaches.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {474–478},
numpages = {5},
keywords = {student retention, predictive models, mixed-effects model, learning analytics, early alert systems},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3502717.3532110,
author = {Smith, Julie M.},
title = {How Do Students Learn to Program? Investigating Theory and Practice with Learning Analytics},
year = {2022},
isbn = {9781450392006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3502717.3532110},
doi = {10.1145/3502717.3532110},
abstract = {This dissertation will use the Blackbox data set to explore which student behaviors are most likely to lead to learning a programming concept, resulting in a model of student learning which will be analyzed to determine which learning theories and models it supports. Finally, whether machine learning can be used to predict student learning will be explored.},
booktitle = {Proceedings of the 27th ACM Conference on on Innovation and Technology in Computer Science Education Vol. 2},
pages = {640–641},
numpages = {2},
keywords = {cs1, learning analytics, learning theories},
location = {Dublin, Ireland},
series = {ITiCSE '22}
}

@inproceedings{10.1145/2723576.2723643,
author = {Duval, Erik and Verbert, Katrien and Klerkx, Joris and Wolpers, Martin and Pardo, Abelardo and Govaerts, Sten and Gillet, Denis and Ochoa, Xavier and Parra, Denis},
title = {VISLA: visual aspects of learning analytics},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723643},
doi = {10.1145/2723576.2723643},
abstract = {In this paper, we briefly describe the goal and activities of the LAK15 workshop on Visual Aspects of Learning analytics.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {394–395},
numpages = {2},
keywords = {visual analytics, learning analytics, information visualisation},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/3268808.3268827,
author = {Nguyen, Viet Anh and Nguyen, Quang Bach and Nguyen, Vuong Thinh},
title = {A Model to Forecast Learning Outcomes for Students in Blended Learning Courses Based On Learning Analytics},
year = {2018},
isbn = {9781450365284},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3268808.3268827},
doi = {10.1145/3268808.3268827},
abstract = {One of the difficulties experienced by online learners is the lack of regular supervision as well as the need to provide instructions to support the learning process more effectively. The analysis of the learning data in the online courses is not only becoming increasingly important in forecasting learning outcomes but also providing effective instructional strategies for learners to help them get the best results. In this paper, we propose a forecast learning outcomes model based on learners' interaction with online learning systems by providing learning analytics dashboard for both learners and teachers to monitor and orient online learners. This approach is mainly based on some machine learning and data mining techniques. This research aims to answer two research questions: (1) Is it possible to accurately predict learners' learning outcomes based on their interactive activities? (2) How to monitor and guide learners in an effective online learning environment? To answer these two questions, our model has been developed and tested by learners participating in the Moodle LMS system. The results show that 75% of students have outcomes close to the predicted results with an accuracy of over 50%. These positive results, though done on a small scale, can also be considered as suggestions for studies of using learning analytics in predicting learning outcomes of learners through learning activities.},
booktitle = {Proceedings of the 2nd International Conference on E-Society, E-Education and E-Technology},
pages = {35–41},
numpages = {7},
keywords = {predictive modeling, learning outcomes, learning activities, forecast model, Learning analytics},
location = {Taipei, Taiwan},
series = {ICSET 2018}
}

@inproceedings{10.1145/3706468.3706477,
author = {Baker, Ryan and Mills, Caitlin and Choi, Jaeyoon},
title = {The Difficulty of Achieving High Precision with Low Base Rates for High-Stakes Intervention},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706477},
doi = {10.1145/3706468.3706477},
abstract = {Automated detectors are routinely used in learning analytics for high-stakes, high-risk interventions. Such interventions depend on detectors with a low rate of false positives (i.e., predicting the construct is present when it is not present) in order to avoid giving an intervention where it is not needed, especially when such interventions can be costly or even harmful. This in turn suggests that such a detector needs to have high precision at the cut-off used by the detector for decision-making.  However, high precision is difficult to achieve for the common case where the base rate of the target construct is low. In this paper, we demonstrate the difficulty of achieving high precision for low base rates, and demonstrate how other metrics (such as F1, Kappa, Specificity, and AUC ROC) are insufficient for this specific use case and situation, despite their merits and advantages for other use cases and situations.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {790–796},
numpages = {7},
keywords = {Automated Detection, Precision, Prediction Model, Unbalanced Data},
location = {
},
series = {LAK '25}
}

@proceedings{10.1145/2567574,
title = {LAK '14: Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our pleasure to welcome you to LAK 2014. This is the fourth meeting of Learning Analytics and Knowledge, and this conference has quickly become an established meeting place for top researchers from learning sciences, data mining, learning analytics, computer sciences and other fields. Since the first conference held in 2011, the Learning Analytics community has grown and matured. The annual meetings of LAK are convened by an international professional society, the Society for Learning Analytics Research (SoLAR), which has also just this year launched a new scholarly journal, the Journal of Learning Analytics. The rapid growth of this field and the public attention drawn to the explosion of online technology platforms and services has convinced many of us that we are engaged in an exciting area that holds great promise for innovating teaching and learning.},
location = {Indianapolis, Indiana, USA}
}

@inproceedings{10.1145/2723576.2723598,
author = {Mouri, Kousuke and Ogata, Hiroaki and Uosaki, Noriko},
title = {Ubiquitous learning analytics in the context of real-world language learning},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723598},
doi = {10.1145/2723576.2723598},
abstract = {This paper describes a method of the visualization and analysis for mining useful learning logs from numerous learning experiences that learners have accumulated in the real world as the ubiquitous learning logs. Ubiquitous Learning Log (ULL) is defined as a digital record of what learners have learned in the daily life using ubiquitous technologies. It allows learners to log their learning experiences with photos, audios, videos, location, RFID tag and sensor data, and to share and reuse ULL with others. By constructing real-world corpora which comprise of accumulated ULLs with information such as what, when, where, and how learners have learned in the real world and by analyzing them, we can support learners to learn more effectively. The proposed system will predict their future learning opportunities including their learning patterns and trends by analyzing their past ULLs. The prediction is made possible both by network analysis based on ULL information such as learners, knowledge, place and time and by learners' self-analysis using time-map. By predicting what they tend to learn next in their learning paths, it provides them with more learning opportunities. Accumulated data are so big and the relationships among the data are so complicated that it is difficult to grasp how closely the ULLs are related each other. Therefore, this paper proposes a system to help learners to grasp relationships among learners, knowledge, place and time, using network graphs and network analysis.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {378–382},
numpages = {5},
keywords = {ubiquitous learning log, time map, network graph, network analysis},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2330601.2330634,
author = {Drachsler, Hendrik and Greller, Wolfgang},
title = {The pulse of learning analytics understandings and expectations from the stakeholders},
year = {2012},
isbn = {9781450311113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2330601.2330634},
doi = {10.1145/2330601.2330634},
abstract = {While there is currently much buzz about the new field of learning analytics [19] and the potential it holds for benefiting teaching and learning, the impression one currently gets is that there is also much uncertainty and hesitation, even extending to scepticism. A clear common understanding and vision for the domain has not yet formed among the educator and research community. To investigate this situation, we distributed a stakeholder survey in September 2011 to an international audience from different sectors of education. The findings provide some further insights into the current level of understanding and expectations toward learning analytics among stakeholders. The survey was scaffolded by a conceptual framework on learning analytics that was developed based on a recent literature review. It divides the domain of learning analytics into six critical dimensions. The preliminary survey among 156 educational practitioners and researchers mostly from the higher education sector reveals substantial uncertainties in learning analytics.In this article, we first briefly introduce the learning analytics framework and its six domains that formed the backbone structure to our survey. Afterwards, we describe the method and key results of the learning analytics questionnaire and draw further conclusions for the field in research and practice. The article finishes with plans for future research on the questionnaire and the publication of both data and the questions for others to utilize.},
booktitle = {Proceedings of the 2nd International Conference on Learning Analytics and Knowledge},
pages = {120–129},
numpages = {10},
keywords = {understanding, survey, privacy, learning technologies, learning analytics, innovation, expectations, attitude},
location = {Vancouver, British Columbia, Canada},
series = {LAK '12}
}

@inproceedings{10.1145/2460296.2460313,
author = {Ferguson, Rebecca and Wei, Zhongyu and He, Yulan and Buckingham Shum, Simon},
title = {An evaluation of learning analytics to identify exploratory dialogue in online discussions},
year = {2013},
isbn = {9781450317856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2460296.2460313},
doi = {10.1145/2460296.2460313},
abstract = {Social learning analytics are concerned with the process of knowledge construction as learners build knowledge together in their social and cultural environments. One of the most important tools employed during this process is language. In this paper we take exploratory dialogue, a joint form of co-reasoning, to be an external indicator that learning is taking place. Using techniques developed within the field of computational linguistics, we build on previous work using cue phrases to identify exploratory dialogue within online discussion. Automatic detection of this type of dialogue is framed as a binary classification task that labels each contribution to an online discussion as exploratory or non-exploratory. We describe the development of a self-training framework that employs discourse features and topical features for classification by integrating both cue-phrase matching and k-nearest neighbour classification. Experiments with a corpus constructed from the archive of a two-day online conference show that our proposed framework outperforms other approaches. A classifier developed using the self-training framework is able to make useful distinctions between the learning dialogue taking place at different times within an online conference as well as between the contributions of individual participants.},
booktitle = {Proceedings of the Third International Conference on Learning Analytics and Knowledge},
pages = {85–93},
numpages = {9},
keywords = {synchronous dialogue, social learning analytics, social learning, self-training framework, learning analytics, exploratory dialogue, educational dialogue, educational assessment, discourse analytics, cue-phrase matching, computational linguistics, SocialLearn, MaxEnt, k-nearest neighbour},
location = {Leuven, Belgium},
series = {LAK '13}
}

@inproceedings{10.1145/3027385.3027396,
author = {Ferguson, Rebecca and Clow, Doug},
title = {Where is the evidence? a call to action for learning analytics},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027396},
doi = {10.1145/3027385.3027396},
abstract = {Where is the evidence for learning analytics? In particular, where is the evidence that it improves learning in practice? Can we rely on it? Currently, there are vigorous debates about the quality of research evidence in medicine and psychology, with particular issues around statistical good practice, the 'file drawer effect', and ways in which incentives for stakeholders in the research process reward the quantity of research produced rather than the quality. In this paper, we present the Learning Analytics Community Exchange (LACE) project's Evidence Hub, an effort to relate research evidence in learning analytics to four propositions about learning analytics: whether they support learning, support teaching, are deployed widely, and are used ethically. Surprisingly little evidence in this strong, specific sense was found, and very little was negative (7%, N=123), suggesting that learning analytics is not immune from the pressures in other areas. We explore the evidence in one particular area in detail (whether learning analytics improve teaching and learners support in the university sector), and set out some of the weaknesses of the evidence available. We conclude that there is considerable scope for improving the evidence base for learning analytics, and set out some suggestions of ways for various stakeholders to achieve this.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {56–65},
numpages = {10},
keywords = {validity, reliability, learning analytics cycle, generalisability, evidence hub, evidence, ethics, access},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/2883851.2883855,
author = {Martinez-Maldonado, Roberto and Hernandez-Leo, Davinia and Pardo, Abelardo and Suthers, Dan and Kitto, Kirsty and Charleer, Sven and Aljohani, Naif Radi and Ogata, Hiroaki},
title = {Cross-LAK: learning analytics across physical and digital spaces},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883855},
doi = {10.1145/2883851.2883855},
abstract = {It is of high relevance to the LAK community to explore blended learning scenarios where students can interact at diverse digital and physical learning spaces. This workshop aims to gather the sub-community of LAK researchers, learning scientists and researchers from other communities, interested in ubiquitous, mobile and/or face-to-face learning analytics. An overarching concern is how to integrate and coordinate learning analytics to provide continued support to learning across digital and physical spaces. The goals of the workshop are to share approaches and identify a set of guidelines to design and connect Learning Analytics solutions according to the pedagogical needs and contextual constraints to provide support across digital and physical learning spaces.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {486–487},
numpages = {2},
keywords = {seamless learning, monitoring, learning analytics, integration},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/3027385.3029424,
author = {Tsai, Yi-Shan and Gasevic, Dragan and Mu\~{n}oz-Merino, Pedro J. and Dawson, Shane},
title = {LA policy: developing an institutional policy for learning analytics using the RAPID outcome mapping approach},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029424},
doi = {10.1145/3027385.3029424},
abstract = {This workshop aims to promote strategic planning for learning analytics in higher education through developing institutional policies. While adoption of learning analytics is predominantly seen in small-scale and bottom-up patterns, it is believed that a systemic implementation can bring the widest impact to the education system and lasting benefits to learners. However, the success of it highly depends on the adopted strategy that meets the needs of various stakeholders and systematically pushes the institution towards achieving its targets. It is imperative to develop a learning analytics policy that ensures a practice that is valid, effective and ethical.The workshop involves two components. The first component includes a set of presentations about the state of learning analytics in higher education, drawing on results from an Australian and a European project examining institutional learning analytics policy and adoption processes. The second component is an interactive session where participants are encouraged to share their motivations for adopting learning analytics and the diversity of challenges they perceive impede analytics adoption in their institution. Using the RAPID Outcome Mapping Approach (ROMA), participants will create a draft policy that articulates how the various challenges can be addressed. This workshop aims to further develop our understanding of how learning analytics operates in an organizational system and promote a cultural change in how such analytics are adopted in higher education.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {494–495},
numpages = {2},
keywords = {policy, learning analytics, higher education},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3029440,
author = {Grover, Shuchi and Bienkowski, Marie and Basu, Satabdi and Eagle, Michael and Diana, Nicholas and Stamper, John},
title = {A framework for hypothesis-driven approaches to support data-driven learning analytics in measuring computational thinking in block-based programming},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029440},
doi = {10.1145/3027385.3029440},
abstract = {K-12 classrooms use block-based programming environments (BBPEs) for teaching computer science and computational thinking (CT). To support assessment of student learning in BBPEs, we propose a learning analytics framework that combines hypothesis- and data-driven approaches to discern students' programming strategies from BBPE log data. We use a principled approach to design assessment tasks to elicit evidence of specific CT skills. Piloting these tasks in high school classrooms enabled us to analyze student programs and video recordings of students as they built their programs. We discuss a priori patterns derived from this analysis to support data-driven analysis of log data in order to better assess understanding and use of CT in BBPEs.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {530–531},
numpages = {2},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3027419,
author = {Whitelock-Wainwright, Alexander and Ga\v{s}evi\'{c}, Dragan and Tejeiro, Ricardo},
title = {What do students want? towards an instrument for students' evaluation of quality of learning analytics services},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027419},
doi = {10.1145/3027385.3027419},
abstract = {Quality assurance in any organization is important for ensuring that service users are satisfied with the service offered. For higher education institutes, the use of service quality measures allows for ideological gaps to be both identified and resolved. The learning analytic community, however, has rarely addressed the concept of service quality. A potential outcome of this is the provision of a learning analytics service that only meets the expectations of certain stakeholders (e.g., managers), whilst overlooking those who are most important (e.g., students). In order to resolve this issue, we outline a framework and our current progress towards developing a scale to assess student expectations and perceptions of learning analytics as a service.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {368–372},
numpages = {5},
keywords = {service quality, learning analytics, action research},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3369255.3369277,
author = {Zhang, Wei and Zhou, Yilin and Yi, Baolin},
title = {An Interpretable Online Learner's Performance Prediction Model Based on Learning Analytics},
year = {2020},
isbn = {9781450372541},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3369255.3369277},
doi = {10.1145/3369255.3369277},
abstract = {Most of student performance prediction model only focused on the accuracy of prediction results, but achieving an interpretable prediction model may be as important as obtaining high accuracy in learning prediction research. This paper proposed a student performance prediction model based on online learning behavior analytics with 19 behavior indicators. This model consists of four steps: data collection and processing, correlation analysis, data analytics, student performance prediction algorithm, prediction and intervention. Moreover, a case have been taken to predict student performance according to the model with rule-based genetic programming algorithm. The experiment results show that the rule-based genetic programming algorithm has a stronger interpretation in ensuring competitive prediction accuracy. The model achieves a good prediction effect.},
booktitle = {Proceedings of the 11th International Conference on Education Technology and Computers},
pages = {148–154},
numpages = {7},
keywords = {student performance model, prediction algorithm, learning behavior analytics, intervention, Online learning platform},
location = {Amsterdam, Netherlands},
series = {ICETC '19}
}

@inproceedings{10.1145/2876034.2893389,
author = {Renz, Jan and Navarro-Suarez, Gerado and Sathi, Rowshan and Staubitz, Thomas and Meinel, Christoph},
title = {Enabling Schema Agnostic Learning Analytics in a Service-Oriented MOOC Platform},
year = {2016},
isbn = {9781450337267},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2876034.2893389},
doi = {10.1145/2876034.2893389},
abstract = {This paper at hand describes the design and implementation of an analytics service to retrieve live usage data from students enrolled in a service-oriented MOOC platform for the purpose of learning analytics (LA) research. A real-time and extensible architecture for consolidating and processing data in versatile analytics stores is introduced.},
booktitle = {Proceedings of the Third (2016) ACM Conference on Learning @ Scale},
pages = {137–140},
numpages = {4},
keywords = {service oriented architecture, mooc, learning analytics},
location = {Edinburgh, Scotland, UK},
series = {L@S '16}
}

@inproceedings{10.1145/3506860.3506872,
author = {Yan, Lixiang and Martinez-Maldonado, Roberto and Zhao, Linxuan and Deppeler, Joanne and Corrigan, Deborah and Gasevic, Dragan},
title = {How do Teachers Use Open Learning Spaces? Mapping from Teachers’ Socio-spatial Data to Spatial Pedagogy},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506872},
doi = {10.1145/3506860.3506872},
abstract = {Teacher’s in-class positioning and interaction patterns (termed spatial pedagogy) are an essential part of their classroom management and orchestration strategies that can substantially impact students’ learning. Yet, effective management of teachers’ spatial pedagogy can become increasingly challenging as novel architectural designs, such as open learning spaces, aim to disrupt teaching conventions by promoting flexible pedagogical approaches and maximising student connectedness. Multimodal learning analytics and indoor positioning technologies may hold promises to support teachers in complex learning spaces by making salient aspects of their spatial pedagogy visible for provoking reflection. This paper explores how granular x-y positioning data can be modelled into socio-spatial metrics that can contain insights about teachers’ spatial pedagogy across various learning designs. A total of approximately 172.63 million position data points were collected during 101 classes over eight weeks. The results illustrate how indoor positioning analytics can help generate a deeper understanding of how teachers use their learning spaces, such as their 1) teaching responsibilities; 2) proactive or passive interactions with students; and 3) supervisory, interactional, collaborative, and authoritative teaching approaches. Implications of the current findings to future learning analytics research and educational practices were also discussed.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {87–97},
numpages = {11},
keywords = {spatial pedagogy, proxemics, multimodal learning analytics, learning analytics, indoor positioning},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3576050.3576063,
author = {Pozdniakov, Stanislav and Martinez-Maldonado, Roberto and Tsai, Yi-Shan and Echeverria, Vanessa and Srivastava, Namrata and Gasevic, Dragan},
title = {How Do Teachers Use Dashboards Enhanced with Data Storytelling Elements According to their Data Visualisation Literacy Skills?},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576063},
doi = {10.1145/3576050.3576063},
abstract = {There is a proliferation of learning analytics (LA) dashboards aimed at supporting teachers. Yet, teachers still find it challenging to make sense of LA dashboards, thereby making informed decisions. Two main strategies to address this are emerging: i) upskilling teachers’ data literacy; ii) improving the explanatory design features of current dashboards (e.g., adding visual cues or text) to minimise the skills required by teachers to effectively use dashboards. While each approach has its own trade-offs, no previous work has explored the interplay between the dashboard design and such "data skills". In this paper, we explore how teachers with varying visualisation literacy (VL) skills use LA dashboards enhanced with (explanatory) data storytelling elements. We conducted a quasi-experimental study with 23 teachers of varied VL inspecting two versions of an authentic multichannel dashboard enhanced with data storytelling elements. We used an eye-tracking device while teachers inspected the students’ data captured from Zoom and Google Docs, followed by interviews. Results suggest that high VL teachers adopted complex exploratory strategies and were more sensitive to subtle inconsistencies in the design; while low VL teachers benefited the most from more explicit data storytelling guidance such as accompanying complex graphs with narrative and semantic colour encoding.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {89–99},
numpages = {11},
keywords = {dashboard, data literacy, data storytelling, human-centred design, learning analytics},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576077,
author = {Poquet, Oleksandra and Jovanovic, Jelena and Pardo, Abelardo},
title = {Student Profiles of Change in a University Course: A Complex Dynamical Systems Perspective},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576077},
doi = {10.1145/3576050.3576077},
abstract = {Learning analytics approaches to profiling students based on their study behaviour remain limited in how they integrate temporality and change. To advance this area of work, the current study examines profiles of change in student study behaviour in a blended undergraduate engineering course. The study is conceptualised through complex dynamical systems theory and its applications in psychological and cognitive science research. Students were profiled based on the changes in their behaviour as observed in clickstream data. Measure of entropy in the recurrence of student behaviour was used to indicate the change of a student state, consistent with the evidence from cognitive sciences. Student trajectories of weekly entropy values were clustered to identify distinct profiles. Three patterns were identified: stable weekly study, steep changes in weekly study, and moderate changes in weekly study. The students with steep changes in their weekly study activity had lower exam grades and showed destabilisation of weekly behaviour earlier in the course. The study investigated the relationships between these profiles of change, student performance, and other approaches to learner profiling, such as self-reported measures of self-regulated learning, and profiles based on the sequences of learning actions.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {197–207},
numpages = {11},
keywords = {complex dynamical systems, learning analytics, self-regulated learning},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576093,
author = {Lewis, Armanda and Ochoa, Xavier and Qamra, Rohini},
title = {Instructor-in-the-Loop Exploratory Analytics to Support Group Work},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576093},
doi = {10.1145/3576050.3576093},
abstract = {This case study examines an interactive, low barrier process, termed instructor-in-the-loop, by which an instructor defines and makes meaning from exploratory metrics and visualizations, and uses this multimodal information to improve a course iteratively. We present potentials for course improvement based on automated learning analytics insights related to students’ participation in small active learning sessions associated with a large lecture course. Automated analytics processes are essential for larger courses where engaging smaller groups is important to ensure participation and understanding, but monitoring a large total number of groups throughout an instructional experience becomes untenable for the instructor. Of interest is providing instructors with easy-to-digest summaries of group performance that do not require complex set up and knowledge of more advanced algorithmic approaches. We explore synthesizing metrics and visualizations as ways to engage instructors in meaning making of complex learning environments, but in a low barrier manner that provides insights quickly.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {284–292},
numpages = {9},
keywords = {automated detection, group work, multimodal learning analytics},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3017680.3017711,
author = {Khosravi, Hassan and Cooper, Kendra M.L.},
title = {Using Learning Analytics to Investigate Patterns of Performance and Engagement in Large Classes},
year = {2017},
isbn = {9781450346986},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3017680.3017711},
doi = {10.1145/3017680.3017711},
abstract = {Educators continue to face significant challenges in providing high quality, post-secondary instruction in large classes including: motivating and engaging diverse populations (e.g., academic ability and backgrounds, generational expectations); and providing helpful feedback and guidance. Researchers investigate solutions to these kinds of challenges from alternative perspectives, including learning analytics (LA). Here, LA techniques are applied to explore the data collected for a large, flipped introductory programming class to (1) identify groups of students with similar patterns of performance and engagement; and (2) provide them with more meaningful appraisals that are tailored to help them effectively master the learning objectives. Two studies are reported, which apply clustering to analyze the class population, followed by an analysis of a subpopulation with extreme behaviours.},
booktitle = {Proceedings of the 2017 ACM SIGCSE Technical Symposium on Computer Science Education},
pages = {309–314},
numpages = {6},
keywords = {personalizing learning, learning analytics, clustering, CS1},
location = {Seattle, Washington, USA},
series = {SIGCSE '17}
}

@inproceedings{10.1145/3636555.3636887,
author = {Saint, John and Fan, Yizhou and Gasevic, Dragan},
title = {Analytics of scaffold compliance for self-regulated learning},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636887},
doi = {10.1145/3636555.3636887},
abstract = {The shift toward digitally-based education has emphasised the need for learners to have strong skills for self-regulated learning (SRL). The use of scaffolding prompts is seen as an effective way to stimulate SRL and enhance academic outcomes. A key aspect of SRL scaffolding prompts is the degree to which they are complied to by students. Compliance is a complex concept, one that is further complicated by the nature of scaffold design in the context of adaptability. These nuances notwithstanding, scaffold compliance demands specific exploration. To that end, we conducted a study in which we: 1) focused specifically on scaffolding interaction behaviour in a timed online assessment task, as opposed to the broader interaction with non-scaffolding artefacts; 2) identified distinct scaffold interaction patterns in the context of compliance and non-compliance to scaffold design; 3) analysed how groups of learners traverse compliant and non-compliant interaction behaviours and engage in SRL processes in response to a sequence of timed and personalised SRL-informed scaffold prompts. We found that scaffold interactions fell into two categories of compliance and non-compliance, and whilst there was a healthy engagement with compliance, it does ebb and flow during an online timed assessment.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {326–337},
numpages = {12},
keywords = {Clustering, Learning Analytics, Process Mining, Scaffolding, Scaffolding Compliance, Self-Regulated Learning},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/2460296.2460328,
author = {Martin, Taylor and Aghababyan, Ani and Pfaffman, Jay and Olsen, Jenna and Baker, Stephanie and Janisiewicz, Philip and Phillips, Rachel and Smith, Carmen Petrick},
title = {Nanogenetic learning analytics: illuminating student learning pathways in an online fraction game},
year = {2013},
isbn = {9781450317856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2460296.2460328},
doi = {10.1145/2460296.2460328},
abstract = {A working understanding of fractions is critical to student success in high school and college math. Therefore, an understanding of the learning pathways that lead students to this working understanding is important for educators to provide optimal learning environments for their students. We propose the use of microgenetic analysis techniques including data mining and visualizations to inform our understanding of the process by which students learn fractions in an online game environment. These techniques help identify important variables and classification algorithms to group students by their learning trajectories.},
booktitle = {Proceedings of the Third International Conference on Learning Analytics and Knowledge},
pages = {165–169},
numpages = {5},
keywords = {rational numbers, process analysis, performance, measurement, mathematics education, keywords microgenetic research, games, fractions, experimentation, documentation},
location = {Leuven, Belgium},
series = {LAK '13}
}

@inproceedings{10.1145/2522848.2533790,
author = {Oviatt, Sharon and Cohen, Adrienne and Weibel, Nadir},
title = {Multimodal learning analytics: description of math data corpus for ICMI grand challenge workshop},
year = {2013},
isbn = {9781450321297},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2522848.2533790},
doi = {10.1145/2522848.2533790},
abstract = {This paper provides documentation on dataset resources for establishing a new research area called multimodal learning analytics (MMLA). Research on this topic has the potential to transform the future of educational practice and technology, as well as computational techniques for advancing data analytics. The Math Data Corpus includes high-fidelity time-synchronized multimodal data recordings (speech, digital pen, images) on collaborating groups of students as they work together to solve mathematics problems that vary in difficulty level. The Math Data Corpus resources include initial coding of problem segmentation, problem-solving correctness, and representational content of students' writing. These resources are made available to participants in the data-driven grand challenge for the Second International Workshop on Multimodal Learning Analytics. The primary goal of this event is to analyze coherent signal, activity, and lexical patterns that can identify domain expertise and change in domain expertise early, reliably, and objectively, as well as learning-oriented precursors. An additional aim is to build an international research community in the emerging area of multimodal learning analytics by organizing a series of workshops that bring together multidisciplinary scientists to work on MMLA topics.},
booktitle = {Proceedings of the 15th ACM on International Conference on Multimodal Interaction},
pages = {563–568},
numpages = {6},
keywords = {speech, multimodal learning analytics, math data corpus, images, domain expertise, digital pen, data resources},
location = {Sydney, Australia},
series = {ICMI '13}
}

@inproceedings{10.1145/3576050.3576146,
author = {Poellhuber, Louis-Vincent and Poellhuber, Bruno and Desmarais, Michel and Leger, Christian and Roy, Normand and Manh-Chien Vu, Mathieu},
title = {Cluster-Based Performance of Student Dropout Prediction as a Solution for Large Scale Models in a Moodle LMS},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576146},
doi = {10.1145/3576050.3576146},
abstract = {Learning management systems provide a wide breadth of data waiting to be analyzed and utilized to enhance student and faculty experience in higher education. As universities struggle to support students’ engagement, success and retention, learning analytics is being used to build predictive models and develop dashboards to support learners and help them stay engaged, to help teachers identify students needing support, and to predict and prevent dropout. Learning with Big Data has its challenges, however: managing great quantities of data requires time and expertise. To predict students at risk, many institutions use machine learning algorithms with LMS data for a given course or type of course, but only a few are trying to make predictions for a large subset of courses. This begs the question: “How can student dropout be predicted on a very large set of courses in an institution Moodle LMS?” In this paper, we use automation to improve student dropout prediction for a very large subset of courses, by clustering them based on course design and similarity, then by automatically training, testing, and selecting machine learning algorithms for each cluster. We developed a promising methodology that outlines a basic framework that can be adjusted and optimized in many ways and that further studies can easily build on and improve.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {592–598},
numpages = {7},
keywords = {Moodle LMS, dropout prediction, engagement, learning analytics},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576107,
author = {Aghaei, Kimia and Hatala, Marek and Mogharrab, Alireza},
title = {How Students’ Emotion and Motivation Changes After Viewing Dashboards with Varied Social Comparison Group: A Qualitative Study},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576107},
doi = {10.1145/3576050.3576107},
abstract = {The need to personalize learning analytics dashboards (LADs) is getting more recognized in learning analytics research community. In order to study the impact of these dashboards on learners, various types of prototypes have been designed and deployed in different settings. Applying Weiner’s attribution theory, our goal in this study was to understand the effect of dashboard information content on learners. We wanted to understand how elements of assignment grade, time spent on an assignment, assignment view, and proficiency in the dashboard affect students’ attribution of achievement and motivation for future work. We designed a qualitative study in which we analyzed participants’ responses and indicated behavioural changes after viewing the dashboard. Through in-depth interviews, we aimed to understand students’ interpretations of the designed dashboard, and to what extent social comparison impacts their judgments of learning. Students used multiple dimensions to attribute their success or failure to their ability and effort. Our results indicate that to maximize the benefits of dashboards as a vehicle for motivating change in students learning, the dashboard should promote effort in both personal and social comparison capacities.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {663–669},
numpages = {7},
keywords = {attribution theory, learning analytics dashboard, motivation, qualitative analysis, social comparison},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3506860.3506935,
author = {Zhao, Linxuan and Yan, Lixiang and Gasevic, Dragan and Dix, Samantha and Jaggard, Hollie and Wotherspoon, Rosie and Alfredo, Riordan and Li, Xinyu and Martinez-Maldonado, Roberto},
title = {Modelling Co-located Team Communication from Voice Detection and Positioning Data in Healthcare Simulation},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506935},
doi = {10.1145/3506860.3506935},
abstract = {In co-located situations, team members use a combination of verbal and visual signals to communicate effectively, among which positional forms play a key role. The spatial patterns adopted by team members in terms of where in the physical space they are standing, and who their body is oriented to, can be key in analysing and increasing the quality of interaction during such face-to-face situations. In this paper, we model the students’ communication based on spatial (positioning) and audio (voice detection) data captured from 92 students working in teams of four in the context of healthcare simulation. We extract non-verbal events (i.e., total speaking time, overlapped speech,and speech responses to team members and teachers) and investigate to what extent they can serve as meaningful indicators of students’ performance according to teachers’ learning intentions. The contribution of this paper to multimodal learning analytics includes: i) a generic method to semi-automatically model communication in a setting where students can freely move in the learning space; and ii) results from a mixed-methods analysis of non-verbal indicators of team communication with respect to teachers’ learning design.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {370–380},
numpages = {11},
keywords = {Nursing simulation, Multimodal learning analytics, Learning analytics, Communication, Collaborative learning, Audio},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3706468.3706509,
author = {Ko, Pakon and Liu, Cong and Law, Nancy and Tan, Yuanru and Shaffer, David Williamson},
title = {Exploring students’ epistemic orientation, learning trajectories, and outcomes},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706509},
doi = {10.1145/3706468.3706509},
abstract = {The influence of students’ epistemic orientations on their learning behavior and outcomes is well-documented. However, limited research explores students’ epistemic orientations in terms of conceptual engagement and learning outcomes. This study, set within the context of higher education, examined the patterns of conceptual engagement among two performance groups and identifies differences in their epistemic orientations. Both epistemic network analysis (ENA) and ordered network analysis (ONA) methods were used. The results from the ENA revealed distinct trajectories and patterns of conceptual engagement between high-performing and low-performing students during different periods in their learning journey. High-performing students were able to establish a more interconnected and distributed epistemic network earlier than their low-performing counterparts. ONA results revealed that (1) high-performing students were more inclined to employ abstract theoretical concepts to address empirical concerns, doing so more frequently and earlier; and (2) low-performing students benefitted from forum interactions with high-performing students to expand their knowledge resources and engagement with theoretical constructs over time. These discoveries contribute to our comprehension of epistemic orientations in different learners. The implications of this study could help generate learning analytics that monitor students’ conceptual engagement in forum discussion and provide feedback to guide the design of learning.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {317–327},
numpages = {11},
keywords = {ENA, ONA, epistemic orientation, learning trajectory},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3303772.3303775,
author = {Fincham, Ed and Whitelock-Wainwright, Alexander and Kovanovi\'{c}, Vitomir and Joksimovi\'{c}, Sre\'{c}ko and van Staalduinen, Jan-Paul and Ga\v{s}evi\'{c}, Dragan},
title = {Counting Clicks is Not Enough: Validating a Theorized Model of Engagement in Learning Analytics},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303775},
doi = {10.1145/3303772.3303775},
abstract = {Student engagement is often considered an overarching construct in educational research and practice. Though frequently employed in the learning analytics literature, engagement has been subjected to a variety of interpretations and there is little consensus regarding the very definition of the construct. This raises grave concerns with regards to construct validity: namely, do these varied metrics measure the same thing? To address such concerns, this paper proposes, quantifies, and validates a model of engagement which is both grounded in the theoretical literature and described by common metrics drawn from the field of learning analytics. To identify a latent variable structure in our data we used exploratory factor analysis and validated the derived model on a separate sub-sample of our data using confirmatory factor analysis. To analyze the associations between our latent variables and student outcomes, a structural equation model was fitted, and the validity of this model across different course settings was assessed using MIMIC modeling. Across different domains, the broad consistency of our model with the theoretical literature suggest a mechanism that may be used to inform both interventions and course design.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {501–510},
numpages = {10},
keywords = {Structural Equation Modeling, Measurement Invariance, MOOCs, Factor Analysis, Engagement},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3706468.3706526,
author = {Huang, Kevin and Ferreira Mello, Rafael and Pereira Junior, Cleon and Rodrigues, Luiz and Baars, Martine and Viberg, Olga},
title = {That's What RoBERTa Said: Explainable Classification of Peer Feedback},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706526},
doi = {10.1145/3706468.3706526},
abstract = {Peer feedback (PF) is essential for improving student learning outcomes, particularly in Computer-Supported Collaborative Learning (CSCL) settings. When using digital tools for PF practices, student data (e.g., PF text entries) is generated automatically. Analyzing these large datasets can enhance our understanding of how students learn and help improve their learning. However, manually processing these large datasets is time-intensive, highlighting the need for automation. This study investigates the use of six machine learning models to classify PF messages from 231 students in a large university course. The models include Multi-Layer Perceptron (MLP), Decision Tree, BERT, RoBERTa, DistilBERT, and ChatGPT4o. The models were evaluated based on Cohen’s accuracy and F1-score. Preprocessing involved removing stop words, and the impact of this on model performance was assessed. Results showed that only the Decision Tree model improved with stop-word removal, while performance decreased in the other models. RoBERTa consistently outperformed the others across all metrics. Explainable AI was used to understand RoBERTa’s decisions by identifying the most predictive words. This study contributes to the automatic classification of peer feedback which is crucial for scaling learning analytics efforts aiming to provide better in-time support to students in CSCL settings.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {880–886},
numpages = {7},
keywords = {Peer feedback, Higher Education, Machine Learning, Explainable artificial intelligence, Computer Supported Collaborative Learning.},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/2090116.2090130,
author = {Ferguson, Rebecca and Buckingham Shum, Simon},
title = {Learning analytics to identify exploratory dialogue within synchronous text chat},
year = {2011},
isbn = {9781450309448},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2090116.2090130},
doi = {10.1145/2090116.2090130},
abstract = {While generic web analytics tend to focus on easily harvested quantitative data, Learning Analytics will often seek qualitative understanding of the context and meaning of this information. This is critical in the case of dialogue, which may be employed to share knowledge and jointly construct understandings, but which also involves many superficial exchanges. Previous studies have validated a particular pattern of 'exploratory dialogue' in learning environments to signify sharing, challenge, evaluation and careful consideration by participants. This study investigates the use of sociocultural discourse analysis to analyse synchronous text chat during an online conference. Key words and phrases indicative of exploratory dialogue were identified in these exchanges, and peaks of exploratory dialogue were associated with periods set aside for discussion and keynote speakers. Fewer individuals posted at these times, but meaningful discussion outweighed trivial exchanges. If further analysis confirms the validity of these markers as learning analytics, they could be used by recommendation engines to support learners and teachers in locating dialogue exchanges where deeper learning appears to be taking place.},
booktitle = {Proceedings of the 1st International Conference on Learning Analytics and Knowledge},
pages = {99–103},
numpages = {5},
keywords = {text chat, synchronous dialogue, learning analytics, instant messaging, exploratory dialogue, educational dialogue},
location = {Banff, Alberta, Canada},
series = {LAK '11}
}

@inproceedings{10.1145/3706468.3706564,
author = {Ramanathan, Sriram and Lim, Lisa-Angelique and Mottaghi, Nazanin Rezazadeh and Buckingham Shum, Simon},
title = {When the Prompt becomes the Codebook: Grounded Prompt Engineering (GROPROE) and its application to Belonging Analytics},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706564},
doi = {10.1145/3706468.3706564},
abstract = {With the emergence of generative AI, the field of Learning Analytics (LA) has increasingly embraced the use of Large Language Models (LLMs) to automate qualitative analysis. Deductive analysis requires theoretical or other conceptual grounding to inform coding. However, few studies detail the process of translating the literature into a codebook, and then into an effective LLM prompt. In this paper, we introduce Grounded Prompt Engineering (GROPROE) as a systematic process to develop a literature-grounded prompt for deductive analysis. We demonstrate our GROPROE process on a dataset of 860 written reflections, coding for students’ affective engagement and sense of belonging. To evaluate the quality of the coding we demonstrate substantial human/LLM Inter-Annotator Reliability (IAR). To evaluate the consistency of LLM coding, a subset of the data was analysed 60 times using the LLM Quotient showing how this stabilized for most codes. We discuss the dynamics of human-AI interaction when following GROPROE, foregrounding how the prompt took over as the iteratively revised codebook, and how the LLM provoked codebook revision. The contributions to the LA field are threefold: (i) GROPROE as a systematic prompt-design process for deductive coding grounded in literature, (ii) a detailed worked example showing its application to Belonging Analytics, and (iii) implications for human-AI interaction in automated deductive analysis.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {713–725},
numpages = {13},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/2536536.2536574,
author = {Minovi\'{c}, Miroslav and Milovanovi\'{c}, Milo\v{s}},
title = {Real-time learning analytics in educational games},
year = {2013},
isbn = {9781450323451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2536536.2536574},
doi = {10.1145/2536536.2536574},
abstract = {Learning analytics is a research area that targets the problem of tracking and evaluating students learning progress. In case of educational games, traditional analytic tools can prove insufficient. Electronic games are dynamic, packed with action and learning is integral part of gameplay. Such a specific learning environment requires a specific real-time analytical tool that will adequately match the dynamic game environment. This paper presents a new form of specific visualization tool for educators to track students learning progress in real-time, while in gameplay session. The tool is based on the specially designed students learning model. Same tool can be used by students to track the game progress. Use of this tool provides educators with real-time tracking of students learning and enables them to react and influence the overall learning process.},
booktitle = {Proceedings of the First International Conference on Technological Ecosystem for Enhancing Multiculturality},
pages = {245–251},
numpages = {7},
keywords = {learning analytics, educational games, data visualisation},
location = {Salamanca, Spain},
series = {TEEM '13}
}

@inproceedings{10.1145/2460296.2460345,
author = {Niemann, Katja and Wolpers, Martin and Stoitsis, Giannis and Chinis, Georgios and Manouselis, Nikos},
title = {Aggregating social and usage datasets for learning analytics: data-oriented challenges},
year = {2013},
isbn = {9781450317856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2460296.2460345},
doi = {10.1145/2460296.2460345},
abstract = {Recent work has studied real-life social and usage datasets from educational applications, highlighting the opportunity to combine or merge them. It is expected that being able to put together different datasets from various applications will make it possible to support learning analytics of a much larger scale and across different contexts. We examine how this can be achieved from a practical perspective by carrying out a study that focuses on three real datasets. More specifically, we combine social data that has been collected from the users of three learning portals and reflect on how they should be handled. We start by studying the data types and formats that these portals use to represent and store social and usage data. Then we develop crosswalks between the different schemas, so that merged versions of the source datasets may be created. The results of this bottom-up, hands-on investigation reveal several interesting issues that need to be overcome before aggregated sets of social and usage data can be actually used to support learning analytics research or services.},
booktitle = {Proceedings of the Third International Conference on Learning Analytics and Knowledge},
pages = {245–249},
numpages = {5},
keywords = {usage data formats, experimental investigation, education, dataset, data-driven analysis},
location = {Leuven, Belgium},
series = {LAK '13}
}

@inproceedings{10.1145/2858796.2858798,
author = {Ihantola, Petri and Vihavainen, Arto and Ahadi, Alireza and Butler, Matthew and B\"{o}rstler, J\"{u}rgen and Edwards, Stephen H. and Isohanni, Essi and Korhonen, Ari and Petersen, Andrew and Rivers, Kelly and Rubio, Miguel \'{A}ngel and Sheard, Judy and Skupas, Bronius and Spacco, Jaime and Szabo, Claudia and Toll, Daniel},
title = {Educational Data Mining and Learning Analytics in Programming: Literature Review and Case Studies},
year = {2015},
isbn = {9781450341462},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2858796.2858798},
doi = {10.1145/2858796.2858798},
abstract = {Educational data mining and learning analytics promise better understanding of student behavior and knowledge, as well as new information on the tacit factors that contribute to student actions. This knowledge can be used to inform decisions related to course and tool design and pedagogy, and to further engage students and guide those at risk of failure. This working group report provides an overview of the body of knowledge regarding the use of educational data mining and learning analytics focused on the teaching and learning of programming. In a literature survey on mining students' programming processes for 2005-2015, we observe a significant increase in work related to the field. However, the majority of the studies focus on simplistic metric analysis and are conducted within a single institution and a single course. This indicates the existence of further avenues of research and a critical need for validation and replication to better understand the various contributing factors and the reasons why certain results occur. We introduce a novel taxonomy to analyse replicating studies and discuss the importance of replicating and reproducing previous work. We describe what is the state of the art in collecting and sharing programming data. To better understand the challenges involved in replicating or reproducing existing studies, we report our experiences from three case studies using programming data. Finally, we present a discussion of future directions for the education and research community.},
booktitle = {Proceedings of the 2015 ITiCSE on Working Group Reports},
pages = {41–63},
numpages = {23},
keywords = {replication, programming, literature review, learning analytics, educational data mining},
location = {Vilnius, Lithuania},
series = {ITICSE-WGR '15}
}

@inproceedings{10.1145/2536536.2536579,
author = {Fulantelli, Giovanni and Taibi, Davide and Arrigo, Marco},
title = {A semantic approach to mobile learning analytics},
year = {2013},
isbn = {9781450323451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2536536.2536579},
doi = {10.1145/2536536.2536579},
abstract = {Mobile learning has reached a considerable level of maturity in recent years, and its role is widely acknowledged in school contexts, university, vocational training, formal and non-formal learning settings, and more generally as an opportunity for lifelong learning. Despite its maturity, evaluation of mobile learning remains an open research issue, especially as regards the activities that take place outside the classroom. In this context, Learning Analytics can provide answers, and offer the appropriate tools to enhance mobile learning experiences. In recent years Learning Analytics has been highly successful in different contexts, but mobile learning exhibits particular characteristics related to the technologies used, student mobility, the possibility of having localized data and information and the social dynamics that characterize the context in which learning takes place. In this paper we propose an innovative approach to support analytics of learners' activities in a mobile learning setting based on the Semantic Web paradigm and on the semantic relationships expressed in the Linked Open Data cloud. MeLOD, a mobile environment for learning with Linked Open Data, is also introduced as a demonstrator for the ideas illustrated in the paper. Potentials and pitfalls of the proposed approach, both for teachers and learners, are reported in the conclusions.},
booktitle = {Proceedings of the First International Conference on Technological Ecosystem for Enhancing Multiculturality},
pages = {287–292},
numpages = {6},
keywords = {semantic web, mobile learning, linked open data, learning analytics},
location = {Salamanca, Spain},
series = {TEEM '13}
}

@inproceedings{10.1145/3706468.3706475,
author = {Alfredo, Riordan and Mejia-Domenzain, Paola and Echeverria, Vanessa and Rahayu, Dwi and Zhao, Linxuan and Alajlan, Haya and Swiecki, Zachari and K\"{a}ser, Tanja and Ga\v{s}evi\'{c}, Dragan and Martinez-Maldonado, Roberto},
title = {TeamTeachingViz: Benefits, Challenges, and Ethical Considerations of Using a Multimodal Analytics Dashboard to Support Team Teaching Reflection},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706475},
doi = {10.1145/3706468.3706475},
abstract = {Team teaching in higher education can be challenging, especially for educators managing large classes with limited pedagogical training and few opportunities to reflect on their practices. Emerging sensing technologies and analytics can capture and analyse patterns of collaboration, communication, and movement of team teaching. Yet, few studies have presented these data to educators for reflection. To address this gap, we examine the benefits, challenges, and concerns of presenting multimodal teaching data (positional, audio, and spatial pedagogy observations) to educators via the TeamTeachingViz dashboard. We evaluated TeamTeachingViz in an authentic classroom context where educators explored their own data and team teaching strategies. Multimodal data was collected from 36 in-the-wild classroom sessions involving 12 educators grouped in various combinations over 4 weeks, followed by semi-structured interviews to reflect on their practices. Findings suggest that educators improved their self-awareness by using data-driven insights to understand their movements and interactions, enabling continuous improvement in team teaching. However, they noted the need for additional data, such as student behaviours and speech content, to better contextualise these insights.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {58–69},
numpages = {12},
keywords = {teaching analytics, LA dashboard, multimodal learning analytics, co-teaching, teaching reflection, spatial pedagogy, in-the-wild},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706518,
author = {Hern\'{a}ndez-Campos, M\'{o}nica and Hilliger, Isabel and Garc\'{\i}a-Pe\~{n}alvo, Francisco-Jos\'{e}},
title = {Evaluating Learning Outcomes Through Curriculum Analytics: Actionable Insights for Curriculum Decision-making: A Design-based research approach to assess learning outcomes in higher education},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706518},
doi = {10.1145/3706468.3706518},
abstract = {Learning analytics (LA) emerged with the promise of improving student learning outcomes (LOs), however, its effectiveness in informing actionable insights remains a challenge. Curriculum analytics (CA), a subfield of LA, seeks to address this by using data to inform curriculum development. This study explores using CA to evaluate LOs through direct standardized measures at the subject level, examining how this process informs curriculum decision-making. Conducted at an engineering-focused higher education institution, the research involved 32 administrators and 153 faculty members, serving 9.906 students across nine programs. By utilizing the Integrative Learning Design Framework, we conducted three phases of this framework and present key results. Findings confirm the importance of stakeholder involvement throughout different design phases, highlighting the need for ongoing training and support. Among the actionable insights that emerged from LOs assessments, we identified faculty reflections regarding the need to incorporate active learning strategies, improve course planning, and acknowledge the need for education-specific training for faculty development. Although the study does not demonstrate whether these insights lead to improvements in LOs, this paper contributes to the CA field by offering a practical approach to evaluating LOs and translating these assessments into actionable improvements within an actual-world educational context.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {384–394},
numpages = {11},
keywords = {Additional Keywords and Phrases},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3636555.3636918,
author = {Feng, Shihui and Yan, Lixiang and Zhao, Linxuan and Maldonado, Roberto Martinez and Ga\v{s}evi\'{c}, Dragan},
title = {Heterogenous Network Analytics of Small Group Teamwork: Using Multimodal Data to Uncover Individual Behavioral Engagement Strategies},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636918},
doi = {10.1145/3636555.3636918},
abstract = {Individual behavioral engagement is an important indicator of active learning in collaborative settings, encompassing multidimensional behaviors mediated through various interaction modes. Little existing work has explored the use of multimodal process data to understand individual behavioral engagement in face-to-face collaborative learning settings. In this study we bridge this gap, for the first time, introducing a heterogeneous tripartite network approach to analyze the interconnections among multimodal process data in collaborative learning. Students’ behavioral engagement strategies are analyzed based on their interaction patterns with various spatial locations and verbal communication types using a heterogeneous tripartite network. The multimodal collaborative learning process data were collected from 15 teams of four students. We conducted stochastic blockmodeling on a projection of the heterogeneous tripartite network to cluster students into groups that shared similar spatial and oral engagement patterns. We found two distinct clusters of students, whose characteristic behavioural engagement strategies were identified by extracting interaction patterns that were statistically significant relative to a multinomial null model. The two identified clusters also exhibited a statistically significant difference regarding students’ perceived collaboration satisfaction and teacher-assessed team performance level. This study advances collaboration analytics methodology and provides new insights into personalized support in collaborative learning.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {587–597},
numpages = {11},
keywords = {collaborative learning, heterogeneous networks, individual engagement, multimodal learning analytics},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3706468.3706531,
author = {Thomas, Danielle R and Borchers, Conrad and Kakarla, Sanjit and Lin, Jionghao and Bhushan, Shambhavi and Guo, Boyuan and Gatz, Erin and Koedinger, Kenneth R},
title = {Do Tutors Learn from Equity Training and Can Generative AI Assess It?},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706531},
doi = {10.1145/3706468.3706531},
abstract = {Equity is a core concern of learning analytics. However, applications that teach and assess equity skills, particularly at scale are lacking, often due to barriers in evaluating language. Advances in generative AI via large language models (LLMs) are being used in a wide range of applications, with this present work assessing its use in the equity domain. We evaluate tutor performance within an online lesson on enhancing tutors’ skills when responding to students in potentially inequitable situations. We apply a mixed-method approach to analyze the performance of 81 undergraduate remote tutors. We find marginally significant learning gains with increases in tutors’ self-reported confidence in their knowledge in responding to middle school students experiencing possible inequities from pretest to posttest. Both GPT-4o and GPT-4-turbo demonstrate proficiency in assessing tutors ability to predict and explain the best approach. Balancing performance, efficiency, and cost, we determine that few-shot learning using GPT-4o is the preferred model. This work makes available a dataset of lesson log data, tutor responses, rubrics for human annotation, and generative AI prompts. Future work involves leveling the difficulty among scenarios and enhancing LLM prompts for large-scale grading and assessment.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {505–515},
numpages = {11},
keywords = {Tutor Training, Generative AI, Large Language Models, Assessment, Equity},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/2723576.2723585,
author = {Prinsloo, Paul and Slade, Sharon},
title = {Student privacy self-management: implications for learning analytics},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723585},
doi = {10.1145/2723576.2723585},
abstract = {Optimizing the harvesting and analysis of student data promises to clear the fog surrounding the key drivers of student success and retention, and provide potential for improved student success. At the same time, concerns are increasingly voiced around the extent to which individuals are routinely and progressively tracked as they engage online. The Internet, the very thing that promised to open up possibilities and to break down communication barriers, now threatens to narrow it again through the panopticon of mass surveillance.Within higher education, our assumptions and understanding of issues surrounding student attitudes to privacy are influenced both by the apparent ease with which the public appear to share the detail of their lives and our paternalistic institutional cultures. As such, it can be easy to allow our enthusiasm for the possibilities offered by learning analytics to outweigh consideration of issues of privacy.This paper explores issues around consent and the seemingly simple choice to allow students to opt-in or opt-out of having their data tracked. We consider how 3 providers of massive open online courses (MOOCs) inform users of how their data is used, and discuss how higher education institutions can work toward an approach which engages and more fully informs students of the implications of learning analytics on their personal data.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {83–92},
numpages = {10},
keywords = {opting out, opt out, learning analytics, informed consent, ethics},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2661172.2661199,
author = {Morse, Robert Kenneth},
title = {Towards Requirements for Supporting Course Redesign with Learning Analytics},
year = {2014},
isbn = {9781450327800},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2661172.2661199},
doi = {10.1145/2661172.2661199},
abstract = {With increased focus on quality, many Colleges and Universities run large enrollment online courses by maintaining course masters or frameworks. A framework enables curriculum managers to instantiate hundreds of course sections by keeping under control the quality of common elements of instruction. Current research in learning analytics tools suggests their applicability for the purposes of course design however, the current set of tools were built to support either course or student interventions. Little work has been done to examine the challenges of developing tools for the purpose of supporting the course redesign process. This research is the first phase of a larger project aimed at developing a "Best Practice Finder" which would aggregate section level data to help curriculum managers improve the design of their frameworks. This phase of the research seeks to investigate key requirements for the support of the use of learning analytic systems for the purpose of course redesign. Responses from 71 program chairs of Ivy Tech Community College of Indiana were collected to gauge the relative importance of curricular, institutional, and statistical knowledge they would need to make sense of learning analytics. Three main approaches to sense making emerged: a course centered, an institution centered, and an information centered approach. This work paves the way for an articulation of a full set of requirements for supporting course design focused learning analytics.},
booktitle = {Proceedings of the 42nd Annual ACM SIGUCCS Conference on User Services},
pages = {89–92},
numpages = {4},
keywords = {online course development, learning analytics, education administration, distance education, analytics},
location = {Salt Lake City, Utah, USA},
series = {SIGUCCS '14}
}

@inproceedings{10.1145/3170358.3170420,
author = {Worsley, Marcelo},
title = {(Dis)engagement matters: identifying efficacious learning practices with multimodal learning analytics},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170420},
doi = {10.1145/3170358.3170420},
abstract = {Video analysis is a staple of the education research community. For many contemporary education researchers, participation in the video coding process serves as a rite of passage. However, recent developments in multimodal learning analytics may help to accelerate and enhance this process by providing researchers with a more nuanced glimpse into a set of learning experiences. As an example of how to use multimodal learning analytics towards these ends, this paper includes a preliminary analysis from 54 college students, who completed two engineering design tasks in pairs. Gesture, speech and electro-dermal activation data were collected as students completed these tasks. The gesture data was used to learn a set of canonical clusters (N=4). A decision tree was trained based on individual students' cluster frequencies, and pre-post learning gains. The nodes in the decision tree were then used to identify a subset of video segments that were human coded based on prior work in learning analytics and engineering design. The combination of machine learning and human inference helps elucidate the practices that seem to correlate with student learning. In particular, both engagement and disengagement seem to correlate with student learning, albeit in a somewhat nuanced fashion.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {365–369},
numpages = {5},
keywords = {qualitative analysis, gesture, engineering design, collaboration},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3706468.3706505,
author = {Butler, Darren and Borchers, Conrad and Asher, Michael and Lee, Yongmin and Karnataki, Sonya and Dangi, Sameeksha and Athreya, Samyukta and Stamper, John and Ogan, Amy and Carvalho, Paulo},
title = {Does the Doer Effect Generalize To Non-WEIRD Populations? Toward Analytics in Radio and Phone-Based Learning},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706505},
doi = {10.1145/3706468.3706505},
abstract = {The Doer Effect states that completing more active learning activities, like practice questions, is more strongly related to positive learning outcomes than passive learning activities, like reading, watching, or listening to course materials. Although broad, most evidence has emerged from practice with tutoring systems in Western, Industrialized, Rich, Educated, and Democratic (WEIRD) populations in North America and Europe. Does the Doer Effect generalize beyond WEIRD populations, where learners may practice in remote locales through different technologies? Through learning analytics, we provide evidence from N = 234 Ugandan students answering multiple-choice questions via phones and listening to lectures via community radio. Our findings support the hypothesis that active learning is more associated with learning outcomes than passive learning. We find this relationship is weaker for learners with higher prior educational attainment. Our findings motivate further study of the Doer Effect in diverse populations. We offer considerations for future research in designing and evaluating contextually relevant active and passive learning opportunities including leveraging familiar technology, increasing the number of practice opportunities, and aligning multiple data sources.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {844–850},
numpages = {7},
keywords = {doer effect, learning by doing, replication, mobile learning, global south, distance learning, equity},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706546,
author = {Liu, Qinyi and Deho, Oscar and Vadiee, Farhad and Khalil, Mohammad and Joksimovic, Srecko and Siemens, George},
title = {Can Synthetic Data be Fair and Private? A Comparative Study of Synthetic Data Generation and Fairness Algorithms},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706546},
doi = {10.1145/3706468.3706546},
abstract = {The increasing use of machine learning in learning analytics (LA) has raised significant concerns around algorithmic fairness and privacy. Synthetic data has emerged as a dual-purpose tool, enhancing privacy and improving fairness in LA models. However, prior research suggests an inverse relationship between fairness and privacy, making it challenging to optimize both. This study investigates which synthetic data generators can best balance privacy and fairness, and whether pre-processing fairness algorithms, typically applied to real datasets, are effective on synthetic data. Our results highlight that the DEbiasing CAusal Fairness (DECAF) algorithm achieves the best balance between privacy and fairness. However, DECAF suffers in utility, as reflected in its predictive accuracy. Notably, we found that applying pre-processing fairness algorithms to synthetic data improves fairness even more than when applied to real data. These findings suggest that combining synthetic data generation with fairness pre-processing offers a promising approach to creating fairer LA models.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {591–600},
numpages = {10},
keywords = {Privacy, Synthetic Data Generation, Algorithmic Fairness, Fairness Metrics, Classifiers},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3576050.3576101,
author = {Emerson, Andrew and Min, Wookhee and Rowe, Jonathan and Azevedo, Roger and Lester, James},
title = {Multimodal Predictive Student Modeling with Multi-Task Transfer Learning},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576101},
doi = {10.1145/3576050.3576101},
abstract = {Game-based learning environments have the distinctive capacity to promote learning experiences that are both engaging and effective. Recent advances in sensor-based technologies (e.g., facial expression analysis and eye gaze tracking) and natural language processing have introduced the opportunity to leverage multimodal data streams for learning analytics. Learning analytics and student modeling informed by multimodal data captured during students’ interactions with game-based learning environments hold significant promise for designing effective learning environments that detect unproductive student behaviors and provide adaptive support for students during learning. Learning analytics frameworks that can accurately predict student learning outcomes early in students’ interactions hold considerable promise for enabling environments to dynamically adapt to individual student needs. In this paper, we investigate a multimodal, multi-task predictive student modeling framework for game-based learning environments. The framework is evaluated on two datasets of game-based learning interactions from two student populations (n=61 and n=118) who interacted with two versions of a game-based learning environment for microbiology education. The framework leverages available multimodal data channels from the datasets to simultaneously predict student post-test performance and interest. In addition to inducing models for each dataset individually, this work investigates the ability to use information learned from one source dataset to improve models based on another target dataset (i.e., transfer learning using pre-trained models). Results from a series of ablation experiments indicate the differences in predictive capacity among a combination of modalities including gameplay, eye gaze, facial expressions, and reflection text for predicting the two target variables. In addition, multi-task models were able to improve predictive performance compared to single-task baselines for one target variable, but not both. Lastly, transfer learning showed promise in improving predictive capacity in both datasets.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {333–344},
numpages = {12},
keywords = {Game-Based Learning, Multimodal Learning Analytics, Predictive Student Modeling},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/2723576.2723659,
author = {Worsley, Marcelo and Blikstein, Paulo},
title = {Using learning analytics to study cognitive disequilibrium in a complex learning environment},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723659},
doi = {10.1145/2723576.2723659},
abstract = {Cognitive disequilibrium has received significant attention for its role in fostering student learning in intelligent tutoring systems and in complex learning environments. In this paper, we both add to and extend this discussion by analyzing the emergence of four affective states associated with disequilibrium: joy, surprise, neutrality and confusion; in a collaborative hands-on, engineering design task. Specifically, we conduct a comparison between two learning strategies to make salient how the strategies are associated with different affective states. This comparison is grounded in the construction of a probabilistic model of student affective state as defined by the frequency of each state, and the rate of transition between affective states. Through this comparison we confirm prior research that highlights the importance of confusion as a marker of knowledge construction, but put to question the notion that surprise is a significant mediator of cognitive disequilibrium. Overall, we show how modeling learner affect is useful for understanding and improving learning in complex, hands-on learning environments.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {426–427},
numpages = {2},
keywords = {learning sciences, cognition, affect},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2669711.2669914,
author = {Ruiz, Javier Santofimia and D\'{\i}az, H\'{e}ctor J. Pijeira and Ruip\'{e}rez-Valiente, Jos\'{e} A. and Mu\~{n}oz-Merino, Pedro J. and Kloos, Carlos Delgado},
title = {Towards the development of a learning analytics extension in open edX},
year = {2014},
isbn = {9781450328968},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2669711.2669914},
doi = {10.1145/2669711.2669914},
abstract = {The emergence of platforms to support MOOCs (Massive Open Online Courses) strengthens the need of a powerful learning analytics support since teachers cannot be aware of so many students. However, the learning analytics support in MOOC platforms is in an early stage nowadays. The edX platform, one of the most important MOOC platforms, has few learning analytics functionalities at present. In this paper, we analyze the learning analytics support given by the edX platform, and the main initiatives to implement learning analytics in edX. We also present our initial steps to implement a learning analytics extension in edX. We review technical aspects, difficulties, solutions, the architecture and the different elements involved. Finally, we present some new visualizations in the edX platform for teachers and students to help them understand the learning process.},
booktitle = {Proceedings of the Second International Conference on Technological Ecosystems for Enhancing Multiculturality},
pages = {299–306},
numpages = {8},
keywords = {visualizations, learning analytics, MOOCs},
location = {Salamanca, Spain},
series = {TEEM '14}
}

@inproceedings{10.1145/2729104.2729126,
author = {Patarakin, Evgeny and Burov, Vasiliy and Parfenov, Roman},
title = {Learning Analytics for Mixed E-Governance-E-Learning Projects},
year = {2014},
isbn = {9781450334013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2729104.2729126},
doi = {10.1145/2729104.2729126},
abstract = {This paper states that e-governance and e-learning both need new types of participants, those who would act as agents in a process of productive activity and would understand that the final product depends entirely on their activity in collaborative work. The related needs of e-governance and e-learning lead to the emergence of a new type of mixed project: 'e-gov-e-learning' projects. This article presents some examples of using learning analytics. A procedure is described for finding sets of key players in mixed 'e-gov-e-learning' projects.},
booktitle = {Proceedings of the 2014 Conference on Electronic Governance and Open Society: Challenges in Eurasia},
pages = {34–37},
numpages = {4},
keywords = {learning analytics, e-learning, crowdsourcing, collaboration, agency, E-governance},
location = {St. Petersburg, Russian Federation},
series = {EGOSE '14}
}

@inproceedings{10.1145/3706468.3706550,
author = {Liu, Naiming and Sonkar, Shashank and Basu Mallick, Debshila and Baraniuk, Richard and Chen, Zhongzhou},
title = {Atomic Learning Objectives and LLMs Labeling: A High-Resolution Approach for Physics Education},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706550},
doi = {10.1145/3706468.3706550},
abstract = {This paper introduces a novel approach to create a high-resolution “map" for physics learning: an "atomic" learning objectives (LOs) system designed to capture detailed cognitive processes and concepts required for problem solving in a college-level introductory physics course. Our method leverages Large Language Models (LLMs) for automated labeling of physics questions and introduces a comprehensive set of metrics to evaluate the quality of the labeling outcomes. The atomic LO system, covering nine chapters of an introductory physics course, uses a “subject-verb-object” structure to represent specific cognitive processes. We apply this system to 131 questions from expert-curated question banks and the OpenStax University Physics textbook. Each question is labeled with 1-8 atomic LOs across three chapters. Through extensive experiments using various prompting strategies and LLMs, we compare automated LOs labeling results against human expert labeling. Our analysis reveals both the strengths and limitations of LLMs, providing insight into LLMs reasoning processes for labeling LOs and identifying areas for improvement in LOs system design. Our work contributes to the field of learning analytics by proposing a more granular approach to mapping learning objectives with questions. Our findings have significant implications for the development of intelligent tutoring systems and personalized learning pathways in STEM education, paving the way for more effective “learning GPS” systems.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {620–630},
numpages = {11},
keywords = {Physics Education, Learning Objectives},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3636555.3636881,
author = {Ain, Qurat Ul and Chatti, Mohamed Amine and Meteng Kamdem, Paul Arthur and Alatrash, Rawaa and Joarder, Shoeb and Siepmann, Clara},
title = {Learner Modeling and Recommendation of Learning Resources using Personal Knowledge Graphs},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636881},
doi = {10.1145/3636555.3636881},
abstract = {Educational recommender systems (ERS) are playing a pivotal role in providing recommendations of personalized resources and activities to students, tailored to their individual learning needs. A fundamental part of generating recommendations is the learner modeling process that identifies students’ knowledge state. Current ERSs, however, have limitations mainly related to the lack of transparency and scrutability of the learner models as well as capturing the semantics of learner models and learning materials. To address these limitations, in this paper we empower students to control the construction of their personal knowledge graphs (PKGs) based on the knowledge concepts that they actively mark as ’did not understand (DNU)’ while interacting with learning materials. We then use these PKGs to build semantically-enriched learner models and provide personalized recommendations of external learning resources. We conducted offline experiments and an online user study (N=31), demonstrating the benefits of a PKG-based recommendation approach compared to a traditional content-based one, in terms of several important user-centric aspects including perceived accuracy, novelty, diversity, usefulness, user satisfaction, and use intentions. In particular, our results indicate that the degree of control students are able to exert over the learner modeling process, has positive consequences on their satisfaction with the ERS and their intention to accept its recommendations.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {273–283},
numpages = {11},
keywords = {Educational Recommender System, Learner Modeling, Learning Analytics, MOOC, Open Learner Model, Personal Knowledge Graph, Sentence Encoder},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3576050.3576086,
author = {Russell, Jae-Eun and Smith, Anna Marie and George, Salim and Damman, Bryce},
title = {Instructional Strategies and Student eTextbook Reading},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576086},
doi = {10.1145/3576050.3576086},
abstract = {Students’ reading is an essential part of learning in college courses. However, many instructors are concerned that students do not complete assigned readings, and multiple studies have found evidence to support this concern. A handful of studies suggest adopting strategies to address students’ lack of reading. This research examines various instructional strategies and student eTextbook reading behaviors validated by page view data. Survey responses related to use of instructional strategies were collected. A total of 86 instructors from four public universities participated. Of these participants, 59 submitted the assigned reading pages for their courses. This resulted in reading data from 3,714 students which were examined in this study. The findings indicated that students read about 37% of the assigned pages on any given day during the semester. Also, of the students that read, two-thirds made at least one annotation and students tend to re-read the pages they annotated. Most importantly, student reading in the courses where strategies were used was almost three times higher than in the courses where no strategies were implemented.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {613–618},
numpages = {6},
keywords = {eTextbooks, education, learning analytics, reading},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3706468.3706504,
author = {Alghamdi, Saleh Ramadhan and Rakovi\'{c}, Mladen and Yang, Kaixun and Fan, Yizhou and Ga\v{s}evi\'{c}, Dragan and Chen, Guanliang},
title = {Analytics of Temporal Patterns of Self-regulated Learners: A Time Series Approach},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706504},
doi = {10.1145/3706468.3706504},
abstract = {Temporal patterns play a significant role in understanding dynamic changes in Self-regulated Learning (SRL) engagement over time. Several previous studies have proposed approaches for automated detection of SRL strategies through analysis of temporal patterns. However, these approaches are mostly focused on the analysis of patterns in sequential ordering of SRL processes. This offers a useful yet limited temporal perspective to SRL. As noted in the literature, temporality of SRL has two dimensions – passage of time and ordering of events. To address this gap, this paper specifically proposes a time series approach that can automatically detect SRL strategies by accounting for both dimensions of temporality. Our approach also explores when specific processes occur and how learners engage metacognitively or cognitively with learning tasks. In particular, this study investigated SRL engagement as students composed essays using multiple sources within a 120-minute time frame. The results indicated that five distinct strategies with varying levels of engagement were detected. The correlation between these identified strategies and students’ scores was not statistically significant; however, further exploration revealed that students who adopted a specific strategy could outperform other groups based on obtained scores. We also noticed additional factors that had a positive effect on learners’ performance.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {283–292},
numpages = {10},
keywords = {Self-regulated Learning (SRL), Time Series, Clustering, Learning Strategies, Learning Analytics.},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/2330601.2330622,
author = {Cambridge, Darren and Perez-Lopez, Kathleen},
title = {First steps towards a social learning analytics for online communities of practice for educators},
year = {2012},
isbn = {9781450311113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2330601.2330622},
doi = {10.1145/2330601.2330622},
abstract = {Learning analytics has the potential to provide actionable insights for managers of online communities of practice. Because the purposes of such communities and the patterns of activity that might further them are diverse, a wider range of methods may be needed than in formal educational settings. This paper describes the proposed learning analytics approach of the U. S. Department of Education's Connected Educatorsproject, and presents preliminary applications of social network analysis to the National Science Teachers Association Learning Center as an illustration.},
booktitle = {Proceedings of the 2nd International Conference on Learning Analytics and Knowledge},
pages = {69–72},
numpages = {4},
keywords = {social network analysis, professional development, paradata, online communities of practice, learning analytics, education},
location = {Vancouver, British Columbia, Canada},
series = {LAK '12}
}

@inproceedings{10.1145/3012430.3012543,
author = {Fidalgo-Blanco, \'{A}ngel and Sein-Echaluce, Mar\'{\i}a Luisa and Esteban-Esca\~{n}o, Javier and Pe\~{n}alvo, Francisco J. Garc\'{\i}a and Conde, Miguel \'{A}ngel},
title = {Learning analytics to identify the influence of leadership on the academic performance of work teams},
year = {2016},
isbn = {9781450347471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3012430.3012543},
doi = {10.1145/3012430.3012543},
abstract = {In the academic context teamwork has a dual mission: to train students in teamwork competence and the active participation of students in their own learning. Authentic leadership of teams is the key to both goals. This paper presents a research which relates leadership, team grades (individual and group) and student-student interactions. The CTMTC teamwork method is used, as it allows continuous monitoring of teamwork and evaluates the work of the leader and the rest of the team members separately. The measurement tools, a survey for the individual opinion on the authentic leader actions, and a learning analytics system to analyze student-student interactions in forums, help to confirm the following hypothesis: that CTMTC encourages leadership role, that leadership skills are related with team grades and that learning analytics systems help predicting the behavior of teams with true leadership.},
booktitle = {Proceedings of the Fourth International Conference on Technological Ecosystems for Enhancing Multiculturality},
pages = {377–382},
numpages = {6},
keywords = {teamwork competence, learning analytics, leadership, CTMTC},
location = {Salamanca, Spain},
series = {TEEM '16}
}

@inproceedings{10.1145/2567574.2567631,
author = {Ferguson, Rebecca and De Liddo, Anna and Whitelock, Denise and de Laat, Maarten and Buckingham Shum, Simon},
title = {DCLA14: second international workshop on discourse-centric learning analytics},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567631},
doi = {10.1145/2567574.2567631},
abstract = {The first international workshop on discourse-centric learning analytics (DCLA) took place at LAK13 in Leuven, Belgium. That workshop succeeded in its aim of catalysing ideas and building community connections between those working in this field of social learning analytics. It also proposed a mission statement for DCLA: to devise and validate analytics that look beyond surface measures in order to quantify linguistic proxies for deeper learning. This year, the focus of the second international DCLA workshop, like that of LAK14, is on the intersection of learning analytics research, theory and practice. Once researchers have developed and validated discourse-centric analytics, how can these be successfully deployed at scale to support learning?},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {283–284},
numpages = {2},
keywords = {visualisation, social learning analytics, learning analytics, discourse, dialogue, deliberation, argumentation},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/3636555.3636879,
author = {Garg, Ryan and Han, Jaeyoung and Cheng, Yixin and Fang, Zheng and Swiecki, Zachari},
title = {Automated Discourse Analysis via Generative Artificial Intelligence},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636879},
doi = {10.1145/3636555.3636879},
abstract = {Coding discourse data is critical to many learning analytics studies. To code their data, researchers may use manual techniques, automated techniques, or a combination thereof. Manual coding can be time-consuming and error prone; automated coding can be difficult to implement for non-technical users. Generative artificial intelligence (GAI) offers a user friendly alternative to automated discourse coding via prompting and APIs. We assessed the ability of GAI, specifically the GPT class of models, at automatically coding discourse in the context of a learning analytics study using a variety of prompting and training strategies. We found that fine-tuning approaches produced the best results; however, no results achieved standard thresholds for reliability in our field.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {814–820},
numpages = {7},
keywords = {Automated Discourse Coding, Generative Artificial Intelligence},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3506860.3506863,
author = {Karumbaiah, Shamya and Baker, Ryan and Tao, Yan and Liu, Ziyang},
title = {How does Students’ Affect in Virtual Learning Relate to Their Outcomes? A Systematic Review Challenging the Positive-Negative Dichotomy},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506863},
doi = {10.1145/3506860.3506863},
abstract = {Several emotional theories that inform the design of Virtual Learning Environments (VLEs) categorize affect as either positive or negative. However, the relationship between affect and learning appears to be more complex than that. Despite several empirical investigations in the last fifteen years, including a few that have attempted to complexify the role of affect in students’ learning in VLE, there has not been an attempt to synthesize the evidence across them. To bridge this gap, we conducted a systematic review of empirical studies that examined the relationship between student outcomes and the affect that arises during their interaction with a VLE. Our synthesis of results across thirty-nine papers suggests that except engagement, all of the commonly studied affective states (confusion, frustration, and boredom) have mixed relationships with outcomes. We further explored the differences in student demographics and study context to explain the variation in the results. Some of our key findings include poorer learning outcomes arising for confusion in classrooms (versus lab studies), differences in brief versus prolonged confusion and resolved versus persistent confusion, more positive (versus null) results for engagement in learning games, and more significant results for rarer affective states like frustration with automated affect detectors (versus student self-reports). We conclude that more careful attention must be paid to contextual differences in affect's role in student learning. We discuss the implication of this review for VLE design and research.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {24–33},
numpages = {10},
keywords = {Virtual learning, Systematic review, Student outcomes, Student affect, Online tutor, Education, Affective computing},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3012430.3012536,
author = {Filv\`{a}, Daniel Amo and Alier, Marc and Casany, Mar\'{\i}a Jos\'{e} and Mayol, Enric},
title = {A learning analytics tool with hybrid graphical and textual interpretation generation},
year = {2016},
isbn = {9781450347471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3012430.3012536},
doi = {10.1145/3012430.3012536},
abstract = {The introduction and use of on-line learning resources has improved students learning process in several aspects. But at the same time, it also introduced more complexity and made more difficult to teachers how to analyze learning evolution and improvement of students. In this paper, we propose a first approach how to visualize and analyze student interaction with on-line learning systems and Virtual Learning Environments (VLE).We present a piece of software that collects information on the interaction of the students with the Moodle VLE and that it displays to teachers in a more analytical way. The interaction data is displayed to support teacher interpretation from a learning point of view, with the inclusion of automatically generated textual explanations about the analysis of such data.},
booktitle = {Proceedings of the Fourth International Conference on Technological Ecosystems for Enhancing Multiculturality},
pages = {327–333},
numpages = {7},
keywords = {virtual learning environments, student interactions, moodle, learning management systems, learning analytics},
location = {Salamanca, Spain},
series = {TEEM '16}
}

@inproceedings{10.1145/2090116.2090131,
author = {Fournier, H\'{e}l\`{e}ne and Kop, Rita and Sitlia, Hanan},
title = {The value of learning analytics to networked learning on a personal learning environment},
year = {2011},
isbn = {9781450309448},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2090116.2090131},
doi = {10.1145/2090116.2090131},
abstract = {Some might argue that the analytics tools at our disposal are currently mainly used for boring purposes, such as improving processes and making money. In this paper we will try to define learning analytics and their purpose for learning and education. We will ponder on the best possible fit of particular types of research methods and their analysis. Methodological concerns related to the analysis of Big Data collected on online networks as well as ethical and privacy concerns will also be highlighted and a case study of the use of learning analytics in a Massive Open Online Course explored.},
booktitle = {Proceedings of the 1st International Conference on Learning Analytics and Knowledge},
pages = {104–109},
numpages = {6},
keywords = {massive open online courses, learning analytics, educational research, big data, analytics tools},
location = {Banff, Alberta, Canada},
series = {LAK '11}
}

@inproceedings{10.1145/2818346.2820737,
author = {Worsley, Marcelo and Scherer, Stefan and Morency, Louis-Philippe and Blikstein, Paulo},
title = {Exploring Behavior Representation for Learning Analytics},
year = {2015},
isbn = {9781450339124},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2818346.2820737},
doi = {10.1145/2818346.2820737},
abstract = {Multimodal analysis has long been an integral part of studying learning. Historically multimodal analyses of learning have been extremely laborious and time intensive. However, researchers have recently been exploring ways to use multimodal computational analysis in the service of studying how people learn in complex learning environments. In an effort to advance this research agenda, we present a comparative analysis of four different data segmentation techniques. In particular, we propose affect- and pose-based data segmentation, as alternatives to human-based segmentation, and fixed-window segmentation. In a study of ten dyads working on an open-ended engineering design task, we find that affect- and pose-based segmentation are more effective, than traditional approaches, for drawing correlations between learning-relevant constructs, and multimodal behaviors. We also find that pose-based segmentation outperforms the two more traditional segmentation strategies for predicting student success on the hands-on task. In this paper we discuss the algorithms used, our results, and the implications that this work may have in non-education-related contexts.},
booktitle = {Proceedings of the 2015 ACM on International Conference on Multimodal Interaction},
pages = {251–258},
numpages = {8},
keywords = {performance., human factors, experimentation, algorithms},
location = {Seattle, Washington, USA},
series = {ICMI '15}
}

@proceedings{10.1145/2460296,
title = {LAK '13: Proceedings of the Third International Conference on Learning Analytics and Knowledge},
year = {2013},
isbn = {9781450317856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the third edition of the Learning Analytics and Knowledge conference. This year, the medieval and, at the same time, modern city of Leuven, Belgium is the venue where researchers and practitioners of this exciting field come together to discuss current status and future trends. Similar to Leuven, Learning Analytics is an old and new field at the same time. Old, because it deals with a problem that exists since Plato's times: how to improve the way students learn. New, because the tools used to achieve this goal, like Big Data and natural language processing, were not feasible merely 10 years ago. Leuven is also the home of beautiful centuries old buildings filled with young, smart and active students. In Learning Analytics, we can also find established researchers in the fields of Educational Research and Technology-Enhanced Learning, collaborating with a large contingent of new and promising researchers that could be called Learning Data Scientists.},
location = {Leuven, Belgium}
}

@inproceedings{10.1145/2876034.2893380,
author = {Lewkow, Nicholas and Feild, Jacqueline and Zimmerman, Neil and Riedesel, Mark and Essa, Alfred and Boulanger, David and Seanosky, Jeremie and Kumar, Vive and Kinshuk and Kode, Sandhya},
title = {A Scalable Learning Analytics Platform for Automated Writing Feedback},
year = {2016},
isbn = {9781450337267},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2876034.2893380},
doi = {10.1145/2876034.2893380},
abstract = {In this paper, we describe a scalable learning analytics platform which runs generalized analytics models on educational data in parallel. As a proof of concept, we use this platform as a base for an end-to-end automated writing feedback system. The system allows students to view feedback on their writing in near real-time, edit their writing based on the feedback provided, and observe the progression of their performance over time. Providing students with detailed feedback is an important part of improving writing skills and an essential component towards solving Bloom's "two sigma" problem in education. We evaluate the effectiveness of the feedback for students with an ongoing pilot study with 800 students who are using the learning analytics platform in a college English course.},
booktitle = {Proceedings of the Third (2016) ACM Conference on Learning @ Scale},
pages = {109–112},
numpages = {4},
keywords = {scalable analytics, performance feedback, natural language processing, automatic essay feedback, analytic tools for learners},
location = {Edinburgh, Scotland, UK},
series = {L@S '16}
}

@inproceedings{10.1145/3706468.3706497,
author = {Fang, Zheng and Wang, Weiqing and Chen, Guanliang and Swiecki, Zachari},
title = {The Company You Keep: Refining Neural Epistemic Network Analysis},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706497},
doi = {10.1145/3706468.3706497},
abstract = {Collaborative problem-solving (CPS) is defined as an inherently sociocognitive phenomena. Despite this, extant learning analytic techniques tend to focus on either the social or cognitive aspects without explicitly considering their interaction. Prior work developed Neural Epistemic Network Analysis (NENA), which used a combination of deep learning methods to simultaneously model the social and cognitive aspects of CPS; however, the method had several limitations. The refined version of NENA presented here addresses these limitations by (a) introducing a simplified autoencoder deep learning architecture; (b) using a combination of social and epistemic networks as input to preserve interpretability in terms of social and cognitive factors; and (c) introducing an isometry loss function to ensure downstream statistical tests are meaningful. We found that the refined version of NENA is able to achieve high performance on criteria we would expect from a network analytic technique in the context of learning analytics: interpretability, goodness of fit, orthogonality and isometry; and discriminatory power. We also demonstrated that this method was comparable in performance to a more traditional learning analytic technique, Epistemic Network Analysis (ENA), while providing information that ENA did not. The results suggest that NENA could be a useful method for exploring the cognitive interactions of a given individual’s social network and thus the influences their network exerts upon them.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {216–226},
numpages = {11},
keywords = {Collaborative Problem-Solving, Epistemic Network Analysis, Social Network Analysis, Graph Neural Networks},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3014362.3014366,
author = {Kulkarni, Sanket and Rajamanickam, Venkatesh},
title = {Designing Data Collection Methods for Applying Learning Analytics in Resource Constrained Schools},
year = {2016},
isbn = {9781450348638},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3014362.3014366},
doi = {10.1145/3014362.3014366},
abstract = {The use ofLearning Analytics (LA) has shown significant improvements in online learning environments. This paper intends to discuss the challenges of applyingLA in the context of elementary schools of India.Out of many challenges, we focus on solving the unavailability of school data in digital format. Most of the data in such schools is stored on physical mediums like notebooks. These schools also lack the critical resources like the availability of computing devices, internet and digitally literate manpower.As the primary outcome of this paper, we propose a data collection system, named as OSLA, which provides multiple ways of data collection in resource constrained environment. OSLA distributes the need of resources across different touchpoints to tolerate the unavailability of specific computing device, internet and digitally literate manpower.},
booktitle = {Proceedings of the 8th Indian Conference on Human-Computer Interaction},
pages = {44–51},
numpages = {8},
keywords = {Elementary Schools, India, Learning Analytics, Learning Environment, Resource Constrained, Resources},
location = {Mumbai, India},
series = {IndiaHCI '16}
}

@inproceedings{10.1145/2090116.2090132,
author = {Blikstein, Paulo},
title = {Using learning analytics to assess students' behavior in open-ended programming tasks},
year = {2011},
isbn = {9781450309448},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2090116.2090132},
doi = {10.1145/2090116.2090132},
abstract = {There is great interest in assessing student learning in unscripted, open-ended environments, but students' work can evolve in ways that are too subtle or too complex to be detected by the human eye. In this paper, I describe an automated technique to assess, analyze and visualize students learning computer programming. I logged hundreds of snapshots of students' code during a programming assignment, and I employ different quantitative techniques to extract students' behaviors and categorize them in terms of programming experience. First I review the literature on educational data mining, learning analytics, computer vision applied to assessment, and emotion detection, discuss the relevance of the work, and describe one case study with a group undergraduate engineering students},
booktitle = {Proceedings of the 1st International Conference on Learning Analytics and Knowledge},
pages = {110–116},
numpages = {7},
keywords = {logging, learning analytics, educational data mining, constructionism, automated assessment},
location = {Banff, Alberta, Canada},
series = {LAK '11}
}

@inproceedings{10.1145/2669711.2669911,
author = {Garaizar, Pablo and Guenaga, Mariluz},
title = {A multimodal learning analytics view of HTML5 APIs: technical benefits and privacy risks},
year = {2014},
isbn = {9781450328968},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2669711.2669911},
doi = {10.1145/2669711.2669911},
abstract = {During the last decade, students and teachers have been using a wide variety of web platforms to enhance education. Traditional forums, blogs, wikis, Learning Management Systems or social networks coexist with an astonishing volume of mobile learning apps. All of these platforms provide insightful logs of users' interactions. This is especially true when educational researchers take advantage of smartphones to gather and analyze learning processes in a multimodal manner (not only users' clicks or touch events, but also audio, video, location, motion, temperature, humidity or luminosity, among others). This paper describes the potential advantages of using HTML5 APIs to enhance education using web apps in mobile environments from a Multimodal Learning Analytics perspective, and discusses the privacy risks of this new scenario.},
booktitle = {Proceedings of the Second International Conference on Technological Ecosystems for Enhancing Multiculturality},
pages = {275–281},
numpages = {7},
keywords = {privacy, multimodal, learning analytics, ethics, education, HTML5, API},
location = {Salamanca, Spain},
series = {TEEM '14}
}

@inproceedings{10.1145/3506860.3506885,
author = {Pozdniakov, Stanislav and Martinez-Maldonado, Roberto and Tsai, Yi-Shan and Cukurova, Mutlu and Bartindale, Tom and Chen, Peter and Marshall, Harrison and Richardson, Dan and Gasevic, Dragan},
title = {The Question-driven Dashboard: How Can We Design Analytics Interfaces Aligned to Teachers’ Inquiry?},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506885},
doi = {10.1145/3506860.3506885},
abstract = {One of the ultimate goals of several learning analytics (LA) initiatives is to close the loop and support students’ and teachers’ reflective practices. Although there has been a proliferation of end-user interfaces (often in the form of dashboards), various limitations have already been identified in the literature such as key stakeholders not being involved in their design, little or no account for sense-making needs, and unclear effects on teaching and learning. There has been a recent call for human-centred design practices to create LA interfaces in close collaboration with educational stakeholders to consider the learning design, and their authentic needs and pedagogical intentions. This paper addresses the call by proposing a question-driven LA design approach to ensure that end-user LA interfaces explicitly address teachers’ questions. We illustrate the approach in the context of synchronous online activities, orchestrated by pairs of teachers using audio-visual and text-based tools (namely Zoom and Google Docs). This study led to the design and deployment of an open-source monitoring tool to be used in real-time by teachers when students work collaboratively in breakout rooms, and across learning spaces.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {175–185},
numpages = {11},
keywords = {online learning, learning analytics, inquiry-driven practice, human-centred design, dashboard, CSCL},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/2723576.2723652,
author = {Hawn, Aaron},
title = {The bridge report: bringing learning analytics to low-income, urban schools},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723652},
doi = {10.1145/2723576.2723652},
abstract = {Widespread adoption of learning analytics for risk prediction faces different challenges at low-income secondary schools than at post-secondary institutions, where such methods have been more widely adopted. To leverage the benefits of learning analytics for under-resourced communities, educators must overcome the barriers to adoption faced by local schools: internet access, data integration, data interpretation, and local alignment. We present the case study of an enhanced reporting tool for parents and teachers, the Bridge Report, locally designed to meet the needs of a low-income secondary school in New York City. Parent and Teacher focus groups suggest that addressing local obstacles to learning analytics can create conditions for enthusiastic adoption by parents and teachers.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {410–411},
numpages = {2},
keywords = {risk prediction, predictive analytics, learning analytics, instructor support},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2460296.2460321,
author = {Vatrapu, Ravi and Reimann, Peter and Bull, Susan and Johnson, Matthew},
title = {An eye-tracking study of notational, informational, and emotional aspects of learning analytics representations},
year = {2013},
isbn = {9781450317856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2460296.2460321},
doi = {10.1145/2460296.2460321},
abstract = {This paper presents an eye-tracking study of notational, informational, and emotional aspects of nine different notational systems (Skill Meters, Smilies, Traffic Lights, Topic Boxes, Collective Histograms, Word Clouds, Textual Descriptors, Table, and Matrix) and three different information states (Weak, Average, &amp; Strong) used to represent student's learning. Findings from the eye-tracking study show that higher emotional activation was observed for the metaphorical notations of traffic lights and smilies and collective representations. Mean view time was higher for representations of the "average" informational learning state. Qualitative data analysis of the think-aloud comments and post-study interview show that student participants reflected on the meaning-making opportunities and action-taking possibilities afforded by the representations. Implications for the design and evaluation of learning analytics representations and discourse environments are discussed.},
booktitle = {Proceedings of the Third International Conference on Learning Analytics and Knowledge},
pages = {125–134},
numpages = {10},
keywords = {teaching analytics, open learner models representational guidance, learning analytics, computer supported collaborative learning (CSCL), affordances},
location = {Leuven, Belgium},
series = {LAK '13}
}

@inproceedings{10.1145/3636555.3636857,
author = {Echeverria, Vanessa and Yan, Lixiang and Zhao, Linxuan and Abel, Sophie and Alfredo, Riordan and Dix, Samantha and Jaggard, Hollie and Wotherspoon, Rosie and Osborne, Abra and Buckingham Shum, Simon and Gasevic, Dragan and Martinez-Maldonado, Roberto},
title = {TeamSlides: a Multimodal Teamwork Analytics Dashboard for Teacher-guided Reflection in a Physical Learning Space},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636857},
doi = {10.1145/3636555.3636857},
abstract = {Advancements in Multimodal Learning Analytics (MMLA) have the potential to enhance the development of effective teamwork skills and foster reflection on collaboration dynamics in physical learning environments. Yet, only a few MMLA studies have closed the learning analytics loop by making MMLA solutions immediately accessible to educators to support reflective practices, especially in authentic settings. Moreover, deploying MMLA solutions in authentic settings can bring new challenges beyond logistic and privacy issues. This paper reports the design and use of TeamSlides, a multimodal teamwork analytics dashboard to support teacher-guided reflection. We conducted an in-the-wild classroom study involving 11 teachers and 138 students. Multimodal data were collected from students working in team healthcare simulations. We examined how teachers used the dashboard in 22 debrief sessions to aid their reflective practices. We also interviewed teachers to discuss their perceptions of the dashboard’s value and the challenges faced during its use. Our results suggest that the dashboard effectively reinforced discussions and augmented teacher-guided reflection practices. However, teachers encountered interpretation conflicts, sometimes leading to mistrust or misrepresenting the information. We discuss the considerations needed to overcome these challenges in MMLA research.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {112–122},
numpages = {11},
keywords = {MMLA, dashboards, reflection, team dynamics, teamwork analytics, visualisation},
location = {Kyoto, Japan},
series = {LAK '24}
}

@article{10.1145/3105910,
author = {Grover, Shuchi and Basu, Satabdi and Bienkowski, Marie and Eagle, Michael and Diana, Nicholas and Stamper, John},
title = {A Framework for Using Hypothesis-Driven Approaches to Support Data-Driven Learning Analytics in Measuring Computational Thinking in Block-Based Programming Environments},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
url = {https://doi.org/10.1145/3105910},
doi = {10.1145/3105910},
abstract = {Systematic endeavors to take computer science (CS) and computational thinking (CT) to scale in middle and high school classrooms are underway with curricula that emphasize the enactment of authentic CT skills, especially in the context of programming in block-based programming environments. There is, therefore, a growing need to measure students’ learning of CT in the context of programming and also support all learners through this process of learning computational problem solving. The goal of this research is to explore hypothesis-driven approaches that can be combined with data-driven ones to better interpret student actions and processes in log data captured from block-based programming environments with the goal of measuring and assessing students’ CT skills. Informed by past literature and based on our empirical work examining a dataset from the use of the Fairy Assessment in the Alice programming environment in middle schools, we present a framework that formalizes a process where a hypothesis-driven approach informed by Evidence-Centered Design effectively complements data-driven learning analytics in interpreting students’ programming process and assessing CT in block-based programming environments. We apply the framework to the design of Alice tasks for high school CS to be used for measuring CT during programming.},
journal = {ACM Trans. Comput. Educ.},
month = aug,
articleno = {14},
numpages = {25},
keywords = {hypothesis-driven, evidence-centered design, data-driven, computational psychometrics, block-based programming environments, K-12 computer science education, Blended learning analytics}
}

@inproceedings{10.1145/2567574.2567626,
author = {Hickey, Daniel T. and Kelley, Tara Alana and Shen, Xinyi},
title = {Small to big before massive: scaling up participatory learning analytics},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567626},
doi = {10.1145/2567574.2567626},
abstract = {This case study describes how course features and individual &amp; social learning analytics were scaled up to support "participatory" learning. An existing online course was turned into a "big open online course" (BOOC) offered to hundreds. Compared to typical open courses, relatively high levels of persistence, individual &amp; social engagement, and achievement were obtained. These results suggest that innovative learning analytics might best be scaled (a) incrementally, (b) using design-based research methods, (c) focusing on engagement in consequential &amp; contextual knowledge, (d) using emerging situative assessment theories.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {93–97},
numpages = {5},
keywords = {social learning analysis, personalized learning, learning analytics, assessment, analytic approaches},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/3636555.3636938,
author = {Kitto, Kirsty and Gibson, Andrew},
title = {Places to intervene in complex learning systems},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636938},
doi = {10.1145/3636555.3636938},
abstract = {Responding to recent questioning of Learning Analytics (LA) as a field that is achieving its aim of understanding and optimising learning and the environments in which it occurs, this paper argues that there is a need to genuinely embrace the complexity of learning when considering the impact of LA. Rather than focusing upon ‘optimisation’, we propose that LA should seek to understand and improve the complex socio-technical system in which it operates. We adopt a framework from systems theory to propose 12 different intervention points for learning systems, and apply it to two case studies. We conclude with an invitation to the community to critique and extend this proposed framework.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {929–935},
numpages = {7},
keywords = {Complex Systems, Intervention, Learning, Theory},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/2723576.2723639,
author = {Hickey, Daniel and Jovanovic, Jelena and Lonn, Steve and Willis, James E.},
title = {2nd int'l workshop on open badges in education (OBIE 2015): from learning evidence to learning analytics},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723639},
doi = {10.1145/2723576.2723639},
abstract = {Open digital badges are Web-enabled tokens of learning and accomplishment. Unlike traditional grades, certificates, and transcripts, badges include specific claims about learning accomplishments and detailed evidence in support of those claims. Considering the richness of data associated with Open Badges, it is reasonable to expect a very powerful predictive element at the intersection of Open Badges and Learning Analytics. This could have substantial implications for recommending and exposing students to a variety of curricular and co-curricular pathways utilizing data sources far more nuanced than grades and achievement tests. Therefore, this workshop was aimed at: i) examining the potentials of Open Badges (including the associated data and resources) to provide new and potentially unprecedented data for analysis; ii) examining the kinds of Learning Analytics methods and techniques that could be suitable for gaining valuable insights from and/or making predictions based on the evidence (data and resources) associated with badges, and iii) connecting Open Badges communities, aiming to allow for the exchange of experiences and learning from different cultures and communities.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {392–393},
numpages = {2},
keywords = {open badges, online learning, learning analytics, education},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/3706468.3706472,
author = {Jaiyeola, Grace D. and Wong, Aaron Y. and Bryck, Richard L. and Mills, Caitlin and Hutt, Stephen},
title = {One Size Does Not Fit All: Considerations when using Webcam-Based Eye Tracking to Models of Neurodivergent Learners’ Attention and Comprehension},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706472},
doi = {10.1145/3706468.3706472},
abstract = {This study investigates the use of webcam-based eye tracking to model attention and comprehension in both neurotypical and neurodivergent learners. Leveraging the WebGazer, a previously used online data collection tool, we collected gaze and interaction data (N=354) during online reading tasks to explore task unrelated thought (TUT) and comprehension in an ecologically valid setting. Our findings challenge the "one size fits all" approach to learner modeling by demonstrating distinct differences in indicators of both constructs between neurotypical and neurodivergent learners. We compared general models trained on the entire population with tailored models specific to neurodivergent and neurotypical groups. Results indicate that diagnosis-specific models provide more accurate predictions (AUROC's .59-.70 vs. .57 for the general model), and through SHAPley analysis, we note that the strongest indicators of each construct vary as the training population is refined, highlighting the limitations of generalized approaches. This work supports the scalability of webcam-based cognitive modeling and underscores the potential for personalized learning analytics and modeling to better support diverse learning needs.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {24–35},
numpages = {12},
keywords = {Comprehension, Eye tracking, Learner modeling, Neurodivergent learners, Task Unrelated Thought (TUT)},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/2723576.2723596,
author = {Milligan, Sandra},
title = {Crowd-sourced learning in MOOCs: learning analytics meets measurement theory},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723596},
doi = {10.1145/2723576.2723596},
abstract = {This paper illustrated the promise of the combination of measurement theory and learning analytics for understanding effective MOOC learning. It reports findings from a study of whether and how MOOC log file data can assist in understanding how MOOC participants use (often) messy, chaotic forums to support complex, unpredictable, contingent learning processes. It is argued that descriptions of posting, voting and viewing behaviours do not in and of themselves provide insights about how learning is generated in MOOC forums. Rather, it is hypothesised that there is a skill involved in using forums to learn; that theory-informed descriptions of this skill illustrate how MOOC participants use forums differently as they progress from novice to expert; that the skill progression can be validated through the use of forum log file data; and that log file data can also be used to assess an individual MOOC participant's position in relation to this progression -- that is, to measure an individual's skill in learning through forums and similar educational settings. These hypotheses were examined using data drawn from forums in a large MOOC run at the University of Melbourne in 2013.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {151–155},
numpages = {5},
keywords = {rasch analysis, on-line forums, measurement theory, learning progression, learning analytics, learner performance, crowd-sourced learning, collaborative learning, analytics tools, MOOC, 21st century skills},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/3576050.3576117,
author = {Davalos, Eduardo and Vatral, Caleb and Cohn, Clayton and Horn Fonteles, Joyce and Biswas, Gautam and Mohammed, Naveeduddin and Lee, Madison and Levin, Daniel},
title = {Identifying Gaze Behavior Evolution via Temporal Fully-Weighted Scanpath Graphs},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576117},
doi = {10.1145/3576050.3576117},
abstract = {Eye-tracking technology has expanded our ability to quantitatively measure human perception. This rich data source has been widely used to characterize human behavior and cognition. However, eye-tracking analysis has been limited in its applicability, as contextualizing gaze to environmental artifacts is non-trivial. Moreover, the temporal evolution of gaze behavior through open-ended environments where learners are alternating between tasks often remains unclear. In this paper, we propose temporal fully-weighted scanpath graphs as a novel representation of gaze behavior and combine it with a clustering scheme to obtain high-level gaze summaries that can be mapped to cognitive tasks via network metrics and cluster mean graphs. In a case study with nurse simulation-based team training, our approach was able to explain changes in gaze behavior with respect to key events during the simulation. By identifying cognitive tasks via gaze behavior, learners’ strategies can be evaluated to create online performance metrics and personalized feedback.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {476–487},
numpages = {12},
keywords = {eye-tracking, learning analytics, network analysis, simulation-based training, temporal},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@proceedings{10.1145/2389268,
title = {MLA '12: Proceedings of the 1st International Workshop on Multimodal Learning Analytics},
year = {2012},
isbn = {9781450315159},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Multimodal learning analytics, learning analytics, and educational data mining all are emerging disciplines concerned with developing techniques to more deeply explore unique data in educational settings. They use the results based on these analyses to understand how students learn. Among other things, this includes how they communicate, collaborate, and use digital and non-digital tools during learning activities, and what the impact of these tools is on acquiring new skills and knowledge. Past research on learning analytics has aimed to model students' knowledge and motivation during learning, to contribute to understanding learning dynamics during computer-supported and collaborative exchange, to diagnose students' failure to progress during learning activities, and to adapt learning activities to students' limitations and learning status. More generally, advances in learning analytics are expected to contribute new empirical findings, theories, methods, and metrics for understanding how students learn. They also can contribute to improving pedagogical support for students' learning through assessment of new digital tools, teaching strategies, and curricula.The most recent direction within this area is multimodal learning analytics, which emphasizes the analysis of natural rich modalities of communication during situated learning activities. This includes students' speech, writing, and nonverbal interaction (e.g., gestures, facial expressions, gaze). A primary objective of multimodal learning analytics is to analyze coherent signal and activity patterns in order to uncover entirely new learning-oriented phenomena. Another is to develop a better collection of converging metrics for learning-related behavior and landmarks. These include metrics that eventually could be analyzed unobtrusively, continuously, automatically, and in natural classroom environments and mobile settings.Progress in these areas will transform our ability to identify and stimulate effective learning, support more rapid feedback and responsive intervention, and facilitate learning in more diverse students and contexts.},
location = {Santa Monica, California}
}

@inproceedings{10.1145/2567574.2567621,
author = {Arnold, Kimberly E. and Lonn, Steven and Pistilli, Matthew D.},
title = {An exercise in institutional reflection: the learning analytics readiness instrument (LARI)},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567621},
doi = {10.1145/2567574.2567621},
abstract = {While the landscape of learning analytics is relatively well defined, the extent to which institutions are ready to embark on an analytics implementation is less known. Further, while work has been done on measuring the maturity of an institution's implementation, this work fails to investigate how an institution that has not implemented analytics to date might become mature over time. To that end, the authors developed and piloted a survey, the Learning Analytics Readiness Instrument (LARI), in an attempt to help institutions successfully prepare themselves for a successfully analytics implementation. The LARI is comprised of 90 items encompassing five factors related to a learning analytics implementation: (1) Ability, (2) Data, (3) Culture and Process, (4) Governance and Infrastructure, and, (5) Overall Readiness Perception. Each of the five factors has a high internal consistency, as does the overall tool. This paper discusses the need for a survey such as the LARI, the tool's psychometric properties, the authors' broad interpretations of the findings, and next steps for the LARI and the research in this field.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {163–167},
numpages = {5},
keywords = {survey design, readiness, learning analytics, higher education},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/3636555.3636873,
author = {Cohausz, Lea and Kappenberger, Jakob and Stuckenschmidt, Heiner},
title = {What Fairness Metrics Can Really Tell You: A Case Study in the Educational Domain},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636873},
doi = {10.1145/3636555.3636873},
abstract = {Recently, discussions on fairness and algorithmic bias have gained prominence in the learning analytics and educational data mining communities. To quantify algorithmic bias, researchers and practitioners often use popular fairness metrics, e.g., demographic parity, without discussing their choices. This can be considered problematic, as the choices should strongly depend on the underlying data generation mechanism, the potential application, and normative beliefs. Likewise, whether and how one should deal with the indicated bias depends on these aspects. This paper presents and discusses several theoretical cases to highlight precisely this. By providing a set of examples, we hope to facilitate a practice where researchers discuss potential fairness concerns by default.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {792–799},
numpages = {8},
keywords = {causal models, education, fairness},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/2567574.2567607,
author = {Harfield, Timothy D.},
title = {Teaching the unteachable: on the compatibility of learning analytics and humane education},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567607},
doi = {10.1145/2567574.2567607},
abstract = {This paper is an exploratory effort to find a place for learning analytics in humane education. After distinguishing humane education from training on the basis of the Aristotelian model of intellectual capabilities, and arguing that humane education is distinct by virtue of its interest in cultivating prudence, which is unteachable, an account of three key characteristics of humane education is provided. Appealing to thinkers of the Italian Renaissance, it is argued that ingenium, eloquence, and self-knowledge constitute the what, how, and why of humane education. Lastly, looking to several examples from recent learning analytics literature, it is demonstrated that learning analytics is not only helpful as set of aids for ensuring success in scientific and technical disciplines, but in the humanities as well. In order to function effectively as an aid to humane education, however, learning analytics must be embedded within a context that encourages continuous reflection, responsiveness, and personal responsibility for learning.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {241–245},
numpages = {5},
keywords = {renaissance, pedagogy, learning analytics, humanities, humanism, aristotle},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/2883851.2883882,
author = {Bakharia, Aneesha and Kitto, Kirsty and Pardo, Abelardo and Ga\v{s}evi\'{c}, Dragan and Dawson, Shane},
title = {Recipe for success: lessons learnt from using xAPI within the connected learning analytics toolkit},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883882},
doi = {10.1145/2883851.2883882},
abstract = {An ongoing challenge for Learning Analytics research has been the scalable derivation of user interaction data from multiple technologies. The complexities associated with this challenge are increasing as educators embrace an ever growing number of social and content-related technologies. The Experience API (xAPI) alongside the development of user specific record stores has been touted as a means to address this challenge, but a number of subtle considerations must be made when using xAPI in Learning Analytics. This paper provides a general overview to the complexities and challenges of using xAPI in a general systemic analytics solution - called the Connected Learning Analytics (CLA) toolkit. The importance of design is emphasised, as is the notion of common vocabularies and xAPI Recipes. Early decisions about vocabularies and structural relationships between statements can serve to either facilitate or handicap later analytics solutions. The CLA toolkit case study provides us with a way of examining both the strengths and the weaknesses of the current xAPI specification, and we conclude with a proposal for how xAPI might be improved by using JSON-LD to formalise Recipes in a machine readable form.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {378–382},
numpages = {5},
keywords = {xAPI, learning record store, learning analytics, architecture, CLRecipe, CLA toolkit},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2522848.2534669,
author = {Morency, Louis-Philippe and Oviatt, Sharon and Scherer, Stefan and Weibel, Nadir and Worsley, Marcelo},
title = {ICMI 2013 grand challenge workshop on multimodal learning analytics},
year = {2013},
isbn = {9781450321297},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2522848.2534669},
doi = {10.1145/2522848.2534669},
abstract = {Advances in learning analytics are contributing new empirical findings, theories, methods, and metrics for understanding how students learn. It also contributes to improving pedagogical support for students' learning through assessment of new digital tools, teaching strategies, and curricula. Multimodal learning analytics (MMLA)[1] is an extension of learning analytics and emphasizes the analysis of natural rich modalities of communication across a variety of learning contexts. This MMLA Grand Challenge combines expertise from the learning sciences and machine learning in order to highlight the rich opportunities that exist at the intersection of these disciplines. As part of the Grand Challenge, researchers were asked to predict: (1) which student in a group was the dominant domain expert, and (2) which problems that the group worked on would be solved correctly or not. Analyses were based on a combination of speech, digital pen and video data. This paper describes the motivation for the grand challenge, the publicly available data resources and results reported by the challenge participants. The results demonstrate that multimodal prediction of the challenge goals: (1) is surprisingly reliable using rich multimodal data sources, (2) can be accomplished using any of the three modalities explored, and (3) need not be based on content analysis.},
booktitle = {Proceedings of the 15th ACM on International Conference on Multimodal Interaction},
pages = {373–378},
numpages = {6},
keywords = {predictive data and models, multimodal learning analytics, empirical and machine learning techniques, domain expertise, collaboration},
location = {Sydney, Australia},
series = {ICMI '13}
}

@proceedings{10.1145/2666633,
title = {MLA '14: Proceedings of the 2014 ACM workshop on Multimodal Learning Analytics Workshop and Grand Challenge},
year = {2014},
isbn = {9781450304887},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Learning Analytics is the "middle-space" where Educational Sciences, Computer Science, Learning Technologies and Data Science converge. The main goal of this new field of knowledge is to contribute to new empirical findings, theories, methods, and metrics for understanding how students learn and to use that knowledge to improve those students' learning. Multimodal Learning Analytics, which emphasizes the analysis of natural rich modalities of communication during situated learning activities, is one of the most challenging but, at time, more promising areas of Learning Analytics. The Third International Workshop on Multimodal Learning Analytics brings together researchers in multimodal interaction and systems, cognitive and learning sciences, educational technologies, and related areas to discuss the recent developments and future opportunities in this sub-field.Following the First International Workshop on Multimodal Learning Analytics in Santa Monica in 2012 and the ICMI Grand Challenge on Multimodal Learning Analytics in Sydney in 2013, this third workshop comprises a mixture of a workshop session and two data-driven grand challenges. The program committee reviewed and accepted the following articles.The workshop session focuses on the presentation of multimodal signal analysis techniques that could be applied in Multimodal Learning Analytics. In this workshop challenges presenters concentrate on the benefits and shortcomings of different research and technical methods used for multimodal analysis of learning signals. This session includes four articles from diverse topics: theoretical and conceptual considerations for different forms of multimodal data fusion; voice analysis to determine the level of rapport in learning exercises; video analysis of live classrooms; and the role of multimodal analysis in the service of studying complex learning environments.Following the successful experience of the previous Multimodal Learning Analytics Grand Challenge in ICMI 2013, this year, this event will provide two data sets with a wealth of research questions to be tackled by interested participants: Math Data Challenge and Presentation Quality Challenge. For the Math Data Challenge, one article presented in this session provides a detailed exploration of how to use the digital pen information to predict the expertise in the group. This work reaches high levels of accuracy (83%) when identifying the expert student among the participants. For the Presentation Quality Challenge three articles are presented. The first one explores the slide presentation files and the audio features to predict the grade obtained by each student. The second work makes use of all the provided modalities (audio, video, Kinect data and slide files) and suggests that multimodal cues can predict human scores on presentation tasks. The final article uses the video and Kinect information to predict human grading.The third Multimodal Learning Analytics Workshop and Grand Challenges (MLA'14) was envisioned as a venue to initiate research in this nascent subfield of Learning Analytics. New challenges and insights will arise from the convergence of practitioners, academics and researchers, which in turn will create opportunities to collaborate and to create applications and tools to assist students, teachers and the community.},
location = {Istanbul, Turkey}
}

@inproceedings{10.1145/2460296.2460327,
author = {d'Aquin, Mathieu and Jay, Nicolas},
title = {Interpreting data mining results with linked data for learning analytics: motivation, case study and directions},
year = {2013},
isbn = {9781450317856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2460296.2460327},
doi = {10.1145/2460296.2460327},
abstract = {Learning Analytics by nature relies on computational information processing activities intended to extract from raw data some interesting aspects that can be used to obtain insights into the behaviours of learners, the design of learning experiences, etc. There is a large variety of computational techniques that can be employed, all with interesting properties, but it is the interpretation of their results that really forms the core of the analytics process. In this paper, we look at a specific data mining method, namely sequential pattern extraction, and we demonstrate an approach that exploits available linked open data for this interpretation task. Indeed, we show through a case study relying on data about students' enrolment in course modules how linked data can be used to provide a variety of additional dimensions through which the results of the data mining method can be explored, providing, at interpretation time, new input into the analytics process.},
booktitle = {Proceedings of the Third International Conference on Learning Analytics and Knowledge},
pages = {155–164},
numpages = {10},
keywords = {sequence mining, linked data, leaning analytics, interpretation, data mining, course enrolment},
location = {Leuven, Belgium},
series = {LAK '13}
}

@inproceedings{10.1145/2883851.2883893,
author = {Drachsler, Hendrik and Greller, Wolfgang},
title = {Privacy and analytics: it's a DELICATE issue a checklist for trusted learning analytics},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883893},
doi = {10.1145/2883851.2883893},
abstract = {The widespread adoption of Learning Analytics (LA) and Educational Data Mining (EDM) has somewhat stagnated recently, and in some prominent cases even been reversed following concerns by governments, stakeholders and civil rights groups about privacy and ethics applied to the handling of personal data. In this ongoing discussion, fears and realities are often indistinguishably mixed up, leading to an atmosphere of uncertainty among potential beneficiaries of Learning Analytics, as well as hesitations among institutional managers who aim to innovate their institution's learning support by implementing data and analytics with a view on improving student success. In this paper, we try to get to the heart of the matter, by analysing the most common views and the propositions made by the LA community to solve them. We conclude the paper with an eight-point checklist named DELICATE that can be applied by researchers, policy makers and institutional managers to facilitate a trusted implementation of Learning Analytics.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {89–98},
numpages = {10},
keywords = {trust, privacy, legal aspects, learning analytics, implementation, ethics, educational data mining, data management},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/3706468.3706561,
author = {Tang, Joe and Gibson, Andrew and Bruza, Peter},
title = {Analysis of exploratory behaviour: A step towards modelling of curiosity},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706561},
doi = {10.1145/3706468.3706561},
abstract = {In this research, we analysed exploratory behaviour trace data for students engaging in learning tasks in a technology-enhanced data analytics course as the first step towards modelling curiosity in learning. Curiosity is a complex phenomenon that is not amenable to direct modelling, but it can be understood through related behaviours like exploration, which is critical to effective learning. We analysed trace data from 40 students using visualisation and network analysis techniques, focusing on their interactions with learning tasks within the JupyterLab environment. Our analysis found that providing sufficient exploration time before explicit instruction or answer revelation, and designing learning tasks that embrace errors as opportunities, encouraged behaviours associated with curiosity-driven learning. These findings highlight the importance of designing learning environments that foster curiosity and promote active exploration.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {690–701},
numpages = {12},
keywords = {exploratory behaviour, curiosity-driven learning, task-centric approach, trace data},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/2388676.2388803,
author = {Scherer, Stefan and Worsley, Marcelo and Morency, Louis-Philippe},
title = {1st international workshop on multimodal learning analytics: extended abstract},
year = {2012},
isbn = {9781450314671},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2388676.2388803},
doi = {10.1145/2388676.2388803},
abstract = {This summary describes the 1st International Workshop on Multimodal Learning Analytics. This area of study brings together the technologies of multimodal analysis with the learning sciences. The intersection of these domains should enable researchers to foster an improved understanding of student learning, lead to the creation of more natural and enriching learning interfaces, and motivate the development of novel techniques for tackling challenges that are specific of education.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimodal Interaction},
pages = {609–610},
numpages = {2},
keywords = {statistical analysis, multimodal learning analytics, machine learning},
location = {Santa Monica, California, USA},
series = {ICMI '12}
}

@inproceedings{10.1145/3636555.3636890,
author = {Zambrano, Andres Felipe and Zhang, Jiayi and Baker, Ryan S.},
title = {Investigating Algorithmic Bias on Bayesian Knowledge Tracing and Carelessness Detectors},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636890},
doi = {10.1145/3636555.3636890},
abstract = {In today's data-driven educational technologies, algorithms have a pivotal impact on student experiences and outcomes. Therefore, it is critical to take steps to minimize biases, to avoid perpetuating or exacerbating inequalities. In this paper, we investigate the degree to which algorithmic biases are present in two learning analytics models: knowledge estimates based on Bayesian Knowledge Tracing (BKT) and carelessness detectors. Using data from a learning platform used across the United States at scale, we explore algorithmic bias following three different approaches: 1) analyzing the performance of the models on every demographic group in the sample, 2) comparing performance across intersectional groups of these demographics, and 3) investigating whether the models trained using specific groups can be transferred to demographics that were not observed during the training process. Our experimental results show that the performance of these models is close to equal across all the demographic and intersectional groups. These findings establish the feasibility of validating educational algorithms for intersectional groups and indicate that these algorithms can be fairly used for diverse students at scale.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {349–359},
numpages = {11},
location = {Kyoto, Japan},
series = {LAK '24}
}

@proceedings{10.1145/2330601,
title = {LAK '12: Proceedings of the 2nd International Conference on Learning Analytics and Knowledge},
year = {2012},
isbn = {9781450311113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {We welcome you to the 2012 Learning Analytics and Knowledge conference, being held in the beautiful city of Vancouver, Canada. Before you explore the city and the University of British Columbia, please join us in acknowledging that Vancouver and UBC are located on the traditional, ancestral, and unceded territory of the Canadian First Nations Musqueam people, and to thank the Musqueam people for their hospitality. Vancouver is a city rich in cultures, including people of First Nations, Asian, British, and many other origins and areas from the original Gastown to Granville Island, and from the high rise towers of downtown to the beaches of Kitsilano. Residents are an active group, with trails facilitating bicycling in the city, mountains close at hand for skiing and hiking, and the sea for boating of all kinds. At very close hand to the conference is the famed Stanley Park with its 22 kilometer (13.7 miles) seawall for walking, jogging or bicycling. For the more hardy, at little further off is Grouse Grind where you can test yourself against the mountains with 2,830 steps, and an 853 meter (2,800 feet) vertical ascent to a magnificent view of the Vancouver area. Or, stroll the streets of Vancouver, visit the UBC campus and perhaps you'll see some familiar spaces and places from the many films made hereWe hope you'll be able to make the most of your visit to Vancouver and of your time at the LAK12 conference. At time of writing, a number of weeks before the conference, we are sold out! This signals to us the importance of this emerging area of learning analytics and of the conference. We are pleased to be involved and helping to promote this new and exciting area of research and practice.We also want to thank those involved in helping make the conference such a success. Establishing a new research conference with proceedings published in the ACM Digital Library demands an extremely competent Program Committee, and we are indebted to our colleagues for their commitment to LAK12. A rigorous review process ensured that each paper was evaluated by at least three program committee members, and in many cases by four. Each paper was discussed in the online forum, with the authors having the option to reply to comments as distilled by the Program Chairs before a final decision was reached.Over our three days, we'll hear from three keynote speakers and an international set of authors in the field of learning analytics. The adjudicated papers include 14 Full Papers accepted from 36 submissions (39%). Of these a further six were accepted in briefer form as Short Papers and two as Design Briefings. We also invited authors to submit Short Papers that share preliminary conceptual, technical and empirical contributions: of the 26 submissions, 15 were accepted (58%). The program also includes three panels aimed to provide more discursive forums. As well as papers, the program includes two full-day and two half-day workshops taking place on April 29th on the UBC campus. In one year we have doubled the conference size. At LAK11, held in Banff in 2011, there were 17 adjudicated papers in a single track. LAK12 more than doubles in size to 40 in two parallel tracks, plus the pre-conference workshops. We have every confidence that this year's LAK will be a great success and will grow in size and reputation as at the end of LAK12 we will pass the torch on to the LAK13 organizers.},
location = {Vancouver, British Columbia, Canada}
}

@inproceedings{10.1145/2666633.2666634,
author = {Worsley, Marcelo},
title = {Multimodal Learning Analytics as a Tool for Bridging Learning Theory and Complex Learning Behaviors},
year = {2014},
isbn = {9781450304887},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2666633.2666634},
doi = {10.1145/2666633.2666634},
abstract = {The recent emergence of several low-cost, high resolution, multimodal sensors has greatly facilitated the ability for researchers to capture a wealth of data across a variety of contexts. Over the past few years, this multimodal technology has begun to receive greater attention within the learning community. Specifically, the Multimodal Learning Analytics community has been capitalizing on new sensor technology, as well as the expansion of tools for supporting computational analysis, in order to better understand and improve student learning in complex learning environments. However, even as the data collection and analysis tools have greatly eased the process, there remain a number of considerations and challenges in framing research in such a way that it lends to the development of learning theory. Moreover, there are a multitude of approaches that can be used for integrating multimodal data, and each approach has different assumptions and implications. In this paper, I describe three different types of multimodal analyses, and discuss how decisions about data integration and fusion have a significant impact on how the research relates to learning theories.},
booktitle = {Proceedings of the 2014 ACM Workshop on Multimodal Learning Analytics Workshop and Grand Challenge},
pages = {1–4},
numpages = {4},
keywords = {learning sciences, constructionism, cognition},
location = {Istanbul, Turkey},
series = {MLA '14}
}

@inproceedings{10.1145/2669711.2669913,
author = {Ray\'{o}n, Alex and Guenaga, Mariluz and N\'{u}\~{n}ez, Asier},
title = {Supporting competency-assessment through a learning analytics approach using enriched rubrics},
year = {2014},
isbn = {9781450328968},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2669711.2669913},
doi = {10.1145/2669711.2669913},
abstract = {Universities have increasingly emphasized competencies as central elements of students' development. However, the assessment of these competencies is not an easy task. The availability of data that learners generate in computer mediated learning offers great potential to study how learning takes place, and thus, to gather evidences for competency-assessment using enriched rubrics. The lack of data interoperability and the decentralization of those educational applications set out a challenge to exploit trace data. To face these problems we have designed and developed SCALA (Scalable Competence Assessment through a Learning Analytics approach), an analytics system that integrates usage -how the user interacts with resources- and social -how students and teachers interact among them-trace data to support competency assessment. The case study of SCALA presents teachers a dashboard with enriched rubrics of blended datasets obtained from six assessment learning activities, performed with a group of 28 students working teamwork competency. In terms of knowledge discovery, we obtain results applying clustering and association rule mining algorithms. Thus, we provide a visual analytics tool ready to support competency-assessment.},
booktitle = {Proceedings of the Second International Conference on Technological Ecosystems for Enhancing Multiculturality},
pages = {291–298},
numpages = {8},
keywords = {learning dashboard, learning analytics, large-scale interoperability, information retrieval, data integration},
location = {Salamanca, Spain},
series = {TEEM '14}
}

@inproceedings{10.1145/3079628.3079683,
author = {Dimitrova, Vania and Mitrovic, Antonija and Piotrkowicz, Alicja and Lau, Lydia and Weerasinghe, Amali},
title = {Using Learning Analytics to Devise Interactive Personalised Nudges for Active Video Watching},
year = {2017},
isbn = {9781450346351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3079628.3079683},
doi = {10.1145/3079628.3079683},
abstract = {Videos can be a powerful medium for acquiring soft skills, where learning requires contextualisation in personal experience and ability to see different perspectives. However, to learn effectively while watching videos, students need to actively engage with video content. We implemented interactive notetaking during video watching in an active video watching system (AVW) as a means to encourage engagement. This paper proposes a systematic approach to utilise learning analytics for the introduction of adaptive intervention - a choice architecture for personalised nudges in the AVW to extend learning. A user study was conducted and used as an illustration. By characterising clusters derived from user profiles, we identify different styles of engagement, such as parochial learning, habitual video watching, and self-regulated learning (which is the target ideal behaviour). To find opportunities for interventions, interaction traces in the AVW were used to identify video intervals with high user interest and relevant behaviour patterns that indicate when nudges may be triggered. A prediction model was developed to identify comments that are likely to have high social value, and can be used as examples in nudges. A framework for interactive personalised nudges was then conceptualised for the case study.},
booktitle = {Proceedings of the 25th Conference on User Modeling, Adaptation and Personalization},
pages = {22–31},
numpages = {10},
keywords = {video-based learning, soft-skills, personalised nudges, analytics},
location = {Bratislava, Slovakia},
series = {UMAP '17}
}

@inproceedings{10.1145/2330601.2330619,
author = {Rahman, Nazim and Dron, Jon},
title = {Challenges and opportunities for learning analytics when formal teaching meets social spaces},
year = {2012},
isbn = {9781450311113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2330601.2330619},
doi = {10.1145/2330601.2330619},
abstract = {Social networking is revolutionizing the world in ways few imagined just a few years ago. The power of social networking technology can also be leveraged to improve education and enhance the instructor and learner experience. Unlike conventional learning management systems, social software environments such as Athabasca Landing provide a persistent space and are flexible enough to support social and learner-led methods of informal, non-formal, and formal learning. Analytics can be used to effectively track and measure personal progress and help uncover extra-curricular factor affecting learner success such as network formation and growth. The paper reports on an attempt to explore this problem through analysis of student behaviour on the Athabasca Landing site within the context of a course. Its findings, explanation, and potential implications are listed. Effects of social learning on learners, based on the learner's behaviour before, during, and after the course are described and discussed. Finally, features of an open source tool created for this analysis, LASSIE is presented.},
booktitle = {Proceedings of the 2nd International Conference on Learning Analytics and Knowledge},
pages = {54–58},
numpages = {5},
keywords = {social systems, informal learning, formal learning, analytics},
location = {Vancouver, British Columbia, Canada},
series = {LAK '12}
}

@inproceedings{10.1145/3448139.3448212,
author = {Tsai, Yi-Shan and Mello, Rafael Ferreira and Jovanovi\'{c}, Jelena and Ga\v{s}evi\'{c}, Dragan},
title = {Student appreciation of data-driven feedback: A pilot study on OnTask},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448212},
doi = {10.1145/3448139.3448212},
abstract = {Feedback plays a crucial role in student learning. Learning analytics (LA) has demonstrated potential in addressing prominent challenges with feedback practice, such as enabling timely feedback based on insights obtained from large data sets. However, there is insufficient research looking into relations between student expectations of feedback and their experience with LA-based feedback. This paper presents a pilot study that examined students’ experience of LA-based feedback, offered with the OnTask system, taking into consideration the factors of students’self-efficacy and self-regulation skills. Two surveys were carried out at a Brazilian university, and the results highlighted important implications for LA-based feedback practice, including leveraging the ‘partnership’ between the human teacher and the computer, and developing feedback literacy among learners.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {511–517},
numpages = {7},
keywords = {learning theories, learning analytics, feedback literacy, feedback, OnTask},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/2567574.2567585,
author = {Dawson, Shane and Ga\v{s}evi\'{c}, Dragan and Siemens, George and Joksimovic, Srecko},
title = {Current state and future trends: a citation network analysis of the learning analytics field},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567585},
doi = {10.1145/2567574.2567585},
abstract = {This paper provides an evaluation of the current state of the field of learning analytics through analysis of articles and citations occurring in the LAK conferences and identified special issue journals. The emerging field of learning analytics is at the intersection of numerous academic disciplines, and therefore draws on a diversity of methodologies, theories and underpinning scientific assumptions. Through citation analysis and structured mapping we aimed to identify the emergence of trends and disciplinary hierarchies that are influencing the development of the field to date. The results suggest that there is some fragmentation in the major disciplines (computer science and education) regarding conference and journal representation. The analyses also indicate that the commonly cited papers are of a more conceptual nature than empirical research reflecting the need for authors to define the learning analytics space. An evaluation of the current state of learning analytics provides numerous benefits for the development of the field, such as a guide for under-represented areas of research and to identify the disciplines that may require more strategic and targeted support and funding opportunities.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {231–240},
numpages = {10},
keywords = {social network analysis, learning analytics, citation analysis, author networks},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/3706468.3706554,
author = {Spitzer, Markus Wolfgang Hermann and Bardach, Lisa and Strittmatter, Younes and Moeller, Korbinian},
title = {Usage and performance declines in a classroom-integrated digital learning software over the course of an academic year},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706554},
doi = {10.1145/3706468.3706554},
abstract = {In increasing numbers of classrooms worldwide, students use digital learning software. However, we know little about the trajectories of usage and the performance within such digital learning software over the academic year. This study analyzed real-world longitudinal data from a mathematics learning software used in classrooms in Germany and the Netherlands (6,000 students who worked on &gt;23 million problems). We evaluated students’ usage and performance trajectories across an academic year by examining the percentage of students using the software, worked-through problems, active days and weeks, as well as performance. Our results indicate a decline in both usage and performance over the course of the academic year, with overall lower usage in Germany than in the Netherlands. Our findings highlight the need for further research into the factors maintaining or increasing the usage of and performance in classroom-integrated digital learning software over extended periods.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {957–962},
numpages = {6},
keywords = {digital learning software, mathematics, academic performance, naturalistic data, expectancy-value theory, expectancy-value-cost theory},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3636555.3636849,
author = {Barrett, Jake and Day, Alasdair and Gal, Kobi},
title = {Improving Model Fairness with Time-Augmented Bayesian Knowledge Tracing},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636849},
doi = {10.1145/3636555.3636849},
abstract = {Modelling student performance is an increasingly popular goal in the learning analytics community. A common method for this task is Bayesian Knowledge Tracing (BKT), which predicts student performance and topic mastery using the student’s answer history. While BKT has strong qualities and good empirical performance, like many machine learning approaches it can be prone to bias. In this study we demonstrate an inherent bias in BKT with respect to students’ income support levels and gender, using publicly available data. We find that this bias is likely a result of the model’s ‘slip’ parameter disregarding answer speed when deciding if a student has lost mastery status. We propose a new BKT model variation that directly considers answer speed, resulting in a significant fairness increase without sacrificing model performance. We discuss the role of answer speed as a potential cause of BKT model bias, as well as a method to minimise bias in future implementations.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {46–54},
numpages = {9},
keywords = {fairness, knowledge tracing},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3706468.3706536,
author = {Pan, Hongchen and Araujo Oliveira, Eduardo and Ferreira Mello, Rafael},
title = {Exploring Human-AI Collaboration in Educational Contexts: Insights from Writing Analytics and Authorship Attribution},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706536},
doi = {10.1145/3706468.3706536},
abstract = {This research investigates the characteristics of student essays written with and without generative AI assistance, using stylometric analysis and deep learning techniques to explore human-AI collaboration in academic writing. To address three research questions, the study examines: (1) patterns in vocabulary diversity, sentence structure, and readability in AI-generated versus student-written essays; (2) the development of a stylometry-based BERT model for authorship attribution, focusing on linguistic features to accurately distinguish between student and AI-generated content; and (3) the application of this model to measure AI involvement at the sentence level in collaborative essays. Using a dataset of student and AI-assisted essays, we observed distinct stylistic differences, with AI-generated content exhibiting higher lexical diversity and readability scores. The BERT model demonstrated high accuracy (85%), precision (79%), and F1-scores (74%) in identifying AI contributions, surpassing the adopted baseline. While limitations such as dataset imbalance and variability in AI outputs remain, this study highlights the potential of stylometric analysis in improving authorship attribution and quantifying AI involvement in academic writing. These findings provide educators with tools to monitor student progress, offer personalised feedback, and maintain academic integrity in the face of growing AI usage in education.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {903–909},
numpages = {7},
keywords = {Generative AI, Authorship Attribution, Writing Analytics},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706567,
author = {Yao, Chengyuan and Cortez, Carmen and Yu, Renzhe},
title = {Towards Fair and Privacy-Aware Transfer Learning for Educational Predictive Modeling: A Case Study on Retention Prediction in Community Colleges},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706567},
doi = {10.1145/3706468.3706567},
abstract = {Predictive analytics is a widely used application of learning analytics, but many resource-constrained institutions lack the capacity to develop their own predictive models or rely on proprietary models trained in different contexts with little transparency. In this context, transfer learning holds promise for expanding reliable and equitable access to predictive analytics, but this potential remains underexplored given existing legal and technical constraints. In this paper, we examine transfer learning strategies in the context of retention prediction at two-year community colleges in the United States, which enroll the most postsecondary students from underserved communities with higher dropout rates than selective universities. We envision a scenario where community colleges can collaborate with each other and with four-year universities to develop retention prediction models under privacy constraints, and evaluate the risks and potential improvement strategies of cross-institutional model transfer for different stakeholders. Using detailed administrative records from 4 research universities and 23 community colleges, which cover more than 800,000 students across 7 cohorts, we first identify performance and fairness degradation when source (external) models are deployed at a target institution without any localization. Fortunately, publicly available institution-level contextual information can be used to forecast these performance drops and offer early guidance for model portability. For model developers under data privacy regulations, sequential training that selects training institutions based on demographic similarities proves useful for enhancing the general fairness of resulting models without compromising performance. For target institutions without local data to fine-tune source models, we find that customizing evaluation thresholds for different sensitive groups is more successful than established transfer learning techniques at improving performance and fairness of deployed models. Our findings suggest the value of transfer learning for more accessible educational predictive modeling and call for judicious use of contextual information in model training, selection, and deployment to achieve reliable and equitable model transfer1.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {738–749},
numpages = {12},
keywords = {Predictive Analytics; Transfer Learning; Algorithmic Fairness; Privacy; Intersectionality; College Retention; Community Colleges; Higher Education},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706507,
author = {Xiao, Changrong and Ma, Wenxing and Song, Qingping and Xu, Sean Xin and Zhang, Kunpeng and Wang, Yufang and Fu, Qi},
title = {Human-AI Collaborative Essay Scoring: A Dual-Process Framework with LLMs},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706507},
doi = {10.1145/3706468.3706507},
abstract = {Receiving timely and personalized feedback is essential for second-language learners, especially when human instructors are unavailable. This study explores the effectiveness of Large Language Models (LLMs), including both proprietary and open-source models, for Automated Essay Scoring (AES). Through extensive experiments with public and private datasets, we find that while LLMs do not surpass conventional state-of-the-art (SOTA) grading models in performance, they exhibit notable consistency, generalizability, and explainability. We propose an open-source LLM-based AES system, inspired by the dual-process theory. Our system offers accurate grading and high-quality feedback, at least comparable to that of fine-tuned proprietary LLMs, in addition to its ability to alleviate misgrading. Furthermore, we conduct human-AI co-grading experiments with both novice and expert graders. We find that our system not only automates the grading process but also enhances the performance and efficiency of human graders, particularly for essays where the model has lower confidence. These results highlight the potential of LLMs to facilitate effective human-AI collaboration in the educational context, potentially transforming learning experiences through AI-generated feedback.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {293–305},
numpages = {13},
keywords = {LLM Application, Automatic Essay Scoring, AI-assisted Learning},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3041021.3054158,
author = {Peng, Jun and Sergis, Stylianos and Wang, Minhong and Sampson, Demetrios},
title = {Combining Smart Web-based Learning Environments with Teaching and Learning Analytics to Support Reflection on Project-based Programming Education},
year = {2017},
isbn = {9781450349147},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3041021.3054158},
doi = {10.1145/3041021.3054158},
abstract = {Project-based learning (PjBL) is a promising approach for supporting learning of computer programming by addressing the gap between the attainment of abstract knowledge and the application of this knowledge to authentic programming tasks. The World Wide Web has considerable potential to expand and improve PjBL environments. However, making implicit aspects of a programming task accessible to learners and instructors to support reflection and improvement can be challenging. This paper discusses the challenge of PjBL in programming education and outlines a set of key aspects and an analysis framework to inform design and analysis of PjBL programming in web-based environments, by exploiting the emerging field of Teaching and Learning Analytics. Based on the proposed analysis framework, the design of an empirical study is outlined, capitalizing on a novel web-based PjBL environment that makes complex cognitive processes accessible and affords the analysis of its effects on learning programming in multiple aspects.},
booktitle = {Proceedings of the 26th International Conference on World Wide Web Companion},
pages = {379–385},
numpages = {7},
keywords = {web-based learning, technology-enabled learning, teaching and learning analytics, project-based learning, learning analytics, computer programming},
location = {Perth, Australia},
series = {WWW '17 Companion}
}

@inproceedings{10.1145/3506860.3506881,
author = {Tzi-Dong Ng, Jeremy and Hu, Xiao and Que, Ying},
title = {Towards Multi-modal Evaluation of Eye-tracked Virtual Heritage Environment},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506881},
doi = {10.1145/3506860.3506881},
abstract = {In times of pandemic-induced challenges, virtual reality (VR) allows audience to learn about cultural heritage sites without temporal and spatial constraints. The design of VR content is largely determined by professionals, while evaluations of content often rely on learners’ self-report data. Learners’ attentional focus and understanding of VR content might be affected by the presence or absence of different multimedia elements including text and audio-visuals. It remains an open question which design variations are more conducive for learning about heritage sites. Leveraging eye-tracking, a technology often adopted in recent multimodal learning analytics (MmLA) research, we conducted an experiment to collect and analyze 40 learners’ eye movement and self-reported data. Results of statistical tests and heatmap elicitation interviews indicate that 1) text in the VR environment helped learners better understand the presented heritage sites, regardless of having audio narration or not, 2) text diverted learners’ attention away from other visual elements that contextualized the heritage sites, 3) exclusively having audio narration best simulated the experience of a real-world heritage tour, 4) narration accompanying text prompted learners to read the text faster. We make recommendations for improving the design of VR learning materials and discuss the implications for MmLA research.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {451–457},
numpages = {7},
keywords = {Virtual reality, Multimodal learning analytics, Multimedia learning, Eye-tracking, Cultural heritage},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3706468.3706480,
author = {Nguyen, Ha and Park, Saerok},
title = {Providing Automated Feedback on Formative Science Assessments: Uses of Multimodal Large Language Models},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706480},
doi = {10.1145/3706468.3706480},
abstract = {Formative assessment in science education often involves multimodality and combines textual and visual representations. We evaluate the capacity of multimodal large language models (MLLMs), including Anthropic’s Claude 3.5 Sonnet, Google’s Gemini 1.5 Flash, and OpenAI’s GPT-4o and GPT-4 Turbo, to score and provide feedback on multimodal science assessments. Overall, the MLLMs can accurately transcribe students’ hand-written text. The best performing models (Claude and GPT4-o) show moderate to substantial agreement with human evaluators in assessing students’ scientific reasoning. MLLMs provided with example responses, scores, and explanations (few-shot learning) generally perform better than those without examples (zero-shot learning). Thematic analysis reveals cases where the models misevaluate the depth in students’ answers, add details not included in the input (i.e., hallucinate), or show incorrect numerical reasoning. Findings demonstrate the feasibility of and considerations for using MLLMs to provide in-time feedback for science assessments. Such feedback can help to revise students’ understanding and inform teachers’ instructional practices.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {803–809},
numpages = {7},
keywords = {multimodal large language model, science assessment, automated evaluation},
location = {
},
series = {LAK '25}
}

@proceedings{10.1145/2090116,
title = {LAK '11: Proceedings of the 1st International Conference on Learning Analytics and Knowledge},
year = {2011},
isbn = {9781450309448},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {We are pleased to present the proceedings of the First International Conference on Learning Analytics and Knowledge (LAK 2011). The conference took place in Banff, Alberta, Canada in the period February 27 - March 1, 2011. The idea for establishing a special dedicated forum to researching learning analytics was motivated by several important indicators:1. The growth of data surpasses the ability of organizations to make sense of it. This concern is particularly pronounced in relation to knowledge, teaching, and learning.2. Learning institutions and corporations make little use of the data learners "throw off" in the process of accessing learning materials, interacting with educators and peers, and creating new content.3. In an age where educational institutions are under growing pressure to reduce costs and increase efficiency, analytics promises to be an important lens through which to view and plan for change at course and institutions levels.},
location = {Banff, Alberta, Canada}
}

@inproceedings{10.1145/3636555.3636911,
author = {Borchers, Conrad and Zhang, Jiayi and Baker, Ryan S. and Aleven, Vincent},
title = {Using Think-Aloud Data to Understand Relations between Self-Regulation Cycle Characteristics and Student Performance in Intelligent Tutoring Systems},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636911},
doi = {10.1145/3636555.3636911},
abstract = {Numerous studies demonstrate the importance of self-regulation during learning by problem-solving. Recent work in learning analytics has largely examined students’ use of SRL concerning overall learning gains. Limited research has related SRL to in-the-moment performance differences among learners. The present study investigates SRL behaviors in relationship to learners’ moment-by-moment performance while working with intelligent tutoring systems for stoichiometry chemistry. We demonstrate the feasibility of labeling SRL behaviors based on AI-generated think-aloud transcripts, identifying the presence or absence of four SRL categories (processing information, planning, enacting, and realizing errors) in each utterance. Using the SRL codes, we conducted regression analyses to examine how the use of SRL in terms of presence, frequency, cyclical characteristics, and recency relate to student performance on subsequent steps in multi-step problems. A model considering students’ SRL cycle characteristics outperformed a model only using in-the-moment SRL assessment. In line with theoretical predictions, students’ actions during earlier, process-heavy stages of SRL cycles exhibited lower moment-by-moment correctness during problem-solving than later SRL cycle stages. We discuss system re-design opportunities to add SRL support during stages of processing and paths forward for using machine learning to speed research depending on the assessment of SRL based on transcription of think-aloud data.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {529–539},
numpages = {11},
keywords = {intelligent tutoring systems, process analysis, self-regulated learning, think-aloud method},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636871,
author = {Paik, Jae H. and Himelfarb, Igor and Yoo, Seung Hee and Yoo, KyoungMi and Ha, Hoyong},
title = {The relationships among school engagement, students' emotions, and academic performance in an elementary online learning},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636871},
doi = {10.1145/3636555.3636871},
abstract = {This study investigated the relationship among school engagement, students’ emotions, and academic performance of students in grades 3-6 in South Korea. A random sampling approach was used to extract data from 1,075 students out of a total of 141,926 students who used the educational learning platform, I-TokTok, adapted as the primary Learning Management System (LMS) at the provincial level. The present study aimed to identify dimensions of school engagement by exploring the behaviors consistent with IMS Caliper Analytics Specifications, a common standard utilized for collecting learning data from digital resources. Exploratory and Confirmatory Factor Analyses revealed a three-factor model of school engagement among the 13 learning behavioral indicators: behavioral engagement, social engagement, and cognitive engagement.&nbsp;Students’ emotions were measured through voluntary daily activities in the platform involving reflecting on, recognizing, and recording of their emotions. Students’ academic performance was assessed with performance in math tests administered within the platform. Consistent with current literature, results demonstrated that dimensions of school engagement (i.e., behavioral and social engagement) and students’ emotions positively predicted their math performance. Lastly, school engagement mediated the relationship between students’ emotions and math performance. The present study emphasizes the importance of investigating the underlying mechanisms through which elementary students emotions and school engagement predict academic achievement in an online learning environment. This relatively new area of educational research deserves attention in the field of learning analytics. We highlight the importance of considering ways to improve both students’ emotions and their school engagement to maximize the student learning outcomes.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {219–230},
numpages = {12},
keywords = {Academic Performance, Elementary Education, Learning Management System, School Engagement, Students’ Emotions},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3706468.3706506,
author = {Xiang, Mengtong and Zhang, Jingjing and Saqr, Mohammed and Jiang, Han and Liu, Wei},
title = {Capturing The Temporal Dynamics of Learner Interactions In Moocs: A Comprehensive Approach With Longitudinal And Inferential Network Analysis},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706506},
doi = {10.1145/3706468.3706506},
abstract = {While research on social network analysis is abundant and less frequently so temporal network analysis, research that uses inferential temporal network methods is barely existent. This paper aims to fill this gap by conducting a comparative analysis of temporal networks and inferential longitudinal network methods in the context of learner interactions in Massive Open Online Courses (MOOCs). We focus on three prominent methods: Temporal Network Analysis (TNA), Temporal Exponential Random Graph Models (TERGM) and Simulation Investigation for Empirical Network Analysis (SIENA). Using a five-week Nature Education MOOC as a case study, we compared the features, metrics of each method as well as their understanding of using network to analyze learner interactions. TNA focuses on describing and visualizing temporal changes in network structure, while TERGM and SIENA view networks as evolving systems influenced by individual behaviors and structural dependencies. TERGM treats network changes as a joint of random processes, while SIENA emphasizes the agency of learners and analyzes continuous network evolution. The findings provide guidelines for researchers and educators to select appropriate network analysis methods for temporal studies in educational contexts.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {851–857},
numpages = {7},
keywords = {MOOCs, Network Analysis, SIENA, TERGM, TNA, Temporal Dynamics},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/2723576.2723589,
author = {Ezen-Can, Aysu and Boyer, Kristy Elizabeth and Kellogg, Shaun and Booth, Sherry},
title = {Unsupervised modeling for understanding MOOC discussion forums: a learning analytics approach},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723589},
doi = {10.1145/2723576.2723589},
abstract = {Massively Open Online Courses (MOOCs) have gained attention recently because of their great potential to reach learners. Substantial empirical study has focused on student persistence and their interactions with the course materials. However, most MOOCs include a rich textual dialogue forum, and these textual interactions are largely unexplored. Automatically understanding the nature of discussion forum posts holds great promise for providing adaptive support to individual students and to collaborative groups. This paper presents a study that applies unsupervised student understanding models originally developed for synchronous tutorial dialogue to MOOC forums. We use a clustering approach to group similar posts, compare the clusters with manual annotations by MOOC researchers, and further investigate clusters qualitatively. This paper constitutes a step toward applying unsupervised models to asynchronous communication, which can enable massive-scale automated discourse analysis and mining to better support students' learning.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {146–150},
numpages = {5},
keywords = {text-based learning analytics, online learning, MOOCs},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2388676.2388755,
author = {Worsley, Marcelo},
title = {Multimodal learning analytics: enabling the future of learning through multimodal data analysis and interfaces},
year = {2012},
isbn = {9781450314671},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2388676.2388755},
doi = {10.1145/2388676.2388755},
abstract = {Project-based learning has found its way into a range of formal and informal learning environments. However, systematically assessing these environments remains a significant challenge. Traditional assessments, which focus on learning outcomes, seem incongruent with the process-oriented goals of project-based learning. Multimodal interfaces and multimodal learning analytics hold significant promise for assessing learning in open-ended learning environments. With its rich integration of a multitude of data streams and naturalistic interfaces, this area of research may help usher in a new wave of education reform by supporting alternative modes of learning.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimodal Interaction},
pages = {353–356},
numpages = {4},
keywords = {probabilistic modeling, learning, data mining, constructionism},
location = {Santa Monica, California, USA},
series = {ICMI '12}
}

@inproceedings{10.1145/3448139.3448148,
author = {Srivastava, Namrata and Nawaz, Sadia and Newn, Joshua and Lodge, Jason and Velloso, Eduardo and M. Erfani, Sarah and Gasevic, Dragan and Bailey, James},
title = {Are you with me? Measurement of Learners’ Video-Watching Attention with Eye Tracking},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448148},
doi = {10.1145/3448139.3448148},
abstract = {Video has become an essential medium for learning. However, there are challenges when using traditional methods to measure how learners attend to lecture videos in video learning analytics, such as difficulty in capturing learners’ attention at a fine-grained level. Therefore, in this paper, we propose a gaze-based metric—“with-me-ness direction” that can measure how learners’ gaze-direction changes when they listen to the instructor’s dialogues in a video-lecture. We analyze the gaze data of 45 participants as they watched a video lecture and measured both the sequences of with-me-ness direction and proportion of time a participant spent looking in each direction throughout the lecture at different levels. We found that although the majority of the time participants followed the instructor’s dialogues, their behaviour of looking-ahead, looking-behind or looking-outside differed by their prior knowledge. These findings open the possibility of using eye-tracking to measure learners’ video-watching attention patterns and examine factors that can influence their attention, thereby helping instructors to design effective learning materials.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {88–98},
numpages = {11},
keywords = {video lecture, learning analytics, gaze direction, eye-tracking, co-attention},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3636555.3636896,
author = {Thomas, Danielle R and Lin, Jionghao and Gatz, Erin and Gurung, Ashish and Gupta, Shivang and Norberg, Kole and Fancsali, Stephen E and Aleven, Vincent and Branstetter, Lee and Brunskill, Emma and Koedinger, Kenneth R},
title = {Improving Student Learning with Hybrid Human-AI Tutoring: A Three-Study Quasi-Experimental Investigation},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636896},
doi = {10.1145/3636555.3636896},
abstract = {Artificial intelligence (AI) applications to support human tutoring have potential to significantly improve learning outcomes, but engagement issues persist, especially among students from low-income backgrounds. We introduce an AI-assisted tutoring model that combines human and AI tutoring and hypothesize this synergy will have positive impacts on learning processes. To investigate this hypothesis, we conduct a three-study quasi-experiment across three urban and low-income middle schools: 1) 125 students in a Pennsylvania school; 2) 385 students (50% Latinx) in a California school, and 3) 75 students (100% Black) in a Pennsylvania charter school, all implementing analogous tutoring models. We compare learning analytics of students engaged in human-AI tutoring compared to students using math software only. We find human-AI tutoring has positive effects, particularly in student’s proficiency and usage, with evidence suggesting lower achieving students may benefit more compared to higher achieving students. We illustrate the use of quasi-experimental methods adapted to the particulars of different schools and data-availability contexts so as to achieve the rapid data-driven iteration needed to guide an inspired creation into effective innovation. Future work focuses on improving the tutor dashboard and optimizing tutor-student ratios, while maintaining annual costs per student of approximately $700 annually.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {404–415},
numpages = {12},
keywords = {AI-assisted tutoring, Design-based research, Human-AI tutoring, Tutoring},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.5555/2655780.2655908,
author = {Jones, Kyle M. L.},
title = {Learning analytics &amp; FERPA: issues of student privacy and new boundaries of student data},
year = {2013},
isbn = {0877155453},
publisher = {American Society for Information Science},
address = {USA},
abstract = {In keeping with the conference's theme of "Rethinking Information Boundaries," this poster describes preliminary research results from a case study regarding how Hammond University is employing analytic technologies to examine student data across boundaries: within the technological domain of their campus and from disparate sources beyond.},
booktitle = {Proceedings of the 76th ASIS&amp;T Annual Meeting: Beyond the Cloud: Rethinking Information Boundaries},
articleno = {128},
numpages = {5},
location = {Montreal, Quebec, Canada},
series = {ASIST '13}
}

@inproceedings{10.1145/3706468.3706495,
author = {Ocumpaugh, Jaclyn and Nasiar, Nidhi and Zambrano, Andres Felipe and Goslen, Alex and Vandenberg, Jessica and Esiason, Jordan and Rowe, Jonathan and Hutt, Stephen},
title = {Refocusing the lens through which we view affect dynamics: The Skills, Difficulty, Value, Efficacy and Time Model},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706495},
doi = {10.1145/3706468.3706495},
abstract = {For more than a decade, a handful of theoretical models have shaped a substantial amount of the research related to students’ emotional experiences during learning. This research has been productive, but articulating the underlying implicit assumptions in existing theories and their implications in our empirical interpretations can help to better investigate the reciprocal relationships between learning and emotion, and subsequently, to develop better interventions. This paper expands upon the existing theoretical frameworks, increasing the types of questions we ask about affect dynamics. We do so within the context of Crystal Island, a virtual world that allows middle school students to investigate microbiology questions. Specifically, we use this data to examine and revise the assumptions that are implicit in these models and the methods we use to investigate them.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {192–203},
numpages = {12},
keywords = {Affect Dynamics, Control Value Theory, Game-based learning, Self-Efficacy},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3041021.3054175,
author = {de Freitas, Sara and Gibson, David and Alvarez, Victor and Irving, Leah and Star, Kam and Charleer, Sven and Verbert, Katrien},
title = {How to Use Gamified Dashboards and Learning Analytics for Providing Immediate Student Feedback and Performance Tracking in Higher Education},
year = {2017},
isbn = {9781450349147},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3041021.3054175},
doi = {10.1145/3041021.3054175},
abstract = {With the wide use of the Internet and digital data sources, there has been a recent emergence of easy access to student data within learning management systems (LMS), grade data through student information systems (SIS) and broader sector data through benchmarking metrics and standards. Learning analytics on top of this data has introduced greater capabilities for improving student performance through immediate feedback. Current literature considers the role of dashboards for student performance and feedback, but few papers consider the efficacy of fast feedback to students or other ways that information can be fed back to learners. In this paper, we consider the work done by three leading groups addressing the impact of gamification in university education, with a specific focus on how data is presented to the learner, that is using elements such as points, levelling up, narrative and progression to scaffold learning. Results indicate increases in student motivation, engagement, satisfaction, retention and performance enhancements.},
booktitle = {Proceedings of the 26th International Conference on World Wide Web Companion},
pages = {429–434},
numpages = {6},
keywords = {serious games, learning analytics, higher education, game-based learning, dashboards},
location = {Perth, Australia},
series = {WWW '17 Companion}
}

@inproceedings{10.1145/3706468.3706483,
author = {Hedlin, Elias and Estling, Ludwig and Wong, Jacqueline and Demmans Epp, Carrie and Viberg, Olga},
title = {Got It! Prompting Readability Using ChatGPT to Enhance Academic Texts for Diverse Learning Needs},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706483},
doi = {10.1145/3706468.3706483},
abstract = {Reading skills are crucial for students' success in education and beyond. However, reading proficiency among K-12 students has been declining globally, including in Sweden, leaving many underprepared for post-secondary education. Additionally, an increasing number of students have reading disorders, such as dyslexia, which require support. Generative artificial intelligence (genAI) technologies, like ChatGPT, may offer new opportunities to improve reading practices by enhancing the readability of educational texts. This study investigates whether ChatGPT-4 can simplify academic texts and which prompting strategies are most effective. We tasked ChatGPT to re-write 136 academic texts using four prompting approaches: Standard, Meta, Roleplay, and Chain-of-Thought. All four approaches improved text readability, with Meta performing the best overall and the Standard prompt sometimes creating texts that were less readable than the original. This study found variability in the simplified texts, suggesting that different strategies should be used based on the specific needs of individual learners. Overall, the findings highlight the potential of genAI tools, like ChatGPT, to improve the accessibility of academic texts, offering valuable support for students with reading difficulties and promoting more equitable learning opportunities.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {115–125},
numpages = {11},
keywords = {Analytics, Equity, Large language models, Literacy, Prompt engineering, Readability},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3027385.3027420,
author = {Quigley, David and Ostwald, Jonathan and Sumner, Tamara},
title = {Scientific modeling: using learning analytics to examine student practices and classroom variation},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027420},
doi = {10.1145/3027385.3027420},
abstract = {Modeling has a strong focus in current science learning frameworks as a critical skill for students to learn. However, understanding students' scientific models and their modeling practices at scale is a difficult task that has not been taken up by the research literature. The complex variables involved in classroom learning, such as teacher differences, increase the difficulty of understanding this problem. This work begins with an exploration of the methods used to explore students' scientific modeling in the learning sciences space and the frameworks developed to characterize student modeling practices. Learning analytics can be used to leverage these frameworks of scientific modeling practices to explore questions around students' scientific models and their modeling practices. These analyses are focused around the use of EcoSurvey, a collaborative, digital tool used in high-school biology classrooms to model the local ecosystem. This tool was deployed in ten biology classrooms and used with varying degrees of success. There are significant teacher-level differences found in the activity sequences of students using the EcoSurvey tool. The theoretical metrics around scientific modeling practices and automatically extracted feature sequences were also used in a classification task to automatically determine a particular student's teacher. These results underline the power of learning analytics methods to give insight into how modeling practices are realized in the classroom. This work also informs changes to modeling tools, associated curricula, and supporting professional development around scientific modeling.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {329–338},
numpages = {10},
keywords = {teacher differences, scientific modeling, collaborative modeling, classification},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3448139.3448211,
author = {Fan, Yizhou and Saint, John and Singh, Shaveen and Jovanovic, Jelena and Ga\v{s}evi\'{c}, Dragan},
title = {A learning analytic approach to unveiling self-regulatory processes in learning tactics},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448211},
doi = {10.1145/3448139.3448211},
abstract = {Investigation of learning tactics and strategies has received increasing attention by the Learning Analytics (LA) community. While previous research efforts have made notable contributions towards identifying and understanding learning tactics from trace data in various blended and online learning settings, there is still a need to deepen our understanding about learning processes that are activated during the enactment of distinct learning tactics. In order to fill this gap, we propose a learning analytic approach to unveiling and comparing self-regulatory processes in learning tactics detected from trace data. Following this approach, we detected four learning tactics (Reading with Quiz Tactic, Assessment and Interaction Tactic, Short Login and Interact Tactic and Focus on Quiz Tactic) as used by 728 learners in an undergrad course. We then theorised and detected five micro-level processes of self-regulated learning (SRL) through an analysis of trace data. We analysed how these micro-level SRL processes were activated during enactment of the four learning tactics in terms of their frequency of occurrence and temporal sequencing. We found significant differences across the four tactics regarding the five micro-level SRL processes based on multivariate analysis of variance and comparison of process models. In summary, the proposed LA approach allows for meaningful interpretation and distinction of learning tactics in terms of the underlying SRL processes. More importantly, this approach shows the potential to overcome the limitations in the interpretation of LA results which stem from the context-specific nature of learning. Specifically, the study has demonstrated how the interpretation of LA results and recommendation of pedagogical interventions can also be provided at the level of learning processes rather than only in terms of a specific course design.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {184–195},
numpages = {12},
keywords = {Self-regulated learning, Process model, Learning tactic, Learning analytics},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/2666633.2666637,
author = {Worsley, Marcelo and Blikstein, Paulo},
title = {Deciphering the Practices and Affordances of Different Reasoning Strategies through Multimodal Learning Analytics},
year = {2014},
isbn = {9781450304887},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2666633.2666637},
doi = {10.1145/2666633.2666637},
abstract = {Multimodal analysis has had demonstrated effectiveness in studying and modeling several human-human and human-computer interactions. In this paper, we explore the role of multimodal analysis in the service of studying complex learning environments. We use a semi-automated multimodal method to examine how students learn in a hands-on, engineering design context. Specifically, we combine, audio, gesture and electro-dermal activation data from a study (N=20) in which students were divided into two experimental conditions. The two experimental conditions, example-based reasoning and principle-based reasoning, have previously been shown to be associated with different learning gains and different levels of design quality. In this paper we study how the two experimental conditions differed in terms of their practices and processes. The practices included four common multimodal behaviors, that we've entitled ACTION, TALK, STRESS and FLOW. Furthermore, we show that individuals from the two experimental conditions differed in their usage of the four common behavior both on aggregate, and when we model their sequence of actions. Details concerning the data, analytic technique, interpretation and implications of this research are discussed.},
booktitle = {Proceedings of the 2014 ACM Workshop on Multimodal Learning Analytics Workshop and Grand Challenge},
pages = {21–27},
numpages = {7},
keywords = {learning sciences, data mining, constructionist, computational},
location = {Istanbul, Turkey},
series = {MLA '14}
}

@inproceedings{10.1145/3706468.3706537,
author = {Li, Hai and Xing, Wanli and Li, Chenglu and Zhu, Wangda and Lyu, Bailing and Zhang, Fan and Liu, Zifeng},
title = {Who Should Be My Tutor? Analyzing the Interactive Effects of Automated Text Personality Styles Between Middle School Students and a Mathematics Chatbot},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706537},
doi = {10.1145/3706468.3706537},
abstract = {Engaging with instructors through question-and-response techniques is an efficient method for delivering mathematics instruction to middle school learners. The flexible nature and sophisticated functionality of large language models (LLMs) have fueled interest in automating this process to strengthen students’ mathematical understanding, with the chatbot’s personality serving as an essential aspect of its design. While much research has explored students’ preferences for chatbot personalities, preferences in the context of learning gains, considering students’ own personalities, remain unclear. This study draws on QA dialogue logs between middle school students and a chatbot from a U.S.-based online mathematics learning platform. An automated feature extraction framework was designed to analyze text style from a personality perspective, extracting features including emotional polarity (reflecting emotional arousal), subjectivity (degree of subjective-neutral expression), and the big five personality traits (indicating potential personality tendencies). Linear regression was then used to analyze the relationship between these features and students’ learning gains in mathematics. Our findings support the complementary hypothesis from interpersonal interaction theory, which posits that students prefer chatbot personalities that complement their own. We discuss the implications for instructional design. Our analysis contributes to the development of more effective conversational AI applications in educational technology.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {910–917},
numpages = {8},
keywords = {conversational AI, chatbot personality, text style},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706530,
author = {Thomas, Danielle R and Borchers, Conrad and Kakarla, Sanjit and Lin, Jionghao and Bhushan, Shambhavi and Guo, Boyuan and Gatz, Erin and Koedinger, Kenneth R},
title = {Does Multiple Choice Have a Future in the Age of Generative AI? A Posttest-only RCT},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706530},
doi = {10.1145/3706468.3706530},
abstract = {The role of multiple-choice questions (MCQs) as effective learning tools has been debated in past research. While MCQs are widely used due to their ease in grading, open response questions are increasingly used for instruction, given advances in large language models (LLMs) for automated grading. This study evaluates MCQs effectiveness relative to open-response questions, both individually and in combination, on learning. These activities are embedded within six tutor lessons on advocacy. Using a posttest-only randomized control design, we compare the performance of 234 tutors (790 lesson completions) across three conditions: MCQ only, open response only, and a combination of both. We find no significant learning differences across conditions at posttest, but tutors in the MCQ condition took significantly less time to complete instruction. These findings suggest that MCQs are as effective, and more efficient, than open response tasks for learning when practice time is limited. To further enhance efficiency, we autograded open responses using GPT-4o and GPT-4-turbo. GPT models demonstrate proficiency for purposes of low-stakes assessment, though further research is needed for broader use. This study contributes a dataset of lesson log data, human annotation rubrics, and LLM prompts to promote transparency and reproducibility.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {494–504},
numpages = {11},
keywords = {Tutoring, Generative AI, Human-AI tutoring, AI-assisted tutoring, Assessment},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706569,
author = {Oh, Hyunju and Liu, Zifeng and Xing, Wanli},
title = {Do Actions Speak Louder Than Words? Unveiling Linguistic Patterns in Online Learning Communities Using Cross Recurrence Quantification Analysis},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706569},
doi = {10.1145/3706468.3706569},
abstract = {This study explores the dynamics of engagement in online learning communities (OLCs), focusing on online math discussion forums. It employs Social Network Analysis (SNA) and Cross-Recurrence Quantification Analysis (CRQA) to examine interaction patterns and linguistic synchrony across participant clusters with varying levels of engagement. SNA reveals three distinct participant groups—core, intermediate, and peripheral—each exhibiting different interaction levels. The study’s findings highlight the significance of coordinated discourse in fostering collaborative learning and engagement in OLCs. This research contributes to the theoretical framework of Social Capital Theory by emphasizing the role of shared language in promoting cohesive communication. The results offer valuable insights for designing more effective online learning environments that encourage sustained student participation and knowledge construction.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {992–998},
numpages = {7},
keywords = {Online Learning Communities, Social Network Analysis, Cross-Recurrence Quantification Analysis},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3341215.3356335,
author = {Pastushenko, Olena},
title = {Gamification in Assignments: Using Dynamic Difficulty Adjustment and Learning Analytics to Enhance Education},
year = {2019},
isbn = {9781450368711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341215.3356335},
doi = {10.1145/3341215.3356335},
abstract = {This paper discusses the opportunities for gamification and dynamic difficulty adjustment based on multimodal learning analytics in assignments. Altogether this covers a broader term of personalized education, which is getting more attention among the researchers in recent years. The difference of this work from other similar researches is that it suggests combining several domains to achieve better results: gamification (in order to improve student's motivation and involvements), and dynamic difficulty adjustment. All this is made possible by applying multimodal learning analytics and creating useful learning dashboards for the teachers.},
booktitle = {Extended Abstracts of the Annual Symposium on Computer-Human Interaction in Play Companion Extended Abstracts},
pages = {47–53},
numpages = {7},
keywords = {dynamic difficulty adjustment, gamification, multimodal learning analytics, personalizedlearning},
location = {Barcelona, Spain},
series = {CHI PLAY '19 Extended Abstracts}
}

@inproceedings{10.1145/3576050.3576052,
author = {Choi, Heeryung and Winne, Philip H. and Brooks, Christopher and Li, Warren and Shedden, Kerby},
title = {Logs or Self-Reports? Misalignment Between Behavioral Trace Data and Surveys When Modeling Learner Achievement Goal Orientation},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576052},
doi = {10.1145/3576050.3576052},
abstract = {While learning analytics researchers have been diligently integrating trace log data into their studies, learners’ achievement goals are still predominantly measured by self-reported surveys. This study investigated the properties of trace data and survey data as representations of achievement goals. Through the lens of goal complex theory, we generated achievement goal clusters using latent variable mixture modeling applied to each kind of data. Findings show significant misalignment between these two data sources. Self-reported goals stated before learning do not translate into goal-relevant behaviors tracked using trace data collected during learning activities. While learners generally articulate an orientation towards mastery learning in self-report surveys, behavioral trace data showed a higher incidence of less engaged learning activities. These findings call into question the utility of survey-based measures when up-to-date achievement goal data are needed. Our results advance methodological and theoretical understandings of achievement goals in the modern age of learning analytics.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {11–21},
numpages = {11},
keywords = {achievement goals, latent variable mixture modeling, survey data, trace data},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/2883851.2883875,
author = {Rienties, Bart and Toetenel, Lisette},
title = {The impact of 151 learning designs on student satisfaction and performance: social learning (analytics) matters},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883875},
doi = {10.1145/2883851.2883875},
abstract = {An increasing number of researchers are taking learning design into consideration when predicting learning behavior and outcomes across different modules. This study builds on preliminary learning design work that was presented at LAK2015 by the Open University UK. In this study we linked 151 modules and 111.256 students with students' satisfaction and performance using multiple regression models. Our findings strongly indicate the importance of learning design in predicting and understanding performance of students in blended and online environments. In line with proponents of social learning analytics, our primary predictor for academic retention was the amount of communication activities, controlling for various institutional and disciplinary factors. Where possible, appropriate communication tasks that align with the learning objectives of the course may be a way forward to enhance academic retention.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {339–343},
numpages = {5},
keywords = {distance learning, data analytics, collaborative learning},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2536536.2536578,
author = {Ruip\'{e}rez-Valiente, Jos\'{e} A. and Mu\~{n}oz-Merino, Pedro J. and Kloos, Carlos Delgado},
title = {An architecture for extending the learning analytics support in the Khan Academy framework},
year = {2013},
isbn = {9781450323451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2536536.2536578},
doi = {10.1145/2536536.2536578},
abstract = {The Khan Academy platform enables powerful on-line courses in which students can watch videos, solve exercises or earn badges. This platform provides an advanced learning analytics module with useful visualizations for teachers and students. Nevertheless, this learning analytics support can be improved with recommendations and new useful higher level visualizations in order to try to improve the learning process. In this paper, we describe our architecture for processing data from the Khan Academy platform in order to show new higher level learning visualizations and recommendations. The different involved elements of the architecture are presented and the different decisions are justified. In addition, we explain some initial examples of new useful visualizations and recommendations for teachers and students as part of our extension of the learning analytics module for the Khan Academy platform. These examples use data from an undergraduate Physics course developed at Universidad Carlos III de Madrid with more than 100 students using the Khan Academy system.},
booktitle = {Proceedings of the First International Conference on Technological Ecosystem for Enhancing Multiculturality},
pages = {277–284},
numpages = {8},
keywords = {visualizations, recommenders, learning analytics, data processing, architectures},
location = {Salamanca, Spain},
series = {TEEM '13}
}

@inproceedings{10.1145/3706468.3706490,
author = {Strugatski, Alona and Alexandron, Giora},
title = {Applying IRT to Distinguish Between Human and Generative AI Responses to Multiple-Choice Assessments},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706490},
doi = {10.1145/3706468.3706490},
abstract = {Generative AI is transforming the educational landscape, raising significant concerns about cheating. Despite the widespread use of multiple-choice questions (MCQs) in assessments, the detection of AI cheating in MCQ-based tests has been almost unexplored, in contrast to the focus on detecting AI-cheating on text-rich student outputs. In this paper, we propose a method based on the application of Item Response Theory (IRT) to address this gap. Our approach operates on the assumption that artificial and human intelligence exhibit different response patterns, with AI cheating manifesting as deviations from the expected patterns of human responses. These deviations are modeled using Person-Fit Statistics (PFS). We demonstrate that this method effectively highlights the differences between human responses and those generated by premium versions of leading chatbots (ChatGPT, Claude, and Gemini), but that it is also sensitive to the amount of AI cheating in the data. Furthermore, we show that the chatbots differ in their reasoning profiles. Our work provides both a theoretical foundation and empirical evidence for the application of IRT to identify AI cheating in MCQ-based assessments.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {817–823},
numpages = {7},
keywords = {Cheating with AI, separating AI from humans, Item-response theory, person-fit statistics},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3506860.3506897,
author = {Nicoll, Serena and Douglas, Kerrie and Brinton, Christopher},
title = {Giving Feedback on Feedback: An Assessment of Grader Feedback Construction on Student Performance},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506897},
doi = {10.1145/3506860.3506897},
abstract = {Feedback is a critical element of student-instructor interaction: it provides a direct manner for students to learn from mistakes. However, with student to teacher ratios growing rapidly, challenges arise for instructors to provide quality feedback to individual students. While significant efforts have been directed at automating feedback generation, relatively little attention has been given to underlying feedback characteristics. We develop a methodology for analyzing instructor-provided feedback and determining how it correlates with changes in student grades using data from online higher education engineering classrooms. Specifically, we featurize written feedback on individual assignments using Natural Language Processing (NLP) techniques including sentiment analysis, bigram splitting, and Named Entity Recognition (NER) to quantify post-, sentence-, and word-dependent attributes of grader writing. We demonstrate that student grade improvement can be well approximated by a multivariate linear model with average fits across course sections between 67% and 83%. We determine several statistically significant contributors to and detractors from student success contained in instructor feedback. For example, our results reveal that inclusion of student name is significantly correlated with an improvement in post-feedback grades, as is inclusion of specific assignment-related keywords. Finally, we discuss how this methodology can be incorporated into educational technology systems to make recommendations for feedback content from observed student behavior.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {239–249},
numpages = {11},
keywords = {Student engagement, Learning analytics, Instructor feedback},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3706468.3706488,
author = {Shin, Insub and Hwang, Su Bhin and Yoo, Yun Joo and Bae, Sooan and Kim, Rae Yeong},
title = {Comparing Student Preferences for AI-Generated and Peer-Generated Feedback in AI-driven Formative Peer Assessment},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706488},
doi = {10.1145/3706468.3706488},
abstract = {Formative assessment can enhance student learning and improve teaching practices by identifying areas for growth and providing feedback. However, practical obstacles remain, such as time constraints and students’ passive participation and the low quality of peer feedback. Artificial intelligence (AI) has been explored for its potential to automate grading and provide timely feedback, making it a valuable tool in formative assessment. Nevertheless, there is still limited research on how AI can be used effectively in the context of formative peer assessment. In this study, we conducted an AI-driven formative peer assessment with 108 secondary school students. During the peer assessment process, students not only evaluated peers’ responses and received peer-generated feedback, but also evaluated AI-generated responses and received AI-generated feedback. This research focused on analyzing the differences in preference between AI-generated and peer-generated feedback using trace data and dispositional data. In scenarios where student participation was low or the quality of peer feedback was insufficient, students showed a higher preference for AI-generated feedback, demonstrating its potential utility. However, students with high Math Confidence and AI Interest preferred peer-generated feedback. Based on these findings, we will propose practical strategies for implementing AI-driven formative peer assessment.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {159–169},
numpages = {11},
keywords = {Formative Peer Assessment, Feedback, Generative Artificial Intelligence, Trace Data, Dispositional Data},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3136907.3136939,
author = {Su\'{a}rez, Angel and Ternier, Stefaan and Helbig, Ren\'{e} and Specht, Marcus},
title = {DojoAnalytics: A Learning Analytics interoperable component for DojoIBL},
year = {2017},
isbn = {9781450352550},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3136907.3136939},
doi = {10.1145/3136907.3136939},
abstract = {DojoIBL1 is a cloud based platform that provides flexible support for collaborative inquiry-based learning processes. It expands the learning process beyond the classroom walls and brings it to an online setting. Such transition requires teachers and learners to have more means to track and to follow up their progress. Learning Analytics dashboards provide such functionality in form of meaningful visualizations. In this paper we present the DojoAnalytics, a new module of DojoIBL that enables connections with third party Learning Analytics dashboards. In order to demonstrate interoperability with the external dashboards, two use case implementations will be described.},
booktitle = {Proceedings of the 16th World Conference on Mobile and Contextual Learning},
articleno = {2},
numpages = {8},
keywords = {performance, learning analytics, learners', interoperability, Inquiry-based learning},
location = {Larnaca, Cyprus},
series = {mLearn 2017}
}

@inproceedings{10.1145/3506860.3506873,
author = {Zhang, Qian and Rutherford, Teomara},
title = {Grade 5 Students’ Elective Replay After Experiencing Failures in Learning Fractions in an Educational Game: When Does Replay After Failures Benefit Learning?},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506873},
doi = {10.1145/3506860.3506873},
abstract = {Despite theoretical benefits of replayability in educational games, empirical studies have found mixed evidence about the effects of replaying a previously passed game (i.e., elective replay) on students’ learning. Particularly, we know little about behavioral features of students’ elective replay process after experiencing failures (i.e., interruptive elective replay) and the relationships between these features and learning outcomes. In this study, we analyzed 5th graders’ log data from an educational game, ST Math, when they studied fractions—one of the most important but challenging math topics. We systematically constructed interruptive elective replay features by following students’ sequential behaviors after failing a game and investigated the relationships between these features and students’ post-test performance, after taking into account pretest performance and in-game performance. Descriptive statistics of the features we constructed revealed individual differences in the elective replay process after failures in terms of when to start replaying, what to replay, and how to replay. Moreover, a Bayesian multi-model linear regression showed that interruptive elective replay after failures might be beneficial for students if they chose to replay previously passed games when failing at a higher, more difficult level in the current game and if they passed the replayed games.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {98–106},
numpages = {9},
keywords = {Replay, Learning Analytics, Educational Games},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3506860.3506883,
author = {Xia, Meng and Zhao, Yankun and Erol, Mehmet Hamza and Hong, Jihyeong and Kim, Juho},
title = {Understanding Distributed Tutorship in Online Language Tutoring},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506883},
doi = {10.1145/3506860.3506883},
abstract = {With the rise of the gig economy, online language tutoring platforms are becoming increasingly popular. They provide temporary and flexible jobs for native speakers as tutors and allow language learners to have one-on-one speaking practices on demand. However, the lack of stable relationships hinders tutors and learners from building long-term trust. “Distributed tutorship”—temporally discontinuous learning experience with different tutors—has been underexplored yet has many implications for modern learning platforms. In this paper, we analyzed tutorship sequences of 15,959 learners and found that around 40% of learners change to new tutors every session; 44% learners change to new tutors while reverting to previous tutors sometimes; only 16% learners change to new tutors and then fix on one tutor. We also found suggestive evidence that higher distributedness—higher diversity and lower continuity in tutorship—is correlated to slower improvements in speaking performance scores with a similar number of sessions. We further surveyed 519 and interviewed 40 learners and found that more learners preferred fixed tutorship while some do not have it due to various reasons. Finally, we conducted semi-structured interviews with three tutors and one product manager to discuss the implications for improving the continuity in learning under distributed tutorship.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {164–174},
numpages = {11},
keywords = {online tutoring platforms, lifelong learning, learning analytics, language learning, distributed tutorship},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3706468.3706538,
author = {Wang, Yeyu and Carpenter, Zack and Swiecki, Zachari and Shaffer, David Williamson},
title = {Qualitative Parameter Triangulation: A Conceptual and Methodological Framework for Event-Based Temporal Models},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706538},
doi = {10.1145/3706468.3706538},
abstract = {Learning is a complex process that occurs over time. To represent this complex process, interests has been rising in conceptualizing and integrating temporality into model constructions. However, the construction of an event-based temporal model is challenging. Specifically, researchers struggle with translating qualitative heuristics and theoretical hypotheses into quantifiable temporal parameters. Existing methods of parameter derivation also suffer from issues of model transparency and oversimplification of learning contexts. Thus, we proposed a conceptual and methodological framework, Qualitative Parameter Triangulation (QPT), to center human interpretation in model construction. Based on human interpretations, QPT constructs a qualitative loss function and derives temporal parameters using an automatical optimization algorithm. The final step is to check consistency between a global representation with local qualitative evidence given specific learning moments. By presenting a worked example of QPT, we demonstrated the process of maintaining pairwise alignments across interpretation, systematization, and approxi-gation. As a proof of concept, QPT is a feasible framework for determining temporal parameters and constructing event-based temporal models.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {537–546},
numpages = {10},
keywords = {Temporal Analysis, Interpretivity, Methodology, Event-based Modeling.},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706568,
author = {Woodrow, Juliette and Piech, Chris},
title = {Soft Grades: A Calibrated and Accurate Method for Course-Grade Estimation that Expresses Uncertainty},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706568},
doi = {10.1145/3706468.3706568},
abstract = {In traditional educational settings, students are often summarized by a single number—a final course grade—that reflects their performance. While final grades are convenient for reporting or comparison, they oversimplify a student’s true ability and do not express uncertainty. In this paper, we introduce a new item-response model for classroom settings that infers a distribution over student abilities and uses this to represent each student’s final grade as a probability distribution. This approach captures the uncertainty that comes from variations in both student performance and grading processes. Practical applications of our approach include enabling teachers to better understand grading confidence, impute missing assignment scores, and make informed decisions when curving final grades. For students, the model offers probabilistic estimates of their final course grades based on current performance, supporting informed academic decisions such as opting for Pass/Fail grading. We evaluate our model using real-world datasets, showing that the Soft Grades model is well-calibrated and surpasses the state-of-the-art polytomous IRT model in accurately predicting future scores. Additionally, we share a web application and Python scripts to make our model available to teachers and students.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {750–760},
numpages = {11},
keywords = {Item Response Theory, Grade Prediction, Soft Grades, Ability Inference},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706560,
author = {Sun, Chen and Shute, Valerie and Stewart, Angela E.B. and D'Mello, Sidney K.},
title = {The Relationship between Collaborative Problem-Solving Skills and Group-to-Individual Learning Transfer in a Game-based Learning Environment},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706560},
doi = {10.1145/3706468.3706560},
abstract = {Collaborative problem solving (CPS) is viewed as an essential 21st century skill for the modern workforce. Accordingly, researchers have been investigating how to conceptualize, assess, and develop pedagogical approaches to improve CPS. These efforts require theoretically-grounded and empirically-validated frameworks of CPS which have been emerging over the past decade with various levels of validity data. The present paper focuses on validating the generalized competency model (GCM) of CPS with respect to predicting individual learning outcomes following CPS among triads. The GCM consists of three main facets–constructing shared knowledge, negotiation/coordination, and maintaining team function–mapped to behavioral indicators (i.e., observable evidence). It hypothesizes that scores on all three facets should positively predict CPS outcomes, including group-to-individual learning transfer. We tested this hypothesis in a study where 249 students who comprised 83 triads engaged in collaborative gameplay with the Physics Playground game environment remotely via videoconferencing. We found that the only CPS facet predicting individual physics learning was maintaining team function, after accounting for pretest scores, students’ perceptions of team collaboration, and their perceived physics self-efficacy. This facet was also the only significant predictor of individual learning regardless of how facet scores were computed (i.e., reverse coding of negative indicators, separating the sums of positive and negative indicators, and no reverse coding of negative indicators). Implications for the GCM and other CPS frameworks are discussed.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {680–689},
numpages = {10},
keywords = {collaborative problem solving, framework validation, physics learning outcome, game-based learning, triads},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706510,
author = {Martins Van Jaarsveld, Gabrielle and Wong, Jacqueline and Baars, Martine and Specht, Marcus and Paas, Fred},
title = {Scaling goal-setting interventions in higher education using a conversational agent: Examining the effectiveness of guidance and adaptive feedback},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706510},
doi = {10.1145/3706468.3706510},
abstract = {Goal setting is the first and driving stage of the self-regulated learning cycle. Studies have shown that supporting goal setting is an effective means of improving academic performance among higher education students. However, doing so can be complex and resource intensive. In this study, a goal-setting conversational agent was designed and deployed to support higher education students in setting academic goals. Across 5-weeks, we tested the effects of goal-setting prompts (guided vs. unguided) and adaptive feedback (with vs. without) when delivered via a goal-setting conversational agent. We explored the effects of these supports (i.e., guidance and feedback) on students’ 1) goal quality and 2) goal attainment. Findings showed that guidance and feedback combined had the largest positive effect on goal quality. They also revealed that guidance alone produced initially high-quality goals which decreased in quality overtime, whereas feedback had a delayed but cumulative effect on quality across multiple goal setting iterations. However, neither guidance nor feedback had significant effects on goal attainment, and there was no significant relationship between goal quality and attainment. This study provides insights into how a goal-setting conversational agent and adaptive feedback can be used to support the academic goal setting process for higher education students.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {328–338},
numpages = {11},
keywords = {Adaptive Support, Conversational Agents, Feedback, Self-Regulated Learning},
location = {
},
series = {LAK '25}
}

@article{10.5555/3417608.3417615,
author = {Phillips, Chistopher and Eickholt, Jesse},
title = {LAGradebook: a tool for course-level comparative learning analytics},
year = {2020},
issue_date = {April 2020},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {35},
number = {6},
issn = {1937-4771},
abstract = {Learning analytics aims to collect and analyze data to improve learning outcomes. Several tools for learning analytics exist and have been successfully used in the context of higher education but their cost and related institutional challenges can be limiting factors. To complement larger, institutional efforts for learning analytics adoption, we have developed a tool to support smaller scale applications. The tool ingests raw, student assessment data and produces a rich, learning analytics enabled spreadsheet. The spreadsheet calculates descriptive statistics and relative performance and contains pair-wise correlation metrics of assessment items and individual student reports. The reports and comparative performance measures help students monitor their relative performance. The low-cost and course-specific nature of the tool supports increased access to learning analytics in more classrooms.},
journal = {J. Comput. Sci. Coll.},
month = apr,
pages = {66–73},
numpages = {8}
}

@inproceedings{10.1145/2330601.2330642,
author = {Leony, Derick and Pardo, Abelardo and de la Fuente Valent\'{\i}n, Luis and de Castro, David S\'{a}nchez and Kloos, Carlos Delgado},
title = {GLASS: a learning analytics visualization tool},
year = {2012},
isbn = {9781450311113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2330601.2330642},
doi = {10.1145/2330601.2330642},
abstract = {The use of technology in every day tasks enables the possibility to collect large amounts of observations of events taking place in different environments. Most tools are capable of storing a detailed account of the operations executed by users in certain files commonly known as logs. These files can be further analyzed to infer information that is not directly visible such as the most popular applications, times of the day with highest activity, calories burnt after a running session, etc. Graphic visualizations of this data can be used to support this type of analysis as shown in [1]. Visualization can also be applied in the domain of learning experiences to track and analyse the data obtained from both learners and instructors. There are several tools that have been proposed in specific environments such as, for example, in personal learning environments [5], to foster self-reflection and awareness [2], and to support instructors in web-based distance learning [3]. These visualizations need to take into account aspects such as how to access and protect personal data, filter management, multi-user support and availability. In this paper, the web-based visualization platform GLASS (Gradient's Learning Analytics System) is presented. The architecture of the tool has been conceived to support a large number of modular visualizations derived from a common dataset containing a large number of recorded events. The tool was developed following a bottom-up methodology to provide a set of basic operations required by any visualization. The design goal is to provide a highly versatile, modular platform that simplifies the implementation of new visualizations.The main functionality elements considered in GLASS are database access, module management, visualization parameters, and the web interface. The platform uses datasets stored using the CAM schema (Contextualized Attention Metadata) [6]. This schema allows to capture events occurring during the use of various computer applications which, in our case, are the tools used by students when working in a learning environment. The process to obtain events from learning environments has been described in [4]. GLASS is able to connect to more than one CAM database, thus allowing access to events obtained in different contexts.The tool is extensible through the installation of modules. A module is a structured set of scripts and resources that, given a dataset of events and a set of filters, generates at least one visualization. In order to simplify the development of new modules, the platform provides an API to manage common visualizations settings such as the date range and other typical filters. A visualization may include a simpler version suitable to be displayed in the user's Dashboard, which is the entry page of the application. Figure 1 shows an example of dashboard in GLASS. Additionally, visualizations can be exported as HTML code to be embedded in another website.The GLASS architecture consists of four layers: data layer, code base, modules and visualizations, as depicted in Figure 2. The data layer is composed of a set of CAM databases and a database to store the platform parameters. The code base is in charge of the main functionalities of GLASS regarding module and user management and interfaces. Modules must comply with the platform specifications to generate visualizations and the settings that can affect their appearance. Currently, the tool includes a default module that provides two visualizations as shown in Figure 1): a frequency time line of activity events and a bar-chart with grouped bars of events generated by different user groups (e.g. events from students individually, or groups). The default module also serves as an example of how to develop a additional modules.Currently, GLASS is able to support new visualizations and is undergoing additional testing in different learning scenarios. Preliminary results obtained from user tests indicate that visualizations need to be very intuitive for both instructors and learners. The current development effort is focused on providing visualizations that show the most-common learners events and the most active learners in a given context. To encourage its use in other institutions, the tool has been released with an open source license and can be obtained from http://glass.mozart.gast.it.uc3m.es. A video demonstrating the tool is available at http://bit.ly/glass-lak12.},
booktitle = {Proceedings of the 2nd International Conference on Learning Analytics and Knowledge},
pages = {162–163},
numpages = {2},
keywords = {visualization system, visualization framework, learning analytics},
location = {Vancouver, British Columbia, Canada},
series = {LAK '12}
}

@inproceedings{10.1145/3706468.3706503,
author = {Zhong, Lina and Lang, Weijie and Rong, Jia and Chen, Guanliang and Fan, Miao},
title = {Enhancing Motivation and Learning in Primary School History Classrooms: The Impact of Virtual Reality},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706503},
doi = {10.1145/3706468.3706503},
abstract = {Conventional classroom instruction often struggles to effectively convey cultural heritage due to constraints in spatial and temporal dimensions, limiting students’ ability to fully engage with and appreciate historical content. In contrast, virtual reality (VR) technology offers a human-centered, immersive way to present cultural heritage, creating a dynamic digital experience particularly beneficial when physical access to heritage sites is unavailable. This study investigates whether VR-based learning can enhance students’ performance in cultural education compared to traditional teaching methods. A sample of 228 primary school students from Grades 5 and 6 was randomly assigned to one of two groups: a high-visual engagement group (VR with 360° video) or a low-visual engagement group (static video and textbook). The findings revealed that students in the high-visual engagement group achieved higher levels of intrinsic motivation and demonstrated greater learning improvements than their counterparts in the low-visual engagement group. Furthermore, the study identified negative user experiences as a significant factor moderating the connection between intrinsic motivation and learning outcomes. These results highlight the value of integrating VR into conventional teaching practices, showcasing its potential to enhance student engagement and improve educational outcomes in history and cultural studies.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {272–282},
numpages = {11},
keywords = {Virtual reality, Cultural education, K-6 learning, Structural equation modeling, Fornell-Larcker criterion, PLS-SEM model, Motivation, Engagement},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3012430.3012534,
author = {Caeiro-Rodr\'{\i}guez, Manuel and Conde, Miguel \'{A}. and Guenaga, Mariluz and Hern\'{a}ndez-Garc\'{\i}a, \'{A}ngel and Larra\~{n}aga, Mikel and Mart\'{\i}nez-Mon\'{e}s, Alejandra and Mu\~{n}oz-Merino, Pedro J. and Pastor-Vargas, Rafael and Perallos-Ruiz, Asier and Rodr\'{\i}guez-Conde, Mar\'{\i}a-Jos\'{e}},
title = {SNOLA: Spanish network of learning analytics},
year = {2016},
isbn = {9781450347471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3012430.3012534},
doi = {10.1145/3012430.3012534},
abstract = {We are living the end of a first wave of transformation of education caused by the incorporation of ICT to educational processes. In the time of MOOCs, informal and social learning, gamification, game-based learning, etc., we are facing a second wave of transformation. This time the wave is cross-technological, and focuses on the study, understanding and improvement of educational processes through the analysis of the educational data collected along such processes, in what is known as learning analytics (LA).There are currently three main barriers to the diffusion and use of LA in educational institutions and companies: the dispersion and fragmentation of LA propositions and solutions, the lack of availability of resources for integration of LA, and the lack of professionals trained in the implementation of LA solutions. SNOLA (Spanish Network of Learning Analytics) emerges as a reflection of Spanish research groups focusing on learning analytics, and as a means to overcome these barriers in Spain from a multidisciplinary networking approach.},
booktitle = {Proceedings of the Fourth International Conference on Technological Ecosystems for Enhancing Multiculturality},
pages = {313–317},
numpages = {5},
keywords = {learning analytics, improvement of learning, educational processes, education, digital society, ICT},
location = {Salamanca, Spain},
series = {TEEM '16}
}

@inproceedings{10.1145/3636555.3636936,
author = {Xu, Zhen and Olson, Joseph and Pochinki, Nicole and Zheng, Zhijian and Yu, Renzhe},
title = {Contexts Matter but How? Course-Level Correlates of Performance and Fairness Shift in Predictive Model Transfer},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636936},
doi = {10.1145/3636555.3636936},
abstract = {Learning analytics research has highlighted that contexts matter for predictive models, but little research has explicated how contexts matter for models’ utility. Such insights are critical for real-world applications where predictive models are frequently deployed across instructional and institutional contexts. Building upon administrative records and behavioral traces from 37,089 students across 1,493 courses, we provide a comprehensive evaluation of performance and fairness shifts of predictive models when transferred across different course contexts. We specifically quantify how differences in various contextual factors moderate model portability. Our findings indicate an average decline in model performance and inconsistent directions in fairness shifts, without a direct trade-off, when models are transferred across different courses within the same institution. Among the course-to-course contextual differences we examined, differences in admin features account for the largest portion of both performance and fairness loss. Differences in student composition can simultaneously amplify drops in performance and fairness while differences in learning design have a greater impact on performance degradation. Given these complexities, our results highlight the importance of considering multiple dimensions of course contexts and evaluating fairness shifts in addition to performance loss when conducting transfer learning of predictive models in education.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {713–724},
numpages = {12},
keywords = {Algorithmic Fairness, Higher Education, Intersectionality, Learning Management System, Predictive Analytics, Transfer Learning},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3706468.3706551,
author = {Yun, Joy and Nie, Allen and Brunskill, Emma and Demszky, Dorottya},
title = {Exploring the Benefit of Customizing Feedback Interventions For Educators and Students With Offline Contextual Multi-Armed Bandits},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706551},
doi = {10.1145/3706468.3706551},
abstract = {Automated feedback to teachers powered by natural language processing has been successful at improving instruction and student outcomes across various learning contexts. However, existing one-size-fits-all feedback interventions may not be equally effective for all educators and students. Understanding whether and how customization might enhance the effectiveness of automated feedback is a timely issue. In this paper we investigate this using data from a randomized controlled trial conducted on a peer SAT math tutoring program where tutors and/or learners were provided with post session feedback on their discourse during tutoring. We employ a partially data-driven, partially expert knowledge driven, process to propose some potential context-specific intervention policies. We then use offline contextual multi-armed bandit policy evaluation measures to estimate the potential performance of these interventions compared to providing a single intervention designed to maximize overall average performance. Our preliminary results did not show substantial or significant gains, but suggest that there may be value in providing differentiated interventions. More generally, our results point to the potential for such analysis to be used as a hypothesis-generating tool for future empirical studies.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {944–949},
numpages = {6},
keywords = {Education, Automated Feedback, Reinforcement Learning, Offline Contextual Multi-Armed Bandit, Policy Evaluation},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/2445196.2445431,
author = {Grafsgaard, Joseph F. and Wiggins, Joseph B. and Boyer, Kristy Elizabeth and Wiebe, Eric N. and Lester, James C.},
title = {Modeling student programming with multimodal learning analytics (abstract only)},
year = {2013},
isbn = {9781450318686},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2445196.2445431},
doi = {10.1145/2445196.2445431},
abstract = {Understanding how students solve computational problems is central to computer science education research. This goal is facilitated by recent advances in the availability and analysis of detailed multimodal data collected during student learning. Drawing on research into student problem-solving processes and findings on human posture and gesture, this poster utilizes a multimodal learning analytics framework that links automatically identified posture and gesture features with student problem-solving and dialogue events during one-on-one human tutoring of introductory computer science. The findings provide new insight into how bodily movements occur during computer science tutoring, and lay the foundation for programming feedback tools and deep analyses of student learning processes.},
booktitle = {Proceeding of the 44th ACM Technical Symposium on Computer Science Education},
pages = {736},
numpages = {1},
keywords = {tutoring, posture, nonverbal behavior, multimodal learning analytics, gesture, computer science education research, computer programming},
location = {Denver, Colorado, USA},
series = {SIGCSE '13}
}

@inproceedings{10.1145/3358961.3358999,
author = {Riquelme, Fabi\'{a}n and N\"{o}el, Ren\'{e} and Munoz, Roberto and Lean, Roberto Mac},
title = {How to enhance collaboration in Latin America to be more competitive worldwide? a multimodal learning analytics approach},
year = {2020},
isbn = {9781450376792},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3358961.3358999},
doi = {10.1145/3358961.3358999},
abstract = {We propose the creation of an interdisciplinary research group related to Multimodal Learning Analytics and Social Network Analysis for the study of collaborative work groups in Latin America. We believe that the advanced work so far justifies the viability of the proposal.},
booktitle = {Proceedings of the IX Latin American Conference on Human Computer Interaction},
articleno = {36},
numpages = {4},
keywords = {social networks, multimodal learning analytics, computer-supported cooperative work, collaboration},
location = {Panama City, Panama},
series = {CLIHC '19}
}

@inproceedings{10.1145/3706468.3706521,
author = {Lee, HaeJin and Belitz, Clara and Nasiar, Nidhi and Bosch, Nigel},
title = {XAI Reveals the Causes of Attention Deficit Hyperactivity Disorder (ADHD) Bias in Student Performance Prediction},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706521},
doi = {10.1145/3706468.3706521},
abstract = {Uncovering algorithmic bias related to sensitive attributes is crucial. However, understanding the underlying causes of bias is even more important to ensure fairer outcomes. This study investigates bias associated with Attention Deficit Hyperactivity Disorder (ADHD) in a machine learning model predicting students’ test scores. While fairness metrics did not reveal significant bias, potential subtle bias indicated by variations in model performance for students with ADHD was observed. To uncover causes of this potential bias, we correlated SHapley Additive exPlanations (SHAP) values with the model’s prediction errors, identifying the features most strongly associated with increasing prediction errors. Behavioral and self-reported survey features designed to measure students’ use of effective learning strategies were identified as potential causes of the model underestimating test grades for students with ADHD. Behavioral features had a stronger correlation between absolute SHAP values and prediction errors (up to r =.354, p =.013) for students with ADHD than for those without ADHD. Students with ADHD often use unique yet effective approaches to studying in online learning environments—approaches that may not be fully captured by traditional measures of typical student behaviors. These insights suggest adjusting feature design to better account for students with ADHD and mitigate bias.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {418–428},
numpages = {11},
keywords = {Explainable AI, Algorithmic bias, Machine Learning, Self-regulated Learning, Attention Deficit Hyperactivity Disorder},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3027385.3027428,
author = {Scheffel, Maren and Drachsler, Hendrik and Kreijns, Karel and de Kraker, Joop and Specht, Marcus},
title = {Widget, widget as you lead, I am performing well indeed! using results from an exploratory offline study to inform an empirical online study about a learning analytics widget in a collaborative learning environment},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027428},
doi = {10.1145/3027385.3027428},
abstract = {The collaborative learning processes of students in online learning environments can be supported by providing learning analytics-based visualisations that foster awareness and reflection about an individual's as well as the team's behaviour and their learning and collaboration processes. For this empirical study we implemented an activity widget into the online learning environment of a live five-months Master course and investigated the predictive power of the widget indicators towards the students' grades and compared the results to those from an exploratory study with data collected in previous runs of the same course where the widget had not been in use. Together with information gathered from a quantitative as well as a qualitative evaluation of the activity widget during the course, the findings of this current study show that there are indeed predictive relations between the widget indicators and the grades, especially those regarding responsiveness, and indicate that some of the observed differences in the last run could be attributed to the implemented activity widget.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {289–298},
numpages = {10},
keywords = {tool evaluation, statistical analysis, learning analytics},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/2968219.2971383,
author = {Pirkl, Gerald and Hevesi, Peter and Lukowicz, Paul and Klein, Pascal and Heisel, Carina and Gr\"{o}ber, Sebastian and Kuhn, Jochen and Sick, Bernhard},
title = {Any problems? a wearable sensor-based platform for representational learning-analytics.},
year = {2016},
isbn = {9781450344623},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2968219.2971383},
doi = {10.1145/2968219.2971383},
abstract = {We describe in this work a sensor-based learning platform which supports both the teacher and the learner during exercises. We use a combination of eye tracker, sensor pen and exercise texts to capture the progress of learners. The eye tracker retrieves information about the gaze, for example reading or scanning for key words; the sensor pen captures trends like number of words or the pressure applied to the paper. Combining this information, the platform should be used to indicate problems of the learner to the teacher. Besides presenting the data information to the teacher, we work on advancing the platform to an adaptive system, which could give individual feedback to the learners themselves according to their individual cognitive and affective requirements.},
booktitle = {Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct},
pages = {353–356},
numpages = {4},
keywords = {sensor supported eduction, sensor pen, eye tracker},
location = {Heidelberg, Germany},
series = {UbiComp '16}
}

@inproceedings{10.1145/3576050.3576092,
author = {Quick, Joshua Dallas and Motz, Benjamin and Morrone, Anastasia},
title = {Lost in Translation: Determining the Generalizability of Temporal Models across Course Contexts&nbsp;},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576092},
doi = {10.1145/3576050.3576092},
abstract = {A common activity in learning analytics research is to demonstrate a new analytical technique by applying it to data from a single course.&nbsp; We explore whether the value of an analytical approach might generalize across course contexts.&nbsp; Accordingly, we conduct a conceptual replication of a well-cited temporal modeling study using self-regulated learning (SRL) taxonomies. We attempt to conceptually replicate this previous work through the analysis of 411 students across 19 courses’ trace event data. Using established SRL categorizations, learner actions are sequenced to identify regular clusters of interaction through hierarchical clustering methods. These clusters are then compared with the entire data corpus and each other through the development of first-order Markov models to develop process maps. Our findings indicate that, although some general patterns of SRL can generalize, these results are more limited at higher scales. Comparing these clusters of interaction along students’ performance in courses also indicates some relationships between activity and outcomes, though this finding is also limited in relation to the complexity introduced by scaling out these methods. We discuss how these temporal models should be viewed when making descriptive and qualitative inferences about students’ activity in digital learning environments.&nbsp;&nbsp;},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {273–283},
numpages = {11},
keywords = {Generalizability, Process Modeling, Replication, Self-regulated Learning, Trace Data},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3706468.3706500,
author = {Duan, Zhangqi and Fernandez, Nigel and Hicks, Alexander and Lan, Andrew},
title = {Test Case-Informed Knowledge Tracing for Open-ended Coding Tasks},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706500},
doi = {10.1145/3706468.3706500},
abstract = {Open-ended coding tasks, which ask students to construct programs according to certain specifications, are common in computer science education. Student modeling can be challenging since their open-ended nature means that student code can be diverse. Traditional knowledge tracing (KT) models that only analyze response correctness may not fully capture nuances in student knowledge from student code. In this paper, we introduce Test case-Informed Knowledge Tracing for Open-ended Coding (TIKTOC), a framework to simultaneously analyze and predict both open-ended student code and whether the code passes each test case. We augment the existing CodeWorkout dataset with the test cases used for a subset of the open-ended coding questions, and propose a multi-task learning KT method to simultaneously analyze and predict 1) whether a student’s code submission passes each test case and 2) the student’s open-ended code, using a large language model as the backbone. We quantitatively show that these methods outperform existing KT methods for coding that only use the overall score a code submission receives. We also qualitatively demonstrate how test case information, combined with open-ended code, helps us gain fine-grained insights into student knowledge.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {238–248},
numpages = {11},
keywords = {Computer Science Education, Large Language Models, Open-ended Coding Questions, Test Cases},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/2460296.2460339,
author = {Monroy, Carlos and Rangel, Virginia Snodgrass and Whitaker, Reid},
title = {STEMscopes: contextualizing learning analytics in a K-12 science curriculum},
year = {2013},
isbn = {9781450317856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2460296.2460339},
doi = {10.1145/2460296.2460339},
abstract = {In this paper, we discuss a scalable approach for integrating learning analytics into an online K-12 science curriculum. A description of the curriculum and the underlying pedagogical framework is followed by a discussion of the challenges to be tackled as part of this integration. We also include examples of data visualization based on real student and teacher data. With more than one million students and fifty thousand teachers using the curriculum, a massive and rich dataset is continuously updated. This repository depicts teacher and students usage of an inquiry-based science program, and offers exciting opportunities to leverage research to improve both teaching and learning. The growing dataset, with more than a hundred million items of activity in six months, also poses technical challenges such as data storage, complex aggregation and analysis with broader implications for pedagogy, big data, and learning.},
booktitle = {Proceedings of the Third International Conference on Learning Analytics and Knowledge},
pages = {210–219},
numpages = {10},
keywords = {online curriculum, learning analytics, big data, STEM education},
location = {Leuven, Belgium},
series = {LAK '13}
}

@inproceedings{10.1145/3706468.3706502,
author = {Cheng, Yixin and Guan, Rui and Li, Tongguang and Rakovi\'{c}, Mladen and Li, Xinyu and Fan, Yizhou and Jin, Flora and Tsai, Yi-Shan and Ga\v{s}evi\'{c}, Dragan and Swiecki, Zachari},
title = {Self-regulated Learning Processes in Secondary Education: A Network Analysis of Trace-based Measures},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706502},
doi = {10.1145/3706468.3706502},
abstract = {While the capacity to self-regulate has been found to be crucial for secondary school students, prior studies often rely on self-report surveys and think-aloud protocols that present notable limitations in capturing self-regulated learning (SRL) processes. This study advances the understanding of SRL in secondary education by using trace data to examine SRL processes during multi-source writing tasks, with higher education participants included for comparison. We collected fine-grained trace data from 66 secondary school students and 59 university students working on the same writing tasks within a shared SRL-oriented learning environment. The data were labelled using Bannert’s validated SRL coding scheme to reflect specific SRL processes, and we examined the relationship between these processes, essay performance, and educational levels. Using epistemic network analysis (ENA) to model and visualise the interconnected SRL processes in Bannert’s coding scheme, we found that: (a) secondary school students predominantly engaged in three SRL processes—Orientation, Re-reading, and Elaboration/Organisation; (b) high-performing secondary students engaged more in Re-reading, while low-performing students showed more Orientation process; and (c) higher education students exhibited more diverse SRL processes such as Monitoring and Evaluation than their secondary education counterparts, who heavily relied on following task instructions and rubrics to guide their writing. These findings highlight the necessity of designing scaffolding tools and developing teacher training programs to enhance awareness and development of SRL skills for secondary school learners.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {260–271},
numpages = {12},
keywords = {Self-regulated learning, K-12 education, Epistemic network analysis, Secondary education},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706535,
author = {Williamson, Kimberly and Kizilcec, Rene and Fath, Sean and Heffernan, Neil},
title = {Algorithm Appreciation in Education: Educators Prefer Complex over Simple Algorithms},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706535},
doi = {10.1145/3706468.3706535},
abstract = {Algorithm aversion among educators can pose a challenge to the adoption of AI tools in education, especially when complex algorithms are involved. This study investigates how providing explanations for a complex algorithm in an intelligent tutoring system (ITS) affects educators’ attitudes, trust, and willingness to adopt the tool. In two randomized experiments (N = 570), we compare educator preferences between a simple heuristic algorithm and a complex (Bayesian Knowledge Tracing) algorithm, focusing on how explanations for the complex algorithm can improve attitudes and adoption. Surprisingly, we found that educators generally preferred the complex over the simple algorithm, and explanations did not improve attitudes or adoption intentions, even when educators had to explain the complex algorithm’s predictions. The complex algorithm scored lower on informational fairness than the simple one, considering it is less transparent, and the explanation was insufficient to overcome this. Overall, the findings suggest that widespread algorithm aversion may have evolved into algorithm appreciation, at least in the context of widely used technologies like ITS.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {527–536},
numpages = {10},
keywords = {Algorithm Aversion, Algorithm Appreciation, AI Literacy, XAI},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706473,
author = {Hui, Bowen and Adeyemi, Opey and Phan, Kiet and Schoenit, Justin and Akins, Seth and Khademi, Keyvan},
title = {Diversity Considerations in Team Formation Design, Algorithm, and Measurement},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706473},
doi = {10.1145/3706468.3706473},
abstract = {Building teams that foster equitable interaction provides the foundation for a positive collaborative learning experience. Existing literature shows that many context-specific algorithms exist to help instructors form teams automatically in large classes, but the field lacks general guidelines for selecting a suitable algorithm in a given pedagogical context and lacks a general evaluation approach that allows for the methodological comparison of these algorithms. This paper presents a general-purpose team formation algorithm that considers diversity and inclusion in its design. We also describe an evaluation framework with diversity metrics to assess team compositions using synthetically generated student data and real class data. Our simulation and classroom experiments show that our algorithm performs competitively against three state-of-the-art algorithms. We hope this work contributes to building a more equitable and collaborative learning environment for students.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {36–46},
numpages = {11},
keywords = {Team formation, evaluation, diversity metrics, hill climbing},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706539,
author = {Zhu, Wangda and Xing, Wanli and Lyu, Bailing and Li, Chenglu and Zhang, Fan and Li, Hai},
title = {Bridging the Gender Gap: The Role of AI-Powered Math Story Creation in Learning Outcomes},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706539},
doi = {10.1145/3706468.3706539},
abstract = {Addressing the gender gap in K-12 math education is essential for providing equitable learning opportunities, as historical disparities in engagement, performance, and confidence between male and female students in mathematics are often linked to educational biases. Integrating Generative AI (GAI) into math education shows promise for bridging the gender gap in K12 math learning. This study proposes an innovative pedagogy and platform that enables students to create math stories powered by GAI, enhancing their conceptual understanding of key mathematical ideas. The platform was implemented in two K5 schools to evaluate its effectiveness and mechanism (N = 86). Pre- and post-intervention surveys and usage logs indicated significant improvements in students’ learning outcomes regarding Math Question (MQ) skills and Math Story (MS) skills. Bayes SEM further modeled the mechanism: students’ creating math stories powered by GAI significantly improves MS, which further improves MQ. We further found female students were significantly more engaged in creating stories on this platform and gained more improvement on MQ than male students. The results suggest that AI-powered math story creation can be an effective tool for deepening students’ mathematical learning outcomes and has the potential to mitigate the gender gap.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {918–923},
numpages = {6},
keywords = {Gender gap, Generative AI, Math story, Learning outcomes},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3576050.3576079,
author = {Fahid, Fahmid Morshed and Lee, Seung and Mott, Bradford and Vandenberg, Jessica and Acosta, Halim and Brush, Thomas and Glazewski, Krista and Hmelo-Silver, Cindy and Lester, James},
title = {Effects of Modalities in Detecting Behavioral Engagement in Collaborative Game-Based Learning},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576079},
doi = {10.1145/3576050.3576079},
abstract = {Collaborative game-based learning environments have significant potential for creating effective and engaging group learning experiences. These environments offer rich interactions between small groups of students by embedding collaborative problem solving within immersive virtual worlds. Students often share information, ask questions, negotiate, and construct explanations between themselves towards solving a common goal. However, students sometimes disengage from the learning activities, and due to the nature of collaboration, their disengagement can propagate and negatively impact others within the group. From a teacher's perspective, it can be challenging to identify disengaged students within different groups in a classroom as they need to spend a significant amount of time orchestrating the classroom. Prior work has explored automated frameworks for identifying behavioral disengagement. However, most prior work relies on a single modality for identifying disengagement. In this work, we investigate the effects of using multiple modalities to detect disengagement behaviors of students in a collaborative game-based learning environment. For that, we utilized facial video recordings and group chat messages of 26 middle school students while they were interacting with Crystal Island: EcoJourneys, a game-based learning environment for ecosystem science. Our study shows that the predictive accuracy of a unimodal model heavily relies on the modality of the ground truth, whereas multimodal models surpass the unimodal models, trading resources for accuracy. Our findings can benefit future researchers in designing behavioral engagement detection frameworks for assisting teachers in using collaborative game-based learning within their classrooms.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {208–218},
numpages = {11},
keywords = {Behavioral engagement, Collaborative game-based learning, K-12 education, Multimodal learning analytics},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3506860.3506922,
author = {Praharaj, Sambit and Scheffel, Maren and Schmitz, Marcel and Specht, Marcus and Drachsler, Hendrik},
title = {Towards Collaborative Convergence: Quantifying Collaboration Quality with Automated Co-located Collaboration Analytics},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506922},
doi = {10.1145/3506860.3506922},
abstract = {Collaboration is one of the four important 21st-century skills. With the pervasive use of sensors, interest on co-located collaboration (CC) has increased lately. Most related literature used the audio modality to detect indicators of collaboration (such as total speaking time and turn taking). CC takes place in physical spaces where group members share their social (i.e., non-verbal audio indicators like speaking time, gestures) and epistemic space (i.e., verbal audio indicators like the content of the conversation). Past literature has mostly focused on the social space to detect the quality of collaboration. In this study, we focus on both social and epistemic space with an emphasis on the epistemic space to understand different evolving collaboration patterns and collaborative convergence and quantify collaboration quality. We conduct field trials by collecting audio recordings in 14 different sessions in a university setting while the university staff and students collaborate over playing a board game to design a learning activity. This collaboration task consists of different phases with each collaborating member having been assigned a pre-fixed role. We analyze the collected group speech data to do role-based profiling and visualize it with the help of a dashboard.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {358–369},
numpages = {12},
keywords = {multimodal learning analytics, collaboration analytics, collaboration, co-located collaboration},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3303772.3303840,
author = {Wise, Alyssa Friend and Cui, Yi},
title = {Top Concept Networks of Professional Education Reflections},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303840},
doi = {10.1145/3303772.3303840},
abstract = {This study explores the application of computational techniques to extract information about dental students' developing conceptions of their profession from digital reflective journal entries. Top concept networks were created for two cohorts of students at the beginning and end of their four-year program. A shift from a collection of general notions about becoming a professional to a more integrated, patient-centered conceptualization was found for both cohorts. The two groups initially differed in their perception of dental school (a mechanism for being able to work as a dentist versus a place to learn the skills to serve patients well) and subsequently in the extent of attention they paid to the feelings of their patients and themselves, as well as the continual growth of skill after graduation. Several useful linguistic markers were identified for examining these same issues in other cohorts. The results suggest that top concept networks can offer a useful window into students' developing conceptions of their profession. This kind of information can support student success on a macro level by offering feedback on existing curricula / informing learning designs to cultivate desired conceptions, and on a micro level through identifying particular ways individuals align with and diverge from the common trajectories.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {260–264},
numpages = {5},
keywords = {professional education, concept network, Reflection},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/2330601.2330629,
author = {Buckingham Shum, Simon and Crick, Ruth Deakin},
title = {Learning dispositions and transferable competencies: pedagogy, modelling and learning analytics},
year = {2012},
isbn = {9781450311113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2330601.2330629},
doi = {10.1145/2330601.2330629},
abstract = {Theoretical and empirical evidence in the learning sciences substantiates the view that deep engagement in learning is a function of a complex combination of learners' identities, dispositions, values, attitudes and skills. When these are fragile, learners struggle to achieve their potential in conventional assessments, and critically, are not prepared for the novelty and complexity of the challenges they will meet in the workplace, and the many other spheres of life which require personal qualities such as resilience, critical thinking and collaboration skills. To date, the learning analytics research and development communities have not addressed how these complex concepts can be modelled and analysed, and how more traditional social science data analysis can support and be enhanced by learning analytics. We report progress in the design and implementation of learning analytics based on a research validated multidimensional construct termed "learning power". We describe, for the first time, a learning analytics infrastructure for gathering data at scale, managing stakeholder permissions, the range of analytics that it supports from real time summaries to exploratory research, and a particular visual analytic which has been shown to have demonstrable impact on learners. We conclude by summarising the ongoing research and development programme and identifying the challenges of integrating traditional social science research, with learning analytics and modelling.},
booktitle = {Proceedings of the 2nd International Conference on Learning Analytics and Knowledge},
pages = {92–101},
numpages = {10},
keywords = {transferable skills, learning power, learning how to learn, learning dispositions, learning analytics, effective lifelong learning inventory, educational assessment, 21st century skills},
location = {Vancouver, British Columbia, Canada},
series = {LAK '12}
}

@inproceedings{10.1145/3576050.3576106,
author = {Cloude, Elizabeth B. and Baker, Ryan S. and Fouh, Eric},
title = {Online help-seeking occurring in multiple computer-mediated conversations affects grades in an introductory programming course},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576106},
doi = {10.1145/3576050.3576106},
abstract = {Computing education researchers often study the impact of online help-seeking behaviors that occur across multiple online resources in isolation. Such separation fails to capture the interconnected nature of online help-seeking behaviors that occur across multiple online resources and its affect on course grades. This is particularly important for programming education, which arguably has more online resources to seek help from other people (e.g., computer-mediated conversations) than other majors. Using data from an introductory programming course (CS1) at a large US university, we found that students (n=301) sought help in multiple computer-mediated conversations, both Q&amp;A forum and online office hours (OHQ), differently. Results showed the more prior knowledge about programming students had, the more they sought help in the Q&amp;A compared to students with less prior knowledge. In general, higher-performing students sought help online in the Q&amp;A more than the lower-performing groups on all the homework assignments, but not for the OHQ. By better understanding how students seek help online across multiple modalities of computer-mediated conversations and the relationship between help-seeking and grades, we can re-design online resources that best support all students in introductory programming courses at scale.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {378–387},
numpages = {10},
keywords = {CS1, Learning analytics at scale, Online help-seeking, Programming},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576104,
author = {Zhang, Fan and Xing, Wanli and Li, Chenglu},
title = {Predicting Students’ Algebra I Performance using Reinforcement Learning with Multi-Group Fairness},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576104},
doi = {10.1145/3576050.3576104},
abstract = {Numerous studies have successfully adopted learning analytics techniques such as machine learning (ML) to address educational issues. However, limited research has addressed the problem of algorithmic bias in ML. In the few attempts to develop strategies to concretely mitigate algorithmic bias in education, the focus has been on debiasing ML models with single group membership. This study aimed to propose an algorithmic strategy to mitigate bias in a multi-group context. The results showed that our proposed model could effectively reduce algorithmic bias in a multi-group setting while retaining competitive accuracy. The findings implied that there could be a paradigm shift from focusing on debiasing a single group to multiple groups in educational attempts on ML.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {657–662},
numpages = {6},
keywords = {fair AI, math achievement prediction, multi-group fairness, reinforcement learning},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3506860.3506877,
author = {Vinker, Efrat and Rubinstein, Amir},
title = {Mining Code Submissions to Elucidate Disengagement in a Computer Science MOOC},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506877},
doi = {10.1145/3506860.3506877},
abstract = {Despite the growing prevalence of Massive Open Online Courses (MOOCs) in the last decade, using them effectively is still challenging. Particularly, when MOOCs involve teaching programming, learners often struggle with writing code without sufficient support, which may increase frustration, attrition, and eventually dropout. In this study, we assess the pedagogical design of a fresh introductory computer science MOOC. Keeping in mind MOOC “end-user” instructors, our analyses are based merely on features easily accessible from code submissions, and methods that are relatively simple to apply and interpret. Using visual data mining we discover common patterns of&nbsp;behavior, provide insights on content that may require reevaluation and detect critical points of attrition in the course timeline. Additionally, we extract students’ code submission profiles that reflect various aspects of engagement and performance. Consequently, we predict disengagement towards programming using classic machine learning methods. To the best of our knowledge, our definition for attrition in terms of disengagement towards programming is novel as it suits the unique active hands-on nature of programming. To our perception, the results emphasize that more attention and further research should be aimed at the pedagogical design of hands-on experience, such as programming, in online learning systems.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {142–151},
numpages = {10},
keywords = {massive open online courses (MOOCs), machine learning, learning analytics, educational data mining, code analysis, automated tutoring systems, Introductory computer science education},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3706468.3706515,
author = {Rodrigues, Luiz and Xavier, Cleon and Costa, Newarney and Batista, Hyan and Silva, Luiz Felipe Bagnhuk and Chaleghi de Melo, Weslei and Gasevic, Dragan and Ferreira Mello, Rafael},
title = {LLMs Performance in Answering Educational Questions in Brazilian Portuguese: A Preliminary Analysis on LLMs Potential to Support Diverse Educational Needs},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706515},
doi = {10.1145/3706468.3706515},
abstract = {Question-answering systems facilitate adaptive learning and respond to student queries, making education more responsive. Despite that, challenges such as natural language understanding and context management complicate their widespread adoption, where Large Language Models (LLMs) offer a promising solution. However, existing research is predominantly focused on English, proprietary models, and often limited to a single question type, subject, or skill, leaving a gap in understanding LLMs’ performance in languages like Brazilian Portuguese and across questions of various characteristics. This study investigates how LLMs could be integrated in an educational question-answering system efficiently to answer different question types (multiple-choice, cloze, open-ended), subjects (mathematics and Portuguese language), and skills (summation/subtraction, multiplication, interpretation, and grammar), evaluating answers by GPT-4 - the main LLM at the time of writing - and Sabi\'{a} - the open-source Brazilian Portuguese LLM - based on grades assigned by two experienced teachers. Overall, both LLMs demonstrated strong overall performance, with mean scores close to 9.8 out of 10. However, specific challenges emerged, with distinct strengths and weaknesses observed for each model, such as GPT-4’s error in a multiple-choice subtraction question and Sabi\'{a}’s misinterpretation of a cloze question.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {865–871},
numpages = {7},
keywords = {GPT, Sabi\'{a}, Question-Answering, Chatbot.},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706501,
author = {Scarlatos, Alexander and Baker, Ryan S. and Lan, Andrew},
title = {Exploring Knowledge Tracing in Tutor-Student Dialogues using LLMs},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706501},
doi = {10.1145/3706468.3706501},
abstract = {Recent advances in large language models (LLMs) have led to the development of artificial intelligence (AI)-powered tutoring chatbots, showing promise in providing broad access to high-quality personalized education. Existing works have studied how to make LLMs follow tutoring principles, but have not studied broader uses of LLMs for supporting tutoring. Up until now, tracing student knowledge and analyzing misconceptions has been difficult and time-consuming to implement for open-ended dialogue tutoring. In this work, we investigate whether LLMs can be supportive of this task: we first use LLM prompting methods to identify the knowledge components/skills involved in each dialogue turn, i.e., a tutor utterance posing a task or a student utterance that responds to it. We also evaluate whether the student responds correctly to the tutor and verify the LLM’s accuracy using human expert annotations. We then apply a range of knowledge tracing (KT) methods on the resulting labeled data to track student knowledge levels over an entire dialogue. We conduct experiments on two tutoring dialogue datasets, and show that a novel yet simple LLM-based method, LLMKT, significantly outperforms existing KT methods in predicting student response correctness in dialogues. We perform extensive qualitative analyses to highlight the challenges in dialogueKT and outline multiple avenues for future work.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {249–259},
numpages = {11},
keywords = {Knowledge Components, Knowledge Tracing, Large Language Models, Tutoring dialogues},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3576050.3576103,
author = {Van Campenhout, Rachel and Jerome, Bill and Dittel, Jeffrey S. and Johnson, Benny G.},
title = {The Doer Effect at Scale: Investigating Correlation and Causation Across Seven Courses},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576103},
doi = {10.1145/3576050.3576103},
abstract = {The future of digital learning should be focused on methods proven to be effective by learning science and learning analytics. One such method is learning by doing—combining formative practice with expository content so students actively engage with their learning resource. This generates the doer effect: the principle that students who do practice while they read have higher outcomes than those who only read [9]. Research on the doer effect has shown it to be causal to learning [10], and these causal findings have previously been replicated in a single course [19]. This study extends the replication of the doer effect by analyzing 15.2 million data events from 18,546 students in seven courses at an online higher education institution, the most students and courses known to date. Furthermore, we analyze each course five ways by using different outcomes, accounting for prior knowledge, and doing both correlational and causal analyses. By performing the doer effect analyses five ways on seven courses, new insights are gained on how this method of learning analytics can contribute to our interpretation of this learning science principle. Practical implications of the doer effect for students are discussed, and future research goals are established.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {357–365},
numpages = {9},
keywords = {Doer effect, causal discovery, course effectiveness, courseware, external validity, learn by doing, learning outcomes, replication},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3706468.3706474,
author = {Yin, Stella Xin and Liu, Zhengyuan and Goh, Dion Hoe-Lian and Quek, Choon Lang and Chen, Nancy F.},
title = {Scaling Up Collaborative Dialogue Analysis: An AI-driven Approach to Understanding Dialogue Patterns in Computational Thinking Education},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706474},
doi = {10.1145/3706468.3706474},
abstract = {Pair programming is a collaborative activity that enhances students’ computational thinking (CT) skills. Analyzing students’ interactions during pair programming provides valuable insights into effective learning. However, interpreting classroom dialogues is a challenging and complex task. Due to the simultaneous interaction between interlocutors and other ambient noise in collaborative learning contexts, previous work heavily relied on manual transcription and coding, which is labor-intensive and time-consuming. Recent advancements in speech and language processing offer promising opportunities to automate and scale up dialogue analysis. Besides, previous work mainly focused on task-related interactions, with little attention to social interactions. To address these gaps, we conducted a four-week CT course with 26 fifth-grade primary school students. We recorded their discussions, transcribed them with speech processing models, and developed a coding scheme and applied LLMs for annotation. Our AI-driven pipeline effectively analyzed classroom recordings with high accuracy and efficiency. After identifying the dialogue patterns, we investigated the relationships between these patterns and CT performance. Four clusters of dialogue patterns have been identified: Inquiry, Constructive Collaboration, Disengagement, and Disputation. We observed that Inquiry and Constructive Collaboration patterns were positively related to students’ CT skills, while Disengagement and Disputation patterns were associated with lower CT performance. This study contributes to the understanding of how dialogue patterns relate to CT performance and provides implications for both research and educational practice in CT learning.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {47–57},
numpages = {11},
keywords = {Collaborative learning, Computational thinking, Dialogue analysis, Large language models, Pair programming, Speech and language processing},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3448139.3448194,
author = {Guillain, L\'{e}onore V. and Schneider, Bertrand},
title = {Facilitators First: Building a Tool With Facilitators to Foster a More Collaborative Makerspace Community Through Movement Traces},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448194},
doi = {10.1145/3448139.3448194},
abstract = {Research indicates that makerspaces equip students with the practical skills needed to build their own projects and thrive in the twenty-first-century workforce. While the appeal of makerspaces lies in their spirit of tinkering and community-driven ethos, these same attributes make it difficult to monitor and facilitate learning. Makerspaces also attract students from diverse backgrounds and skills, further challenging facilitators to accommodate the needs of each student and their self-directed projects. We propose a dashboard interface that visualizes Kinect sensor data to aid facilitators in monitoring student collaboration. The tool was designed with an iterative and participatory approach. Five facilitators were involved at each phase of the design process, from need-finding to prototyping to implementation and evaluation. Insights derived from interviews were used to inform the design decisions of the final interface. The final evaluation suggests that the use of normalized summary scores and an interactive network graph can successfully support facilitators in tasks related to improving collaboration. Moreover, the use of a red-green color scheme and the inclusion of student photos improved the usability for facilitators, but issues of trustworthiness need to be further examined.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {533–539},
numpages = {7},
keywords = {physical learning analytics, learning analytics dashboards, human-computer interaction},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3636555.3636916,
author = {Ramanathan, Sriram and Buckingham Shum, Simon and Lim, Lisa-Angelique},
title = {To what extent do responses to a single survey question provide insights into students' sense of belonging?},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636916},
doi = {10.1145/3636555.3636916},
abstract = {A student's “sense of belonging” is critical to retention and success in higher education. However, belonging is a multifaceted and dynamic concept, making monitoring and supporting it with timely action challenging. Conventional approaches to researching belonging depend on lengthy surveys and/or focus groups, and while often insightful, these are resource-intensive, slow, and cannot be repeated too often. “Belonging Analytics” is an emerging concept pointing to the potential of learning analytics to address this challenge, and to illustrate this concept, this paper investigates the feasibility of asking students a single question about what promotes their sense of belonging. To validate this, responses were analysed using a form of topic modelling, and these were triangulated by examining alignment with (i) students’ responses to Likert scale items in a belonging scale and (ii)&nbsp;the literature on the drivers of belonging. These alignments support our proposal that this is a practical tool to gain timely insight into a cohort's sense of belonging. Reflecting our focus on practical tools, the approach is implemented using analytics products readily available to educational institutions — Linguistic Inquiry Word Count (LIWC) and Statistical Program for Social Sciences (SPSS).},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {878–884},
numpages = {7},
keywords = {Belonging, LIWC, Meaning Extraction Method, Natural Language Processing, Topic Modelling},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3576050.3576116,
author = {Kim, Yunsung and Piech, Chris},
title = {The Student Zipf Theory: Inferring Latent Structures in Open-Ended Student Work To Help Educators},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576116},
doi = {10.1145/3576050.3576116},
abstract = {Are there structures underlying student work that are universal across every open-ended task? We demonstrate that, across many subjects and assignment types, the probability distribution underlying student-generated open-ended work is close to Zipf’s Law. Inferring this latent structure for classroom assignments can help learning analytics researchers, instruction designers, and educators understand the landscape of various student approaches, assess the complexity of assignments, and prioritise pedagogical attention. However, typical classrooms are way too small to witness even the contour of the Zipfian pattern, and it is generally impossible to perform inference for Zipf’s law from such small number of samples. We formalise this difficult task as the Zipf Inference Challenge: (1) Infer the ordering of student-generated works by their underlying probabilities, and (2) Estimate the shape parameter of the underlying distribution in a typical-sized classroom. Our key insight in addressing this challenge is to leverage the densities of the student response landscapes represented by semantic similarity. We show that our “Semantic Density Estimation” method is able to do a much better job at inferring the latent Zipf shape and the probability-ordering of student responses for real world education datasets.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {464–475},
numpages = {12},
keywords = {Constructed Response, Open-Ended Response, Probabilistic Modeling, Student Work, Zipf’s Law},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3375462.3375538,
author = {Nguyen, Quan},
title = {Rethinking time-on-task estimation with outlier detection accounting for individual, time, and task differences},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375538},
doi = {10.1145/3375462.3375538},
abstract = {Time-on-task estimation, measured as the duration between two consecutive clicks using student log-files data, has been one of the most frequently used metrics in learning analytics research. However, the process of handling outliers (i.e., excessively long durations) in time-on-task estimation is under-explored and often not explicitly reported in many studies. One common approach to handle outliers in time-to-task estimation is to 'trim' all durations using a cut-off threshold, such as 60 or 30 minutes. This paper challenges this existing approach by demonstrating that the treatment of outliers in an educational context should be individual-specific, time-specific, and task-specific. In other words, what can be considered as outliers in time-on-task depends on the learning pattern of each student, the stages during the learning process, and the nature of the task involved. The analysis showed that predictive models using time-on-task estimation accounting for individual, time, and task differences could explain 3--4% more variances in academic performance than models using an outlier trimming approach. As an implication, this study provides a theoretically grounded and replicable outlier detection approach for future learning analytics research when using time-on-task estimation.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {376–381},
numpages = {6},
keywords = {time-on-task, temporal analysis, outlier detection, measurement, learning analytics},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/2460296.2460357,
author = {Buckingham Shum, Simon and de Laat, Maarten and De Liddo, Anna and Ferguson, Rebecca and Kirschner, Paul and Ravenscroft, Andrew and S\'{a}ndor, \'{A}gnes and Whitelock, Denise},
title = {DCLA13: 1st International Workshop on Discourse-Centric Learning Analytics},
year = {2013},
isbn = {9781450317856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2460296.2460357},
doi = {10.1145/2460296.2460357},
abstract = {This workshop anticipates that an important class of learning analytic will emerge at the intersection of research into learning dynamics, online discussion platforms, and computational linguistics. Written discourse is arguably the primary class of data that can give us insights into deeper learning and higher order qualities such as critical thinking, argumentation, mastery of complex ideas, empathy, collaboration and interpersonal skills. Moreover, the ability to write in a scholarly manner is a core competence, often taking the form of discourse with oneself and the literature. Computational linguistics research has developed a rich array of tools for machine interpretation of human discourse, but work to develop these tools in the context of learning is at a relatively early stage. Moreover, there is a significant difference between designing tools to assist researchers in discourse analysis, and their deployment on platforms to provide meaningful analytics for the learners and educators who are conducting that discourse. This workshop aims to catalyse ideas and build community connections among those who want to shape this field.},
booktitle = {Proceedings of the Third International Conference on Learning Analytics and Knowledge},
pages = {282},
numpages = {1},
keywords = {visualization, learning analytics, discourse, dialogue, deliberation, argumentation},
location = {Leuven, Belgium},
series = {LAK '13}
}

@inproceedings{10.1145/2876034.2893430,
author = {Cobos, Ruth and Gil, Silvia and Lareo, Angel and Vargas, Francisco A.},
title = {Open-DLAs: An Open Dashboard for Learning Analytics},
year = {2016},
isbn = {9781450337267},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2876034.2893430},
doi = {10.1145/2876034.2893430},
abstract = {In this paper a learning analytics dashboard for MOOCs is proposed. It visualises the progress of learners' activity taking into account navigation, social interactions and interaction with educational resources. This approach was tested with the MOOCs created by the University Auton\'{o}ma of Madrid (Spain) in the edX platform. Nowadays, the dashboard is being improved taking into account the received feedback from MOOCs instructors and assistants. Finally, a new version is presented to work along with edX and Open edX.},
booktitle = {Proceedings of the Third (2016) ACM Conference on Learning @ Scale},
pages = {265–268},
numpages = {4},
keywords = {moocs, learning analytics, dashboard},
location = {Edinburgh, Scotland, UK},
series = {L@S '16}
}

@inproceedings{10.1145/3375462.3375464,
author = {Alexandron, Giora and Wiltrout, Mary Ellen and Berg, Aviram and Ruip\'{e}rez-Valiente, Jos\'{e} A.},
title = {Assessment that matters: balancing reliability and learner-centered pedagogy in MOOC assessment},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375464},
doi = {10.1145/3375462.3375464},
abstract = {Learner-centered pedagogy highlights active learning and formative feedback. Instructors often incentivize learners to engage in such formative assessment activities by crediting their completion and score in the final grade, a pedagogical practice that is very relevant to MOOCs as well. However, previous studies have shown that too many MOOC learners exploit the anonymity to abuse the formative feedback, which is critical in the learning process, to earn points without effort. Unfortunately, limiting feedback and access to decrease cheating is counter-pedagogic and reduces the openness of MOOCs. We aimed to identify and analyze a MOOC assessment strategy that balances this tension between learner-centered pedagogy, incentive design, and reliability of the assessment. In this study, we evaluated an assessment model that MITx Biology introduced in a MOOC to reduce cheating with respect to its effect on two aspects of learner behavior - the amount of cheating and learners' engagement in formative course activities. The contribution of the paper is twofold. First, this work provides MOOC designers with an 'analytically-verified' MOOC assessment model to reduce cheating without compromising learner engagement in formative assessments. Second, this study provides a learning analytics methodology to approximate the effect of such an intervention.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {512–517},
numpages = {6},
keywords = {learning analytics, assessment, MOOCs},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/2090116.2090118,
author = {Duval, Erik},
title = {Attention please! learning analytics for visualization and recommendation},
year = {2011},
isbn = {9781450309448},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2090116.2090118},
doi = {10.1145/2090116.2090118},
abstract = {This paper will present the general goal of and inspiration for our work on learning analytics, that relies on attention metadata for visualization and recommendation. Through information visualization techniques, we can provide a dashboard for learners and teachers, so that they no longer need to "drive blind". Moreover, recommendation can help to deal with the "paradox of choice" and turn abundance from a problem into an asset for learning.},
booktitle = {Proceedings of the 1st International Conference on Learning Analytics and Knowledge},
pages = {9–17},
numpages = {9},
keywords = {visualization, recommendation, learning analytics},
location = {Banff, Alberta, Canada},
series = {LAK '11}
}

@inproceedings{10.1145/3706468.3706469,
author = {Le Tallec, Julie and Prihar, Ethan and K\"{a}ser, Tanja},
title = {The Effect of Different Support Strategies on Student Affect},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706469},
doi = {10.1145/3706468.3706469},
abstract = {Within many online learning platforms, struggling students are provided with support to guide them through challenging material. Support comes in many forms, and is typically evaluated based on its ability to improve students’ performance on future tasks. However, there is little experimentation to evaluate how these supports impact students’ emotional states. Student’s emotional state, or affect, significantly impacts their motivation to engage with learning material and persist through challenges. Positive emotions can foster intrinsic engagement and deeper commitment, whereas negative emotions may lead to disengagement and avoidance of challenging tasks. In this work, we use publicly available data from online experiments and affect modeling to causally evaluate the impact that different support strategies have on students’ affect. Through analysis of 25 experiments with 6,463 total participants, we find multiple significant positive and negative changes in students’ affect when receiving hints, examples, or scaffolding questions, despite all three having a positive impact on performance, revealing the need for more nuanced evaluations of support strategies to uncover their impact beyond just performance. The code for this project is available at https://osf.io/74dgx.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {783–789},
numpages = {7},
keywords = {Online Tutoring, Affect Detection, Randomized Controlled Experiments},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3506860.3506876,
author = {Rakovic, Mladen and Fan, Yizhou and van der Graaf, Joep and Singh, Shaveen and Kilgour, Jonathan and Lim, Lyn and Moore, Johanna and Bannert, Maria and Molenaar, Inge and Gasevic, Dragan},
title = {Using Learner Trace Data to Understand Metacognitive Processes in Writing from Multiple Sources},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506876},
doi = {10.1145/3506860.3506876},
abstract = {Writing from multiple sources is a commonly administered learning task across educational levels and disciplines. In this task, learners are instructed to comprehend information from source documents and integrate it into a coherent written composition to fulfil the assignment requirements. Even though educationally potent, multi-source writing tasks are considered challenging to many learners, in particular because many learners underuse monitoring and control, critical metacognitive processes for productive engagement in multi-source writing. To understand these processes, we conducted a laboratory study involving 44 university students. They engaged in multi-source writing task hosted in digital learning environment. Adding to previous research, we unobtrusively measured metacognitive processes using learners’ trace data collected via multiple data channels and in both writing and reading space of the multi-source writing task. We further investigated how these processes affect the quality of a written product, i.e., essay score. In the analysis, we utilised both automatically and human-generated essay score. The rating performance of the essay scoring algorithm was comparable to that of human raters. Our results largely support the theoretical assumptions that engagement in metacognitive monitoring and control benefits the quality of written product. Moreover, our results can inform the development of analytics-based tools that support student writing by making use of trace data and automated essay scoring.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {130–141},
numpages = {12},
keywords = {writing from multiple sources, semantic similarity, reading, monitoring},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3706468.3706478,
author = {Baker, Ryan and Hutt, Stephen},
title = {MORF: A Post-Mortem},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706478},
doi = {10.1145/3706468.3706478},
abstract = {There has been increasing interest in data enclaves in recent years, both in education and other fields. Data enclaves make it possible to conduct analysis on large-scale and higher-risk data sets, while protecting the privacy of the individuals whose data is included in the data sets, thus mitigating risks around data disclosure. In this article, we provide a post-mortem on the MORF (MOoc Replication Framework) 2.1 infrastructure, a data enclave expected to sunset and be replaced in the upcoming years, reviewing the core factors that reduced its usefulness for the community. We discuss challenges to researchers in terms of usability, including challenges involving learning to use core technologies, working with data that cannot be directly viewed, debugging, and working with restricted outputs. Our post-mortem discusses possibilities for ways that future infrastructures could get past these challenges.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {797–802},
numpages = {6},
keywords = {Data enclave, MORF, Privacy},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706487,
author = {Li, Ziqing and Cukurova, Mutlu and Bulathwela, Sahan},
title = {A Novel Approach to Scalable and Automatic Topic-Controlled Question Generation in Education},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706487},
doi = {10.1145/3706468.3706487},
abstract = {The development of Automatic Question Generation (QG) models has the potential to significantly improve educational practices by reducing the teacher workload associated with creating educational content. This paper introduces a novel approach to educational question generation that controls the topical focus of questions. The proposed Topic-Controlled Question Generation (T-CQG) method enhances the relevance and effectiveness of the generated content for educational purposes. Our approach uses fine-tuning on a pre-trained T5-small model, employing specially created datasets tailored to educational needs. The research further explores the impacts of pre-training strategies, quantisation, and data augmentation on the model’s performance. We specifically address the challenge of generating semantically aligned questions with paragraph-level contexts, thereby improving the topic specificity of the generated questions. In addition, we introduce and explore novel evaluation methods to assess the topical relatedness of the generated questions. Our results, validated through rigorous offline and human-backed evaluations, demonstrate that the proposed models effectively generate high-quality, topic-focused questions. These models have the potential to reduce teacher workload and support personalised tutoring systems by serving as bespoke question generators. With its relatively small number of parameters, the proposals not only advance the capabilities of question generation models for handling specific educational topics but also offer a scalable solution that reduces infrastructure costs. This scalability makes them feasible for widespread use in education without reliance on proprietary large language models like ChatGPT.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {148–158},
numpages = {11},
keywords = {Educational Question Generation, Formative Assessment, Summative Assessment, Personalised Testing, Natural Language Processing},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706482,
author = {Garg, Manika and Goel, Anita},
title = {Towards Fair Assessments: A Machine Learning-based Approach for Detecting Cheating in Online Assessments},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706482},
doi = {10.1145/3706468.3706482},
abstract = {Academic cheating poses a significant challenge to conducting fair online assessments. One common way is collusion, where students unethically share answers during the assessment. While several researchers proposed solutions, there is lack of clarity regarding the specific types they target among the different types of collusion. Researchers have used statistical techniques to analyze basic attributes collected by the platforms, for collusion detection. Only few works have used machine learning, considering two or three attributes only; the use of limited features leading to reduced accuracy and increased risk of false accusations.In this work, we focus on In-Parallel Collusion, where students simultaneously work together on an assessment. For data collection, a quiz tool is improvised to capture clickstream data at a finer level of granularity. We use feature engineering to derive seven features and create a machine learning model for collusion detection. The results show: 1) Random Forest exhibits the best accuracy (98.8%), and 2) In contrast to less features as used in earlier works, the full feature set provides the best result; showing that considering multiple facets of similarity enhance the model accuracy. The findings provide platform designers and teachers with insights into optimizing quiz platforms and creating cheat-proof assessments.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {104–114},
numpages = {11},
keywords = {Academic dishonesty, Cheating, Feature engineering, Integrity, Machine learning, Online education},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/2485760.2485805,
author = {Ahn, June and Gubbels, Michael and Yip, Jason and Bonsignore, Elizabeth and Clegg, Tamara},
title = {Using social media and learning analytics to understand how children engage in scientific inquiry},
year = {2013},
isbn = {9781450319188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2485760.2485805},
doi = {10.1145/2485760.2485805},
abstract = {Children are increasingly using social media tools in their lives. In addition, there is great interest in understanding how to design and evaluate social technologies to aid in children's learning and development. We describe two research endeavors that begin to address these issues. First, we introduce SINQ, a social media application that encourages children to practice Scientific INQuiry skills through collaborative participation. Second, we conducted a case study of SINQ with six children, ages 8-11, and collected log data of their interactions in the app. We applied learning analytics on this log data using a visual analytic tool called LifeFlow. The event-sequence visualizations showed how children engaged with scientific inquiry within the SINQ app, and most importantly illuminated how inquiry is not a linear process with a defined start and end. The children in our study traversed the inquiry process via diverse pathways, all of which were supported by the SINQ app.},
booktitle = {Proceedings of the 12th International Conference on Interaction Design and Children},
pages = {427–430},
numpages = {4},
keywords = {social media, science learning, learning analytics, children},
location = {New York, New York, USA},
series = {IDC '13}
}

@inproceedings{10.1145/3706468.3706529,
author = {Hassany, Mohammad and Brusilovsky, Peter and Savelka, Jaromir and Lekshmi Narayanan, Arun Balajiee and Akhuseyinoglu, Kamil and Agarwal, Arav and Hendrawan, Rully Agus},
title = {Generating Effective Distractors for Introductory Programming Challenges: LLMs vs Humans},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706529},
doi = {10.1145/3706468.3706529},
abstract = {As large language models (LLMs) show great promise in generating a wide spectrum of educational materials, robust yet cost-effective assessment of the quality and effectiveness of such materials becomes an important challenge. Traditional approaches, including expert-based quality assessment and student-centered evaluation, are resource-consuming, and do not scale efficiently. In this work, we explored the use of pre-existing student learning data as a promising approach to evaluate LLM-generated learning materials. Specifically, we used a dataset where students were completing the program construction challenges by picking the correct answers among human-authored distractors to evaluate the quality of LLM-generated distractors for the same challenges. The dataset included responses from 1,071 students across 22 classes taught from Fall 2017 to Spring 2023. We evaluated five prominent LLMs (OpenAI-o1, GPT-4, GPT-4o, GPT-4o-mini, and Llama-3.1-8b) across three different prompts to see which combinations result in more effective distractors, i.e., those that are plausible (often picked by students), and potentially based on common misconceptions. Our results suggest that GPT-4o was the most effective model, matching close to 50% of the functional distractors originally authored by humans. At the same time, all of the evaluated LLMs generated many novel distractors, i.e., those that did not match the pre-existing human-authored ones. Our preliminary analysis shows that those appear to be promising. Establishing their effectiveness in real-world classroom settings is left for future work.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {484–493},
numpages = {10},
keywords = {Large Language Models (LLMs), Distractor Generation and Evaluation, Student Learning Data, Introductory Programming, GPT, LLaMA},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3636555.3636897,
author = {Wang, Karen D. and Chen, Zhongzhou and Wieman, Carl},
title = {Can Crowdsourcing Platforms Be Useful for Educational Research?},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636897},
doi = {10.1145/3636555.3636897},
abstract = {A growing number of social science researchers, including educational researchers, have turned to online crowdsourcing platforms such as Prolific and MTurk for their experiments. However, there is a lack of research investigating the quality of data generated by online subjects and how they compare with traditional subject pools of college students in studies that involve cognitively demanding tasks. Using an interactive problem-solving task embedded in an educational simulation, we compare the task engagement and performance based on the interaction log data of college students recruited from Prolific to those from an introductory physics course. Results show that Prolific participants performed on par with participants from the physics class in obtaining the correct solutions. Furthermore, the physics course students who submitted incorrect answers were more likely than Prolific participants to make rushed cursory attempts to solve the problem. These results suggest that with thoughtful study design and advanced learning analytics and data mining techniques, crowdsourcing platforms can be a viable tool for conducting research on teaching and learning in higher education.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {416–425},
numpages = {10},
keywords = {crowdsourcing research, log data, online experiments, postsecondary STEM education, problem solving},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3706468.3706524,
author = {Russell, Jae-Eun and Smith, Anna Marie and George, Salim and Pratt, Jonah and Fodale, Brian and Monk, Cassandra and Brummett, Adam},
title = {Unlocking Insights: Investigating Student AI Tutor Interactions in a Large Introductory STEM Course},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706524},
doi = {10.1145/3706468.3706524},
abstract = {This study explored the use of an AI tutor and its relationship to performance outcomes in a large introductory undergraduate STEM course, where the AI tutor was integrated into the online homework system. The course included 13 weekly homework assignments, comprising 221 questions that contributed 19.5% to the final grade. Results showed that students predominantly completed homework problems without AI tutor assistance, using it selectively to address specific challenges. Patterns of AI interaction varied at both the problem and student levels, with demographic factors having little to no relationship to AI usage. Notably, the frequency of AI use was not linked to exam performance. A multi-level cluster analysis identified distinct patterns in students’ use of the AI tutor during problem-solving. These patterns of use had more significant associations with performance than frequency of use alone. This paper explores these interaction patterns in depth and discusses the study’s limitations and implications.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {451–461},
numpages = {11},
keywords = {AI tutors, Higher Education, STEM},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706532,
author = {Lyu, Bailing and Li, Chenglu and Li, Hai and Oh, Hyunju and Song, Yukyeong and Zhu, Wangda and Xing, Wanli},
title = {Exploring the Role of Teachable AI Agents’ Personality Traits in Shaping Student Interaction and Learning in Mathematics Education},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706532},
doi = {10.1145/3706468.3706532},
abstract = {With advances in artificial intelligence (AI), educational researchers have integrated AI into mathematics education to offer scalable instructional practices and personalized learning. One such innovation is teachable AI agents, designed as learners to facilitate learning by teaching. Previous research has evidenced the benefits of learning by teaching, and its effectiveness depends on the quality of tutor-tutee interaction. However, few studies have explored how features of teachable agents, particularly personality traits, influence student interactions and the agents’ effectiveness. Given the documented importance of personality traits in student learning, this empirical study examines the relationship between teachable AI agents’ personality traits and students’ math learning experiences in a naturalistic setting. Results indicated that students provided more cognitive support when interacting with teachable agents characterized by neuroticism, openness, and conscientiousness, while more affect management and non-responsive behaviors were observed with agents displaying extraversion. These interaction patterns impacted the effectiveness of the teachable agents, providing implications for the integration of AI systems into education.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {887–894},
numpages = {8},
keywords = {Teachable agent, mathematics education, personality traits},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706470,
author = {Ooge, Jeroen and Vanneste, Arno and Szymanski, Maxwell and Verbert, Katrien},
title = {Designing Visual Explanations and Learner Controls to Engage Adolescents in AI-Supported Exercise Selection},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706470},
doi = {10.1145/3706468.3706470},
abstract = {E-learning platforms that personalise content selection with AI are often criticised for lacking transparency and controllability. Researchers have therefore proposed solutions such as open learner models and letting learners select from ranked recommendations, which engage learners before or after the AI-supported selection process. However, little research has explored how learners – especially adolescents – could engage during such AI-supported decision-making. To address this open challenge, we iteratively designed and implemented a control mechanism that enables learners to steer the difficulty of AI-compiled exercise series before practice, while interactively analysing their control’s impact in a what-if visualisation. We evaluated our prototypes through four qualitative studies involving adolescents, teachers, EdTech professionals, and pedagogical experts, focusing on different types of visual explanations for recommendations. Our findings suggest that why explanations do not always meet the explainability needs of young learners but can benefit teachers. Additionally, what-if explanations were well-received for their potential to boost motivation. Overall, our work illustrates how combining learner control and visual explanations can be operationalised on e-learning platforms for adolescents. Future research can build upon our designs for why and what-if explanations and verify our preliminary findings.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {1–12},
numpages = {12},
keywords = {explainable artificial intelligence, learner control, human-centred design, education, K-12, adaptive learning, self-regulated learning},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/2460296.2460323,
author = {Ahn, June},
title = {What can we learn from Facebook activity? using social learning analytics to observe new media literacy skills},
year = {2013},
isbn = {9781450317856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2460296.2460323},
doi = {10.1145/2460296.2460323},
abstract = {Social media platforms such as Facebook are now a ubiquitous part of everyday life for many people. New media scholars posit that the participatory culture encouraged by social media gives rise to new forms of literacy skills that are vital to learning. However, there have been few attempts to use analytics to understand the new media literacy skills that may be embedded in an individual's participation in social media. In this paper, I collect raw activity data that was shared by an exploratory sample of Facebook users. I then utilize factor analysis and regression models to show how (a) Facebook members' online activity coalesce into distinct categories of social media behavior and (b) how these participatory behaviors correlate with and predict measures of new media literacy skills. The study demonstrates the use of analytics to understand the literacies embedded in people's social media activity. The implications speak to the potential of social learning analytics to identify and predict new media literacy skills from data streams in social media platforms.},
booktitle = {Proceedings of the Third International Conference on Learning Analytics and Knowledge},
pages = {135–144},
numpages = {10},
keywords = {social media, social learning analytics, new media literacy, literacy, learning analytics},
location = {Leuven, Belgium},
series = {LAK '13}
}

@inproceedings{10.1145/3375462.3375534,
author = {Matcha, Wannisa and Ga\v{s}evi\'{c}, Dragan and Jovanovi\'{c}, Jelena and Uzir, Nora'ayu Ahmad and Oliver, Chris W and Murray, Andrew and Gasevic, Danijela},
title = {Analytics of learning strategies: the association with the personality traits},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375534},
doi = {10.1145/3375462.3375534},
abstract = {Studying online requires well-developed self-regulated learning skills to properly manage one's learning strategies. Learning analytics research has proposed novel methods for extracting theoretically meaningful learning strategies from trace data originating from formal learning settings (online, blended, or flipped classroom). Thus identified strategies proved to be associated with academic achievement. However, automated extraction of theoretically meaningful learning strategies from trace data in the context of massive open online courses (MOOCs) is still under-explored. Moreover, there is a lacuna in research on the relations between automatically detected strategies and the established psychological constructs. The paper reports on a study that (a) applied a state-of-the-art analytic method that combines process and sequence mining techniques to detect learning strategies from the trace data collected in a MOOC (N=1,397), and (b) explored associations of the detected strategies with academic performance and personality traits (Big Five). Four learning strategies detected with the adopted analytics method were shown to be theoretically interpretable as the well-known approaches to learning. The results also revealed that the four detected learning strategies were predicted by conscientiousness, emotional instability, and agreeableness and were associated with academic performance. Implications for theoretical validity and practical application of analytics-detected learning strategies are also provided.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {151–160},
numpages = {10},
keywords = {personality traits, learning strategies, learning analytics, approaches to learning},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3448139.3448199,
author = {Evrard, August and Schulz, Kyle and Hayward, Caitlin},
title = {How Did You Get that A? Selectivity’s Role in Rising Undergraduate Grades at a Large Public University},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448199},
doi = {10.1145/3448139.3448199},
abstract = {For nearly a century, pre-college standardized test scores and undergraduate letter grades have been de facto industry standard measures of achievement in US higher education. We examine a sample of millions of grades and a half million pre-college test scores earned by undergraduates between 2006 and 2019 at a large public research university that became increasingly selective, in terms of test scores of matriculated students, over that time. A persistent, moderate correlation between test score and grades within the period motivates us to employ a simple importance sampling model to address the question, “How much is increased selectivity driving up campus grades?”. Of the overall 0.213 rise in mean undergraduate grade points over the thirteen-year period, we find that nearly half, 0.098 ± 0.004, can be ascribed to increased selectivity. The fraction is higher, nearly 70%, in engineering, business and natural science subjects. Removing selectivity’s influence to surface curricular-related grade inflation within academic domains, we find a factor four range, from a low of ∼ 0.05 in business and engineering to a high of 0.18 in the humanities, over the thirteen year period.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {565–571},
numpages = {7},
keywords = {Undergraduate Education, Student Grades, Standardized Tests, Learning Analytics},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3636555.3636870,
author = {Xiang, Mengtong and Zhang, Jingjing and Li, Yue},
title = {Understanding Knowledge Convergence in a Cross-cultural Online Context: An Individual and Collective Approach},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636870},
doi = {10.1145/3636555.3636870},
abstract = {The concept of knowledge convergence refers to building a shared cognitive understanding among individuals through social interaction. It is considered as a crucial aspect of collaborative learning and plays a significant role in the process of consensus building. However, there is a lack of research exploring knowledge convergence in the context of online learning, especially in cross-cultural settings. Collaborative learning primarily focuses on constructing cognitive knowledge representations at the individual level, while online learning emphasizes the social mechanism of knowledge diffusion and flow at the collective level. This study aims to investigate individual online knowledge convergence through content analysis of social annotations within a cross-cultural course and using Simulation Investigation for Empirical Network Analysis (SIENA) to depict the collective social interaction. The findings reveal that online knowledge convergence exhibits distinct characteristics, quick consensus building could foster a harmonious community and similar experiences compensated for limited interactions, triggering deep consensus. Individual convergence leads to emergent properties such as reciprocity and transitivity within a dynamic collective interactive network, which can serve as novel indicators for evaluating knowledge convergence at the collective level. By approaching knowledge convergence from multifaceted perspectives, this study contributes to a comprehensive understanding of the concept across diverse learning contexts.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {779–784},
numpages = {6},
keywords = {Knowledge convergence, collaborative learning, complex network analysis, cross-cultural online course},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3706468.3706486,
author = {Yang, Tianyuan and Ren, Baofeng and Gu, Chenghao and Ma, Boxuan and He, Tianjia and Konomi, Shin'Ichi},
title = {Towards Better Course Recommendations: Integrating Multi-Perspective Meta-Paths and Knowledge Graphs},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706486},
doi = {10.1145/3706468.3706486},
abstract = {Course recommender systems demonstrate their potential in assisting students with course selection and effectively alleviating the problem of information overload. Current course recommender systems focus predominantly on collaborative information and fail to consider the multi-perspective information and the bi-directional relationship between students and courses. This paper introduces a novel Multi-perspective Aware Explainable Course Recommendation model (MAECR) that leverages knowledge graphs and multi-perspective meta-paths to enhance both the accuracy and explainability of course recommendations. By the dual-side modeling from both the student and the course for each meta-path, MAECR can identify and understand the interests and needs of students in each course, as well as evaluate the attractiveness and suitability of the courses for individual students. Following the dual-side modeling for each meta-path, we aggregate multi-perspective meta-paths of each student and course using a carefully designed attention mechanism. The attention weights generated by this mechanism serve as explanations for the recommendation results, representing the preference score for each perspective. MAECR thus provides personalized and explainable recommendations. Comprehensive experiments are implemented to demonstrate the effectiveness and improved interpretability of the proposed model.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {137–147},
numpages = {11},
keywords = {Course recommendation, Explainable recommender systems, Knowledge graphs},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706481,
author = {Ferreira Mello, Rafael and Pereira Junior, Cleon and Rodrigues, Luiz and Pereira, Filipe Dwan and Cabral, Luciano and Costa, Newarney and Ramalho, Geber and Gasevic, Dragan},
title = {Automatic Short Answer Grading in the LLM Era: Does GPT-4 with Prompt Engineering beat Traditional Models?},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706481},
doi = {10.1145/3706468.3706481},
abstract = {Assessing short answers in educational settings is challenging due to the need for scalability and accuracy, which led to the field of Automatic Short Answer Grading (ASAG). Traditional machine learning models, such as ensemble and embeddings, have been widely researched in ASAG, but they often suffer from generalizability issues. Recently, Large Language Models (LLMs) emerged as an alternative to optimize ASAG systems. However, previous research has failed to present a comprehensive analysis of LLMs’ performance powered by prompt engineering strategies and compare its capabilities to traditional models. This study presents a comparative analysis between traditional machine learning models and GPT-4 in the context of ASAG. We investigated the effectiveness of different models and text representation techniques and explored prompt engineering strategies for LLMs. The results indicate that traditional machine learning models outperform LLMs. However, GPT-4 showed promising capabilities, especially when configured with optimized prompt components, such as few-shot examples and clear instructions. This study contributes to the literature by providing a detailed evaluation of LLM performance compared to traditional machine learning models in a multilingual ASAG context, offering insights for developing more efficient automatic grading systems.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {93–103},
numpages = {11},
keywords = {Automatic short answer grading, Natural Language Processing, Assessment, LLM, GPT},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3012430.3012608,
author = {Conde, Miguel \'{A}. and Rodr\'{\i}guez-Sedano, Francisco J. and S\'{a}nchez-Gonz\'{a}lez, Lidia and Fern\'{a}ndez-Llamas, Camino and Rodr\'{\i}guez-Lera, Francisco J. and Matell\'{a}n-Olivera, Vicente},
title = {Evaluation of teamwork competence acquisition by using CTMTC methodology and learning analytics techniques},
year = {2016},
isbn = {9781450347471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3012430.3012608},
doi = {10.1145/3012430.3012608},
abstract = {Most workplace task and processes involve collaborative work between peers. This makes teamwork competence acquisition a key issue for educational institutions. This type of competence uses to be associated to project-based learning activities. In these activities students' assessment is something complex and mostly based on the outcome developed by the team of students that works on each project. It is not easy to assess the participation of each member of the team, which is also an issue to take into account. In order to facilitate this process CTMTC methodology was defined. It makes easier the assessment of individual and group outcomes in collaborative learning. This paper describes and evaluates the application of CTMTC methodology in several subjects of Computer Science Degree and Electronics Degree of the University of Le\'{o}n. After the application of the methodology it is possible to see a high level of acceptation of the methodology. In addition, the students perceive that they have improve their teamwork skills.},
booktitle = {Proceedings of the Fourth International Conference on Technological Ecosystems for Enhancing Multiculturality},
pages = {787–794},
numpages = {8},
keywords = {teamwork competence, project-based learning, individual performance, group assessment, assessment, CTMTC},
location = {Salamanca, Spain},
series = {TEEM '16}
}

@inproceedings{10.1145/3576050.3576058,
author = {Alfredo, Riordan Dervin and Nie, Lanbing and Kennedy, Paul and Power, Tamara and Hayes, Carolyn and Chen, Hui and McGregor, Carolyn and Swiecki, Zachari and Ga\v{s}evi\'{c}, Dragan and Martinez-Maldonado, Roberto},
title = {"That Student Should be a Lion Tamer!" StressViz: Designing a Stress Analytics Dashboard for Teachers},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576058},
doi = {10.1145/3576050.3576058},
abstract = {In recent years, there has been a growing interest in creating multimodal learning analytics (LA) systems that automatically analyse students’ states that are hard to see with the "naked eye", such as cognitive load and stress levels, but that can considerably shape their learning experience. A rich body of research has focused on detecting such aspects by capturing bodily signals from students using wearables and computer vision. Yet, little work has aimed at designing end-user interfaces that visualise physiological data to support tasks deliberately designed for students to learn from stressful situations. This paper addresses this gap by designing a stress analytics dashboard that encodes students’ physiological data into stress levels during different phases of an authentic team simulation in the context of nursing education. We conducted a qualitative study with teachers to understand (i) how they made sense of the stress analytics dashboard; (ii) the extent to which they trusted the dashboard in relation to students’ cortisol data; and (iii) the potential adoption of this tool to communicate insights and aid teaching practices.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {57–67},
numpages = {11},
keywords = {Affective computing, Healthcare education, LA dashboard, Multimodal dataset, Stress detection, Visualisation},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3448139.3448185,
author = {Poquet, Oleksandra},
title = {Why Birds of a Feather Flock Together: Factors Triaging Students in Online Forums},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448185},
doi = {10.1145/3448139.3448185},
abstract = {Peer effects, an influence that peers can have on one’s learning and development, have been shown to affect student achievement and attitudes. A large-scale analysis of social influences in digital online interactions showed that students interact in online university forums with peers of similar performance. Mechanisms driving this observed similarity remain unclear. To shed light as to why similar peers interact online, the current study examined the role of organizing factors in the formation of similarity patterns in online university forums, using four-years of forum interaction data of a university cohort. In the study, experiments randomized the timing of student activity, relationship between student activity levels within specific courses, and relationship between student activity and performance. Analysis suggests that similarity between students interacting online is shaped by implications of the course design on individual student behaviour, less so by social processes of selection. Social selection may drive observed similarity in later years of student experience, but its role is relatively small compared to other factors. The results highlight the need to consider what social influences are enacted by the course design and technological scaffolding of learner behaviour in online interactions, towards diversifying student social influences.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {469–474},
numpages = {6},
keywords = {university forums, student activity, learning analytics, digital learning, course design, communication networks},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3706468.3706514,
author = {Imhof, Christof and Hlosta, Martin and Bergamin, Per},
title = {Will they or won't they make it in time? The role of contextual and behavioral predictors in reaching deadlines of mandatory assignments},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706514},
doi = {10.1145/3706468.3706514},
abstract = {Procrastination and other forms of irrational delay are widespread among university students, leading to an array of potential negative consequences. While the reasons for this type of behavior are manifold and many facilitating factors have been identified, which of these factors are able to predict dilatory behavior in online/distance education has received comparatively little attention in the literature so far. In this study, we intended to compare the performance of two sets of objective predictors of delay, namely contextual variables based on characteristics of the assignment, and behavioral variables based on log data. Using historical data drawn from our university's learning management system, we calculated Bayesian multilevel models. The strongest and most consistent predictors of dilatory behavior turned out to be interval between the first click on the assignment and its deadline, the interval between the start of a block and the first click on the assignment, the number of clicks on the assignment, and the deadline type. The combination of both sets of predictors slightly improved the model's performance.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {362–372},
numpages = {11},
keywords = {deadlines, dilatory behavior, log data, predictive models, procrastination},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3636555.3636899,
author = {Leon, Amelia and Nie, Allen and Chandak, Yash and Brunskill, Emma},
title = {Estimating the Causal Treatment Effect of Unproductive Persistence},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636899},
doi = {10.1145/3636555.3636899},
abstract = {There has been considerable work in classifying and predicting unproductive persistence, but much less in understanding its causal impact on downstream outcomes of interest, like external assessments. In general, it is experimentally challenging to understand the causal impact because, unlike in many other settings, we cannot directly intervene (to conduct a randomized control trial) and cause students to struggle unproductively in an authentic manner. In this work, we use data from a prior study that used virtual reality headsets to alert teacher’s attention to students who were unproductively struggling. We show that we can use this as an instrumental variable, and use a two-stage least squares analysis to provide a causal estimate of the treatment effect of unproductive persistence on post-test performance. Our results further strengthen the importance of unproductive struggle and highlight the potential of leveraging instruments to identify causal treatment effects of student behaviors during the use of educational technology.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {843–849},
numpages = {7},
keywords = {Causal Inference, Education, Instrumental Variable, Unproductive Persistence},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636875,
author = {Cloude, Elizabeth B. and Munshi, Anabil and Andres, J. M. Alexandra and Ocumpaugh, Jaclyn and Baker, Ryan S. and Biswas, Gautam},
title = {Exploring Confusion and Frustration as Non-linear Dynamical Systems},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636875},
doi = {10.1145/3636555.3636875},
abstract = {Numerous studies aim to enhance learning in digital environments through emotionally-sensitive interventions. The D’Mello and Graesser (2012) model of affect dynamics hypothesizes that when a learner encounters confusion, the degree to which it is prolonged (and transitions into frustration) or resolved, significantly affects their learning outcomes in digital environments. However, studies yield inconclusive results regarding relations between confusion, frustration, and learning. More research is needed to explore how confusion and frustration manifest during learning and its relation to outcomes. We go beyond past work looking at the rate, duration, and transitions of confusion and frustration by treating these affective states as non-linear dynamical systems consisting of expressive and behavioral components. We examined the frequency and recurrence of facial expressions associated with basic emotions (as automatically labeled by AffDex, a standard tool for analyzing emotions with video data) during confused and frustrated states (as automatically labeled with BROMP-based detectors applied to students’ interaction data). We compare these co-occurring patterns to learning outcomes (pre-tests, post-tests, and learning gains) within a digital learning environment, Betty’s Brain. Results showed that the frequency and recurrence rate of basic emotions expressed during confusion and frustration are complex and remain incompletely understood. Specifically, we show that confusion and frustration have different relationships with learning outcomes, depending on which basic emotion expressions they co-occur with. Implications of this study open avenues for better understanding these emotions as complex and non-linear dynamical systems, in the long-term enabling personalized feedback and emotional support within digital learning environments that enhance learning outcomes.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {241–252},
numpages = {12},
keywords = {Confusion, Digital Learning Environments, Frustration, Learning Outcomes, Non-linear Dynamical Systems},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3506860.3506916,
author = {Aguilar, Stephen J},
title = {Experimental Evidence of Performance Feedback vs. Mastery Feedback on Students’ Academic Motivation},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506916},
doi = {10.1145/3506860.3506916},
abstract = {Work throughout the learning analytics community has examined associations between Learning Analytics Dashboard (LAD) features and a number of important student outcomes, including academic motivation and self-regulated learning strategies. While there are many potential implications of visualized academic information within a LAD on student outcomes, there remains an unanswered question: are there causal differences between showing performance information (e.g., comparing students’ progress to the class average) vs. mastery information (e.g., their individual score) on students’ motivation? Grounded in Achievement Goal Theory, this study answers this question experimentally by analyzing the difference between college students’ (n=445) reported achievement goal orientations as well as their motivated information seeking orientations after being presented with performance or mastery feedback. Results indicate that students in a performance condition which displayed ”above average” achievement on an academic measure reported lower performance-avoidance goals (e.g., not wanting to do worse than everyone else), and performance-avoidance information-seeking goals (e.g., not wanting to seek out information showing that one does worse than peers) when compared to students in the mastery control condition. This study contributes to our understanding of the motivational implications of academic feedback presented to students, and suggests that comparative information has direct effects on student motivation. Results thus uncover a potential tension between what might seem intuitive feedback to give students versus what might be more motivationally appropriate. The implications of this work point to the need to understand LADs not simply as feedback mechanisms, but as embedded features of a learning environment that influence how students engage with course content.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {556–562},
numpages = {7},
keywords = {Visualizations, Non-cognitive factors, Motivation, Higher Education},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3576050.3576115,
author = {Wong, Aaron Y. and Bryck, Richard L. and Baker, Ryan S. and Hutt, Stephen and Mills, Caitlin},
title = {Using a Webcam Based Eye-tracker to Understand Students’ Thought Patterns and Reading Behaviors in Neurodivergent Classrooms},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576115},
doi = {10.1145/3576050.3576115},
abstract = {Previous learning analytics efforts have attempted to leverage the link between students’ gaze behaviors and learning experiences to build effective real-time interventions. Historically, however, these technologies have not been scalable due to the high cost of eye-tracking devices. Further, such efforts have been almost exclusively focused on neurotypical students, despite recent work that suggests a “one size fits many” approach can disadvantage neurodivergent students. Here we attempt to address these limitations by examining the validity and applicability of using scalable, webcam-based eye tracking as a basis for adaptively responding to neurodivergent students in an educational setting. Forty-three neurodivergent students read a text and answered questions about their in-situ thought patterns while a webcam-based eye tracker assessed their gaze locations. Results indicate that eye-tracking measures were sensitive to: 1) moments when students experienced difficulty disengaging from their own thoughts and 2) students’ familiarity with the text. Our findings highlight the fact that a free, open-source, webcam-based eye-tracker can be used to assess differences in reading patterns and online thought patterns. We discuss the implications and possible applications of these results, including the idea that webcam-based eye tracking may be a viable solution for designing real-time interventions for neurodivergent student populations.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {453–463},
numpages = {11},
keywords = {Educational technology, Eye-Tracking, Neurodivergence, Webcam-based tracking},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/2723576.2723599,
author = {Whitelock, Denise and Twiner, Alison and Richardson, John T. E. and Field, Debora and Pulman, Stephen},
title = {OpenEssayist: a supply and demand learning analytics tool for drafting academic essays},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723599},
doi = {10.1145/2723576.2723599},
abstract = {This paper focuses on the use of a natural language analytics engine to provide feedback to students when preparing an essay for summative assessment. OpenEssayist is a real-time learning analytics tool, which operates through the combination of a linguistic analysis engine that processes the text in the essay, and a web application that uses the output of the linguistic analysis engine to generate the feedback. We outline the system itself and present analysis of observed patterns of activity as a cohort of students engaged with the system for their module assignments. We report a significant positive correlation between the number of drafts submitted to the system and the grades awarded for the first assignment. We can also report that this cohort of students gained significantly higher overall grades than the students in the previous cohort, who had no access to OpenEssayist. As a system that is content free, OpenEssayist can be used to support students working in any domain that requires the writing of essays.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {208–212},
numpages = {5},
keywords = {online distance education, natural language processing, educational performance, automated formative feedback, academic essay writing},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/3448139.3448171,
author = {Saint, John and Fan, Yizhou and Singh, Shaveen and Gasevic, Dragan and Pardo, Abelardo},
title = {Using process mining to analyse self-regulated learning: a systematic analysis of four algorithms},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448171},
doi = {10.1145/3448139.3448171},
abstract = {The conceptualisation of self-regulated learning (SRL) as a process that unfolds over time has influenced the way in which researchers approach analysis. This gave rise to the use of process mining in contemporary SRL research to analyse data about temporal and sequential relations of processes that occur in SRL. However, little attention has been paid to the choice and combinations of process mining algorithms to achieve the nuanced needs of SRL research. We present a study that 1) analysed four process mining algorithms that are most commonly used in the SRL literature – Inductive Miner, Heuristics Miner, Fuzzy Miner, and pMineR; and 2) examined how the metrics produced by the four algorithms complement each. The study looked at micro-level processes that were extracted from trace data collected in an undergraduate course (N=726). The study found that Fuzzy Miner and pMineR offered better insights into SRL than the other two algorithms. The study also found that a combination of metrics produced by several algorithms improved interpretation of temporal and sequential relations between SRL processes. Thus, it is recommended that future studies of SRL combine the use of process mining algorithms and work on new tools and algorithms specifically created for SRL research.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {333–343},
numpages = {11},
keywords = {Self-Regulated Learning, Process Mining, Micro-level Process Analysis, Learning Analytics},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3636555.3636863,
author = {Song, Yukyeong and Li, Chenglu and Xing, Wanli and Li, Shan and Lee, Hakeoung Hannah},
title = {A Fair Clustering Approach to Self-Regulated Learning Behaviors in a Virtual Learning Environment},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636863},
doi = {10.1145/3636555.3636863},
abstract = {While virtual learning environments (VLEs) are widely used in K-12 education for classroom instruction and self-study, young students’ success in VLEs highly depends on their self-regulated learning (SRL) skills. Therefore, it is important to provide personalized support for SRL. One important precursor of designing personalized SRL support is to understand students’ SRL behavioral patterns. Extensive studies have clustered SRL behaviors and prescribed personalized support for each cluster. However, limited attention has been paid to the algorithm bias and fairness of clustering results. In this study, we “fairly” clustered the behavioral patterns of SRL using fair-capacitated clustering (FCC), an algorithm that incorporates constraints to ensure fairness in the assignment of data points. We used data from 14,251 secondary school learners in a virtual math learning environment. The results of FCC showed that it could capture six clusters of SRL behaviors in a fair way; three clusters belonging to high-performing (i.e., H-1. Help-provider, H-2) Active SRL learner, H-3) Active onlooker), and three clusters in low-performing groups (i.e., L-1) Quiz-taker, L-2) Dormant learner, and L-3) Inactive onlooker). The findings provide a better understanding of SRL patterns in online learning and can potentially guide the design of personalized support for SRL.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {771–778},
numpages = {8},
keywords = {Fair clustering, Self-regulated learning, Virtual learning environment},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3706468.3706519,
author = {Aguinalde, Anna Pauline and Shin, Jinnie},
title = {Talking in Sync: How Linguistic Synchrony Shapes Teacher-Student Conversation in English as a Second Language Tutoring Environment},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706519},
doi = {10.1145/3706468.3706519},
abstract = {Linguistic synchrony, or alignment, has been shown to be critical for student learning, particularly for L2 students (second language learners), whose patterns of synchrony often differ from fluent speakers due to proficiency constraints. While many studies have explored various dimensions of synchrony in global language tutoring contexts, there is a gap in understanding how linguistic synchrony evolves dynamically over the course of a tutoring session and how tutors’ pedagogical strategies influence this process. This study incorporates three dimensions of synchrony—lexical, syntactic, and semantic—along with tutors’ dialogue acts to evaluate their association with student performance using multivariate time-series analysis. Results indicate that lower-performing L2 students tend to lexically align with their tutor more consistently in the long term and with higher intensity in the short term. In contrast, higher-performing students demonstrate greater alignment with the tutor in syntactic and semantic dimensions. Furthermore, the dialogue acts of eliciting, scaffolding, and enquiry were found to play the strongest roles in influencing synchrony and impacting learning outcomes.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {395–406},
numpages = {12},
keywords = {Linguistic Synchrony, Dialogue acts, Language learner, Tutoring Conversation, Time-series analysis},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3636555.3636862,
author = {Pishtari, Gerti and Sarmiento-M\'{a}rquez, Edna and Rodr\'{\i}guez-Triana, Mar\'{\i}a Jes\'{u}s and Wagner, Marlene and Ley, Tobias},
title = {Mirror mirror on the wall, what is missing in my pedagogical goals? The Impact of an AI-Driven Feedback System on the Quality of Teacher-Created Learning Designs},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636862},
doi = {10.1145/3636555.3636862},
abstract = {Given the rising prominence of Artificial Intelligence (AI) in education, understanding its impact on teacher practices is essential. This paper presents an ABAB reversal design study conducted during a teacher training, where an AI-driven feedback system helped 19 teachers to create learning designs. It investigates the impact that the AI-driven feedback had on the quality of designs and assesses pre- and post-training shifts in teachers’ perceptions of the technology. We observed statistical differences between designs crafted without (in phase A1) and with AI (B1). Notably, a small positive influence persisted even after AI withdrawal (A2). This hints that specialized AI algorithms for learning design can assist teachers in effectively achieving their design objectives. Despite noticeable shifts in teachers’ perceived understanding and usefulness of AI, their trust and intention to use remained unchanged. For a successful teacher-AI partnership, future research should explore the long-term impact that AI usage can have on learning design practices and strategies to nurture trust.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {145–156},
numpages = {12},
keywords = {Artificial Intelligence, Design Analytics, Inquiry-Based Learning, Learning Design, Mobile Learning, Teacher Training},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3706468.3706494,
author = {Jansen, Thorben and Horbach, Andrea and Meyer, Jennifer},
title = {Feedback from Generative AI: Correlates of Student Engagement in Text Revision from 655 Classes from Primary and Secondary School},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706494},
doi = {10.1145/3706468.3706494},
abstract = {Writing is fundamental in knowledge-based societies, and engaging students in text revision through feedback is critical for developing students’ writing skills. Automated feedback offers a promising solution to teachers’ time constraints creating feedback. However, prior research indicates that 20 to 71 percent of students receiving feedback do not engage in any text revision. Despite these concerning figures, students’ non-engagement has not received widespread attention, likely due to fragmented evidence from a few grade levels and writing tasks disconnected from regular teaching. Further, whether the issue persists when generative AI generates the feedback is unclear. The present study investigates what percentage of students behaviorally engage with feedback from generative AI in authentic classroom learning contexts. We analyzed data from an educational technology company, including 655 teacher-generated writing tasks involving 14,236 students across grades 1-12. Our findings show that around half of the students did not revise a single character in the text after receiving feedback. The percentage was similar across grade levels, task types, or feedback characteristics. We discuss the importance of including the percentage of engaged students as an additional metric in feedback research to achieve the goal that no student is left behind.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {831–836},
numpages = {6},
keywords = {student engagement, automated feedback, writing, generative AI},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3506860.3506899,
author = {Hicks, Ben and Kitto, Kirsty and Payne, Leonie and Buckingham Shum, Simon},
title = {Thinking with causal models: A visual formalism for collaboratively crafting assumptions},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506899},
doi = {10.1145/3506860.3506899},
abstract = {Learning Analytics (LA) is a bricolage field that requires a concerted effort to ensure that all stakeholders it affects are able to contribute to its development in a meaningful manner. We need mechanisms that support collaborative sense-making. This paper argues that graphical causal models can help us to span the disciplinary divide, providing a new apparatus to help educators understand, and potentially challenge, the technical models developed by LA practitioners as they form. We briefly introduce causal modelling, highlighting its potential benefits in helping the field to move from associations to causal claims, and illustrate how graphical causal models can help us to reason about complex statistical models. The approach is illustrated by applying it to the well known problem of at-risk modelling.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {250–259},
numpages = {10},
keywords = {causal models, diagrammatic reasoning, directed acyclic graphs, transdisciplinary collaboration},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3636555.3636929,
author = {Pradhan, Siddhartha and Gurung, Ashish and Ottmar, Erin},
title = {Gamification and Deadending: Unpacking Performance Impacts in Algebraic Learning},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636929},
doi = {10.1145/3636555.3636929},
abstract = {This study explores the effects of varying problem-solving strategies on students’ future performance within the gamified algebraic learning platform From Here To There! (FH2T). The study focuses on the procedural pathways students adopted, transitioning from a start state to a goal state in solving algebraic problems. By dissecting the nature of these pathways—optimal, sub-optimal, incomplete, and dead-end—we sought correlations with post-test outcomes. A striking observation was that students who frequently engaged in what we term ‘regular dead-ending behavior’, were significantly correlated with higher post-test performance. This finding underscores the potential of exploratory learner behavior within a low-stakes gamified framework in bolstering algebraic comprehension. The implications of our findings are twofold: they accentuate the significance of tailoring gamified platforms to student behaviors and highlight the potential benefits of fostering an environment that promotes exploration without retribution. Moreover, our insights hint at the notion that fostering exploratory behavior could be instrumental in cultivating mathematical flexibility.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {899–906},
numpages = {8},
keywords = {algebraic learning, gamification, math flexibility, networks, procedural pathways},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636853,
author = {Singh, Anjali and Brooks, Christopher and Wang, Xu and Li, Warren and Kim, Juho and Wilson, Deepti},
title = {Bridging Learnersourcing and AI: Exploring the Dynamics of Student-AI Collaborative Feedback Generation},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636853},
doi = {10.1145/3636555.3636853},
abstract = {This paper explores the space of optimizing feedback mechanisms in complex domains such as data science, by combining two prevailing approaches: Artificial Intelligence (AI) and learnersourcing. Towards addressing the challenges posed by each approach, this work compares traditional learnersourcing with an AI-supported approach. We report on the results of a randomized controlled experiment conducted with 72 Master’s level students in a data visualization course, comparing two conditions: students writing hints independently versus revising hints generated by GPT-4. The study aimed to evaluate the quality of learnersourced hints, examine the impact of student performance on hint quality, gauge learner preference for writing hints with versus without AI support, and explore the potential of the student-AI collaborative exercise in fostering critical thinking about LLMs. Based on our findings, we provide insights for designing learnersourcing activities leveraging AI support and optimizing students’ learning as they interact with LLMs.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {742–748},
numpages = {7},
keywords = {Data Visualization, Feedback Generation, GPT-4, Learnersourcing},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636867,
author = {Henricks, Genevieve and Perry, Michelle and Bhat, Suma},
title = {The Relation Among Gender, Language, and Posting Type in Online Chemistry Course Discussion Forums},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636867},
doi = {10.1145/3636555.3636867},
abstract = {This study explored gendered language used in an online chemistry course’s discussion forums, to understand how using gendered language might help or hinder learning outcomes, while considering the goal of various posting structures required in the course. Findings revealed that although gendered-language use did not differ between men and women, gendered forms of language were widely used throughout the forums. The use of gendered language appeared strategic, however, and reliably varied by the goal of the discussion post (i.e., posting a solution to a homework problem, asking a question, or answering a question). Ultimately, gender, language and posting type were found to be related to final grade.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {189–199},
numpages = {11},
keywords = {Gendered language, Online discussion, STEM education},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3706468.3706479,
author = {R\"{u}dian, Sylvio and Podelo, Julia and Ku\v{z}\'{\i}lek, Jakub and Pinkwart, Niels},
title = {Feedback on Feedback: Student’s Perceptions for Feedback from Teachers and Few-Shot LLMs},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706479},
doi = {10.1145/3706468.3706479},
abstract = {Large language models (LLMs) can be a valuable resource for generating texts and performing various instruction-based tasks. In this paper, we explored the use of LLMs, particularly for generating feedback for students in higher education. More precisely, we conducted an experiment to examine students’ perceptions regarding LLM-generated feedback. This has the overall aim of assisting teachers in the feedback creation process. First, we examine the different student perceptions regarding the feedback that students got without being aware of whether it was created by their teacher or an LLM. Our results reveal that the feedback source has not impacted how it was perceived by the students, except in cases where repetitive content has been generated, which is a known limitation of LLMs. Second, students have been asked to identify whether the feedback comes from an LLM or the teacher. The results demonstrate, that students were unable to identify the feedback source. A small subset of indicators has been identified, that clearly revealed from whom the feedback comes from. Third, student perceptions are analyzed while knowing that feedback has been auto-generated. This examination indicates that generated feedback is likely to be met with resistance. It contradicts the findings of the first examination. This emphasizes the need of a teacher-in-the-loop approach when employing auto-generated feedback in higher education.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {82–92},
numpages = {11},
keywords = {Large Language Models, Prompt Engineering, Feedback Indicators, Language Learning},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706468.3706566,
author = {Yeung, Steven},
title = {A comparative study of rule-based, machine learning and large language model approaches in automated writing evaluation (AWE)},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706566},
doi = {10.1145/3706468.3706566},
abstract = {Automated Writing Evaluation (AWE) tools have proved beneficial to writing development. Research on AWE methods is essential for improving tool performance and further comparative studies are needed as new methods emerge. This study examines the performance of several AWE approaches, comparing rule-based and statistical methods, machine learning (ML) models, and a large language model (LLM). These three AWE methods were applied to a representative sample of academic essays from the TOEFL11 dataset to compare their assessment performance. Results show that the selected LLM, GPT-4, outperformed the other two approaches in terms of QWK and Pearson’s correlation coefficient, while the Support Vector Machine (SVM) model in the ML approach had the highest accuracy and the lowest mean absolute error. This paper provides a detailed comparison of these three approaches and discusses implications for educational practice and future research around AWE.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {984–991},
numpages = {8},
keywords = {Rule-based method, machine learning, large language model, automated writing evaluation, automated essay scoring, generative AI},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3576050.3576069,
author = {Zamecnik, Andrew and Joksimovic, Srecko and Kovanovic, Vitomir and Grossmann, Georg and Ladjal, Djazia and Pardo, Abelardo},
title = {Exploring the Feedback Provision of Mentors and Clients for Teams in Work-Integrated Learning Environments},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576069},
doi = {10.1145/3576050.3576069},
abstract = {Industry supervisors play a pivotal role in ongoing learner support and guidance within a work-integrated learning context. Effective provisional feedback from industry supervisors in work-integrated learning environments is essential for increasing a team’s metacognitive awareness and ability to evaluate their performance. However, research that examines the usefulness and type of feedback from industry supervisors for teams remains limited. In this study, we investigate the quality of provisional feedback by comparing the teams’ helpfulness rating of the feedback from two types of industry supervisors (i.e., clients and mentors), based on the feedback type (task, process, regulatory and self-level oriented) using learning analytics. The results show that teams rated the perceived helpfulness scores of clients and mentors as very useful, with mentors providing slightly more helpful feedback. We also found that mentors provide more co-occurrences of feedback classifications than clients. The overall results show that teams perceive mentor feedback as more helpful than clients and that the mentor targets feedback that is more beneficial to the teams learning than the clients. Our findings can aid in developing guidelines that aim to validate and improve existing or new feedback quality frameworks by leveraging backward evaluation data.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {566–571},
numpages = {6},
keywords = {epistemic network analysis, feedback, helpfulness rating, teams, work-integrated learning},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/2462476.2462496,
author = {Fernandez-Medina, Carlos and P\'{e}rez-P\'{e}rez, Juan Ram\'{o}n and \'{A}lvarez-Garc\'{\i}a, V\'{\i}ctor M. and Paule-Ruiz, M. del Puerto},
title = {Assistance in computer programming learning using educational data mining and learning analytics},
year = {2013},
isbn = {9781450320788},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2462476.2462496},
doi = {10.1145/2462476.2462496},
abstract = {The learning of programming presents many difficulties for students. Nowadays, a number of software tools are available that enable students in programming courses to develop and exercise their knowledge and skills. However, these tools do not examine their work or provide students with indications on their learning process. In this paper we introduce a learning approach for programming based on the analysis of students' mistakes during practical lessons in programming subjects. This approach makes use of compiler messages to analyse their quantity and semantic value, and report the individual and comparative learning progress. This approach is illustrated in practice by a case study conducted in a class of undergraduate students of computer science. This study makes it possible to provide an analytic representation of reflective learning practice, giving us a better understanding on programming learning processes.},
booktitle = {Proceedings of the 18th ACM Conference on Innovation and Technology in Computer Science Education},
pages = {237–242},
numpages = {6},
keywords = {e-learning, eclipse plug-ins, integrated development environment, learning analytics, programming errors, programming language},
location = {Canterbury, England, UK},
series = {ITiCSE '13}
}

@inproceedings{10.1145/3109859.3109870,
author = {Karypis, George},
title = {Improving Higher Education: Learning Analytics &amp; Recommender Systems Research},
year = {2017},
isbn = {9781450346528},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109859.3109870},
doi = {10.1145/3109859.3109870},
abstract = {An enduring issue in higher education is student retention to successful graduation. Studies in the U.S. report that average six-year graduation rates across higher-education institutions is 59% and have remained relatively stable over the last 15 years. For those that do complete a college degree, less than half complete within four-years. Requiring additional terms or leaving college without receiving a bachelor's degree has high human and monetary costs and deprives students from the economic benefits of a college credential (over $1 million in a lifetime and even higher in STEM fields). Moreover, when students do not succeed in graduating, local and national communities struggle to create an educated workforce. Estimates indicate that by 2020 over 64% of the jobs in the U.S. will require at least some post-secondary education. These challenges have been recognized by the U.S. National Research Council, which identified that there is a critical need to develop innovative approaches to enable higher-education institutions retain students, ensure their timely graduation, and are well-trained and workforce ready in their field of study. Failure to do so represents a significant problem as it deprives the U.S. of the highly skilled workforce that it needs to successfully compete in the modern world.This talk describes various efforts under way to develop "Big Data" methods to analyze in a comprehensive manner, the large and diverse types of education and learning-related data in order to improve undergraduate education. These methods are motivated by and are designed to address various interrelated issues that have a significant impact on college student success and include: (i) academic pathways towards successful and timely graduation from the student perspective; (ii) effective pedagogy by instructors; and (iii) retention and persistence of students from the institutional and advisor perspective. In addition, the talk will discuss areas in which research methods and approaches originally developed by the recommender systems community can be applied to this domain.},
booktitle = {Proceedings of the Eleventh ACM Conference on Recommender Systems},
pages = {2},
numpages = {1},
keywords = {educational data mining, learning analytics, student modeling},
location = {Como, Italy},
series = {RecSys '17}
}

@inproceedings{10.1145/3576050.3576111,
author = {Ng, Jeremy T. D. and Liu, Yiming and Chui, Didier S. Y. and Man, Jack C. H. and Hu, Xiao},
title = {Leveraging LMS Logs to Analyze Self-Regulated Learning Behaviors in a Maker-based Course},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576111},
doi = {10.1145/3576050.3576111},
abstract = {Existing learning analytics (LA) studies on self-regulated learning (SRL) have rarely focused on maker education that emphasizes student autonomy in their learning process. Towards using LA methods for generating evidence of SRL in maker-based courses, this study leverages logs of a learning management system (LMS) with its activity design aligned with the maker-based pedagogy. We explored frequencies and sequential patterns of students’ SRL behaviors as reflected in the LMS logs and their relations with learning performance. Adopting a mixed method approach, we collected and triangulated both quantitative (i.e., system logs, performance scores) and qualitative (i.e., student-written reflections) data sources from 104 students. Based on current LA-based SRL research, we developed an LMS log-based analytic framework to define the SRL phases and behaviors applicable to maker activities. Statistical, data mining, and qualitative analysis methods were conducted on 48,602 logged events and 131 excerpts extracted from student reflections. Results reveal that high-performing students demonstrated some SRL behaviors (e.g., Making Personal Plans, Evaluation) more frequently than their low-performing counterparts, yet the two groups showcased fairly similar sequences of SRL behaviors. Theoretical, methodological and pedagogical implications are drawn for LA-based SRL research and maker education.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {670–676},
numpages = {7},
keywords = {Learning Management System, Maker education, Self-regulated learning},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3636555.3636903,
author = {Xu, Yinuo and Pardos, Zach A.},
title = {Extracting Course Similarity Signal using Subword Embeddings},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636903},
doi = {10.1145/3636555.3636903},
abstract = {Several studies have shown the utility of neural network models in learning course similarities and providing insightful course recommendations from enrollment data. In this study, we explore if additional signals can be found in the morphological structure of course names. We train skip-gram, FastText, and other combination models on these course sequence data from the past nine years and compare results with state-of-the-art models. We find a 97.95% improvement in model performance (as measured by recall @ 10 in similarity-based course recommendations) from skip-gram to FastText, and 80.75% improvement from the current best combination model to the previous state-of-the-art model, indicating that the naming convention of courses (e.g., PHYS_H101) carries valuable signals. We define attributes with which to categorize course pairs from our validation set and present an analysis of which models are strongest and weakest at predicting the similarity of which categories of course pairs. Additionally, we also explore course-taking culture, analyzing if courses with the same demographic features are learned to be more similar. Our approach could help students find alternatives to full courses, improve existing course recommendation systems and course articulations between institutions, and assist institutions in course policy-making.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {857–863},
numpages = {7},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636905,
author = {Suraworachet, Wannapon and Seon, Jennifer and Cukurova, Mutlu},
title = {Predicting challenge moments from students' discourse: A comparison of GPT-4 to two traditional natural language processing approaches},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636905},
doi = {10.1145/3636555.3636905},
abstract = {Effective collaboration requires groups to strategically regulate themselves to overcome challenges. Research has shown that groups may fail to regulate due to differences in members’ perceptions of challenges which may benefit from external support. In this study, we investigated the potential of leveraging three distinct natural language processing models: an expert knowledge rule-based model, a supervised machine learning (ML) model and a Large Language model (LLM), in challenge detection and challenge dimension identification (cognitive, metacognitive, emotional and technical/other challenges) from student discourse, was investigated. The results show that the supervised ML and the LLM approaches performed considerably well in both tasks, in contrast to the rule-based approach, whose efficacy heavily relies on the engineered features by experts. The paper provides an extensive discussion of the three approaches’ performance for automated detection and support of students’ challenge moments in collaborative learning activities. It argues that, although LLMs provide many advantages, they are unlikely to be the panacea to issues of the detection and feedback provision of socially shared regulation of learning due to their lack of reliability, as well as issues of validity evaluation, privacy and confabulation. We conclude the paper with a discussion on additional considerations, including model transparency to explore feasible and meaningful analytical feedback for students and educators using LLMs.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {473–485},
numpages = {13},
keywords = {Challenge moments, Collaborative learning, Discourse analysis, Natural language processing},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3375462.3375493,
author = {Uzir, Nora'ayu Ahmad and Ga\v{s}evi\'{c}, Dragan and Jovanovi\'{c}, Jelena and Matcha, Wannisa and Lim, Lisa-Angelique and Fudge, Anthea},
title = {Analytics of time management and learning strategies for effective online learning in blended environments},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375493},
doi = {10.1145/3375462.3375493},
abstract = {This paper reports on the findings of a study that proposed a novel learning analytics methodology that combines three complimentary techniques - agglomerative hierarchical clustering, epistemic network analysis, and process mining. The methodology allows for identification and interpretation of self-regulated learning in terms of the use of learning strategies. The main advantage of the new technique over the existing ones is that it combines the time management and learning tactic dimensions of learning strategies, which are typically studied in isolation. The new technique allows for novel insights into learning strategies by studying the frequency of, strength of connections between, and ordering and time of execution of time management and learning tactics. The technique was validated in a study that was conducted on the trace data of first-year undergraduate students who were enrolled into two consecutive offerings (N2017 = 250 and N2018 = 232) of a course at an Australian university. The application of the proposed technique identified four strategy groups derived from three distinct time management tactics and five learning tactics. The tactics and strategies identified with the technique were correlated with academic performance and were interpreted according to the established theories and practices of self-regulated learning.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {392–401},
numpages = {10},
keywords = {blended learning, learning analytics, learning strategies, self-regulated learning, time management strategies},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3636555.3636924,
author = {Demszky, Dorottya and Wang, Rose and Geraghty, Sean and Yu, Carol},
title = {Does Feedback on Talk Time Increase Student Engagement? Evidence from a Randomized Controlled Trial on a Math Tutoring Platform},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636924},
doi = {10.1145/3636555.3636924},
abstract = {Providing ample opportunities for students to express their thinking is pivotal to their learning of mathematical concepts. We introduce the Talk Meter, which provides in-the-moment automated feedback on student-teacher talk ratios. We conduct a randomized controlled trial on a virtual math tutoring platform (n=742 tutors) to evaluate the effectiveness of the Talk Meter at increasing student talk. In one treatment arm, we show the Talk Meter only to the tutor, while in the other arm we show it to both the student and the tutor. We find that the Talk Meter increases student talk ratios in both treatment conditions by 13-14%; this trend is driven by the tutor talking less in the tutor-facing condition, whereas in the student-facing condition it is driven by the student expressing significantly more mathematical thinking. Through interviews with tutors, we find the student-facing Talk Meter was more motivating to students, especially those with introverted personalities, and was effective at encouraging joint effort towards balanced talk time. These results demonstrate the promise of in-the-moment joint talk time feedback to both teachers and students as a low cost, engaging, and scalable way to increase students’ mathematical reasoning.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {632–644},
numpages = {13},
keywords = {automated feedback, joint feedback to students and teachers, math tutoring, randomized controlled trial, student engagement, talk time},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636858,
author = {Kandemir, Erva Nihan and Vie, Jill-J\^{e}nn and Sanchez-Ayte, Adam and Palombi, Olivier and Ramus, Franck},
title = {Adaptation of the Multi-Concept Multivariate Elo Rating System to Medical Students' Training Data},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636858},
doi = {10.1145/3636555.3636858},
abstract = {Accurate estimation of question difficulty and prediction of student performance play key roles in optimizing educational instruction and enhancing learning outcomes within digital learning platforms. The Elo rating system is widely recognized for its proficiency in predicting student performance by estimating both question difficulty and student ability while providing computational efficiency and real-time adaptivity. This paper presents an adaptation of a multi-concept variant of the Elo rating system to the data collected by a medical training platform—a platform characterized by a vast knowledge corpus, substantial inter-concept overlap, a huge question bank with significant sparsity in user-question interactions, and a highly diverse user population, presenting unique challenges. Our study is driven by two primary objectives: firstly, to comprehensively evaluate the Elo rating system’s capabilities on this real-life data, and secondly, to tackle the issue of imprecise early-stage estimations when implementing the Elo rating system for online assessments. Our findings suggest that the Elo rating system exhibits comparable accuracy to the well-established logistic regression model in predicting final exam outcomes for users within our digital platform. Furthermore, results underscore that initializing Elo rating estimates with historical data remarkably reduces errors and enhances prediction accuracy, especially during the initial phases of student interactions.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {123–133},
numpages = {11},
keywords = {Elo-based learning model, knowledge tracing, logistic regression},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636917,
author = {Chandler, Chelsea and Breideband, Thomas and Reitman, Jason G. and Chitwood, Marissa and Bush, Jeffrey B. and Howard, Amanda and Leonhart, Sarah and Foltz, Peter W. and Penuel, William R. and D'Mello, Sidney K.},
title = {Computational Modeling of Collaborative Discourse to Enable Feedback and Reflection in Middle School Classrooms},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636917},
doi = {10.1145/3636555.3636917},
abstract = {Collaboration analytics has the potential to empower teachers and students with valuable insights to facilitate more meaningful and engaging collaborative learning experiences. Towards this end, we developed computational models of student speech during small group work, identifying instances of uplifting behavior related to three Community Agreements: community building, moving thinking forward, and being respectful. Pre-trained RoBERTa language models were fine-tuned and evaluated on human annotated data (N = 9,607 student utterances from 100 unique 5-minute classroom recordings). The models achieved moderate accuracies (AUROCs between 0.67-0.84) and were robust to speech recognition errors. Preliminary generalizability studies indicated that the models generalized well to two other domains (transfer ratios between 0.46-0.85; with 1.0 indicating perfect transfer). We also developed four approaches to provide qualitative feedback in the form of noticings (i.e., specific exemplars) of positive instances of the Community Agreements, finding moderate alignment with human ratings. This research contributes to the computational modeling of the relationship dimension of collaboration from noisy classroom data, selection of positive examples for qualitative feedback, and towards the empowerment of teachers to support diverse learners during collaborative learning.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {576–586},
numpages = {11},
keywords = {Collaboration analytics, Natural language processing},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3506860.3506880,
author = {Tzi-Dong Ng, Jeremy and Wang, Zuo and Hu, Xiao},
title = {Needs Analysis and Prototype Evaluation of Student-facing LA Dashboard for Virtual Reality Content Creation},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506880},
doi = {10.1145/3506860.3506880},
abstract = {Being a promising constructionist pedagogy in recent years, maker education empowers students to take agency of their learning process through constructing both knowledge and real-world physical or digital products and fosters peer interactions for collective innovation. Learning Analytics (LA) excels at generating personalized, fine-grained feedback in near real-time and holds much potential in supporting process-oriented and peer-supported learning activities, including maker activities. In the context of virtual reality (VR) content creation for cultural heritage education, this study qualitatively solicited 27 students’ needs on progress monitoring, reflection, and feedback during their making process. Findings have inspired the prototype design of a student-facing LA dashboard (LAVR). Leveraging multimodal learning analytics (MmLA) such as text and audio analytics to fulfill students’ needs, the prototype has various features and functions including automatic task reminders, content quality detection, and real-time feedback on quality of audio-visual elements. A preliminary evaluation of the prototype with 10 students confirms its potential in supporting students’ self-regulated learning during the making process and for improving the quality of VR content. Implications on LA design for supporting maker education are discussed. Future work is planned to include implementation and evaluation of the dashboard in classrooms.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {444–450},
numpages = {7},
keywords = {Dashboard, Needs analysis, Prototype evaluation, VR content creation},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3448139.3448149,
author = {Chan, Wai-Lun and Yeung, Dit-Yan},
title = {Clickstream Knowledge Tracing: Modeling How Students Answer Interactive Online Questions},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448149},
doi = {10.1145/3448139.3448149},
abstract = {Knowledge tracing (KT) is a research topic which seeks to model the knowledge acquisition process of students by analyzing their past performance in answering questions, based on which their performance in answering future questions is predicted. However, existing KT models only consider whether a student answers a question correctly when the answer is submitted but not the in-question activities. We argue that the interaction involved in the in-question activities can at least partially reveal the thinking process of the student, and hopefully even the competence of acquiring or understanding each piece of the knowledge required for the question. Based on real student interaction clickstream data collected from an online learning platform on which students solve mathematics problems, we conduct clustering analysis for each question to show that clickstreams can reflect different student behaviors. We then propose the first clickstream-based KT model, dubbed clickstream knowledge tracing (CKT), which augments a basic KT model by modeling the clickstream activities of students when answering questions. We apply different variants of CKT and compare them with the baseline KT model which does not use clickstream data. Despite the limited number of questions with clickstream data and its noisy nature which may compromise the data quality, we show that incorporating clickstream data leads to performance improvement. Through this pilot study, we hope to open a new direction in KT research to analyze finer-grained interaction data of students on online learning platforms.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {99–109},
numpages = {11},
keywords = {Clickstream Analytics, Interactive Questions, Knowledge Modeling, Knowledge Tracing, Learner Modeling, Learning Analytics, Student Behavior Clustering},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3506860.3506891,
author = {Matz, Rebecca L and Lee, Albert M and Fowler, Robin R and Hayward, Caitlin},
title = {Teammates Stabilize over Time in How They Evaluate Their Team Experiences},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506891},
doi = {10.1145/3506860.3506891},
abstract = {It is difficult for instructors, and even students themselves, to become aware in real-time of inequitable behaviors occurring on student teams. Here, we explored a potential measure for inequitable teamwork drawing on data from a digital pedagogical tool designed to surface and disrupt such team behaviors. Students in a large, undergraduate business course completed seven surveys about team health (called team checks) at regular intervals throughout the term, providing information about team dynamics, contributions, and processes. The ways in which changes in students’ scores from team check to team check compared to the median changes for their team were used to identify the proportions of teams with outlier student scores. The results show that for every team size and team check item, the proportion of teams with outliers at the end of the term was smaller than at the beginning of the semester, indicating stabilization in how teammates evaluated their team experiences. In all but two cases, outlying students were not disproportionately likely to identify with historically marginalized groups based on gender or race/ethnicity. Thus, we did not broadly identify teamwork inequities in this specific context, but the method provides a basis for future studies about inequitable team behavior.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {485–491},
numpages = {7},
keywords = {Educational technology, equity, gender, groupwork, higher education, learning analytics, peer evaluation, race/ethnicity, teamwork, undergraduate education},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3375462.3375510,
author = {Torre, Manuel Valle and Tan, Esther and Hauff, Claudia},
title = {edX log data analysis made easy: introducing ELAT: An open-source, privacy-aware and browser-based edX log data analysis tool},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375510},
doi = {10.1145/3375462.3375510},
abstract = {Massive Open Online Courses (MOOCs), delivered on platforms such as edX and Coursera, have led to a surge in large-scale learning research. MOOC platforms gather a continuous stream of learner traces, which can amount to several Gigabytes per MOOC, that learning analytics researchers use to conduct exploratory analyses as well as to evaluate deployed interventions. edX has proven to be a popular platform for such experiments, as the data each MOOC generates is easily accessible to the institution running the MOOC. One of the issues researchers face is the preprocessing, cleaning and formatting of those large-scale learner traces. It is a tedious process that requires considerable computational skills. To reduce this burden, a number of tools have been proposed and released with the aim of simplifying this process. Those tools though still have a significant setup cost, are already out-of-date or require already preprocessed data as a starting point. In contrast, in this paper we introduce ELAT, the edX Log file Analysis Tool, which is browser-based (i.e., no setup costs), keeps the data local (i.e., no server is necessary and the privacy-sensitive learner data is not send anywhere) and takes edX data dumps as input. ELAT does not only process the raw data, but also generates semantically meaningful units (learner sessions instead of just click events) that are visualized in various ways (learning paths, forum participation, video watching sequences). We report on two evaluations we conducted: (i) a technological evaluation and a (ii) user study with potential end users of ELAT. ELAT is open-source and available at https://mvallet91.github.io/ELAT/.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {502–511},
numpages = {10},
keywords = {edX log, learning analytics, log data analysis, massive open online course},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3636555.3636868,
author = {Jiang, Lan and Belitz, Clara and Bosch, Nigel},
title = {Synthetic Dataset Generation for Fairer Unfairness Research},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636868},
doi = {10.1145/3636555.3636868},
abstract = {Recent research has made strides toward fair machine learning. Relatively few datasets, however, are commonly examined to evaluate these fairness-aware algorithms, and even fewer in education domains, which can lead to a narrow focus on particular types of fairness issues. In this paper, we describe a novel dataset modification method that utilizes a genetic algorithm to induce many types of unfairness into datasets. Additionally, our method can generate an unfairness benchmark dataset from scratch (thus avoiding data collection in situations that might exploit marginalized populations), or modify an existing dataset used as a reference point. Our method can increase the unfairness by 156.3% on average across datasets and unfairness definitions while preserving AUC scores for models trained on the original dataset (just 0.3% change, on average). We investigate the generalization of our method across educational datasets with different characteristics and evaluate three common unfairness mitigation algorithms. The results show that our method can generate datasets with different types of unfairness, large and small datasets, different types of features, and which affect models trained with different classifiers. Datasets generated with this method can be used for benchmarking and testing for future research on the measurement and mitigation of algorithmic unfairness.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {200–209},
numpages = {10},
keywords = {data generation, datasets, fair machine learning, student data},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3506860.3506905,
author = {Li, Qiujie and Jung, Yeonji and d'Anjou, Bernice and Wise, Alyssa Friend},
title = {Unpacking Instructors’ Analytics Use: Two Distinct Profiles for Informing Teaching},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506905},
doi = {10.1145/3506860.3506905},
abstract = {This study addresses the gap in knowledge about differences in how instructors use analytics to inform teaching by examining the ways that thirteen college instructors engaged with a set of university-provided analytics. Using multiple walk-through interviews with the instructors and qualitative inductive coding, two profiles of instructor analytics use were identified that were distinct from each other in terms of the goals of analytics use, how instructors made sense of and took actions upon the analytics, and the ways that ethical concerns were conceived. Specifically, one group of instructors used analytics to help students get aligned to and engaged in the course, whereas the other group used analytics to align the course to meet students’ needs. Instructors in both profiles saw ethical questions as central to their learning analytics use, with instructors in one profile focusing on transparency and the other on student privacy and agency. These findings suggest the need to view analytics use as an integrated component of instructor teaching practices and envision complementary sets of technical and pedagogical support that can best facilitate the distinct activities aligned with each profile.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {528–534},
numpages = {7},
keywords = {Data-informed teaching, Instructional dashboards, Teacher inquiry},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3636555.3636860,
author = {Li, Hai and Li, Chenglu and Xing, Wanli and Baral, Sami and Heffernan, Neil},
title = {Automated Feedback for Student Math Responses Based on Multi-Modality and Fine-Tuning},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636860},
doi = {10.1145/3636555.3636860},
abstract = {Open-ended mathematical problems are a commonly used method for assessing students’ abilities by teachers. In previous automated assessments, natural language processing focusing on students’ textual answers has been the primary approach. However, mathematical questions often involve answers containing images, such as number lines, geometric shapes, and charts. Several existing computer-based learning systems allow students to upload their handwritten answers for grading. Yet, there are limited methods available for automated scoring of these image-based responses, with even fewer multi-modal approaches that can simultaneously handle both texts and images. In addition to scoring, another valuable scaffolding to procedurally and conceptually support students while lacking automation is comments. In this study, we developed a multi-task model to simultaneously output scores and comments using students’ multi-modal artifacts (texts and images) as inputs by extending BLIP, a multi-modal visual reasoning model. Benchmarked with three baselines, we fine-tuned and evaluated our approach on a dataset related to open-ended questions as well as students’ responses. We found that incorporating images with text inputs enhances feedback performance compared to using texts alone. Meanwhile, our model can effectively provide coherent and contextual feedback in mathematical settings.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {763–770},
numpages = {8},
keywords = {auto-scoring, automated comment, fine-tuning, image response, multi-modality, open-ended response},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636928,
author = {Dwivedi, Deep and Garg, Ritik and Baghel, Shiva and Thareja, Rushil and Kulshrestha, Ritvik and Mohania, Mukesh and Shukla, Jainendra},
title = {Effecti-Net: A Multimodal Framework and Database for Educational Content Effectiveness Analysis},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636928},
doi = {10.1145/3636555.3636928},
abstract = {Amid the evolving landscape of education, evaluating the impact of educational video content on students remains a challenge. Existing methods for assessment often rely on heuristics and self-reporting, leaving room for subjectivity and limited insight. This study addresses this issue by leveraging physiological sensor data to predict student-perceived content effectiveness. Within the realm of educational content evaluation, prior studies focused on conventional approaches, leaving a gap in understanding the nuanced responses of students to educational materials. To bridge this gap, our research introduces a novel perspective, building upon previous work in multimodal physiological data analysis. Our primary contributions encompass two key elements. First, we present the ’Effecti-Net’ architecture, a sophisticated deep learning model that integrates data from multiple sensor modalities, including Electroencephalogram (EEG), Eye Tracker, Galvanic Skin Response (GSR), and Photoplethysmography (PPG). Second, we introduce the ’DECEP’ dataset, a repository comprising 597 minutes of multimodal sensor data. To assess the effectiveness of our approach, we benchmark it against conventional methods. Remarkably, our model achieves a lowest MSE of 0.1651 and MAE of 0.3544 on the DECEP dataset. It offers educators and content creators a comprehensive framework that promotes the development of more engaging educational content.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {667–677},
numpages = {11},
keywords = {Affective Datasets, Deep Learning, Machine Learning, Multi-Modal frameworks, Physiological Data Analysis, User Studies},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636889,
author = {Sonkar, Shashank and Chen, Xinghe and Le, Myco and Liu, Naiming and Basu Mallick, Debshila and Baraniuk, Richard},
title = {Code Soliloquies for Accurate Calculations in Large Language Models},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636889},
doi = {10.1145/3636555.3636889},
abstract = {High-quality conversational datasets are crucial for the successful development of Intelligent Tutoring Systems (ITS) that utilize a Large Language Model (LLM) backend. Synthetic student-teacher dialogues, generated using advanced GPT-4 models, are a common strategy for creating these datasets. However, subjects like physics that entail complex calculations pose a challenge. While GPT-4 presents impressive language processing capabilities, its limitations in fundamental mathematical reasoning curtail its efficacy for such subjects. To tackle this limitation, we introduce in this paper an innovative stateful prompt design. Our design orchestrates a mock conversation where both student and tutorbot roles are simulated by GPT-4. Each student response triggers an internal monologue, or ‘code soliloquy’ in the GPT-tutorbot, which assesses whether its subsequent response would necessitate calculations. If a calculation is deemed necessary, it scripts the relevant Python code and uses the Python output to construct a response to the student. Our approach notably enhances the quality of synthetic conversation datasets, especially for subjects that are calculation-intensive. The preliminary Subject Matter Expert evaluations reveal that our Higgs model, a fine-tuned LLaMA model, effectively uses Python for computations, which significantly enhances the accuracy and computational reliability of Higgs’ responses.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {828–835},
numpages = {8},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636869,
author = {Belitz, Clara and Lee, HaeJin and Nasiar, Nidhi and Fancsali, Stephen E. and Ritter, Steve and Almoubayyed, Husni and Baker, Ryan S. and Ocumpaugh, Jaclyn and Bosch, Nigel},
title = {Hierarchical Dependencies in Classroom Settings Influence Algorithmic Bias Metrics},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636869},
doi = {10.1145/3636555.3636869},
abstract = {Measuring algorithmic bias in machine learning has historically focused on statistical inequalities pertaining to specific groups. However, the most common metrics (i.e., those focused on individual- or group-conditioned error rates) are not currently well-suited to educational settings because they assume that each individual observation is independent from the others. This is not statistically appropriate when studying certain common educational outcomes, because such metrics cannot account for the relationship between students in classrooms or multiple observations per student across an academic year. In this paper, we present novel adaptations of algorithmic bias measurements for regression for both independent and nested data structures. Using hierarchical linear models, we rigorously measure algorithmic bias in a machine learning model of the relationship between student engagement in an intelligent tutoring system and year-end standardized test scores. We conclude that classroom-level influences had a small but significant effect on models. Examining significance with hierarchical linear models helps determine which inequalities in educational settings might be explained by small sample sizes rather than systematic differences.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {210–218},
numpages = {9},
keywords = {Algorithmic bias, Intelligent tutoring systems, Interactive learning environments, Predictive analytics},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636904,
author = {Vanacore, Kirk and Gurung, Ashish and Sales, Adam and Heffernan, Neil T.},
title = {The Effect of Assistance on Gamers: Assessing The Impact of On-Demand Hints &amp; Feedback Availability on Learning for Students Who Game the System},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636904},
doi = {10.1145/3636555.3636904},
abstract = {Gaming the system, characterized by attempting to progress through a learning activity without engaging in essential learning behaviors, remains a persistent problem in computer-based learning platforms. This paper examines a simple intervention to mitigate the harmful effects of gaming the system by evaluating the impact of immediate feedback on students prone to gaming the system. Using a randomized controlled trial comparing two conditions - one with immediate hints and feedback and another with delayed access to such resources - this study employs a Fully Latent Principal Stratification model to determine whether students inclined to game the system would benefit more from the delayed hints and feedback. The results suggest differential effects on learning, indicating that students prone to gaming the system may benefit from restricted or delayed access to on-demand support. However, removing immediate hints and feedback did not fully alleviate the learning disadvantage associated with gaming the system. Additionally, this paper highlights the utility of combining detection methods and causal models to comprehend and effectively respond to students’ behaviors. Overall, these findings contribute to our understanding of effective intervention design that addresses gaming the system behaviors, consequently enhancing learning outcomes in computer-based learning platforms.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {462–472},
numpages = {11},
keywords = {Causal Inference, Computer-Based Learning Platforms, Feedback, Gaming the System Detection, Hints},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636891,
author = {Cho, Ji Yong and Tao, Yan and Yeomans, Michael and Tingley, Dustin and Kizilcec, Ren\'{e} F.},
title = {Which Planning Tactics Predict Online Course Completion?},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636891},
doi = {10.1145/3636555.3636891},
abstract = {Planning is a self-regulated learning strategy and widely used behavior change technique that can help learners achieve academic goals (e.g., pass an exam, apply to college, or complete an online course). Numerous studies have tested the effects of planning interventions, but few have examined the content of learners’ plans and how it relates to their academic outcomes. Building on a large-scale intervention study, we conducted a qualitative content analysis of 650 learner plans sampled from 15 massive open online courses (MOOCs). We identified a number of planning tactics, compared their prevalence, and examined which ones significantly predict course progress and completion using regression analyses. We found that learners whose plans specify a time of day (e.g., morning, afternoon, night) are significantly more likely to complete a MOOC, but only 25% of the learners in our sample used this tactic. The high degree of variation in the effectiveness of planning tactics may contribute to mixed intervention findings in scale-up studies. Models of plan effectiveness can be used to provide feedback on the quality of learners’ plans and encourage them to use effective tactics to achieve their learning goals.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {360–370},
numpages = {11},
keywords = {MOOCs, content analysis, online learning, planning intervention},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3706468.3706471,
author = {Musabirov, Ilya and Reza, Mohi and Song, Haochen and Moore, Steven and Chen, Pan and Kumar, Harsh and Li, Tong and Stamper, John and Bier, Norman and Rafferty, Anna and Price, Thomas and Deliu, Nina and Durand, Audrey and Liut, Michael and Williams, Joseph Jay},
title = {Platform-based Adaptive Experimental Research in Education: Lessons Learned from The Digital Learning Challenge},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706471},
doi = {10.1145/3706468.3706471},
abstract = {Adaptive Experimentation is one of the most promising approaches to support complex decision-making in learning experience design and delivery. This paper reports on our experience with a real-world, multi-experimental evaluation of an adaptive experimentation platform within the XPRIZE Digital Learning Challenge framework, and summarizes data-driven lessons learned and best practices for Adaptive Experimentation in education. We outline key scenarios of the applicability of platform-supported experiments and reflect on lessons learned from this two-year project, focusing on implications relevant to platform developers, researchers, practitioners, and policy stakeholders to integrate Adaptive Experiments in real-world courses.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {13–23},
numpages = {11},
keywords = {adaptive experiments, posterior sampling, experimentation platforms, educational technology, human-computer interaction},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3636555.3636848,
author = {Cloude, Elizabeth B. and Kumar, Pranshu and Baker, Ryan S. and Fouh, Eric},
title = {Novice programmers inaccurately monitor the quality of their work and their peers’ work in an introductory computer science course},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636848},
doi = {10.1145/3636555.3636848},
abstract = {A student’s ability to accurately evaluate the quality of their work holds significant implications for their self-regulated learning and problem-solving proficiency in introductory programming. A widespread cognitive bias that frequently impedes accurate self- assessment is overconfidence, which often stems from a misjudgment of contextual and task-related cues, including students’ judgment of their peers’ competencies. Little research has explored the role of overconfidence on novice programmers’ ability to accurately monitor their own work in comparison to their peers’ work and its impact on performance in introductory programming courses. The present study examined whether novice programmers exhibited a common cognitive bias called the "hard-easy effect", where students believe their work is better than their peers on easier tasks (overplace) but worse than their peers on harder tasks (underplace). Results showed a reversal of the hard-easy effect, where novices tended to overplace themselves on harder tasks, yet underplace themselves on easier ones. Remarkably, underplacers performed better on an exam compared to overplacers. These findings advance our understanding of relationships between the hard-easy effect, monitoring accuracy across multiple tasks, and grades within introductory programming. Implications of this study can be used to guide instructional decision making and design to improve novices’ metacognitive awareness and performance in introductory programming courses.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {35–45},
numpages = {11},
keywords = {CS1, Hard-easy Effect, Metacognition, Overconfidence},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3576050.3576088,
author = {Berland, Matthew and Kumar, Vishesh},
title = {Joint Choice Time: A Metric for Better Understanding Collaboration in Interactive Museum Exhibits},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576088},
doi = {10.1145/3576050.3576088},
abstract = {In this paper, we propose a new metric – Joint Choice Time (JCT) – to measure how and when visitors are collaborating around an interactive museum exhibit. This extends dwell time, one of the most commonly used metrics for museum engagement – which tends to be individual, and sacrifices insight into activity and learning details for measurement simplicity. We provide an exemplar of measuring JCT using a common “diversity metric” for collaborative choices and potential outcomes. We provide an implementable description of the metric, results from using the metric with our own data, and potential implications for designing museum exhibits and easily measuring social engagement. Here, we apply JCT to an interactive exhibit game called “Rainbow Agents” where museum visitors can play independently or work together to tend to a virtual garden using computer science concepts. Our data showed that diversity of meaningful choices positively correlated with both dwell time and diversity of positive and creative outcomes. JCT - as a productive as well as easy to access measure of social work - provides an example for learning analytics practitioners and researchers (especially in museums) to consider centering social engagement and work as a rich space for easily assessing effective learning experiences for museum visitors.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {626–629},
numpages = {4},
keywords = {Collaboration, Engagement, Games, Museums, Play, Social participation},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3706468.3706499,
author = {Lee, Melissa and Huang, Chun-Wei and Collins, Kelly and Feng, Mingyu},
title = {Examining the Relationship between Math Anxiety, Effort, and Learning Outcomes Using Latent Class Analysis},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706499},
doi = {10.1145/3706468.3706499},
abstract = {Math anxiety has been found to negatively correlate with math achievement, affecting students’ choices to take fewer math classes and avoid math educational opportunities. Educational technology tools can ameliorate some of the negative effects of math anxiety. We examined students’ math anxiety, effort in an educational technology platform, and their relationship with students’ math achievement. Multilevel latent class analysis was used to identify student profiles of math anxiety. Regression analysis was used to examine how students of different profiles interacted with MathSpring, an adaptive intelligent tutor that provides affective supports to students during math problem-solving. The student's math achievement was measured by a standardized test. Our analysis indicated heterogeneity in math anxiety, and students could fall into one of three groups: Highly Anxious, Performance Anxious, and Calm. Highly Anxious students tended to give up more often when solving questions in MathSpring and had the lowest math achievement outcomes. For these students, using hints to solve problems in MathSpring was significantly associated with increased math outcomes. These findings have implications for the field's understanding of how students of different math anxiety profiles can demonstrate varying efforts in math educational technology platforms, and different math learning outcomes.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {227–237},
numpages = {11},
keywords = {latent class analysis, math anxiety, math learning, social-emotional learning},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3375462.3375474,
author = {Jovanovi\'{c}, Jelena and Dawson, Shane and Joksimovi\'{c}, Sre\'{c}ko and Siemens, George},
title = {Supporting actionable intelligence: reframing the analysis of observed study strategies},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375474},
doi = {10.1145/3375462.3375474},
abstract = {Models and processes developed in learning analytics research are increasing in sophistication and predictive power. However, the ability to translate analytic findings to practice remains problematic. This study aims to address this issue by establishing a model of learner behaviour that is both predictive of student course performance, and easily interpreted by instructors. To achieve this aim, we analysed fine grained trace data (from 3 offerings of an undergraduate online course, N=1068) to establish a comprehensive set of behaviour indicators aligned with the course design. The identified behaviour patterns, which we refer to as observed study strategies, proved to be associated with the student course performance. By examining the observed strategies of high and low performers throughout the course, we identified prototypical pathways associated with course success and failure. The proposed model and approach offers valuable insights for the provision of process-oriented feedback early in the course, and thus can aid learners in developing their capacity to succeed online.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {161–170},
numpages = {10},
keywords = {explanatory models, learner behaviour, learning analytics, learning tactics and strategies, trace data},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/2330601.2330666,
author = {Arnold, Kimberly E. and Pistilli, Matthew D.},
title = {Course signals at Purdue: using learning analytics to increase student success},
year = {2012},
isbn = {9781450311113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2330601.2330666},
doi = {10.1145/2330601.2330666},
abstract = {In this paper, an early intervention solution for collegiate faculty called Course Signals is discussed. Course Signals was developed to allow instructors the opportunity to employ the power of learner analytics to provide real-time feedback to a student. Course Signals relies not only on grades to predict students' performance, but also demographic characteristics, past academic history, and students' effort as measured by interaction with Blackboard Vista, Purdue's learning management system. The outcome is delivered to the students via a personalized email from the faculty member to each student, as well as a specific color on a stoplight -- traffic signal -- to indicate how each student is doing. The system itself is explained in detail, along with retention and performance outcomes realized since its implementation. In addition, faculty and student perceptions will be shared.},
booktitle = {Proceedings of the 2nd International Conference on Learning Analytics and Knowledge},
pages = {267–270},
numpages = {4},
keywords = {college student success, early intervention, learning analytics, retention},
location = {Vancouver, British Columbia, Canada},
series = {LAK '12}
}

@inproceedings{10.1145/3636555.3636846,
author = {Phung, Tung and P\u{a}durean, Victor-Alexandru and Singh, Anjali and Brooks, Christopher and Cambronero, Jos\'{e} and Gulwani, Sumit and Singla, Adish and Soares, Gustavo},
title = {Automating Human Tutor-Style Programming Feedback: Leveraging GPT-4 Tutor Model for Hint Generation and GPT-3.5 Student Model for Hint Validation},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636846},
doi = {10.1145/3636555.3636846},
abstract = {Generative AI and large language models hold great promise in enhancing programming education by automatically generating individualized feedback for students. We investigate the role of generative AI models in providing human tutor-style programming hints to help students resolve errors in their buggy programs. Recent works have benchmarked state-of-the-art models for various feedback generation scenarios; however, their overall quality is still inferior to human tutors and not yet ready for real-world deployment. In this paper, we seek to push the limits of generative AI models toward providing high-quality programming hints and develop a novel technique, GPT4HINTS-GPT3.5VAL. As a first step, our technique leverages GPT-4 as a “tutor” model to generate hints – it boosts the generative quality by using symbolic information of failing test cases and fixes in prompts. As a next step, our technique leverages GPT-3.5, a weaker model, as a “student” model to further validate the hint quality – it performs an automatic quality validation by simulating the potential utility of providing this feedback. We show the efficacy of our technique via extensive evaluation using three real-world datasets of Python programs covering a variety of concepts ranging from basic algorithms to regular expressions and data analysis using pandas library.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {12–23},
numpages = {12},
keywords = {ChatGPT, Feedback Generation, GPT4, Generative AI, Programming Education},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3576050.3576147,
author = {Swamy, Vinitra and Du, Sijia and Marras, Mirko and Kaser, Tanja},
title = {Trusting the Explainers: Teacher Validation of Explainable Artificial Intelligence for Course Design},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576147},
doi = {10.1145/3576050.3576147},
abstract = {Deep learning models for learning analytics have become increasingly popular over the last few years; however, these approaches are still not widely adopted in real-world settings, likely due to a lack of trust and transparency. In this paper, we tackle this issue by implementing explainable AI methods for black-box neural networks. This work focuses on the context of online and blended learning and the use case of student success prediction models. We use a pairwise study design, enabling us to investigate controlled differences between pairs of courses. Our analyses cover five course pairs that differ in one educationally relevant aspect and two popular instance-based explainable AI methods (LIME and SHAP). We quantitatively compare the distances between the explanations across courses and methods. We then validate the explanations of LIME and SHAP with 26 semi-structured interviews of university-level educators regarding which features they believe contribute most to student success, which explanations they trust most, and how they could transform these insights into actionable course design decisions. Our results show that quantitatively, explainers significantly disagree with each other about what is important, and qualitatively, experts themselves do not agree on which explanations are most trustworthy. All code, extended results, and the interview protocol are provided at https://github.com/epfl-ml4ed/trusting-explainers.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {345–356},
numpages = {12},
keywords = {Counterfactuals, Explainable AI, LIME, LSTMs, MOOCs, SHAP, Student Performance Prediction},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/2808580.2808619,
author = {Conde, Miguel \'{A}. and Hern\'{a}ndez-Garc\'{\i}a, \'{A}ngel and Oliveira, Am\'{\i}lcar},
title = {Endless horizons? addressing current concerns about learning analytics},
year = {2015},
isbn = {9781450334426},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808580.2808619},
doi = {10.1145/2808580.2808619},
abstract = {A few years after the emergence of learning analytics as a relevant topic in educational research, which has been accompanied by several publications devoted to the theme, we still see how the different topics seem to alternate both converging and diverging trends. Thus, whereas sometimes different approaches seem to converge into the study of the same issue or situation, other times the same approaches are applied to solve different problems in a divergent way. This situation becomes more complex with the added inclusion of new technological advances and analysis techniques, as a result of which the extension of the scope of learning analytics as a discipline may be perceived as endless or boundless. This is an introduction to the Track on Learning Analytics within the Technological Ecosystems for Enhancing Multiculturality 2015 Conference, which aims to address some of the current concerns on learning analytics and help clearing the horizon. This introductory paper provides an overview of the rationale of the track's proposal, including different topics open for discussion; the introduction is followed by an insight of the submission management and participants' selection process. Then, a detailed summary of the manuscripts accepted for participation in the conference is presented.},
booktitle = {Proceedings of the 3rd International Conference on Technological Ecosystems for Enhancing Multiculturality},
pages = {259–262},
numpages = {4},
keywords = {big data, common frameworks, decision making, ethical issues, integration, learning analytics, tiny data},
location = {Porto, Portugal},
series = {TEEM '15}
}

@inproceedings{10.1145/3636555.3636850,
author = {Hutt, Stephen and DePiro, Allison and Wang, Joann and Rhodes, Sam and Baker, Ryan S and Hieb, Grayson and Sethuraman, Sheela and Ocumpaugh, Jaclyn and Mills, Caitlin},
title = {Feedback on Feedback: Comparing Classic Natural Language Processing and Generative AI to Evaluate Peer Feedback},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636850},
doi = {10.1145/3636555.3636850},
abstract = {Peer feedback can be a powerful tool as it presents learning opportunities for both the learner receiving feedback as well as the learner providing feedback. Despite its utility, it can be difficult to implement effectively, particularly for younger learners, who are often novices at providing feedback. It can be difficult for students to learn what constitutes “good” feedback – particularly in open-ended problem-solving contexts. To address this gap, we investigate both classical natural language processing techniques and large language models, specifically ChatGPT, as potential approaches to devise an automated detector of feedback quality (including both student progress towards goals and next steps needed). Our findings indicate that the classical detectors are highly accurate and, through feature analysis, we elucidate the pivotal elements influencing its decision process. We find that ChatGPT is less accurate than classical NLP but illustrate the potential of ChatGPT in evaluating feedback, by generating explanations for ratings, along with scores. We discuss how the detector can be used for automated feedback evaluation and to better scaffold peer feedback for younger learners.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {55–65},
numpages = {11},
keywords = {Generative AI, Language Analytics, Large Language Models, Natural Language Processing, Peer Feedback},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3706468.3706552,
author = {Gao, Ge and Leon, Amelia and Jetten, Andrea and Turner, Jasmine and Almoubayyed, Husni and Fancsali, Stephen and Brunskill, Emma},
title = {Predicting Long-Term Student Outcomes from Short-Term EdTech Log Data},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706552},
doi = {10.1145/3706468.3706552},
abstract = {Educational stakeholders are often particularly interested in sparse, delayed student outcomes, like end-of-year statewide exams. The rare occurrence of such assessments makes it harder to identify students likely to fail such assessments, as well as making it slow for researchers and educators to be able to assess the effectiveness of particular educational tools. Prior work has primarily focused on using logs from students full usage (e.g. year-long) of an educational product to predict outcomes, or considered predictive accuracy using a few minutes to predict outcomes after a short (e.g. 1 hour) session. In contrast, we investigate machine learning predictors using students’ logs during their first few hours of usage can provide useful predictive insight into those students’ end-of-school year external assessment. We do this on three diverse datasets: from students in Uganda using a literacy game product, and from students in the US using two mathematics intelligent tutoring systems. We consider various measures of the accuracy of the resulting predictors, including its ability to identify students at different parts along the assessment performance distribution. Our findings suggest that short-term log usage data, from 2-5 hours, can be used to provide valuable signal about students’ long term external performance.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {631–641},
numpages = {11},
keywords = {Long-term student outcomes prediction, Short-horizon data, Quantitative analysis, K-12 education, Data mining across educational contexts},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3506860.3506890,
author = {Echeverria, Vanessa and Wong-Villacres, Marisol and Ochoa, Xavier and Chiluiza, Katherine},
title = {An Exploratory Evaluation of a Collaboration Feedback Report},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506890},
doi = {10.1145/3506860.3506890},
abstract = {Providing formative feedback to foster collaboration and improve students’ practice has been an emerging topic in CSCL and LA research communities. However, this pedagogical practice could be unrealistic in authentic classrooms, as observing and annotating improvements for every student and group exceeds the teacher’s capabilities. In the research area of group work and collaborative learning, current learning analytics solutions have reported accurate computational models to understand collaboration processes, yet evaluating formative collaboration feedback, where the final user is the student, is an under-explored research area. This paper reports an exploratory evaluation to understand the effects a collaboration feedback report through an authentic study conducted in regular classes. Fifty students from a Computer Science undergraduate program participated in the study. We followed an user-centered design approach to define six collaboration aspects that are relevant to students. These aspects were part of initial prototypes for the feedback report. From the exploratory intervention, we did not find effects between students who received the feedback (experimental condition) report and those who did not (control condition). Finally, this paper discusses design implications for further feedback report designs and interventions.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {478–484},
numpages = {7},
keywords = {collaboration analytics, collaboration feedback report, human-centered design},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3636555.3636906,
author = {Glandorf, Dominik and Lee, Hye Rin and Orona, Gabe Avakian and Pumptow, Marina and Yu, Renzhe and Fischer, Christian},
title = {Temporal and Between-Group Variability in College Dropout Prediction},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636906},
doi = {10.1145/3636555.3636906},
abstract = {Large-scale administrative data is a common input in early warning systems for college dropout in higher education. Still, the terminology and methodology vary significantly across existing studies, and the implications of different modeling decisions are not fully understood. This study provides a systematic evaluation of contributing factors and predictive performance of machine learning models over time and across different student groups. Drawing on twelve years of administrative data at a large public university in the US, we find that dropout prediction at the end of the second year has a 20% higher AUC than at the time of enrollment in a Random Forest model. Also, most predictive factors at the time of enrollment, including demographics and high school performance, are quickly superseded in predictive importance by college performance and in later stages by enrollment behavior. Regarding variability across student groups, college GPA has more predictive value for students from traditionally disadvantaged backgrounds than their peers. These results can help researchers and administrators understand the comparative value of different data sources when building early warning systems and optimizing decisions under specific policy goals.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {486–497},
numpages = {12},
keywords = {administrative data, college dropout prediction, machine learning, student heterogeneity, temporal dynamics},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3227609.3227669,
author = {Melesko, Jaroslav and Kurilovas, Eugenijus},
title = {Semantic Technologies in e-Learning: Learning Analytics and Artificial Neural Networks in Personalised Learning Systems},
year = {2018},
isbn = {9781450354899},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3227609.3227669},
doi = {10.1145/3227609.3227669},
abstract = {The paper presents semantic clustering and artificial neural network (ANN) based learning analytics software agent for personalised adaptive multi-agent learning system. First of all, systematic literature review on application of ANN to personalise learning in Web of Science database was performed. After that, methodology of application of ANN and semantic clustering of learning material in personalised adaptive multi-agent learning system is presented. In the paper, personalisation in multi-agent learning system is based on learning styles model that requires the use of psychological questionnaire to determine student's learning styles. The results of filling in the questionnaire could be incorrect since some students may answer the questionnaire dishonestly, irresponsibly, or make mistakes in self-diagnosis. This results in creation of an incorrect student's model. This causes the system to provide suboptimal learning objects and scenarios to the student. The authors present a model of ANN based learning analytics agent to be used in the system. Proposed agent uses ANN to associate students' learning styles with their real behaviour within the learning environment. The key factor describing the behaviour of students within the system is the learning content they seek out independently. Clustering the visited documents based on semantic content categorizes students into groups. Belonging to a semantic cluster is one of the inputs that can be used to train ANN agent. After training, the agent could identify potentially faulty student models by looking for anomalous behaviour for those learning styles. Such problems can be resolved by providing alternative learning objects or scenarios to the students and observing their choices.1},
booktitle = {Proceedings of the 8th International Conference on Web Intelligence, Mining and Semantics},
articleno = {34},
numpages = {7},
keywords = {Personalised learning systems, artificial neural networks, e-Learning, learning analytics, multagent systems},
location = {Novi Sad, Serbia},
series = {WIMS '18}
}

@inproceedings{10.1145/3375462.3375487,
author = {Saint, John and Ga\v{s}evi\'{c}, Dragan and Matcha, Wannisa and Uzir, Nora'Ayu Ahmad and Pardo, Abelardo},
title = {Combining analytic methods to unlock sequential and temporal patterns of self-regulated learning},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375487},
doi = {10.1145/3375462.3375487},
abstract = {The temporal and sequential nature of learning is receiving increasing focus in Learning Analytics circles. The desire to embed studies in recognised theories of self-regulated learning (SRL) has led researchers to conceptualise learning as a process that unfolds and changes over time. To that end, a body of research knowledge is growing which states that traditional frequency-based correlational studies are limited in narrative impact. To further explore this, we analysed trace data collected from online activities of a sample of 239 computer engineering undergraduate students enrolled on a course that followed a flipped class-room pedagogy. We employed SRL categorisation of micro-level processes based on a recognised model of learning, and then analysed the data using: 1) simple frequency measures; 2) epistemic network analysis; 3) temporal process mining; and 4) stochastic process mining. We found that a combination of analyses provided us with a richer insight into SRL behaviours than any one single method. We found that better performing learners employed more optimal behaviours in their navigation through the course's learning management system.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {402–411},
numpages = {10},
keywords = {epistemic network analysis, learning analytics, micro-level processes, process mining, self-regulated learning},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3636555.3636898,
author = {Frej, Jibril and Shah, Neel and Knezevic, Marta and Nazaretsky, Tanya and K\"{a}ser, Tanja},
title = {Finding Paths for Explainable MOOC Recommendation: A Learner Perspective},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636898},
doi = {10.1145/3636555.3636898},
abstract = {The increasing availability of Massive Open Online Courses (MOOCs) has created a necessity for personalized course recommendation systems. These systems often combine neural networks with Knowledge Graphs (KGs) to achieve richer representations of learners and courses. While these enriched representations allow more accurate and personalized recommendations, explainability remains a significant challenge which is especially problematic for certain domains with significant impact such as education and online learning. Recently, a novel class of recommender systems that uses reinforcement learning and graph reasoning over KGs has been proposed to generate explainable recommendations in the form of paths over a KG. Despite their accuracy and interpretability on e-commerce datasets, these approaches have scarcely been applied to the educational domain and their use in practice has not been studied. In this work, we propose an explainable recommendation system for MOOCs that uses graph reasoning. To validate the practical implications of our approach, we conducted a user study examining user perceptions of our new explainable recommendations. We demonstrate the generalizability of our approach by conducting experiments on two educational datasets: COCO and Xuetang.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {426–437},
numpages = {12},
keywords = {Explainable AI, MOOCs, Recommendation, User study},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636910,
author = {Hou, Chenyu and Zhu, Gaoxia and Zheng, Juan and Zhang, Lishan and Huang, Xiaoshan and Zhong, Tianlong and Li, Shan and Du, Hanxiang and Ker, Chin Lee},
title = {Prompt-based and Fine-tuned GPT Models for Context-Dependent and -Independent Deductive Coding in Social Annotation},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636910},
doi = {10.1145/3636555.3636910},
abstract = {GPT has demonstrated impressive capabilities in executing various natural language processing (NLP) and reasoning tasks, showcasing its potential for deductive coding in social annotations. This research explored the effectiveness of prompt engineering and fine-tuning approaches of GPT for deductive coding of context-dependent and context-independent dimensions. Coding context-dependent dimensions (i.e., Theorizing, Integration, Reflection) requires a contextualized understanding that connects the target comment with reading materials and previous comments, whereas coding context-independent dimensions (i.e., Appraisal, Questioning, Social, Curiosity, Surprise) relies more on the comment itself. Utilizing strategies such as prompt decomposition, multi-prompt learning, and a codebook-centered approach, we found that prompt engineering can achieve fair to substantial agreement with expert-labeled data across various coding dimensions. These results affirm GPT's potential for effective application in real-world coding tasks. Compared to context-independent coding, context-dependent dimensions had lower agreement with expert-labeled data. To enhance accuracy, GPT models were fine-tuned using 102 pieces of expert-labeled data, with an additional 102 cases used for validation. The fine-tuned models demonstrated substantial agreement with ground truth in context-independent dimensions and elevated the inter-rater reliability of context-dependent categories to moderate levels. This approach represents a promising path for significantly reducing human labor and time, especially with large unstructured datasets, without sacrificing the accuracy and reliability of deductive coding tasks in social annotation. The study marks a step toward optimizing and streamlining coding processes in social annotation. Our findings suggest the promise of using GPT to analyze qualitative data and provide detailed, immediate feedback for students to elicit deepening inquiries.&nbsp;},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {518–528},
numpages = {11},
keywords = {Context-Dependent, Fine-tuning, GPT, Prompt Engineering, Social Annotation, deductive coding},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3506860.3506913,
author = {Rotelli, Daniela and Monreale, Anna},
title = {Time-on-Task Estimation by data-driven Outlier Detection based on Learning Activities},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506913},
doi = {10.1145/3506860.3506913},
abstract = {Temporal analysis has been demonstrated to be relevant in Learning Analytics research, and capturing time-on-task, i.e., the amount of time spent by students in quality learning, as a proxy to model learning behaviour, predict performance, and avoid drop-out has been the focus of a number of investigations. Nonetheless, most studies do not provide enough information on how their data were prepared for their findings to be easily replicated, even though data pre-processing decisions have an impact on the analysis’ outcomes and can lead to inaccurate predictions. One of the key aspects in the preparation of learning data for temporal analysis is the detection of anomalous values of temporal duration of students’ activities. Most of the works in the literature address this problem without taking into account the fact that different activities can have very different typical execution times. In this paper, we propose a methodology for estimating time-on-task that starts with a well-defined data consolidation and then applies an outlier detection strategy to the data based on a distinct study of each learning activity and its peculiarities. Our real-world data experiments show that the proposed methodology outperforms the current state of the art, providing more accurate time estimations for students’ learning tasks.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {336–346},
numpages = {11},
keywords = {Data consolidation, Data pre-processing, Learning log data, Outlier detection., Time-on-task},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/2876034.2893402,
author = {Pijeira D\'{\i}az, H\'{e}ctor J. and Santofimia Ruiz, Javier and Ruip\'{e}rez-Valiente, Jos\'{e} A. and Mu\~{n}oz-Merino, Pedro J. and Delgado Kloos, Carlos},
title = {A Demonstration of ANALYSE: A Learning Analytics Tool for Open edX},
year = {2016},
isbn = {9781450337267},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2876034.2893402},
doi = {10.1145/2876034.2893402},
abstract = {Education is being powered by technology in many ways. One of the main advantages is making use of data to improve the learning process. The massive open online course (MOOC) phenomenon became viral some years ago, and with it many different platforms emerged. However most of them are proprietary solutions (i.e. Coursera, Udacity) and cannot be used by interested stakeholders. At the moment Open edX is placed as the primary open source application to support MOOCs. The community using Open edX is growing at a fast pace with many interested institutions. Nevertheless, the learning analytics support of Open edX is still in its first steps. In this paper we present an overview and demonstration of ANALYSE, an open source learning analytics tool for Open edX. ANALYSE includes currently 12 new visualizations that can be used by both instructors and students.},
booktitle = {Proceedings of the Third (2016) ACM Conference on Learning @ Scale},
pages = {329–330},
numpages = {2},
keywords = {learning analytics, moocs, open edx, visualizations},
location = {Edinburgh, Scotland, UK},
series = {L@S '16}
}

@inproceedings{10.1145/3636555.3636882,
author = {Dunder, Nora and Lundborg, Saga and Wong, Jacqueline and Viberg, Olga},
title = {Kattis vs ChatGPT: Assessment and Evaluation of Programming Tasks in the Age of Artificial Intelligence},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636882},
doi = {10.1145/3636555.3636882},
abstract = {AI-powered education technologies can support students and teachers in computer science education. However, with the recent developments in generative AI, and especially the increasingly emerging popularity of ChatGPT, the effectiveness of using large language models for solving programming tasks has been underexplored. The present study examines ChatGPT’s ability to generate code solutions at different difficulty levels for introductory programming courses. We conducted an experiment where ChatGPT was tested on 127 randomly selected programming problems provided by Kattis, an automatic software grading tool for computer science programs, often used in higher education. The results showed that ChatGPT independently could solve 19 out of 127 programming tasks generated and assessed by Kattis. Further, ChatGPT was found to be able to generate accurate code solutions for simple problems but encountered difficulties with more complex programming tasks. The results contribute to the ongoing debate on the utility of AI-powered tools in programming education.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {821–827},
numpages = {7},
keywords = {Academic Integrity, Automated Grading, ChatGPT, Programming Education},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636919,
author = {Sheel, Shreya and Anastasopoulos, Ioannis and Pardos, Zach A.},
title = {Comparing Authoring Experiences with Spreadsheet Interfaces vs GUIs},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636919},
doi = {10.1145/3636555.3636919},
abstract = {There is little consensus over whether graphical user interfaces (GUIs) or programmatic systems are better for word processing. Even less is known about each interfaces’ affordances and limitations in the context of creating content for adaptive tutoring systems. In order to afford instructors the use of such systems with their own or adapted pedagogies, we must study their experiences in inputting their content. In this study, we conduct a between-subjects A/B test with two content authoring interfaces, a GUI and spreadsheet, to explore 32 instructors’ experiences in authoring algebra content with hints, scaffolds, images, and special characters. We study their experiences by measuring time taken, accuracy, and their perceptions of each interfaces’ usability. Our findings indicate no significant relationship between interface used and time taken authoring problems but significantly more accuracy in authoring problems in the spreadsheet interface over the GUI. Although both interfaces performed reasonably well in time taken and accuracy, both were perceived as average to low in usability, highlighting a dissonance between instructors’ perceptions and actual performances. Since both interfaces are reasonable in authoring content, other factors can be explored, such as cost and author incentive, when deciding which interface approach to take for authoring tutor content.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {598–607},
numpages = {10},
keywords = {A/B testing, adaptive tutoring systems, content authoring, human-computer interaction, usability},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636851,
author = {Zambrano, Andres Felipe and Baker, Ryan S.},
title = {Long-Term Prediction from Topic-Level Knowledge and Engagement in Mathematics Learning},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636851},
doi = {10.1145/3636555.3636851},
abstract = {During middle school, students' learning experiences begin to influence their future decisions about college enrollment and career selection. Prior research indicates that both knowledge gained and the disengagement and affect experienced during this period are predictors of these future outcomes. However, this past research has investigated affect, disengagement, and knowledge in an overall fashion – looking at the average manifestation of these constructs across all topics studied across a year of mathematics. It may be that some mathematics topics are more associated with these outcomes than others. In this study, we use data from middle school students interacting with a digital mathematics learning platform, to analyze the interplay of these features across different topic areas. Our findings show that mastering Functions is the most important predictor of both college enrollment and STEM career selection, while the importance of knowing other topic areas varies across the two outcomes. Furthermore, while subject knowledge tends to be the most relevant predictor for general college enrollment, affective states, especially confusion and engaged concentration, become more important for predicting STEM career selection.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {66–77},
numpages = {12},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636937,
author = {Booth, Brandon M. and Jacobs, Jennifer and Bush, Jeffrey B. and Milne, Brent and Fischaber, Tom and DMello, Sidney K.},
title = {Human-tutor Coaching Technology (HTCT): Automated Discourse Analytics in a Coached Tutoring Model},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636937},
doi = {10.1145/3636555.3636937},
abstract = {High-dosage tutoring has become an effective strategy for bolstering K-12 academic performance and combating education declines accelerated by the COVID-19 pandemic. To achieve high-dosage tutoring at scale, tutoring programs often rely on paraprofessional tutors—recruited tutors with college degrees who lack formal training in education—however, these tutors may require consistent and targeted feedback from instructional coaches for improvement. Accordingly, we developed a human-tutor coaching technology (HTCT) system to automatically extract discourse analytics pertaining to accountable talk moves (or academically productive talk) from tutoring sessions and provide feedback visualizations to coaches to aid their coaching sessions with tutors. We deployed HTCT in a user study using a virtual tutoring platform with 11 real coaches, 40 tutors, and their students to investigate coaches’ usage patterns with HTCT, perceptions of its utility, and changes in tutors’ talk. Overall, we found that coaches had positive perceptions of the system. We also observed an increase in accountable talk from tutors whose coaches used HTCT compared to tutors whose coaches did not. We discuss implications for AI-based applications which offer coaches a promising way to provide personalized, automated, and data-driven feedback to scale high-dosage tutoring.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {725–735},
numpages = {11},
keywords = {coached tutoring, discourse analytics, in situ user study, natural language processing},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3303772.3303776,
author = {Di Mitri, Daniele and Schneider, Jan and Klemke, Roland and Specht, Marcus and Drachsler, Hendrik},
title = {Read Between the Lines: An Annotation Tool for Multimodal Data for Learning},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303776},
doi = {10.1145/3303772.3303776},
abstract = {This paper introduces the Visual Inspection Tool (VIT) which supports researchers in the annotation of multimodal data as well as the processing and exploitation for learning purposes. While most of the existing Multimodal Learning Analytics (MMLA) solutions are tailor-made for specific learning tasks and sensors, the VIT addresses the data annotation for different types of learning tasks that can be captured with a customisable set of sensors in a flexible way. The VIT supports MMLA researchers in 1) triangulating multimodal data with video recordings; 2) segmenting the multimodal data into time-intervals and adding annotations to the time-intervals; 3) downloading the annotated dataset and using it for multimodal data analysis. The VIT is a crucial component that was so far missing in the available tools for MMLA research. By filling this gap we also identified an integrated workflow that characterises current MMLA research. We call this workflow the Multimodal Learning Analytics Pipeline, a toolkit for orchestration, the use and application of various MMLA tools.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {51–60},
numpages = {10},
keywords = {Internet of Things, Learning Analytics, Multimodal data, sensors},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3636555.3636893,
author = {Shingjergji, Krist and Urlings, Corrie and Iren, Deniz and Klemke, Roland},
title = {Shaping and evaluating a system for affective computing in online higher education using a participatory design and the system usability scale},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636893},
doi = {10.1145/3636555.3636893},
abstract = {Online learning’s popularity has surged. However, teachers face the challenge of the lack of non-verbal communication with students, making it difficult to perceive their learning-centered affective states (LCAS), leading to missed intervention opportunities. Addressing this challenge requires a system that detects students’ LCAS from their non-verbal cues and informs teachers in an actionable way. To design such a system, it is essential to explore field experts’ needs and requirements. Therefore, we conducted design-based research focus groups with teachers to determine which LCAS they find important to know during online lectures and their preferred communication methods. The results indicated that confusion, engagement, boredom, frustration, and curiosity are the most important LCAS and that the proposed system should take into account teachers’ cognitive load and give them autonomy in the choice of content and frequency of the information. Considering the obtained feedback, a prototype of two versions was developed. The prototype was evaluated by teachers utilizing the System Usability Scale (SUS). Results indicated an average SUS score of 80.5 and 74.5 for each version, suggesting acceptable usability. These findings can guide the design and development of a system that can help teachers recognize students’ LCAS, thus improving synchronous online learning.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {382–391},
numpages = {10},
keywords = {affective computing, emotions in learning, focus groups, learning-centered affective states, online learning, participatory design, prototype evaluation, system usability},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3303772.3303825,
author = {Mangaroska, Katerina and Vesin, Boban and Giannakos, Michail},
title = {Cross-Platform Analytics: A step towards Personalization and Adaptation in Education},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303825},
doi = {10.1145/3303772.3303825},
abstract = {Learning analytics are used to track learners' progress and empower educators and learners to make well-informed data-driven decisions. However, due to the distributed nature of the learning process, analytics need to be combined to offer broader insights into learner's behavior and experiences. Consequently, this paper presents an architecture of a learning ecosystem, that integrates and utilizes cross-platform analytics. The proposed cross-platform architecture has been put into practice via a Java programming course. After a series of studies, a proof of concept was derived that shows how cross-platform analytics amplify the relevant analytics for the learning process. Such analytics could improve educators' and learners' understanding of their own actions and the environments in which learning occurs.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {71–75},
numpages = {5},
keywords = {architecture, learning analytics, multimodal systems},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3636555.3636902,
author = {Baucks, Frederik and Schmucker, Robin and Wiskott, Laurenz},
title = {Gaining Insights into Course Difficulty Variations Using Item Response Theory},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636902},
doi = {10.1145/3636555.3636902},
abstract = {Curriculum analytics (CA) studies curriculum structure and student data to ensure the quality of educational programs. To gain statistical robustness, most existing CA techniques rely on the assumption of time-invariant course difficulty, preventing them from capturing variations that might occur over time. However, ensuring low temporal variation in course difficulty is crucial to warrant fairness in treating individual student cohorts and consistency in degree outcomes. We introduce item response theory (IRT) as a CA methodology that enables us to address the open problem of monitoring course difficulty variations over time. We use statistical criteria to quantify the degree to which course performance data meets IRT’s theoretical assumptions and verify validity and reliability of IRT-based course difficulty estimates. Using data from 664 Computer Science and 1,355 Mechanical Engineering undergraduate students, we show how IRT can yield valuable CA insights: First, by revealing temporal variations in course difficulty over several years, we find that course difficulty has systematically shifted downward during the COVID-19 pandemic. Second, time-dependent course difficulty and cohort performance variations confound conventional course pass rate measures. We introduce IRT-adjusted pass rates as an alternative to account for these factors. Our findings affect policymakers, student advisors, accreditation, and course articulation.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {450–461},
numpages = {12},
keywords = {curriculum analytics, fairness., item response theory},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3159450.3162295,
author = {Dorodchi, Mohsen and Bendict, Aileen and Desai, Devansh and Mahzoon, Mohammad J.},
title = {Reflections are Good! Analysis of Combination of Grades and Students' Reflections using Learning Analytics (Abstract Only)},
year = {2018},
isbn = {9781450351034},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3159450.3162295},
doi = {10.1145/3159450.3162295},
abstract = {Reflection is a way to get quick opinions from users, clients, etc. Analysis of reflection often involves subjective review and interpretations. On the other hand, data analytics provides guidelines to collect and measure as well as analyze and reflect on data. In our case, learning analytics of students' reflections reveals information about learners, their learning experience, and all their related contexts. Eventually, learning analytics aim is to understand and optimize learning and the corresponding environments (in which learning occurs). It plays a critical role in evaluating students performance and making decisions on how to improve students' success and overall retention. In our study, we focus on applying learning analytics to a heterogeneous data set collected in the introductory programming course. This data set integrated self-assessment reflections along with the existing active learning group activities. By integrating self-assessment reflections, large amounts of valuable data can be gathered to facilitate continuous assessment of students' learning. Using activity-based active learning and peer-instruction, the effectiveness of the content interventions targeting students to understand the fundamental concepts of computer programming is also evaluated. For analysis purposes, we applied a time-based learning analytics model called sequence analysis to learn about the pattern of our at-risk students and use the learned model to predict at-risk students based on their reflections as well as performance in the course.},
booktitle = {Proceedings of the 49th ACM Technical Symposium on Computer Science Education},
pages = {1077},
numpages = {1},
keywords = {cs1, learning analytics, reflection},
location = {Baltimore, Maryland, USA},
series = {SIGCSE '18}
}

@inproceedings{10.1145/3706468.3706558,
author = {Thapa Magar, Abisha and Shakya, Anup and Fancsali, Stephen E. and Rus, Vasile and Murphy, April and Ritter, Steve and Venugopal, Deepak},
title = {"Can A Language Model Represent Math Strategies?": Learning Math Strategies from Big Data using BERT},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706558},
doi = {10.1145/3706468.3706558},
abstract = {AI models have shown a remarkable ability to perform representation learning using large-scale data. In particular, the emergence of Large Language Models (LLMs) attests to the capability of AI models to learn complex hidden structures in a bottom-up manner without requiring a lot of human expertise. In this paper, we leverage these models to learn Math learning strategies at scale. Specifically, we use student interaction data from the MATHia Intelligent Tutoring System to learn strategies based on sequences of actions performed by students. To do this, we develop an AI model based on BERT (Bidirectional Encoder Representations From Transformers) that has two main components. First, we pre-train BERT using an approach known as Masked Language Modeling to learn embeddings for strategies. The embeddings represent strategies in a vector form while preserving their semantics. Next, we fine-tune the model to predict if students are likely to apply a correct strategy to solve a novel problem. We demonstrate using a large dataset collected from 655 schools that our approach where we pre-train to learn strategies from a sample of schools can be fine-tuned with a small number of examples to make accurate predictions over student data collected from other schools.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {655–666},
numpages = {12},
keywords = {Math strategies, Representation learning, BERT, Big Data, Transformers},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3636555.3636864,
author = {Fang, Zheng and Wang, Weiqing and Chen, Guanliang and Swiecki, Zachari},
title = {Neural Epistemic Network Analysis: Combining Graph Neural Networks and Epistemic Network Analysis to Model Collaborative Processes},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636864},
doi = {10.1145/3636555.3636864},
abstract = {We report on the design and evaluation of a novel technique for analysing the sociocognitive nature of collaborative problem-solving—neural epistemic network analysis (NENA). NENA combines the computational power and representational ability of graph neural networks (GNNs) to naturally incorporate social and cognitive features in the analysis with the interpretative advantages of epistemic network analysis (ENA). Comparing NENA and ENA on two datasets from collaborative problem-solving contexts, we found that NENA improves upon ENA’s ability to distinguish between known subgroups in CPS data, while also improving the interpretability and explainability of GNN results.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {157–166},
numpages = {10},
keywords = {Collaborative Problem-Solving, Epistemic Network Analysis, Graph Neural Networks, Social Network Analysis},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3506860.3506866,
author = {Nazaretsky, Tanya and Cukurova, Mutlu and Alexandron, Giora},
title = {An Instrument for Measuring Teachers’ Trust in AI-Based Educational Technology},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506866},
doi = {10.1145/3506860.3506866},
abstract = {Evidence from various domains underlines the key role that human factors, and especially, trust, play in the adoption of technology by practitioners. In the case of Artificial Intelligence (AI) driven learning analytics tools, the issue is even more complex due to practitioners’ AI-specific misconceptions, myths, and fears (i.e., mass unemployment and ethical concerns). In recent years, artificial intelligence has been introduced increasingly into K-12 education. However, little research has been conducted on the trust and attitudes of K-12 teachers regarding the use and adoption of AI-based Educational Technology (EdTech). The present study introduces a new instrument to measure teachers’ trust in AI-based EdTech, provides evidence of its internal structure validity, and uses it to portray secondary-level school teachers’ attitudes toward AI. First, we explain the instrument items creation process based on our preliminary research and review of existing tools in other domains. Second, using Exploratory Factor Analysis we analyze the results from 132 teachers’ input. The results reveal eight factors influencing teachers’ trust in adopting AI-based EdTech: Perceived Benefits of AI-based EdTech, AI-based EdTech’s Lack of Human Characteristics, AI-based EdTech’s Perceived Lack of Transparency, Anxieties Related to Using AI-based EdTech, Self-efficacy in Using AI-based EdTech, Required Shift in Pedagogy to Adopt AI-based EdTech, Preferred Means to Increase Trust in AI-based EdTech, and AI-based EdTech vs Human Advice/Recommendation. Finally, we use the instrument to discuss 132 high-school Biology teachers’ responses to the survey items and to what extent they align with the findings from the literature in relevant domains. The contribution of this research is twofold. First, it introduces a reliable instrument to investigate the role of teachers’ trust in AI-based EdTech and the factors influencing it. Second, the findings from the teachers’ survey can guide creators of teacher professional development courses and policymakers on improving teachers’ trust in, and in turn their willingness to adopt, AI-based EdTech in K-12 education.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {56–66},
numpages = {11},
keywords = {AI, Blended Learning, Human Factors, K-12 Education, Teachers, Trust},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3448139.3448160,
author = {Matz, Rebecca and Schulz, Kyle and Hanley, Elizabeth and Derry, Holly and Hayward, Benjamin and Koester, Benjamin and Hayward, Caitlin and McKay, Timothy},
title = {Analyzing the Efficacy of ECoach in Supporting Gateway Course Success Through Tailored Support},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448160},
doi = {10.1145/3448139.3448160},
abstract = {Large courses act as gateways for college students and often have poor outcomes, particularly in STEM fields where the pace of improvement has been glacial. Students encounter barriers to persistence like low grades, competitive cultures, and a lack of motivation and belonging. Tailored technology systems offer one promising path forward. In this observational study, we report on the use of one such system, called ECoach, that provides students resources based on their psychosocial profile, performance metrics, and pattern of ECoach usage. We investigated ECoach efficacy in five courses enrolling 3,599 students using a clustering method to group users by engagement level and subsequent regression analyses. We present results showing significant positive relationships with small effect sizes between ECoach engagement and final course grade as well as grade anomaly, a performance measure that takes into account prior course grades. The courses with the strongest relationship between ECoach engagement and performance offered nominal extra credit incentives yet show improved grades well above this “investment” from instructors. Such small incentives may act as a catalyst that spurs deeper engagement with the platform. The impact of specific ECoach features and areas for future study are discussed.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {216–225},
numpages = {10},
keywords = {Academic achievement, STEM, educational technology, feedback, higher education, large enrollment courses, learning analytics, undergraduate education},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3375462.3375499,
author = {Lecailliez, Louis and Flanagan, Brendan and Chen, Mei-Rong Alice and Ogata, Hiroaki},
title = {Smart dictionary for e-book reading analytics},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375499},
doi = {10.1145/3375462.3375499},
abstract = {Reading, be it intensive or extensive, is one of the key skills required to master English as a foreign language (EFL) learner. Computerized e-book systems provide convenient access to learning materials inside and outside class. Students may regularly check the meaning of a word or expression using a separate tool to progress on their reading, which is not only disruptive but can lead to other learning problems. An example of a particular issue faced in EFL is when a student learns an inappropriate meaning of a polysemous word for the context in which it is presented. This is also a problem for teachers as they often need to investigate the cause. In this paper, we propose a smart dictionary integrated into an e-book reading platform. It allows the learner to search and note word definitions directly with the purpose of reducing context switching and improve vocabulary retention. Finally, we propose that learner interactions with the system can be analyzed to support EFL teachers in identifying possible problems that arise through dictionary use while reading.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {89–93},
numpages = {5},
keywords = {dictionary, e-book, english education, learning analytics},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3706468.3706556,
author = {Woollaston, Steve and Flanagan, Brendan and Ocheja, Patrick and Toyokawa, Yuko and Ogata, Hiroaki},
title = {ARCHIE: Exploring Language Learner Behaviors in LLM Chatbot-Supported Active Reading Log Data with Epistemic Network Analysis},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706556},
doi = {10.1145/3706468.3706556},
abstract = {With the increasing integration of technology in education, chatbots and e-readers have emerged as promising tools for enhancing language learning experiences. This study investigates how students engage with digital texts and a purpose-built chatbot designed to promote active reading for EFL students. We analysed student interactions and compared high-proficiency and low-proficiency English learners. Results indicate that while all students perceived the chatbot as easy to use, useful, and enjoyable, significant behavioural differences emerged between proficiency groups. High-proficiency students exhibited more frequent interactions with the chatbot, engaged in more active reading strategies like backtracking, and demonstrated less help seeking behaviours. Epistemic Network Analysis revealed distinct co-occurrence patterns, highlighting the stronger connection between navigation and review behaviours in the high-proficiency group. These findings underscore the potential of chatbot-assisted language learning and emphasise the importance of incorporating active reading strategies for improved comprehension.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {642–654},
numpages = {13},
keywords = {chatbot, e-readers, EFL, active reading, epistemic network analysis (ENA), log data, backtracking},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3636555.3636907,
author = {Wang, Karen D. and Liu, Haoyu and DeLiema, David and Haber, Nick and Salehi, Shima},
title = {Discovering Players’ Problem-Solving Behavioral Characteristics in a Puzzle Game through Sequence Mining},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636907},
doi = {10.1145/3636555.3636907},
abstract = {Digital games offer promising platforms for assessing student higher-order competencies such as problem-solving. However, processing and analyzing the large volume of interaction log data generated in these platforms to uncover meaningful behavioral patterns remain a complex research challenge. In this study, we employ sequence mining and clustering techniques to examine students’ log data in an interactive puzzle game that requires player to change rules to win the game. Our goal is to identify behavioral characteristics associated with the problem-solving practices adopted by individual students. The findings indicate that the most effective problem solvers made fewer rule changes and took longer time to make those changes across both an introductory and a more advanced level of the game. Conversely, rapid rule change actions were linked to ineffective problem-solving. This research underscores the potential of sequence mining and cluster analysis as generalizable methods for understanding student higher-order competencies through log data in digital gaming and learning environments. It also suggests future directions on how to provide just-in-time, in-game feedback to enhance student problem-solving competences.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {498–506},
numpages = {9},
keywords = {cluster analysis, digital games, log data, problem-solving, sequence mining},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3012430.3012541,
author = {Villama\~{n}e, Mikel and Larra\~{n}aga, Mikel and \'{A}lvarez, Ainhoa and Ferrero, Bego\~{n}a},
title = {RubricVis: enriching rubric-based formative assessment with visual learning analytics},
year = {2016},
isbn = {9781450347471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3012430.3012541},
doi = {10.1145/3012430.3012541},
abstract = {The implementation of the Bologna Process has implied, in many Spanish universities, the introduction of continuous evaluation processes, in which formative assessment has a great relevance. In such kinds of scenarios, rubrics provide many benefits that can remarkably be improved using Visual Learning Analytics techniques. This paper presents some of the visualization capabilities provided by RubricVis, a system that uses Visual Learning Analytics techniques to enhance rubric-based formative assessment. The objective of these visualizations is to enrich the feedback for both students and teachers in such evaluation environments. This feedback will help users to understand the learning and evaluation processes in order to improve them.},
booktitle = {Proceedings of the Fourth International Conference on Technological Ecosystems for Enhancing Multiculturality},
pages = {363–368},
numpages = {6},
keywords = {formative-assessment, open learner models, rubrics, visual learning analytics},
location = {Salamanca, Spain},
series = {TEEM '16}
}

@inproceedings{10.1145/3636555.3636861,
author = {Steinbeck, Hendrik and Elhayany, Mohamed and Meinel, Christoph},
title = {Millions of Views, But Does It Promote Learning? Analyzing Popular SciComm Production Styles Regarding Learning Success, User Behavior and Perception},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636861},
doi = {10.1145/3636555.3636861},
abstract = {With a rising amount of highly successful educational content on major video platforms, science communication (SciComm) can be considered mainstream. Although the success in terms of social media metrics (e.g. followers and watch time) is undoubtedly given, the learning mechanisms of these production styles is under-researched. Through a between-subject-design of 980 adult learners in a MOOC about data science, this study analyzes how much of a difference four popular SciComm production styles about relational databases make in regard to perceived quality, learning success and technical user behavior. Testing the isolated effect showed no statistical difference in the grand scheme of things. Additionally, a multivariate regression model, estimating the overall course points with robust standard errors showed six significant variables: The time spend with the material and the number of exercise submissions are particular noteworthy. Based on our results, an underlying (video) script is more relevant than the actual production style. Prioritizing the preparation of this material instead following a specific, pre-existing video production style is recommended.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {134–144},
numpages = {11},
keywords = {field-experiment, science communication, video production styles, video-based education, youtube},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636859,
author = {Wong, Cheryl Sze Yin and Ramasamy, Savitha},
title = {Architectural Adaptation and Regularization of Attention Networks for Incremental Knowledge Tracing},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636859},
doi = {10.1145/3636555.3636859},
abstract = {EdTech platforms continuously refresh their database with new questions and concepts with evolving course syllabus. The state-of-the-art knowledge tracing models are unable to adapt to these changes, as the size of the question embedding layers is typically fixed. In this work, we propose an incremental learning algorithm for knowledge tracing that is capable of adapting itself to growing pool of concepts and questions, through its architectural adaptation and regularization strategies. The algorithm, referred as, "Architectural adaptation and Regularization of Attention network for Incremental Knowledge Tracing (ARAIKT)", is capable of adapting the embeddings with increasing concepts and question bank, while preserving representations of the previous concepts and question banks. Furthermore, they are robust to distributional drifts in the data, and are capable of preserving privacy of data across study centers and EdTech platforms. We demonstrate the effectiveness of the ARAIKT by evaluating its performance on subsets of study centers/academic years within ASSISTment2009 and ASSISTment2017 data sets, respectively.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {757–762},
numpages = {6},
keywords = {Incremental, Knowledge Tracing},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3576050.3576089,
author = {Thomas, Danielle and Yang, Xinyu and Gupta, Shivang and Adeniran, Adetunji and Mclaughlin, Elizabeth and Koedinger, Kenneth},
title = {When the Tutor Becomes the Student: Design and Evaluation of Efficient Scenario-based Lessons for Tutors},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576089},
doi = {10.1145/3576050.3576089},
abstract = {Tutoring is among the most impactful educational influences on student achievement, with perhaps the greatest promise of combating student learning loss. Due to its high impact, organizations are rapidly developing tutoring programs and discovering a common problem- a shortage of qualified, experienced tutors. This mixed methods investigation focuses on the impact of short (∼15 min.), online lessons in which tutors participate&nbsp;in situational judgment tests based on everyday tutoring scenarios. We developed three lessons on strategies for supporting student self-efficacy and motivation and tested them with 80 tutors from a national, online tutoring organization. Using a mixed-effects logistic regression model, we found a statistically significant learning effect indicating tutors performed about 20% higher post-instruction than pre-instruction (β = 0.811, p &lt; 0.01). Tutors scored ∼30% better on selected compared to constructed responses at posttest with evidence that tutors are learning from selected-response questions alone. Learning analytics and qualitative feedback suggest future design modifications for larger scale deployment, such as creating more authentically challenging selected-response options, capturing common misconceptions using learnersourced data, and varying modalities of scenario delivery with the aim of maintaining learning gains while reducing time and effort for tutor participants and trainers.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {250–261},
numpages = {12},
keywords = {Design-based research, Learnersourcing, Scenario-based learning, Tutoring},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3636555.3636883,
author = {Xu, Austin and Monroe, Will and Bicknell, Klinton},
title = {Large language model augmented exercise retrieval for personalized language learning},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636883},
doi = {10.1145/3636555.3636883},
abstract = {We study the problem of zero-shot exercise retrieval in the context of online language learning, to give learners the ability to explicitly request personalized exercises via natural language. Using real-world data collected from language learners, we observe that vector similarity approaches poorly capture the relationship between exercise content and the language that learners use to express what they want to learn. This semantic gap between queries and content dramatically reduces the effectiveness of general-purpose retrieval models pretrained on large scale information retrieval datasets like MS MARCO&nbsp;[2]. We leverage the generative capabilities of large language models to bridge the gap by synthesizing hypothetical exercises based on the learner’s input, which are then used to search for relevant exercises. Our approach, which we call mHyER, overcomes three challenges: (1) lack of relevance labels for training, (2) unrestricted learner input content, and (3) low semantic similarity between input and retrieval candidates. mHyER outperforms several strong baselines on two novel benchmarks created from crowdsourced data and publicly available data.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {284–294},
numpages = {11},
keywords = {large language models, online language learning, personalization, zero-shot exercise retrieval},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3636555.3636901,
author = {Van Campenhout, Rachel and Kimball, Murray and Clark, Michelle and Dittel, Jeffrey S. and Jerome, Bill and Johnson, Benny G.},
title = {An Investigation of Automatically Generated Feedback on Student Behavior and Learning},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636901},
doi = {10.1145/3636555.3636901},
abstract = {Decades of research have focused on the feedback delivered to students after answering questions—when to deliver feedback and what kind of feedback is most beneficial for learning. While there is a well-established body of research on feedback, new advances in technology have led to new methods for developing feedback and large-scale usage provides new data for understanding how feedback impacts learners. This paper focuses on feedback that was developed using artificial intelligence for an automatic question generation system. The automatically generated questions were placed alongside text as a formative learning tool in an e-reader platform. Three types of feedback were randomized across the questions: outcome feedback, context feedback, and common answer feedback. In this study, we investigate the effect of different feedback types on student behavior. This analysis is significant to the expanding body of research on automatic question generation, as little research has been reported on automatically generated feedback specifically, as well as the additional insights that microlevel data can reveal on the relationship between feedback and student learning behaviors.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {850–856},
numpages = {7},
keywords = {Automatic question generation, Automatically generated feedback, Feedback, Formative practice, Learning by doing, Student behavior},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3375462.3375489,
author = {Hilliger, Isabel and Aguirre, Camila and Miranda, Constanza and Celis, Sergio and P\'{e}rez-Sanagust\'{\i}n, Mar},
title = {Design of a curriculum analytics tool to support continuous improvement processes in higher education},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375489},
doi = {10.1145/3375462.3375489},
abstract = {Curriculum analytics (CA) emerged as a sub-field of learning analytics, aiming to use evidence to drive curriculum decision-making and program improvement. However, its overall impact on program outcomes remains unknown. In this context, this paper presents work-in-progress of a large research project to understand how CA could support continuous improvement processes at a program-level. We followed an approach based on design-based research to develop a CA tool: The Integrative Learning Design Framework. This paper describes three out of four phases of this framework and its main results, including the evaluation of the local impact of this CA tool. This evaluation consisted of an instrumental case study to evaluate its use to support 124 teaching staff in a 3-year continuous improvement process in a Latin American university. Lessons learned indicate that the tool helped staff to collect information for curriculum discussions, facilitating the availability of evidence regarding student competency attainment. To generalize these lessons, future work will consist of evaluating the tool in different university settings.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {181–186},
numpages = {6},
keywords = {curriculum analytics, higher education, learning analytics},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375494,
author = {Van Goidsenhoven, Steven and Bogdanova, Daria and Deeva, Galina and Broucke, Seppe vanden and De Weerdt, Jochen and Snoeck, Monique},
title = {Predicting student success in a blended learning environment},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375494},
doi = {10.1145/3375462.3375494},
abstract = {Blended learning is gaining ground in contemporary education. However, studies on predictive learning analytics in the context of blended learning remain relatively scarce compared to Massive Open Online Courses (MOOCs), where such applications have gained a strong foothold. Data sets obtained from blended learning environments suffer from a high dimensionality and typically expose a limited number of instances, which makes predictive analysis a challenging task. In this work, we explore the log data of a master-level blended course to predict the students' grades based entirely on the data obtained from an online module (a small private online course), using and comparing logistic regression and random forest-based predictive models. The results of the analysis show that, despite the limited data, success vs. fail predictions can be made as early as in the middle of the course. This could be used in the future for timely interventions, both for failure prevention as well as for reinforcing positive learning behaviours of students.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {17–25},
numpages = {9},
keywords = {blended learning, e-learning, feature extraction, grade prediction, learning analytics, logistic regression, machine learning, random forest classification},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/2567574.2567578,
author = {Jayaprakash, Sandeep M. and Laur\'{\i}a, Eitel J. M.},
title = {Open academic early alert system: technical demonstration},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567578},
doi = {10.1145/2567574.2567578},
abstract = {This paper synthesizes some of the technical decisions, design strategies &amp; concepts developed during the execution of Open Academic Analytics Initiative (OAAI), a research program aimed at improving student retention rates in colleges, by deploying an open-source academic early alert system to identify the students at academic risk. The paper explains the prototype demonstration of the system, detailing several dimensions of data mining &amp; analysis such as: data integration, predictive modelling and scoring with reporting. The paper should be relevant to practitioners and academicians who want to better understand the implementation of an OAAI academic early-alert system.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {267–268},
numpages = {2},
keywords = {course management systems, data mining, intervention, interventions, learning analytics, open source, portability, retention},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/3576050.3576100,
author = {Hutchins, Nicole and Biswas, Gautam},
title = {Using Teacher Dashboards to Customize Lesson Plans for a Problem-Based, Middle School STEM Curriculum},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576100},
doi = {10.1145/3576050.3576100},
abstract = {Keeping K-12 teachers engaged during students’ learning and problem solving in technology-enhanced, integrated problem-based learning (PBL) has been shown to support deeper student involvement, and, therefore, better success learning difficult science, computing, and engineering concepts and practices. However, students’ learning processes and corresponding difficulties are not easily noticed by teachers as students learn from these environments as processes are captured through mouse clicks, drag and drop actions, and other low-level activities. As such, teachers find it difficult to set up meaningful interactions with students while also maintaining the focus on student-centered learning. Little research has examined dashboard-supported responsive teaching practices for K-12 PBL. This study examined 8 teachers as they used a co-designed teacher dashboard to assess and respond to students’ learning and strategies during an integrated, PBL STEM curriculum. Teachers completed a series of 5 “planning period simulations” leveraging the dashboard and think-aloud protocols were implemented, supported by semi-structured interview questions, to enable the teachers to verbalize their thought and evaluation processes. Content analysis and epistemic network analysis were conducted to analyze the simulations. Understanding how teachers use dashboards to support evidence-based teaching practices during technology-enhanced curricula is critical for improving teacher support and preparation.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {324–332},
numpages = {9},
keywords = {co-design, computational modeling, responsive teaching, teacher dashboards},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3303772.3303787,
author = {Matcha, Wannisa and Ga\v{s}evi\'{c}, Dragan and Uzir, Nora'Ayu Ahmad and Jovanovi\'{c}, Jelena and Pardo, Abelardo},
title = {Analytics of Learning Strategies: Associations with Academic Performance and Feedback},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303787},
doi = {10.1145/3303772.3303787},
abstract = {Learning analytics has the potential to detect and explain characteristics of learning strategies through analysis of trace data and communicate the findings via feedback. However, the role of learning analytics-based feedback in selection and regulation of learning strategies is still insufficiently explored and understood. This research aims to examine the sequential and temporal characteristics of learning strategies and investigate their association with feedback. Three years of trace data were collected from online pre-class activities of a flipped classroom, where different types of feedback were employed in each year. Clustering, sequence mining, and process mining were used to detect and interpret learning tactics and strategies. Inferential statistics were used to examine the association of feedback with the learning performance and the detected learning strategies. The results suggest a positive association between the personalised feedback and the effective strategies.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {461–470},
numpages = {10},
keywords = {Data Mining, Feedback, Learning Analytics, Learning Strategies, Learning Tactics, Self-regulated Learning},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3636555.3636931,
author = {Rashid, M Parvez and Gehringer, Edward and Khosravi, Hassan},
title = {Navigating (Dis)agreement: AI Assistance to Uncover Peer Feedback Discrepancies},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636931},
doi = {10.1145/3636555.3636931},
abstract = {Engaging students in the peer review process has been recognized as a valuable educational tool. It not only nurtures a collaborative learning environment where reviewees receive timely and rich feedback but also enhances the reviewer’s critical thinking skills and encourages reflective self-evaluation. However, a common concern arises when students encounter misaligned or conflicting feedback. Not only can such feedback confuse students; but it can also make it difficult for the instructor to rely on the reviews when assigning a score to the work. Addressing this pressing issue, our paper introduces an innovative, AI-assisted approach that is designed to detect and highlight disagreements within formative feedback. We’ve harnessed extensive data from 170 students, analyzing 15,500 instances of peer feedback from a software development course. By utilizing clustering techniques coupled with sophisticated natural language processing (NLP) models, we transform feedback into distinct feature vectors to pinpoint disagreements. The findings from our study underscore the effectiveness of our approach in enhancing text representations to significantly boost the capability of clustering algorithms in discerning disagreements in feedback. These insights bear implications for educators and software development courses, offering a promising route to streamline and refine the peer review process for the betterment of student learning outcomes.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {907–914},
numpages = {8},
keywords = {NLP, clustering, disagreement, feedback, peer-assessment},
location = {Kyoto, Japan},
series = {LAK '24}
}

@article{10.1145/2687917.2686744,
author = {Knox, Jeremy},
title = {From MOOCs to Learning Analytics: Scratching the surface of the 'visual'},
year = {2014},
issue_date = {11-01-2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2014},
number = {11},
url = {https://doi.org/10.1145/2687917.2686744},
doi = {10.1145/2687917.2686744},
abstract = {The visualization of big MOOC data enables us to see trends in student behaviors and activities around the globe, but what is it that we are not seeing?},
journal = {ELearn},
month = nov,
articleno = {3}
}

@inproceedings{10.1145/3284179.3284291,
author = {Kloos, Carlos Delgado and Dimitriadis, Yannis and Hern\'{a}ndez-Leo, Davinia and Mu\~{n}oz-Merino, Pedro J. and Bote-Lorenzo, Miguel L. and Carri\'{o}, Mar and Alario-Hoyos, Carlos and G\'{o}mez-S\'{a}nchez, Eduardo and Santos, Patricia},
title = {SmartLET: Learning analytics to enhance the design and orchestration in scalable, IoT-enriched, and ubiquitous Smart Learning Environments},
year = {2018},
isbn = {9781450365185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3284179.3284291},
doi = {10.1145/3284179.3284291},
abstract = {This paper presents the SmartLET project, a coordinated research project funded by the Spanish Ministry of Science, Innovation and Universities, which just started in 2018. The main aim of this project is to provide support for the design and orchestration of Smart Learning Environments (SLEs) with the support of learning analytics and the Internet of Things. This paper gives an overview of our conception of SLEs based on previous works, provides some ideas about the connection of learning design and orchestration with SLEs, and analyses different ethical and privacy issues for SLEs. In addition, an initial hypothesis and some specific objectives for a support environment for SLEs are proposed.},
booktitle = {Proceedings of the Sixth International Conference on Technological Ecosystems for Enhancing Multiculturality},
pages = {648–653},
numpages = {6},
keywords = {Internet of Things, Learning analytics, learning design, orchestration, smart learning},
location = {Salamanca, Spain},
series = {TEEM'18}
}

@inproceedings{10.1145/3636555.3636895,
author = {Li, Chenglu and Zhu, Wangda and Xing, Wanli and Guo, Rui},
title = {Analyzing Student Attention and Acceptance of Conversational AI for Math Learning: Insights from a Randomized Controlled Trial},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636895},
doi = {10.1145/3636555.3636895},
abstract = {The significance of nurturing a deep conceptual understanding in math learning cannot be overstated. Grounded in the pedagogical strategies of induction, concretization, and exemplification (ICE), we designed and developed a conversational AI using both rule- and generation-based techniques to facilitate math learning. Serving as a preliminary step, this study employed an experimental design involving 151 U.S.-based college students to reveal students’ attention patterns, technology acceptance model, and qualitative feedback when using the developed ConvAI. Our findings suggest that participants in the ConvAI group generally exhibit higher attention levels than those in the control group, aside from the initial stage where the control group was more attentive. Meanwhile, participants appreciated their experience with the ConvAI, particularly valuing the ICE support features. Finally, qualitative analysis of participants’ feedback was conducted to inform future refinement and to inspire educational researchers and practitioners.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {836–842},
numpages = {7},
keywords = {Conversational AI, Large language models, Math learning, Technology design and development},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3630970.3631007,
author = {Martinez-Maldonado, Roberto},
title = {Data Storytelling: Revolutionising Human-Data Interaction or Just Passing Hype?},
year = {2024},
isbn = {9798400716577},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3630970.3631007},
doi = {10.1145/3630970.3631007},
abstract = {Supporting stakeholders in various sectors to interpret dashboards and visualisations presents significant design challenges that might often be overlooked. The interpretation of visualised data by decision-makers and professionals is essentially the construction of a narrative about the underlying processes. Implementing data storytelling techniques in the design of these visualisations can foster deeper insights by aligning the intended objectives, goals, and outcomes with visual elements. The purpose of this tutorial is to guide participants in integrating data storytelling techniques into their visualisation and dashboard designs, ensuring the communication of meaningful insights.},
booktitle = {Proceedings of the XI Latin American Conference on Human Computer Interaction},
articleno = {37},
numpages = {2},
keywords = {data storytelling, data visualisation, human-data interaction},
location = {Puebla, Mexico},
series = {CLIHC '23}
}

@inproceedings{10.1145/2839509.2850551,
author = {Almatrafi, Omaima and Rangwala, Huzefa and Aditya, Johri and Lester, Jaime},
title = {Using Learning Analytics to Trace Academic Trajectories of CS and IT Students to Better Understanding Successful Pathways to Graduation (Abstract Only)},
year = {2016},
isbn = {9781450336857},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2839509.2850551},
doi = {10.1145/2839509.2850551},
abstract = {We present findings from a study examining students' course-taking pathways to graduation and identify the factors regarding course-taking choices that can affect students' performance. The data for the study was collected from two majors within an engineering school at a large public university: computer science (CS) and information technology (IT). Although we look in depth at CS students, we use the other data for a comparative analysis. CS and IT are the most popular and largest majors, respectively, at our institution with almost 2,500 undergraduate students enrolled in them. The results show that there are differences in specific patterns of courses and illustrate relationships between the frequent courses in each semester and the relationships between courses taken in two consecutive semesters. Some major insights from the analysis of trajectory of frequent courses for both groups include: low performers postponed some courses toward the end of the program, and take a collection of courses together that their counterparts do not usually take. This work has direct implications for advising of prospective and current students and can improve programs' curriculum and students' performance. In the next stage of this study we will compare trajectories of students who graduate with those of students who either leave CS and IT or take longer to graduate. This preliminary research is part of a NSF CISE/EHR funded grant project on "BigData and Education".},
booktitle = {Proceedings of the 47th ACM Technical Symposium on Computing Science Education},
pages = {691},
numpages = {1},
keywords = {course-taking patterns, learning analytics, students' performance},
location = {Memphis, Tennessee, USA},
series = {SIGCSE '16}
}

@inproceedings{10.1145/3576050.3576152,
author = {Oliveira, Hil\'{a}rio and Ferreira Mello, Rafael and Barreiros Rosa, Bruno Alexandre and Rakovic, Mladen and Miranda, Pericles and Cordeiro, Thiago and Isotani, Seiji and Bittencourt, Ig and Gasevic, Dragan},
title = {Towards explainable prediction of essay cohesion in Portuguese and English},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576152},
doi = {10.1145/3576050.3576152},
abstract = {Textual cohesion is an essential aspect of a formally written text, related to linguistic mechanisms that connect elements such as words, sentences, and paragraphs. Several studies have proposed approaches to estimate textual cohesion in essays automatically. There is limited research that aims to study the extent to which the use of machine learning approaches can predict the textual cohesion of essays written in different languages (not just English). This paper reports on the findings of a study that aimed to propose and evaluate approaches that automatically estimate the cohesion of essays in Portuguese and English. The study proposed regression-based models grounded in conventional feature-based machine learning methods and deep learning-based pre-trained language models. The study also examined the explainability of automated approaches to scrutinize their predictions. We analyzed two datasets composed of 4,570 (Portuguese) and 7,101 (English) essays. The results demonstrate that a deep learning-based model achieved the best performance on both datasets with a moderate Pearson correlation with human-rated cohesion scores. However, the explainability of the automatic cohesion estimations based on conventional machine learning models offered a stronger potential than that of the deep learning model.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {509–519},
numpages = {11},
keywords = {Essay analysis, explainable artificial intelligence, regression models, textual cohesion},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/2910896.2925437,
author = {Di Nunzio, Giorgio Maria},
title = {Can you learn it? Probably! Developing Learning Analytics Tools in R},
year = {2016},
isbn = {9781450342292},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2910896.2925437},
doi = {10.1145/2910896.2925437},
abstract = {Automatic text categorization is an effective way to organize large text datasets in Digital Libraries (DL). However, most of the available machine learning tools are complex and go beyond the scope of what a digital library curator need or is able to do in order to classify the objects of a DL. Drawing inspiration from the field of Learning Analytics and Interactive Machine Learning, we design and implement visual interactive classifiers that are intuitive to train and easy to use. In this poster, we present an interactive Web application in R that allows users to use text classifier in an innovative way. The source code of the application is available at the following link: https://github.com/gmdn/educational-data-mining},
booktitle = {Proceedings of the 16th ACM/IEEE-CS on Joint Conference on Digital Libraries},
pages = {213–214},
numpages = {2},
keywords = {automated text classification, interactive machine learning, learning analytics, na\'{\i}ve bayes, r programming, r shiny},
location = {Newark, New Jersey, USA},
series = {JCDL '16}
}

@inproceedings{10.1145/3375462.3375468,
author = {Malekian, Donia and Bailey, James and Kennedy, Gregor},
title = {Prediction of students' assessment readiness in online learning environments: the sequence matters},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375468},
doi = {10.1145/3375462.3375468},
abstract = {Online learning environments are now pervasive in higher education. While not exclusively the case, in these environments, there is often modest teacher presence, and students are provided with access to a range of learning, assessment, and support materials. This places pressure on their study skills, including self-regulation. In this context, students may access assessment material without being fully prepared. This may result in limited success and, in turn, raise a significant risk of disengagement. Therefore, if the prediction of students' assessment readiness was possible, it could be used to assist educators or online learning environments to postpone assessment tasks until students were deemed "ready". In this study, we employed a range of machine learning techniques with aggregated and sequential representations of students' behaviour in a Massive Open Online Course (MOOC), to predict their readiness for assessment tasks. Based on our results, it was possible to successfully predict students' readiness for assessment tasks, particularly if the sequential aspects of behaviour were represented in the model. Additionally, we used sequential pattern mining to investigate which sequences of behaviour differed between high or low level of performance in assessments. We found that a high level of performance had the most sequences related to viewing and reviewing the lecture materials, whereas a low level of performance had the most sequences related to successive failed submissions for an assessment. Based on the findings, implications for supporting specific behaviours to improve learning in online environments are discussed.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {382–391},
numpages = {10},
keywords = {LSTM, MOOCs, assessment readiness prediction, learning analytics, sequential pattern mining},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3152771.3156171,
author = {Gibson, Andrew and Martinez-Maldonado, Roberto},
title = {That dashboard looks nice, but what does it mean? towards making meaning explicit in learning analytics design},
year = {2017},
isbn = {9781450353793},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152771.3156171},
doi = {10.1145/3152771.3156171},
abstract = {As learning analytics (LA) systems become more common, teachers and students are often required to not only make sense of the user interface (UI) elements of a system, but also to make meaning that is pedagogically appropriate to the learning context. However, we suggest that the dominant way of thinking about the relationship between representation and meaning results in an overemphasis on the UI, and that re-thinking this relationship is necessary to create systems that can facilitate deeper meaning making. We propose a conceptual view as a basis for discussion among the LA and HCI communities around a different way of thinking about meaning making, specifically that it should be explicit in the design process, provoking greater consideration of system level elements such as algorithms, data structures and information flow. We illustrate the application of the conceptualisation with two cases of LA design in the areas of Writing Analytics and Multi-modal Dashboards.},
booktitle = {Proceedings of the 29th Australian Conference on Computer-Human Interaction},
pages = {528–532},
numpages = {5},
keywords = {embodied cognition, information systems, learning analytics, meaning making, user interface design},
location = {Brisbane, Queensland, Australia},
series = {OzCHI '17}
}

@inproceedings{10.1145/3706468.3706548,
author = {He, Xinyun and Shu, Qi and Zhang, Mo and Huang, Wei and Zhao, Han and Zhu, Mengxiao},
title = {Beyond Final Products: Multi-Dimensional Essay Scoring Using Keystroke Logs and Deep Learning},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706548},
doi = {10.1145/3706468.3706548},
abstract = {Essay assessment plays a crucial role in evaluating students’ abilities in logical reasoning, critical thinking, and creativity. However, traditional manual scoring methods often suffer from inefficiencies due to fatigue, bias, and emotional factors, compromising objectivity. Automated Essay Scoring (AES) systems offer a more efficient and impartial alternative, yet most existing systems focus primarily on evaluating the final written product, overlooking the valuable data captured during the writing process. To address this issue, we introduce an innovative model called KAES, which explores the potential of integrating writing process data to enhance AES performance. The KAES model leverages a variety of data sources, including text content, prompts, keystroke dynamics, and manually extracted features, to extract meaningful insights. It employs a multi-task learning approach to evaluate essays across both linguistic and argumentative dimensions. Extensive experiments on the real-world CBAL dataset demonstrate that the KAES model significantly outperforms traditional baseline models, highlighting the effectiveness of incorporating writing process data into AES tasks.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {601–610},
numpages = {10},
keywords = {Automated Essay Scoring, Keystroke Logs, Deep learning, Feature Engineering, Multi-Task Learning},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/2663204.2668318,
author = {Ochoa, Xavier and Worsley, Marcelo and Chiluiza, Katherine and Luz, Saturnino},
title = {MLA'14: Third Multimodal Learning Analytics Workshop and Grand Challenges},
year = {2014},
isbn = {9781450328852},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2663204.2668318},
doi = {10.1145/2663204.2668318},
abstract = {This paper summarizes the third Multimodal Learning Analytics Workshop and Grand Challenges (MLA'14). This subfield of Learning Analytics focuses on the interpretation of the multimodal interactions that occurs in learning environments, both digital and physical. This is a hybrid event that includes presentations about methods and techniques to analyze and merge the different signals captured from these environments (workshop session) and more concrete results from the application of Multimodal Learning Analytics techniques to predict the performance of students while solving math problems or presenting in the classroom (challenges sessions). A total of eight articles will be presented in this event. The main conclusion from this event is that Multimodal Learning Analytics is a desirable research endeavour that could produce results that can be currently applied to improve the learning process.},
booktitle = {Proceedings of the 16th International Conference on Multimodal Interaction},
pages = {531–532},
numpages = {2},
keywords = {educational data mining, learning analytics, multimodal interaction},
location = {Istanbul, Turkey},
series = {ICMI '14}
}

@inproceedings{10.1145/3636555.3636927,
author = {Iqbal, Sehrish and Rakovic, Mladen and Chen, Guanliang and Li, Tongguang and Bajaj, Jasmine and Mello, Rafael Ferreira and Fan, Yizhou and Aljohani, Naif Radi and Gasevic, Dragan},
title = {Towards Improving Rhetorical Categories Classification and Unveiling Sequential Patterns in Students' Writing},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636927},
doi = {10.1145/3636555.3636927},
abstract = {To meet the growing demand for future professionals who can present information to an audience and create quality written products, educators are increasingly assigning writing assignments that require students to gather information from multiple sources, reorganise and reinterpret knowledge from source materials, and plan for rhetorical structure goals in order to meet the task requirements. When evaluating an essay coherence, scorers manually look for the presence of required rhetorical categories, which takes time. Supervised Machine Learning (ML) techniques have proven to be an effective tool for automatic detection of rhetorical categories that approximate students’ cognitive engagement with source information. Previous studies that addressed this problem used relatively small datasets and reported relatively low kappa scores for accuracy, limiting the use of such models in real-world scenarios. Moreover, to empower educators to effectively evaluate the overall quality of students’ writing, the associations between the sequential patterns of rhetorical categories in students’ writing and writing performance must be examined, which remains largely unexplored in educational domain. Therefore, to fill these gaps, our study aimed to i) investigate the impact of data augmentation approaches on the performance of deep learning algorithms in classifying rhetorical categories in student essays according to Bloom‘s taxonomy ii) and explore the sequential patterns of rhetorical categories in students’ writing that can influence writing performance. Our findings showed that deep learning-based model BERT on Easy Data Augmentation (EDA) based augmented data achieved 20% higher Cohen’s kappa than normal (non-augmented) data, and we discovered that students in different performance groups were statistically different in terms of rhetorical patterns. Our proposed study is valuable in terms of building a data analytic foundation that can be used to create formative feedback on students’ writings based on the patterns of rhetorical categories to improve essay quality.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {656–666},
numpages = {11},
keywords = {Rhetorical structure, automatic classification, deep learning, essay analysis, ordered network analysis},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/2460296.2460352,
author = {Slotta, James D. and Tissenbaum, Mike and Lui, Michelle},
title = {Orchestrating of complex inquiry: three roles for learning analytics in a smart classroom infrastructure},
year = {2013},
isbn = {9781450317856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2460296.2460352},
doi = {10.1145/2460296.2460352},
abstract = {This paper presents our research of a pedagogical model known as Knowledge Community and Inquiry (KCI), focusing on our design of a technological infrastructure for the orchestration of the complex CSCL scripts that characterize KCI curricula. We first introduce the KCI model including some basic design principles, and describe its dependency on real time learning analytics. Next, we describe our technology, known as SAIL Smart Space (S3), which provides scaffolding and analytic support of sequenced interactions amongst people, materials, tools and environments. We outline the critical role of the teacher in our designs and describe how S3 supports their active role in orchestration. Finally we outline two implementations of KCI/S3 and the role of learning analytics, in supporting dynamic collective visualizations, real time orchestrational logic, and ambient displays.},
booktitle = {Proceedings of the Third International Conference on Learning Analytics and Knowledge},
pages = {270–274},
numpages = {5},
keywords = {ambient display, intelligent agents, orchestration, smart classrooms},
location = {Leuven, Belgium},
series = {LAK '13}
}

@inproceedings{10.1145/3706468.3706570,
author = {Barrett, Alex and Ke, Fengfeng and Zhang, Nuodi and Dai, Chih-Pu and Bhowmik, Saptarshi and Yuan, Xin},
title = {Pattern analysis of ambitious science talk between preservice teachers and AI-powered student agents},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706570},
doi = {10.1145/3706468.3706570},
abstract = {New frontiers in simulation-based teacher training have been unveiled with the advancement of artificial intelligence (AI). Integrating AI into virtual student agents increases the accessibility and affordability of teacher training simulations, but little is known about how preservice teachers interact with AI-powered student agents. This study analyzed the discourse behavior of 15 preservice teachers who undertook simulation-based training with AI-powered student agents. Using a framework of ambitious science teaching, we conducted a pattern analysis of teacher and student talk moves, looking for evidence of academically productive discourse. Comparisons are made with patterns found in real classrooms with professionally trained science teachers. Results indicated that preservice teachers generated academically productive discourse with AI-powered students by using ambitious talk moves. The pattern analysis also revealed coachable moments where preservice teachers succumbed to cycles of unproductive discourse. This study highlights the utility of analyzing classroom discourse to understand human-AI communication in simulation-based teacher training.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {761–770},
numpages = {10},
keywords = {Artificial intelligence, Discourse pattern analysis, Simulation, Teacher training},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3576050.3576122,
author = {Lee, Yoon and Specht, Marcus},
title = {Can We Empower Attentive E-reading with a Social Robot? An Introductory Study with a Novel Multimodal Dataset and Deep Learning Approaches},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576122},
doi = {10.1145/3576050.3576122},
abstract = {Reading on digital devices has become more commonplace, while it often poses challenges to learners’ attention. In this study, we hypothesized that allowing learners to reflect on their reading phases with an empathic social robot companion might enhance learners’ attention in e-reading. To verify our assumption, we collected a novel dataset (SKEP) in an e-reading setting with social robot support. It contains 25 multimodal features from various sensors and logged data that are direct and indirect cues of attention. Based on the SKEP dataset, we comprehensively compared the difference between HRI-based (treatment) and GUI-based (control) feedback and obtained insights for intervention design. Based on the human annotation of the nearly 40 hours of video data streams from 60 subjects, we developed a machine learning model to capture attention-regulation behaviors in e-reading. We exploited a two-stage framework to recognize learners’ observable self-regulatory behaviors and conducted attention analysis. The proposed system showed a promising performance with high prediction results of e-reading with HRI, such as 72.97% accuracy in recognizing attention regulation behaviors, 74.29% accuracy in predicting knowledge gain, 75.00% for perceived interaction experience, and 75.00% for perceived social presence. We believe our work can inspire the future design of HRI-based e-reading and its analysis.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {520–530},
numpages = {11},
keywords = {Attention Self-regulation, Deep Learning, E-reading, Human-Robot Interaction, Novel dataset},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3303772.3303821,
author = {Mitra, Ritayan and Chavan, Pankaj},
title = {DEBE feedback for large lecture classroom analytics},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303821},
doi = {10.1145/3303772.3303821},
abstract = {Learning Analytics (LA) research has demonstrated the potential of LA in detecting and monitoring cognitive-affective parameters and improving student success. But most of it has been applied to online and computerized learning environments whereas physical classrooms have largely remained outside the scope of such research. This paper attempts to bridge that gap by proposing a student feedback model in which they report on the difficult/easy and engaging/boring aspects of their lecture. We outline the pedagogical affordances of an aggregated time-series of such data and discuss it within the context of LA research.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {426–430},
numpages = {5},
keywords = {Large lectures, learning analytics, live feedback, mobile application, quantified self},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3576050.3576105,
author = {Almoubayyed, Husni and Fancsali, Stephen E. and Ritter, Steve},
title = {Instruction-Embedded Assessment for Reading Ability in Adaptive Mathematics Software},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576105},
doi = {10.1145/3576050.3576105},
abstract = {Adaptive educational software is likely to better support broader and more diverse sets of learners by considering more comprehensive views (or models) of such learners. For example, recent work proposed making inferences about “non-math” factors like reading comprehension while students used adaptive software for mathematics to better support and adapt to learners. We build on this proposed approach to more comprehensive learning modeling by providing an empirical basis for making inferences about students’ reading ability from their performance on activities in adaptive software for mathematics. We lay out an approach to predicting middle school students’ reading ability using their performance on activities within Carnegie Learning’s MATHia, a widely used intelligent tutoring system for mathematics. We focus on how performance in an early, introductory activity as an especially powerful place to consider instruction-embedded assessment of non-math factors like reading comprehension to guide adaptation based on factors like reading ability. We close by discussing opportunities to extend this work by focusing on particular knowledge components or skills tracked by MATHia that may provide important “levers” for driving adaptation based on students’ reading ability while they learn and practice mathematics.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {366–377},
numpages = {12},
keywords = {assessments, intelligent tutoring systems, machine learning, predictive modeling},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3643834.3660682,
author = {Fernandez-Nieto, Gloria Milena and Swiecki, Zachari and Tsai, Yi-Shan and Sha, Lele and Wei, Yinwei and Wen, Jim and Li, Yuheng and Jin, Yueqiao and Feraud, Ivan Silva and Li, Yuan-Fang and Wang, Weiqing and Chen, Guanliang and Gasevic, Dragan},
title = {Co-designing a knowledge management tool for educator communities of practice},
year = {2024},
isbn = {9798400705830},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643834.3660682},
doi = {10.1145/3643834.3660682},
abstract = {Knowledge management involves finding, expanding, and using knowledge in an organisation to achieve goals. Its role is crucial in higher education to improve problem-solving, research, and teaching by acquiring, sharing, and applying knowledge. Higher education institutions can promote knowledge management through Communities of Practice, but doing so remains challenging due to cultural, organisational, and technological reasons. We present findings of the first step of co-design workshops with authentic higher education teaching teams that sought to understand (a) their practices as a community and any motivators and impediments to their community development; (b) how they perceived the tools they use for knowledge management; and (c) the kinds of tools they believed could help them better conduct knowledge management and develop as Communities of Practice. Our findings suggested four essential design requirements and informed our development of a new tool to support the knowledge management needs of higher education teaching teams.},
booktitle = {Proceedings of the 2024 ACM Designing Interactive Systems Conference},
pages = {1970–1990},
numpages = {21},
keywords = {co-design, communities of practice, education, knowledge management},
location = {Copenhagen, Denmark},
series = {DIS '24}
}

@inproceedings{10.1145/3576050.3576081,
author = {Borchers, Conrad and Pardos, Zachary A.},
title = {Insights into undergraduate pathways using course load analytics},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576081},
doi = {10.1145/3576050.3576081},
abstract = {Course load analytics (CLA) inferred from LMS and enrollment features can offer a more accurate representation of course workload to students than credit hours and potentially aid in their course selection decisions. In this study, we produce and evaluate the first machine-learned predictions of student course load ratings and generalize our model to the full 10,000 course catalog of a large public university. We then retrospectively analyze longitudinal differences in the semester load of student course selections throughout their degree. CLA by semester shows that a student’s first semester at the university is among their highest load semesters, as opposed to a credit hour-based analysis, which would indicate it is among their lowest. Investigating what role predicted course load may play in program retention, we find that students who maintain a semester load that is low as measured by credit hours but high as measured by CLA are more likely to leave their program of study. This discrepancy in course load is particularly pertinent in STEM and associated with high prerequisite courses. Our findings have implications for academic advising, institutional handling of the freshman experience, and student-facing analytics to help students better plan, anticipate, and prepare for their selected courses.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {219–229},
numpages = {11},
keywords = {course load analytics, higher education, on-time graduation, stop-out, workload},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3448139.3448179,
author = {Jivet, Ioana and Wong, Jacqueline and Scheffel, Maren and Valle Torre, Manuel and Specht, Marcus and Drachsler, Hendrik},
title = {Quantum of Choice: How learners’ feedback monitoring decisions, goals and self-regulated learning skills are related},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448179},
doi = {10.1145/3448139.3448179},
abstract = {Learning analytics dashboards (LADs) are designed as feedback tools for learners, but until recently, learners rarely have had a say in how LADs are designed and what information they receive through LADs. To overcome this shortcoming, we have developed a customisable LAD for Coursera MOOCs on which learners can set goals and choose indicators to monitor. Following a mixed-methods approach, we analyse 401 learners’ indicator selection behaviour in order to understand the decisions they make on the LAD and whether learner goals and self-regulated learning skills influence these decisions. We found that learners overwhelmingly chose indicators about completed activities. Goals are not associated with indicator selection behaviour, while help-seeking skills predict learners’ choice of monitoring their engagement in discussions and time management skills predict learners’ interest in procrastination indicators. The findings have implications for our understanding of learners’ use of LADs and their design.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {416–427},
numpages = {12},
keywords = {customisable dashboard, feedback, learner goal, learning dashboard, self-regulated learning},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3375462.3375501,
author = {Saqr, Mohammed and Nouri, Jalal},
title = {High resolution temporal network analysis to understand and improve collaborative learning},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375501},
doi = {10.1145/3375462.3375501},
abstract = {There has been significant efforts in studying collaborative and social learning using aggregate networks. Such efforts have demonstrated the worth of the approach by providing insights about the interactions, student and teacher roles, and predictability of performance. However, using an aggregated network discounts the fine resolution of temporal interactions. By doing so, we might overlook the regularities/irregularities of students' interactions, the process of learning regulation, and how and when different actors influence each other. Thus, compressing a complex temporal process such as learning may be oversimplifying and reductionist. Through a temporal network analysis of 54 students interactions (in total 3134 interactions) in an online medical education course, this study contributes with a methodological approach to building, visualizing and quantitatively analyzing temporal networks, that could help educational practitioners understand important temporal aspects of collaborative learning that might need attention and action. Furthermore, the analysis conducted emphasize the importance of considering the time characteristics of the data that should be used when attempting to, for instance, implement early predictions of performance and early detection of students and groups that need support and attention.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {314–319},
numpages = {6},
keywords = {collaborative learning, learning analytics, medical education, problem-based learning, social network analysis, temporal networks, temporarily},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3448139.3448140,
author = {Effenberger, Tomas and Pel\'{a}nek, Radek},
title = {Validity and Reliability of Student Models for Problem-Solving Activities},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448140},
doi = {10.1145/3448139.3448140},
abstract = {Student models are typically evaluated through predicting the correctness of the next answer. This approach is insufficient in the problem-solving context, especially for student models that use performance data beyond binary correctness. We propose more comprehensive methods for validating student models and illustrate them in the context of introductory programming. We demonstrate the insufficiency of the next answer correctness prediction task, as it is neither able to reveal low validity of student models that use just binary correctness, nor does it show increased validity of models that use other performance data. The key message is that the prevalent usage of the next answer correctness for validating student models and binary correctness as the only input to the models is not always warranted and limits the progress in learning analytics.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {1–11},
numpages = {11},
keywords = {difficulties, introductory programming, performance measures, problem solving, reliability, skills, student modeling, validity},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3027385.3029425,
author = {Knight, Simon and Allen, Laura and Gibson, Andrew and McNamara, Danielle and Buckingham Shum, Simon},
title = {Writing analytics literacy: bridging from research to practice},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029425},
doi = {10.1145/3027385.3029425},
abstract = {There is untapped potential in achieving the full impact of learning analytics through the integration of tools into practical pedagogic contexts. To meet this potential, more work must be conducted to support educators in developing learning analytics literacy. The proposed workshop addresses this need by building capacity in the learning analytics community and developing an approach to resourcing for building 'writing analytics literacy'.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {496–497},
numpages = {2},
keywords = {analytics for action, automated writing evaluation, learning analytics, learning analytics literacy, practitioner knowledge, writing analytics},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3706468.3706528,
author = {Fang, Yu and Huang, Shihong and Ogan, Amy},
title = {A Cross-Cultural Confusion Model for Detecting and Evaluating Students’ Confusion In a Large Classroom},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706528},
doi = {10.1145/3706468.3706528},
abstract = {In traditional lecture delivery setting, it is very challenging to identify which part of the lecture material that students are struggling with. One approach to identify difficult concepts is to capture students’ confusion during class time. However, most existing confusion detectors focus on an individual student rather than a classroom, and only on a single ethnicity group which could propagate bias when developing pedagogical technologies. In this paper, we leverage two existing ‘Confused’ facial expression datasets (DAiSEE and DevEmo) with an East Asian ‘Confused’ facial expression dataset that we collected. Through model performance and explainableAI, we address potential cultural biases in detecting emotions, particularly in confusion, and identified culturally-specific features that align with prior research. As a proof-of-concept, we deployed this cross-cultural confusion machine learning model in a live semester-long class. This work to integrate cross-cultural facial features highlights the importance of fostering inclusivity in educational technologies.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {473–483},
numpages = {11},
keywords = {Cross-cultural models, Confusion, Affective computing, Retrieval-augmented generation},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3576050.3576087,
author = {Dood, Amber and Das, Kapotaksha and Qian, Zhen and Finkenstaedt-Quinn, Solaire and Gere, Anne and Shultz, Ginger},
title = {A Dashboard to Provide Instructors with Automated Feedback on Students’ Peer Review Comments},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576087},
doi = {10.1145/3576050.3576087},
abstract = {Writing-to-Learn (WTL) is an evidence-based instructional practice which can help students construct knowledge across many disciplines. Though it is known to be an effective practice, many instructors do not implement WTL in their courses due to time constraints and inability to provide students with personalized feedback. One way to address this is to include peer review, which allows students to receive feedback on their writing and benefits them as they act as reviewers. To further ease the implementation of peer review and provide instructors with feedback on their students’ work, we labeled students’ peer review comments across courses for type of feedback provided and trained a machine learning model to automatically classify those comments, improving upon models reported in prior work. We then created a dashboard which takes students’ comments, labels the comments using the model, and allows instructors to filter through their students’ comments based on how the model labels the comments. This dashboard can be used by instructors to monitor the peer review collaborations occurring in their courses. The dashboard will allow them to efficiently use information provided by peers to identify common issues in their students’ writing and better evaluate the quality of their students’ peer review.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {619–625},
numpages = {7},
keywords = {Writing-to-Learn, automated feedback, instructor dashboards, machine learning, peer review},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3375462.3375502,
author = {Fadljevi\'{c}, Leon and Maitz, Katharina and Kowald, Dominik and Pammer-Schindler, Viktoria and Gasteiger-Klicpera, Barbara},
title = {Slow is good: the effect of diligence on student performance in the case of an adaptive learning system for health literacy},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375502},
doi = {10.1145/3375462.3375502},
abstract = {This paper describes the analysis of temporal behavior of 11--15 year old students in a heavily instructionally designed adaptive e-learning environment. The e-learning system is designed to support student's acquisition of health literacy. The system adapts text difficulty depending on students' reading competence, grouping students into four competence levels. Content for the four levels of reading competence was created by clinical psychologists, pedagogues and medicine students. The e-learning system consists of an initial reading competence assessment, texts about health issues, and learning tasks related to these texts. The research question we investigate in this work is whether temporal behavior is a differentiator between students despite the system's adaptation to students' reading competence, and despite students having comparatively little freedom of action within the system. Further, we also investigated the correlation of temporal behaviour with performance. Unsupervised clustering clearly separates students into slow and fast students with respect to the time they take to complete tasks. Furthermore, topic completion time is linearly correlated with performance in the tasks. This means that we interpret working slowly in this case as diligence, which leads to more correct answers, even though the level of text difficulty matches student's reading competence. This result also points to the design opportunity to integrate advice on overarching learning strategies, such as working diligently instead of rushing through, into the student's overall learning activity. This can be done either by teachers, or via additional adaptive learning guidance within the system.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {112–117},
numpages = {6},
keywords = {adaptive e-learning system, clustering, differentiation, diversity, health literacy, learning analytics, reading competence},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3448139.3448155,
author = {Huang, Yun and Lobczowski, Nikki G. and Richey, J. Elizabeth and McLaughlin, Elizabeth A. and Asher, Michael W. and Harackiewicz, Judith M. and Aleven, Vincent and Koedinger, Kenneth R.},
title = {A General Multi-method Approach to Data-Driven Redesign of Tutoring Systems},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448155},
doi = {10.1145/3448139.3448155},
abstract = {Analytics of student learning data are increasingly important for continuous redesign and improvement of tutoring systems and courses. There is still a lack of general guidance on converting analytics into better system design, and on combining multiple methods to maximally improve a tutor. We present a multi-method approach to data-driven redesign of tutoring systems and its empirical evaluation. Our approach systematically combines existing and new learning analytics and instructional design methods. In particular, our methods involve identifying difficult skills and creating focused tasks for learning these difficult skills effectively following content redesign strategies derived from analytics. In our past work, we applied this approach to redesigning an algebraic modeling unit and found initial evidence of its effectiveness. In the current work, we extended this approach and applied it to redesigning two other tutor units in addition to a second iteration of redesigning the previously redesigned unit. We conducted a one-month classroom experiment with 129 high school students. Compared to the original tutor, the redesigned tutor led to significantly higher learning outcomes, with time mainly allocated to focused tasks rather than original full tasks. Moreover, it reduced over- and under-practice, yielded a more effective practice experience, and selected skills progressing from easier to harder to a greater degree. Our work provides empirical evidence of the effectiveness and generality of a multi-method approach to data-driven instructional redesign.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {161–172},
numpages = {12},
keywords = {adaptivity, data mining, instructional design, learning design, learning engineering},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3576050.3576108,
author = {Murali, Rohit and Conati, Cristina and Azevedo, Roger},
title = {Predicting Co-occurring Emotions in MetaTutor when Combining Eye-Tracking and Interaction Data from Separate User Studies},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576108},
doi = {10.1145/3576050.3576108},
abstract = {Learning can be improved by providing personalized feedback adapting to the emotions that the learner may be experiencing. There is initial evidence that co-occurring emotions can be predicted during learning in Intelligent Tutoring Systems (ITS) through eye-tracking and interaction data. Predicting co-occurring emotions is a complex task and merging datasets has the potential to improve predictive performance. In this paper, we combine data from two user studies with an ITS, and analyze whether there is an improvement in predictive performance of co-occurring emotions, despite the user studies using different eye-trackers. In the pursuit towards developing real affect-aware ITS, we look at whether we can isolate classifiers that perform better than a baseline. In this regard we perform a series of statistical analyses and test out the predictive performance of standard machine learning models as well as an ensemble classifier for the task of predicting co-occurring emotions.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {388–398},
numpages = {11},
keywords = {Co-occurring emotions, eye-tracking, interaction},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576073,
author = {Vanacore, Kirk and Gurung, Ashish and Mcreynolds, Andrew and Liu, Allison and Shaw, Stacy and Heffernan, Neil},
title = {Impact of Non-Cognitive Interventions on Student Learning Behaviors and Outcomes: An analysis of seven large-scale experimental inventions},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576073},
doi = {10.1145/3576050.3576073},
abstract = {As evidence grows supporting the importance of non-cognitive factors in learning, computer-assisted learning platforms increasingly incorporate non-academic interventions to influence student learning and learning related-behaviors. Non-cognitive interventions often attempt to influence students’ mindset, motivation, or metacognitive reflection to impact learning behaviors and outcomes. In the current paper, we analyze data from five experiments, involving seven treatment conditions embedded in mastery-based learning activities hosted on a computer-assisted learning platform focused on middle school mathematics. Each treatment condition embodied a specific non-cognitive theoretical perspective. Over seven school years, 20,472 students participated in the experiments. We estimated the effects of each treatment condition on students’ response time, hint usage, likelihood of mastering knowledge components, learning efficiency, and post-tests performance. Our analyses reveal a mix of both positive and negative treatment effects on student learning behaviors and performance. Few interventions impacted learning as assessed by the post-tests. These findings highlight the difficulty in positively influencing student learning behaviors and outcomes using non-cognitive interventions.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {165–174},
numpages = {10},
keywords = {A/B Testing, Causal Inference, Computer Assisted Learning Platform, Non-Cognitive Factors},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3448139.3448201,
author = {Hassan, Javaria and Leong, Jovin and Schneider, Bertrand},
title = {Multimodal Data Collection Made Easy: The EZ-MMLA Toolkit: A data collection website that provides educators and researchers with easy access to multimodal data streams.},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448201},
doi = {10.1145/3448139.3448201},
abstract = {While Multimodal Learning Analytics (MMLA) is becoming a popular methodology in the LAK community, most educational researchers still rely on traditional instruments for capturing learning processes (e.g., click-stream, log data, self-reports, qualitative observations). MMLA has the potential to complement and enrich traditional measures of learning by providing high frequency data on learners’ behavior, cognition and affects. However, there is currently no easy-to-use toolkit for recording multimodal data streams. Existing methodologies rely on the use of physical sensors and custom-written code for accessing sensor data. In this paper, we present the EZ-MMLA toolkit. This toolkit was implemented as a website that provides easy access to the latest machine learning algorithms for collecting a variety of data streams from webcams: attention (eye-tracking), physiological states (heart rate), body posture (skeletal data), hand gestures, emotions (from facial expressions and speech), and lower-level computer vision algorithms (e.g., fiducial / color tracking). This toolkit can run from any browser and does not require special hardware or programming experience. We compare this toolkit with traditional methods and describe a case study where the EZ-MMLA toolkit was used in a classroom context. We conclude by discussing other applications of this toolkit, potential limitations, and future steps.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {579–585},
numpages = {7},
keywords = {Computer Visions, Data Collection Toolkit, Multimodal Analytics},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3576050.3576112,
author = {Iqbal, Sehrish and Rakovic, Mladen and Chen, Guanliang and Li, Tongguang and Ferreira Mello, Rafael and Fan, Yizhou and Fiorentino, Giuseppe and Radi Aljohani, Naif and Gasevic, Dragan},
title = {Towards Automated Analysis of Rhetorical Categories in Students Essay Writings using Bloom’s Taxonomy},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576112},
doi = {10.1145/3576050.3576112},
abstract = {Essay writing has become one of the most common learning tasks assigned to students enrolled in various courses at different educational levels, owing to the growing demand for future professionals to effectively communicate information to an audience and develop a written product (i.e. essay). Evaluating a written product requires scorers who manually examine the existence of rhetorical categories, which is a time-consuming task. Machine Learning (ML) approaches have the potential to alleviate this challenge. As a result, several attempts have been made in the literature to automate the identification of rhetorical categories using Rhetorical Structure Theory (RST). However, RST do not provide information regarding students’ cognitive level, which motivates the use of Bloom’s Taxonomy. Therefore, in this research we propose to: i) investigate the extent to which classification of rhetorical categories can be automated based on Bloom’s taxonomy by comparing the traditional ML classifiers with the pre-trained language model BERT, ii) explore the associations between rhetorical categories and writing performance. Our results showed that BERT model outperformed the traditional ML-based classifiers with 18% better accuracy, indicating it can be used in future analytics tool. Moreover, we found a statistical difference between the associations of rhetorical categories in low-achiever, medium-achiever and high-achiever groups which implies that rhetorical categories can be predictive of writing performance.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {418–429},
numpages = {12},
keywords = {Rhetorical structure, epistemic network analysis, essay analysis, machine learning},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3448139.3448161,
author = {Zhou, Jianing and Bhat, Suma},
title = {Modeling Consistency Using Engagement Patterns in Online Courses},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448161},
doi = {10.1145/3448139.3448161},
abstract = {Consistency of learning behaviors is known to play an important role in learners’ engagement in a course and impact their learning outcomes. Despite significant advances in the area of learning analytics (LA) in measuring various self-regulated learning behaviors, using LA to measure consistency of online course engagement patterns remains largely unexplored. This study focuses on modeling consistency of learners in online courses to address this research gap. Toward this, we propose a novel unsupervised algorithm that combines sequence pattern mining and ideas from information retrieval with a clustering algorithm to first extract engagement patterns of learners, represent learners in a vector space of these patterns and finally group them into groups with similar consistency levels. Using clickstream data recorded in a popular learning management system over two offerings of a STEM course, we validate our proposed approach to detect learners that are inconsistent in their behaviors. We find that our method not only groups learners by consistency levels, but also provides reliable instructor support at an early stage in a course.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {226–236},
numpages = {11},
keywords = {behavior modeling, cluster, consistency analysis},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3375462.3375527,
author = {Iraj, Hamideh and Fudge, Anthea and Faulkner, Margaret and Pardo, Abelardo and Kovanovi\'{c}, Vitomir},
title = {Understanding students' engagement with personalised feedback messages},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375527},
doi = {10.1145/3375462.3375527},
abstract = {Feedback is a major factor of student success within higher education learning. However, recent changes - such as increased class sizes and socio-economic diversity of the student population - challenged the provision of effective student feedback. Although the use of educational technology for personalised feedback to diverse students has gained traction, the feedback gap still exists: educators wonder which students respond to feedback and which do not. In this study, a set of trackable Call to Action (CTA) links was embedded in two sets of feedback messages focusing on students' time management, with the goal of (1) examining the association between feedback engagement and course success and (2), to predict students' reaction to provided feedback. We also conducted two focus groups to further examine students' perception of provided feedback messages. Our results revealed that early engagement with the feedback was associated with higher chances of succeeding in the course. Likewise, previous engagement with feedback was highly predictive of students' engagement in the future, and also that certain student sub-populations, (e.g., female students), were more likely to engage than others. Such insight enables instructors to ask "why" questions, improve feedback processes and narrow the feedback gap. Practical implications of our findings are further discussed.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {438–447},
numpages = {10},
keywords = {data-driven approaches, feedback, feedback gap, higher education, learning analytics},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375518,
author = {Zhang, Yingbin and Paquette, Luc and Baker, Ryan S. and Ocumpaugh, Jaclyn and Bosch, Nigel and Munshi, Anabil and Biswas, Gautam},
title = {The relationship between confusion and metacognitive strategies in Betty's Brain},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375518},
doi = {10.1145/3375462.3375518},
abstract = {Confusion has been shown to be prevalent during complex learning and has mixed effects on learning. Whether confusion facilitates or hampers learning may depend on whether it is resolved or not. Confusion resolution, behind which is the resolution of cognitive disequilibrium, requires learners to possess some skills, but it is unclear what these skills are. One possibility may be metacognitive strategies (MS), strategies for regulating cognition. This study examined the relationship between confusion and actions related to MS in Betty's Brain, a computer-based learning environment. The results revealed that MS behavior differed during and outside confusion. However, confusion resolution was not related to MS behavior, and MS did not moderate the effect of confusion on learning.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {276–284},
numpages = {9},
keywords = {confusion, confusion resolution, learning analytics, metacognitive strategy},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3448139.3448144,
author = {Yan, Lixiang and Martinez-Maldonado, Roberto and Cordoba, Beatriz Gallo and Deppeler, Joanne and Corrigan, Deborah and Nieto, Gloria Fernandez and Gasevic, Dragan},
title = {Footprints at School: Modelling In-class Social Dynamics from Students’ Physical Positioning Traces},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448144},
doi = {10.1145/3448139.3448144},
abstract = {Schools are increasingly becoming into complex learning spaces where students interact with various physical and digital resources, educators, and peers. Although the field of learning analytics has advanced in analysing logs captured from digital tools, less progress has been made in understanding the social dynamics that unfold in physical learning spaces. Among the various rapidly emerging sensing technologies, position tracking may hold promises to reveal salient aspects of activities in physical learning spaces such as the formation of interpersonal ties among students. This paper explores how granular x-y physical positioning data can be analysed to model social interactions among students and teachers. We conducted an 8-week longitudinal study in which positioning traces of 98 students and six teachers were automatically captured every day in an open-plan public primary school. Positioning traces were analysed using social network analytics (SNA) to extract a set of metrics to characterise students’ positioning behaviours and social ties at cohort and individual levels. Results illustrate how analysing positioning traces through the lens of SNA can enable the identification of certain pedagogical approaches that may be either promoting or discouraging in-class social interaction, and students who may be socially isolated.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {43–54},
numpages = {12},
keywords = {classroom analytics, indoor positioning, proxemics, social networks analysis, social ties},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3576050.3576091,
author = {Kong, ByeongJo and Hemberg, Erik and Bell, Ana and O'Reilly, Una-May},
title = {Investigating Student's Problem-solving Approaches in MOOCs using Natural Language Processing},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576091},
doi = {10.1145/3576050.3576091},
abstract = {Problem-solving approaches are an essential part of learning. Knowing how students approach solving problems can help instructors improve their instructional designs and effectively guide the learning process of students. We propose a natural language processing (NLP) driven method to capture online learners’ problem-solving approaches at scale while using Massive Open Online Courses (MOOCs) as a learning platform. We employ an online survey to gather data, NLP techniques, and existing educational theories to investigate this in the lens of both computer science and education. The paper shows how NLP techniques, i.e. preprocessing, topic modeling, and text summarization, must be tuned to extract information from a large-scale text corpus. The proposed method discovered 18 problem-solving approaches from the text data, such as using pen and paper, peer learning, trial and error, etc. We also observed topics that appear over the years, such as clarifying code logic, watching videos, etc. We observed that students heavily rely on "tools" for solving programming problems and can expect that such selection of methods can vary depending on the type of task.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {262–272},
numpages = {11},
keywords = {MOOCs, problem-solving methods, text summarization, topic modeling},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3506860.3506869,
author = {Li, Chenglu and Xing, Wanli and Leite, Walter L.},
title = {Do Gender and Race Matter? Supporting Help-Seeking with Fair Peer Recommenders in an Online Algebra Learning Platform},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506869},
doi = {10.1145/3506860.3506869},
abstract = {Discussion forums are important for students’ knowledge inquiry in online contexts, with help-seeking being an essential learning strategy in discussion forums. This study aimed to explore innovative methods to build a peer recommender that can provide fair and accurate intelligence to support help-seeking in online learning. Specifically, we have examined existing network embedding models, Node2Vec and FairWalk, to benchmark with the proposed fair network embedding (Fair-NE). A dataset of 187,450 post-reply pairs by 10,182 Algebra I students from 2015 to 2020 was sampled from Algebra Nation, an online algebra learning platform. The dataset was used to train and evaluate the engines of peer recommenders. We evaluated models with representation fairness, predictive accuracy, and predictive fairness. Our findings suggest that constructing fairness-aware models in learning analytics (LA) is crucial to tackling the potential bias in data and to creating trustworthy LA systems.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {432–437},
numpages = {6},
keywords = {discussion forums, fair machine learning, help-seeking, online learning, peer recommenders},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3636555.3636933,
author = {Szymanski, Maxwell and Ooge, Jeroen and De Croon, Robin and Vanden Abeele, Vero and Verbert, Katrien},
title = {Feedback, Control, or Explanations? Supporting Teachers With Steerable Distractor-Generating AI},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636933},
doi = {10.1145/3636555.3636933},
abstract = {Recent advancements in Educational AI have focused on models for automatic question generation. Yet, these advancements face challenges: (1) their "black-box" nature limits transparency, thereby obscuring the decision-making process; and (2) their novelty sometimes causes inaccuracies due to limited feedback systems. Explainable AI (XAI) aims to address the first limitation by clarifying model decisions, while Interactive Machine Learning (IML) emphasises user feedback and model refinement. However, both XAI and IML solutions primarily serve AI experts, often neglecting novices like teachers. Such oversights lead to issues like misaligned expectations and reduced trust. Following the user-centred design method, we collaborated with teachers and ed-tech experts to develop an AI-aided system for generating multiple-choice question distractors, which incorporates feedback, control, and visual explanations. Evaluating these through semi-structured interviews with 12 teachers, we found a strong preference for the feedback feature, enabling teacher-guided AI improvements. Control and explanations’ usefulness was largely dependent on model performance: they were valued when the model performed well. If the model did not perform well, teachers sought context over AI-centric explanations, suggesting a tilt towards data-centric explanations. Based on these results, we propose guidelines for creating tools that enable teachers to steer and interact with question-generating AI models.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {690–700},
numpages = {11},
keywords = {Interactive Machine Learning, XAI, automated question generation, user control, user studies},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/2669711.2669909,
author = {Hern\'{a}ndez-Garc\'{\i}a, \'{A}ngel and Conde, Miguel \'{A}ngel},
title = {Dealing with complexity: educational data and tools for learning analytics},
year = {2014},
isbn = {9781450328968},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2669711.2669909},
doi = {10.1145/2669711.2669909},
abstract = {The evolution of information technologies and their widespread use have caused an increase in complexity of the educational landscape, as institutions and instructors try to absorb and incorporate these innovations to learning processes. This in turn poses new and countless new challenges to educational research in general, and to new disciplines based on educational data analysis such as learning analytics in particular. In this paper, we introduce the Track on Learning Analytics within the Technological Ecosystems for Enhancing Multiculturality 2014 Conference, a track that aims to present new approaches that allow dealing with this complexity and solving some of these challenges.The paper provides an overview of the motivations behind the proposal of this track, with a general introduction to learning analytics in this complex context and a presentation of the main challenges in current learning analytics research, both from a data analysis perspective and a tool analysis approach; this introduction is followed by an insight of the submission management and participants' selection process. Then, a detailed summary of the manuscripts accepted for participation in the conference is presented.},
booktitle = {Proceedings of the Second International Conference on Technological Ecosystems for Enhancing Multiculturality},
pages = {263–268},
numpages = {6},
keywords = {academic analytics, educational data mining, educational technologies, knowledge management, learning analytics, predictive analytics, visual analytics},
location = {Salamanca, Spain},
series = {TEEM '14}
}

@inproceedings{10.1145/3576050.3576090,
author = {Hur, Paul and Machaka, Nessrine and Krist, Christina and Bosch, Nigel},
title = {Informing Expert Feature Engineering through Automated Approaches: Implications for Coding Qualitative Classroom Video Data},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576090},
doi = {10.1145/3576050.3576090},
abstract = {While classroom video data are detailed sources for mining student learning insights, their complex and unstructured nature makes them less than straightforward for researchers to analyze. In this paper, we compared the differences between the processes of expert-informed manual feature engineering and automated feature engineering using positional data for predicting student group interaction in four middle school and high school mathematics classroom videos. Our results highlighted notable differences, including improved model accuracy for the combined (manual features + automated features) models compared to the only-manual-features models (mean AUC = .778 vs. .706) at the cost of feature interpretability, increased number of features for automated feature engineering (1523 vs. 178), and engineering approach (domain-agnostic in automated vs. domain-knowledge-informed in manual). We carried out feature importance analyses and discuss the implications of the results for potentially augmenting human perspectives about qualitatively coding classroom video data by confirming and expanding views on which body areas and characteristics may be relevant to the target interaction behavior. Lastly, we discuss our study’s limitations and future work.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {630–636},
numpages = {7},
keywords = {classroom video data, expert-informed feature engineering, student positional data},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3506860.3506939,
author = {Tsai, Yi-Shan and Singh, Shaveen and Rakovic, Mladen and Lim, Lisa-Angelique and Roychoudhury, Anushka and Gasevic, Dragan},
title = {Charting Design Needs and Strategic Approaches for Academic Analytics Systems through Co-Design},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506939},
doi = {10.1145/3506860.3506939},
abstract = {Academic analytics focuses on collecting, analysing and visualising educational data to generate institutional insights and improve decision-making for academic purposes. However, challenges that arise from navigating a complex organisational structure when introducing analytics systems have called for the need to engage key stakeholders widely to cultivate a shared vision and ensure that implemented systems create desired value. This paper presents a study that takes co-design steps to identify design needs and strategic approaches for the adoption of academic analytics, which serves the purpose of enhancing the measurement of educational quality utilising institutional data. Through semi-structured interviews with 54 educational stakeholders at a large research university, we identified particular interest in measuring student engagement and the performance of courses and programmes. Based on the observed perceptions and concerns regarding data use to measure or evaluate these areas, implications for adoption strategy of academic analytics, such as leadership involvement, communication, and training, are discussed.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {381–391},
numpages = {11},
keywords = {academic analytics, co-design, educational quality, higher education, implementation strategy},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3576050.3576098,
author = {Morris, Wesley and Crossley, Scott and Holmes, Langdon and Trumbore, Anne},
title = {Using Transformer Language Models to Validate Peer-Assigned Essay Scores in Massive Open Online Courses (MOOCs)},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576098},
doi = {10.1145/3576050.3576098},
abstract = {Massive Open Online Courses (MOOCs) such as those offered by Coursera are popular ways for adults to gain important skills, advance their careers, and pursue their interests. Within these courses, students are often required to compose, submit, and peer review written essays, providing a valuable pedagogical experience for the student and a wealth of natural language data for the educational researcher. However, the scores provided by peers do not always reflect the actual quality of the text, generating questions about the reliability and validity of the scores. This study evaluates methods to increase the reliability of MOOC peer-review ratings through a series of validation tests on peer-reviewed essays. Reliability of reviewers was based on correlations between text length and essay quality. Raters were pruned based on score variance and the lexical diversity observed in their comments to create sub-sets of raters. Each subset was then used as training data to finetune distilBERT large language models to automatically score essay quality as a measure of validation. The accuracy of each language model for each subset was evaluated. We find that training language models on data subsets produced by more reliable raters based on a combination of score variance and lexical diversity produce more accurate essay scoring models. The approach developed in this study should allow for enhanced reliability of peer-reviewed scoring in MOOCS affording greater credibility within the systems.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {315–323},
numpages = {9},
keywords = {moocs, natural language processing, rater reliability, transformers},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576094,
author = {Liu, Qianhui and Paquette, Luc},
title = {Using submission log data to investigate novice programmers’ employment of debugging strategies},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576094},
doi = {10.1145/3576050.3576094},
abstract = {Debugging is a distinct subject in programming that is both comprehensive and challenging for novice programmers. However, instructors have limited opportunities to gain insights into the difficulties students encountered in isolated debugging processes. While qualitative studies have identified debugging strategies that novice programmers use and how they relate to theoretical debugging frameworks, limited larger scale quantitative analyses have been conducted to investigate how students’ debugging behaviors observed in log data align with the identified strategies and how they relate to successful debugging. In this study, we used submission log data to understand how the existing debugging strategies are employed by students in an introductory CS course when solving homework problems. We identified strategies from existing debugging literature that can be observed with trace data and extracted features to reveal how efficient debugging is associated with debugging strategy usage. Our findings both align with and contradict past assumptions from previous studies by suggesting that minor code edition can be a beneficial strategy and that width and depth aggregations of the same debugging behavior can reveal opposite effects on debugging efficiency.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {637–643},
numpages = {7},
keywords = {Computer Science education, Debugging strategies, Submission log data},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3448139.3448154,
author = {Karumbaiah, Shamya and Lan, Andrew and Nagpal, Sachit and Baker, Ryan S. and Botelho, Anthony and Heffernan, Neil},
title = {Using Past Data to Warm Start Active Machine Learning: Does Context Matter?},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448154},
doi = {10.1145/3448139.3448154},
abstract = {Despite the abundance of data generated from students’ activities in virtual learning environments, the use of supervised machine learning in learning analytics is limited by the availability of labeled data, which can be difficult to collect for complex educational constructs. In a previous study, a subfield of machine learning called Active Learning (AL) was explored to improve the data labeling efficiency. AL trains a model and uses it, in parallel, to choose the next data sample to get labeled from a human expert. Due to the complexity of educational constructs and data, AL has suffered from the cold-start problem where the model does not have access to sufficient data yet to choose the best next sample to learn from. In this paper, we explore the use of past data to warm start the AL training process. We also critically examine the implications of differing contexts (urbanicity) in which the past data was collected. To this end, we use authentic affect labels collected through human observations in middle school mathematics classrooms to simulate the development of AL-based detectors of engaged concentration. We experiment with two AL methods (uncertainty sampling, L-MMSE) and random sampling for data selection. Our results suggest that using past data to warm start AL training could be effective for some methods based on the target population's urbanicity. We provide recommendations on the data selection method and the quantity of past data to use when warm starting AL training in the urban and suburban schools.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {151–160},
numpages = {10},
keywords = {Affect detection, Model generalization, Urbanicity, Warm start},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3506860.3506898,
author = {Boothe, Maurice and Yu, Collin and Lewis, Armanda and Ochoa, Xavier},
title = {Towards a Pragmatic and Theory-Driven Framework for Multimodal Collaboration Feedback},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506898},
doi = {10.1145/3506860.3506898},
abstract = {This paper proposes an overarching framework for automated collaboration feedback that bridges theory and tool as well as technology and pedagogy. This pragmatic and theory-driven framework guides our thinking by outlining the components involved in converting theoretical collaboration constructs into features that can be automatically extracted and then converted into actionable feedback. Focusing on the pedagogical components of the framework, the constructs are validated by mapping them onto a selection of multi-disciplinary collaboration frameworks. The resulting behavioral indicators are then applied to measure collaboration in a sample scenario and those measurements are then used to exemplify how feedback analytics could be calculated. The paper concludes with a discussion on how those analytics could be converted into feedback for students and the next steps needed to advance the technological part of the framework.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {507–513},
numpages = {7},
keywords = {collaboration analytics, learning collaboration, teaching collaboration},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3576050.3576149,
author = {Cock, Jade Mai and Bilal, Muhammad and Davis, Richard and Marras, Mirko and Kaser, Tanja},
title = {Protected Attributes Tell Us Who, Behavior Tells Us How: A Comparison of Demographic and Behavioral Oversampling for Fair Student Success Modeling},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576149},
doi = {10.1145/3576050.3576149},
abstract = {Algorithms deployed in education can shape the learning experience and success of a student. It is therefore important to understand whether and how such algorithms might create inequalities or amplify existing biases. In this paper, we analyze the fairness of models which use behavioral data to identify at-risk students and suggest two novel pre-processing approaches for bias mitigation. Based on the concept of intersectionality, the first approach involves intelligent oversampling on combinations of demographic attributes. The second approach does not require any knowledge of demographic attributes and is based on the assumption that such attributes are a (noisy) proxy for student behavior. We hence propose to directly oversample different types of behaviors identified in a cluster analysis. We evaluate our approaches on data from (i) an open-ended learning environment and (ii) a flipped classroom course. Our results show that both approaches can mitigate model bias. Directly oversampling on behavior is a valuable alternative, when demographic metadata is not available. Source code and extended results are provided in https://github.com/epfl-ml4ed/behavioral-oversampling.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {488–498},
numpages = {11},
keywords = {Behavioral data, Bias, Fairness, Machine Learning, Oversampling, Student success},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576072,
author = {Han, Songhee and Ji, Hyangeun and Jiang, Zilu and West, Michael and Liu, Min},
title = {What do students want to know while taking massive open online courses? Examining massive open online course students’ needs based on online forum discussions from the Universal Design for Learning approach},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576072},
doi = {10.1145/3576050.3576072},
abstract = {We identified the nine most dominant massive open online course (MOOC) students’ needs by topic modeling and qualitative analysis of forum discussion posts (n = 3645) among students, staff, and instructors from 21 courses. We examined the implications of these needs using three main Universal Design for Learning (UDL) principles (representation, action and expression, and engagement). We then offered suggestions for what course providers can do to promote an equitable learning experience for MOOC students. The three suggestions are as follows: (1) providing tools such as a direct messaging application to encourage students’ socializing behaviors, (2) modifying course activities to promote more hands-on projects and sharing them, and (3) implementing a bidirectional channel, such as a natural language processing-based chatbot so that students can access useful information whenever they feel the need. We argue that it is critical to include minority students’ voices when examining needs in courses, and our methodology reflects this purpose. We also discuss how the UDL approach helped us recognize students’ needs, create more accessible MOOC learning experiences, and explore future research directions.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {572–578},
numpages = {7},
keywords = {Massive open online courses, Natural language processing, Universal Design for Learning},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576095,
author = {Nazeri, Sina and Hatala, Marek and Salehian Kia, Fatemeh},
title = {When to Intervene? Utilizing Two Facets of Temporality in Students’ SRL Processes in a Programming Course},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576095},
doi = {10.1145/3576050.3576095},
abstract = {This study explored two aspects of temporality in students’ SRL behaviours to understand the dynamics of SRL phase transitions. In the first aspect, which refers to the temporal order of Self-regulated learning (SRL) phases, we characterized four types of SRL processes based on phase transitions and the cyclical nature of SRL. The SRL types were mapped into the kinds of iterative behaviours over SRL phases which correspond to the theorized self-regulatory behaviours of students at different levels of SRL skills. We found a significant association between SRL types and the assignment grades that suggests the higher achieved learning outcomes, i.e., programming skills demonstrated in the assignments, being associated with more advanced SRL processes. This study also focused on the second aspect of temporality, which refers to the instance of time. We revealed the temporal dynamics between SRL phase transitions by analyzing time profiles for transitions in each SRL process type. Next, we showed that a two-day interval is a threshold by which most students iteratively transition from adapting to enactment phases, which provides a suitable time to intervene if the transition is not observed.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {293–302},
numpages = {10},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3636555.3636908,
author = {Gurung, Ashish and Vanacore, Kirk and Mcreynolds, Andrew A. and Ostrow, Korinn S. and Worden, Eamon and Sales, Adam C. and Heffernan, Neil T.},
title = {Multiple Choice vs. Fill-In Problems: The Trade-off Between Scalability and Learning},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636908},
doi = {10.1145/3636555.3636908},
abstract = {Learning experience designers consistently balance the trade-off between open and close-ended activities. The growth and scalability of Computer Based Learning Platforms (CBLPs) have only magnified the importance of these design trade-offs. CBLPs often utilize close-ended activities (i.e. Multiple-Choice Questions [MCQs]) due to feasibility constraints associated with the use of open-ended activities. MCQs offer certain affordances, such as immediate grading and the use of distractors, setting them apart from open-ended activities. Our current study examines the effectiveness of Fill-In problems as an alternative to MCQs for middle school mathematics. We report on a randomized study conducted from 2017 to 2022, with a total of 6,768 students from middle schools across the US. We observe that, on average, Fill-In problems lead to better post-test performance than MCQs; albeit deeper explorations indicate differences between the two design paradigms to be more nuanced. We find evidence that students with higher math knowledge benefit more from Fill-In problems than those with lower math knowledge.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {507–517},
numpages = {11},
keywords = {Causal Inference, Fill-In Problems, Learning Experience Design, Learning Outcomes, Multiple Choice Questions},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3375462.3375498,
author = {Sharma, Kshitij and Papamitsiou, Zacharoula and Olsen, Jennifer K. and Giannakos, Michail},
title = {Predicting learners' effortful behaviour in adaptive assessment using multimodal data},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375498},
doi = {10.1145/3375462.3375498},
abstract = {Many factors influence learners' performance on an activity beyond the knowledge required. Learners' on-task effort has been acknowledged for strongly relating to their educational outcomes, reflecting how actively they are engaged in that activity. However, effort is not directly observable. Multimodal data can provide additional insights into the learning processes and may allow for effort estimation. This paper presents an approach for the classification of effort in an adaptive assessment context. Specifically, the behaviour of 32 students was captured during an adaptive self-assessment activity, using logs and physiological data (i.e., eye-tracking, EEG, wristband and facial expressions). We applied k-means to the multimodal data to cluster students' behavioural patterns. Next, we predicted students' effort to complete the upcoming task, based on the discovered behavioural patterns using a combination of Hidden Markov Models (HMMs) and the Viterbi algorithm. We also compared the results with other state-of-the-art classification algorithms (SVM, Random Forest). Our findings provide evidence that HMMs can encode the relationship between effort and behaviour (captured by the multimodal data) in a more efficient way than the other methods. Foremost, a practical implication of the approach is that the derived HMMs also pinpoint the moments to provide preventive/prescriptive feedback to the learners in real-time, by building-upon the relationship between behavioural patterns and the effort the learners are putting in.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {480–489},
numpages = {10},
keywords = {adaptive assessment, effort classification, hidden Markov models, multimodal learning analytics},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3506860.3506893,
author = {Chen, Bodong and Zhu, Xinran and Shui, Hong},
title = {Socio-Semantic Network Motifs Framework for Discourse Analysis},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506893},
doi = {10.1145/3506860.3506893},
abstract = {Effective collaborative discourse requires both cognitive and social engagement of students. To investigate complex socio-cognitive dynamics in collaborative discourse, this paper proposes to model collaborative discourse as a socio-semantic network (SSN) and then use network motifs – defined as recurring, significant subgraphs – to characterize the network and hence the discourse. To demonstrate the utility of our SSN motifs framework, we applied it to a sample dataset. While more work needs to be done, the SSN motifs framework shows promise as a novel, theoretically informed approach to discourse analysis.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {500–506},
numpages = {7},
keywords = {collaboration, discourse, networks, two-mode networks},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3576050.3576068,
author = {Hui, Bowen},
title = {Are They Learning or Guessing? Investigating Trial-and-Error Behavior with Limited Test Attempts},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576068},
doi = {10.1145/3576050.3576068},
abstract = {Mastery learning and deliberate practice promote personalized learning, allowing the learner to improve through a repetitive and targeted approach. Unfortunately, this pedagogy is challenging to implement in classrooms where everyone is expected to learn at the same pace. Various studies have successfully demonstrated that certain aspects of mastery learning can be integrated into the curriculum. We adopt a similar pedagogical strategy and explain our implementation approach with online assessments. Since this is a new pedagogical approach to assessing student learning, we collected data to investigate test-taking behavior and evaluated potential learning gains in this new test format. As part of this endeavor, we developed a model to detect trial-and-error sequences in test attempts. Our results point to a small percentage of guessing behavior, which is encouraging evidence supporting this test approach is a viable way to implement mastery learning in our curriculum.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {133–144},
numpages = {12},
keywords = {Assessment, guessing behavior, mastery learning, trial-and-error pattern},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3506860.3506868,
author = {Sloan-Lynch, Jay and Gay, Nathanael and Watkins, Robert},
title = {Too Fast for Their Own Good: Analyzing a Decade of Student Exercise Responses to Explore the Impact of Math Solving Photo Apps},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506868},
doi = {10.1145/3506860.3506868},
abstract = {The introduction of math solving photo apps in late 2014 presented students with a tempting new way to solve math problems quickly and accurately. Despite widespread acknowledgement that students increasingly use these apps to complete their coursework, as well as growing concerns about cheating as more students learn online, the prevalence and impact of this technology remains largely unexplored. This study uses a large dataset consisting of 700 unique math exercises and over 82 million student submissions to investigate changes in exercise answering speeds during the last decade. Through a series of exploratory analyses, we identify dramatic shifts in exercise submission speed distributions in recent years, with increasing numbers of rapid responses suggesting growing student reliance on math solving photo technology to answer math problems on homework and exams. Our analyses also reveal that decreases in exercise answering speeds have occurred contemporaneously with the introduction and proliferation of math solving photo apps in education and we further substantiate the role of these tools by verifying that exercise susceptibility to math solving photo apps is associated with decreases in submission speeds. We discuss potential applications of our findings to improve math assessment design and support students in adopting better learning strategies.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {67–76},
numpages = {10},
keywords = {Photomath, academic integrity, math solving photo apps, response times},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3506860.3506911,
author = {Lahza, Hatim and Khosravi, Hassan and Demartini, Gianluca and Gasevic, Dragan},
title = {Effects of Technological Interventions for Self-regulation: A Control Experiment in Learnersourcing},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506911},
doi = {10.1145/3506860.3506911},
abstract = {The benefits of incorporating scaffolds that promote strategies of self-regulated learning (SRL) to help student learning are widely studied and recognised in the literature. However, the best methods for incorporating them in educational technologies and empirical evidence about which scaffolds are most beneficial to students are still emerging. In this paper, we report our findings from conducting an in-the-field controlled experiment with 797 post-secondary students to evaluate the impact of incorporating scaffolds for promoting SRL strategies in the context of assisting students in creating novel content, also known as learnersourcing. The experiment had five conditions, including a control group that had access to none of the scaffolding strategies for creating content, three groups each having access to one of the scaffolding strategies (planning, externally-facilitated monitoring and self-assessing) and a group with access to all of the aforementioned scaffolds. The results revealed that the addition of the scaffolds for SRL strategies increased the complexity and effort required for creating content, were not positively assessed by learners and led to slight improvements in the quality of the generated content. We discuss the implications of our findings for incorporating SRL strategies in educational technologies.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {542–548},
numpages = {7},
keywords = {learnersourcing, metacognition, self-regulation, software-based scaffolding},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3506860.3506864,
author = {Nguyen, Ha and Young, William},
title = {Knowledge Construction and Uncertainty in Real World Argumentation: A Text Analysis Approach},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506864},
doi = {10.1145/3506860.3506864},
abstract = {Collaborative argumentation is key to promoting understanding of scientific issues. However, classroom structures may not always prepare students to engage in argumentation. To address this challenge, education researchers have examined the importance of social knowledge construction and managing uncertainty in group understanding. In this study, we explore these processes using data from /r/ChangeMyView, an online forum on Reddit where users present their opinions, engage others in critiquing ideas, and acknowledge when the discussion has modified their opinions. This unfacilitated environment can illuminate how argumentation evolves naturally towards refined opinions. We employ automated text analyses (LIWC) and discourse analyses to understand the features and discourse sequences of successful arguments. We find that argumentative threads are more likely to be successful if they focus on idea articulation, coherence, and semantic diversity. Findings highlight the role of uncertainty: threads with more certainty words are less likely to be successful. Furthermore, successful arguments are characterized by cycles of raising, managing, and reducing uncertainty, with more occurrences of evidence and idea incorporation. We discuss how learning environments can create norms for idea construction, coherence, and uncertainty, and the potential to provide adaptive prompts to maintain and reduce uncertainty when unproductive argumentative sequences are detected.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {34–44},
numpages = {11},
keywords = {argumentation, online communities, text analysis, uncertainty},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3027385.3029433,
author = {Vigentini, Lorenzo and Le\'{o}n Urrutia, Manuel and Fields, Ben},
title = {FutureLearn data: what we currently have, what we are learning and how it is demonstrating learning in MOOCs},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029433},
doi = {10.1145/3027385.3029433},
abstract = {Compared to other platforms such as Coursera and EdX, FutureLearn is a relatively new player in the MOOC arena and received limited coverage in the Learning Analytics and Educational Data Mining research. Founded by a partnership between the Open University in the UK, the BBC, The British Library and (originally) 12 universities in the UK, FutureLearn has two distinctive features relevant to the way their data is displayed and analyzed: 1) it was designed with a specific educational philosophy in mind which focuses on the social dimension of learning and 2) every learning activity provide opportunities for formal discussion and commenting. This workshop provides an opportunity to invite contributions and connect individual and groups to share their research activities on an international stage. As the first of its kind, this workshop will bring in a number of scholars and practitioners, as well as data scientists and analyst involved in the reporting, researching and developments emerging from the data offered by the platform.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {512–513},
numpages = {2},
keywords = {MOOCs, learning analytics, visualization dashboard},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3506860.3506882,
author = {Takami, Kyosuke and Dai, Yiling and Flanagan, Brendan and Ogata, Hiroaki},
title = {Educational Explainable Recommender Usage and its Effectiveness in High School Summer Vacation Assignment},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506882},
doi = {10.1145/3506860.3506882},
abstract = {Explainable recommendations, which provide explanations about why an item is recommended, help to improve the transparency, persuasiveness, and trustworthiness. However, few research in educational technology utilize explainable recommendations. We developed an explanation generator using the parameters from Bayesian knowledge tracing models. We used this educational explainable recommendation system to investigate the effects of explanation on the summer vacation assignment for high school students. Comparing the click counts of recommended quizzes with and without explanations, we found that the number of clicks was significantly higher for quizzes with explanations. Furthermore, system usage pattern mining revealed that students can be divided to three clusters— none, steady and late users. In the cluster of steady users, recommended quizzes with explanations were continuously used. These results suggest the effectiveness of an explainable recommendation system in the field of education.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {458–464},
numpages = {7},
keywords = {A/B test, Effectiveness of explanation, Explainable recommendation, Long vacation period, Pattern mining},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3027385.3027409,
author = {Nguyen, Quan and Rienties, Bart and Toetenel, Lisette},
title = {Unravelling the dynamics of instructional practice: a longitudinal study on learning design and VLE activities},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027409},
doi = {10.1145/3027385.3027409},
abstract = {Substantial progress has been made in understanding how teachers design for learning. However, there remains a paucity of evidence of the actual students' response towards leaning designs. Learning analytics has the power to provide just-in-time support, especially when predictive analytics is married with the way teachers have designed their course, or so-called a learning design. This study investigates how learning designs are configured over time and their impact on student activities by analyzing longitudinal data of 38 modules with a total of 43,099 registered students over 30 weeks at the Open University UK, using social network analysis and panel data analysis. Our analysis unpacked dynamic configurations of learning designs between modules over time, which allows teachers to reflect on their practice in order to anticipate problems and make informed interventions. Furthermore, by controlling for the heterogeneity between modules, our results indicated that learning designs were able to explain up to 60% of the variability in student online activities, which reinforced the importance of pedagogical context in learning analytics.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {168–177},
numpages = {10},
keywords = {learning analytics, learning design, longitudinal, panel data analysis, social network analysis},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3576050.3576097,
author = {Barboza, Luiz and Mello, Rafael and Modell, Micah and Teixeira, Erico Souza},
title = {Blockly-DS: Blocks Programming for Data Science with Visual, Statistical, Descriptive and Predictive Analysis},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576097},
doi = {10.1145/3576050.3576097},
abstract = {Interest in data science has been growing across industries - both STEM and non-STEM. Non-STEM students often have difficulties with programming and data analysis tools. These entry barriers can be minimized, and these concepts can be easily absorbed when using visual tools. Thus, for this specific audience, the use of visual tools has been essential for teaching data science. Several of these tools are available, but they all have limitations. This work presents Blockly-DS: a new tool capable of assisting in teaching data science to a non-STEM audience. The Blockly-DS tool is being tested in two Brazilian higher education institutions, one, IBMEC, a business undergraduate university, and the other, FIAP, a STEM school that offers an MBA as well as corporate and undergraduate courses. The preliminary results presented in this article refers to a validation with two groups of training sessions for junior financial analysts of a major Brazilian bank in partnership with FIAP.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {644–649},
numpages = {6},
keywords = {block-based programming, data science, visual programming tool},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576099,
author = {Rajarathinam, Robin Jephthah and D'Angelo, Cynthia M.},
title = {Turn-taking analysis of small group collaboration in an engineering discussion classroom},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576099},
doi = {10.1145/3576050.3576099},
abstract = {This preliminary study focuses on using voice activity detection (VAD) algorithms to extract turn information of small group work detected from recorded individual audio stream data from undergraduate engineering discussion sections. Video data along with audio were manually coded for collaborative behavior of students and teacher-student interaction. We found that individual audio data can be used to obtain features that can describe group work in noisy classrooms. We observed patterns in student turn taking and talk duration during various sections of the classroom which matched with the video coded data. Results show that high quality individual audio data can be effective in describing collaborative processes that occurs in the classroom. Future directions on using prosodic features and implications on how we can conceptualize collaborative group work using audio data are discussed.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {650–656},
numpages = {7},
keywords = {audio analysis, collaborative problem solving, discussion patterns, voice activity detection},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3636555.3636876,
author = {Sch\"{u}tt, Anan and Huber, Tobias and Nasir, Jauwairia and Conati, Cristina and Andr\'{e}, Elisabeth},
title = {Does Difficulty even Matter? Investigating Difficulty Adjustment and Practice Behavior in an Open-Ended Learning Task},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636876},
doi = {10.1145/3636555.3636876},
abstract = {Difficulty adjustment in practice exercises has been shown to be beneficial for learning. However, previous research has mostly investigated close-ended tasks, which do not offer the students multiple ways to reach a valid solution. Contrary to this, in order to learn in an open-ended learning task, students need to effectively explore the solution space as there are multiple ways to reach a solution. For this reason, the effects of difficulty adjustment could be different for open-ended tasks. To investigate this, as our first contribution, we compare different methods of difficulty adjustment in a user study conducted with 86 participants. Furthermore, as the practice behavior of the students is expected to influence how well the students learn, we additionally look at their practice behavior as a post-hoc analysis. Therefore, as a second contribution, we identify different types of practice behavior and how they link to students’ learning outcomes and subjective evaluation measures as well as explore the influence the difficulty adjustment methods have on the practice behaviors. Our results suggest the usefulness of taking into account the practice behavior in addition to only using the practice performance to inform adaptive intervention and difficulty adjustment methods.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {253–262},
numpages = {10},
keywords = {Adaptive Practice, Clustering, Difficulty Adjustment, Educational Data Mining},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3506860.3506908,
author = {Iqbal, Sehrish and Swiecki, Zachari and Joksimovic, Srecko and Ferreira Mello, Rafael and Aljohani, Naif and Ul Hassan, Saeed and Gasevic, Dragan},
title = {Uncovering Associations Between Cognitive Presence and Speech Acts: A Network-Based Approach},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506908},
doi = {10.1145/3506860.3506908},
abstract = {This research aimed to explore the relationship between different indicators of the depth and quality of participation in computer-mediated learning environments. By using network analyses and statistical tests, we discovered significant associations between the cognitive presence phases of the Community of Inquiry framework and speech acts, and examined the impact of two different instructional interventions on these associations. We found that there are strong associations between some speech acts and cognitive presence phases. In addition, the study revealed that the association between speech acts and cognitive presence is moderated by external facilitation, but not affected by user role assignment. The results suggest that speech acts can plausibly be used to provide feedback in relation to cognitive presence and can potentially be used to increase the generalizability of cognitive presence classification.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {315–325},
numpages = {11},
keywords = {cognitive presence, community of inquiry, discourse analysis, discussion forums, epistemic network analysis, speech acts},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3576050.3576053,
author = {Watts, Field M. and Dood, Amber J. and Shultz, Ginger V.},
title = {Automated, content-focused feedback for a writing-to-learn assignment in an undergraduate organic chemistry course},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576053},
doi = {10.1145/3576050.3576053},
abstract = {Writing-to-learn (WTL) pedagogy supports the implementation of writing assignments in STEM courses to engage students in conceptual learning. Recent studies in the undergraduate STEM context demonstrate the value of implementing WTL, with findings that WTL can support meaningful learning and elicit students’ reasoning. However, the need for instructors to provide feedback on students’ writing poses a significant barrier to implementing WTL; this barrier is especially notable in the context of introductory organic chemistry courses at large universities, which often have large enrollments. This work describes one approach to overcome this barrier by presenting the development of an automated feedback tool for providing students with formative feedback on their responses to an organic chemistry WTL assignment. This approach leverages machine learning models to identify features of students’ mechanistic reasoning in response to WTL assignments in a second-semester, introductory organic chemistry laboratory course. The automated feedback tool development was guided by a framework for designing automated feedback, theories of self-regulated learning, and the components of effective WTL pedagogy. Herein, we describe the design of the automated feedback tool and report our initial evaluation of the tool through pilot interviews with organic chemistry students.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {531–537},
numpages = {7},
keywords = {automated feedback, self-regulated learning, writing-to-learn},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576062,
author = {Ma, Boxuan and Hettiarachchi, Gayan Prasad and Fukui, Sora and Ando, Yuji},
title = {Each Encounter Counts: Modeling Language Learning and Forgetting},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576062},
doi = {10.1145/3576050.3576062},
abstract = {Language learning applications usually estimate the learner’s language knowledge over time to provide personalized practice content for each learner at the optimal timing. However, accurately predicting language knowledge or linguistic skills is much more challenging than math or science knowledge, as many language tasks involve memorization and retrieval. Learners must memorize a large number of words and meanings, which are prone to be forgotten without practice. Although a few studies consider forgetting when modeling learners’ language knowledge, they tend to apply traditional models, consider only partial information about forgetting, and ignore linguistic features that may significantly influence learning and forgetting. This paper focuses on modeling and predicting learners’ knowledge by considering their forgetting behavior and linguistic features in language learning. Specifically, we first explore the existence of forgetting behavior and cross-effects in real-world language learning datasets through empirical studies. Based on these, we propose a model for predicting the probability of recalling a word given a learner’s practice history. The model incorporates key information related to forgetting, question formats, and semantic similarities between words using the attention mechanism. Experiments on two real-world datasets show that the proposed model improves performance compared to baselines. Moreover, the results indicate that combining multiple types of forgetting information and item format improves performance. In addition, we find that incorporating semantic features, such as word embeddings, to model similarities between words in a learner’s practice history and their effects on memory also improves the model.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {79–88},
numpages = {10},
keywords = {Educational data mining, Forgetting behavior, Knowledge tracing, Language learning},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3506860.3506902,
author = {Vasquez Verdugo, Jonathan and Gitiaux, Xavier and Ortega, Cesar and Rangwala, Huzefa},
title = {FairEd: A Systematic Fairness Analysis Approach Applied in a Higher Educational Context},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506902},
doi = {10.1145/3506860.3506902},
abstract = {Higher education institutions increasingly rely on machine learning models. However, a growing body of evidence shows that these algorithms may not serve underprivileged communities well and at times discriminate against them. This is all the more concerning in education as negative outcomes have long-term implications. We propose a systematic process for framing, detecting, documenting, and reporting unfairness risks. The systematic approach’s outcomes are merged into a framework named FairEd, which would help decision-makers to understand unfairness risks along the environmental and analytical fairness dimension. The tool allows to decide (i) whether the dataset contains risks of unfairness; (ii) how the models could perform along many fairness dimensions; (iii) whether potentially unfair outcomes can be mitigated without degrading performance. The systematic approach is applied to a Chilean University case study, where a predicting student dropout model is aimed to build. First, we capture the nuances of the Chilean context where unfairness emerges along income lines and demographic groups. Second, we highlight the benefit of reporting unfairness risks along a diverse set of metrics to shed light on potential discrimination. Third, we find that measuring the cost of fairness is an important quantity to report on when doing the model selection.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {271–281},
numpages = {11},
keywords = {algorithmic fairness, educational data mining, student dropout},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3448139.3448192,
author = {Jang, JiWoong and Lee, Jaewook and Echeverria, Vanessa and Lawrence, LuEttaMae and Aleven, Vincent},
title = {Explorations of Designing Spatial Classroom Analytics with Virtual Prototyping},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448192},
doi = {10.1145/3448139.3448192},
abstract = {Despite the potential of spatial displays for supporting teachers’ classroom orchestration through real-time classroom analytics, the process to design these displays is a challenging and under-explored topic in the learning analytics (LA) community. This paper proposes a mid-fidelity Virtual Prototyping method (VPM), which involves simulating a classroom environment and candidate designs in virtual space to address these challenges. VPM allows for rapid prototyping of spatial features, requires no specialized hardware, and enables teams to conduct remote evaluation sessions. We report observations and findings from an initial exploration with five potential users through a design process utilizing VPM to validate designs for an AR-based spatial display in the context of middle-school orchestration tools. We found that designs created using virtual prototyping sufficiently conveyed a sense of three-dimensionality to address subtle design issues like occlusion and depth perception. We discuss the opportunities and limitations of applying virtual prototyping, particularly its potential to allow for more robust co-design with stakeholders earlier in the design process.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {518–524},
numpages = {7},
keywords = {augmented reality, classroom analytics, mixed reality, spatial classroom displays, virtual prototyping},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3303772.3303790,
author = {Sher, Varshita and Hatala, Marek and Ga\v{s}evi\'{c}, Dragan},
title = {On multi-device use: Using technological modality profiles to explain differences in students' learning},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303790},
doi = {10.1145/3303772.3303790},
abstract = {With increasing abundance and ubiquity of mobile phones, desktop PCs, and tablets in the last decade, we are seeing students intermixing these modalities to learn and regulate their learning. However, the role of these modalities in educational settings is still largely under-researched. Similarly, little attention has been paid to the research on the extension of learning analytics to analyze the learning processes of students adopting various modalities during a learning activity. Traditionally, research on how modalities affect the way in which activities are completed has mainly relied upon self-reported data or mere counts of access from each modality. We explore the use of technological modalities in regulating learning via learning management systems (LMS) in the context of blended courses. We used data mining techniques to analyze patterns in sequences of actions performed by learners (n = 120) across different modalities in order to identify technological modality profiles of sequences. These profiles were used to detect the technological modality strategies adopted by students. We found a moderate effect size (∈2 = 0.12) of students' adopted strategies on the final course grade. Furthermore, when looking specifically at online discussion engagement and performance, students' adopted technological modality strategies explained a large amount of variance (η2 = 0.68) in their engagement and quality of contributions. The result implications and further research are discussed.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {1–10},
numpages = {10},
keywords = {Blended learning, Learning analytics, Mobile Learning, Multi-device use, Online discussions, Trace Analysis},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3506860.3506887,
author = {Zhang, Tom and Taub, Michelle and Chen, Zhongzhou},
title = {A Multi-Level Trace Clustering Analysis Scheme for Measuring Students’ Self-Regulated Learning Behavior in a Mastery-Based Online Learning Environment},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506887},
doi = {10.1145/3506860.3506887},
abstract = {This study introduces a new analysis scheme to analyze trace data and visualize students’ self-regulated learning strategies in a mastery-based online learning modules platform. The pedagogical design of the platform resulted in fewer event types and less variability in student trace data. The current analysis scheme overcomes those challenges by conducting three levels of clustering analysis. On the event level, mixture-model fitting is employed to distinguish between abnormally short and normal assessment attempts and study events. On the module level, trace level clustering is performed with three different methods for generating distance metrics between traces, with the best performing output used in the next step. On the sequence level, trace level clustering is performed on top of module-level clusters to reveal students’ change of learning strategy over time. We demonstrated that distance metrics generated based on learning theory produced better clustering results than pure data-driven or hybrid methods. The analysis showed that most students started the semester with productive learning strategies, but a significant fraction shifted to a multitude of less productive strategies in response to increasing content difficulty and stress. The observations could prompt instructors to rethink conventional course structure and implement interventions to improve self-regulation at optimal times.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {197–207},
numpages = {11},
keywords = {Click-stream data, Online learning environments, Self-regulated learning},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/2883851.2883911,
author = {Khan, Imran and Pardo, Abelardo},
title = {Data2U: scalable real time student feedback in active learning environments},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883911},
doi = {10.1145/2883851.2883911},
abstract = {The majority of applications and products that use learning analytics to understand and improve learning experiences assume the creation of actionable items that will affect students through an intermediary. Much less focus is devoted to exploring how to provide insight directly to students. Furthermore, student engagement has always been a relevant aspect to increase the quality of a learning experience. Learning analytics techniques can be used to provide real-time insight tightly integrated with the learning outcomes directly to the students. This paper describes a case study deployed in a first year engineering course using a flipped learning strategy to explore the behavior of students interacting with a dashboard updated in real time providing indicators of their engagement with the course activities. The results show different patterns of use and their evolution throughout the experience and shed some light on how students perceived this resource.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {249–253},
numpages = {5},
keywords = {dashboard, feedback, learning analytics, visualizations},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/3170358.3170398,
author = {Nguyen, Quan and Huptych, Michal and Rienties, Bart},
title = {Linking students' timing of engagement to learning design and academic performance},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170398},
doi = {10.1145/3170358.3170398},
abstract = {In recent years, the connection between Learning Design (LD) and Learning Analytics (LA) has been emphasized by many scholars as it could enhance our interpretation of LA findings and translate them to meaningful interventions. Together with numerous conceptual studies, a gradual accumulation of empirical evidence has indicated a strong connection between how instructors design for learning and student behaviour. Nonetheless, students' timing of engagement and its relation to LD and academic performance have received limited attention. Therefore, this study investigates to what extent students' timing of engagement aligned with instructor learning design, and how engagement varied across different levels of performance. The analysis was conducted over 28 weeks using trace data, on 387 students, and replicated over two semesters in 2015 and 2016. Our findings revealed a mismatch between how instructors designed for learning and how students studied in reality. In most weeks, students spent less time studying the assigned materials on the VLE compared to the number of hours recommended by instructors. The timing of engagement also varied, from in advance to catching up patterns. High-performing students spent more time studying in advance, while low-performing students spent a higher proportion of their time on catching-up activities. This study reinforced the importance of pedagogical context to transform analytics into actionable insights.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {141–150},
numpages = {10},
keywords = {engagement, higher education, learning analytics, learning design, temporal, virtual learning environment},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3506860.3506901,
author = {Krumm, Andrew and Coulson, Andrew and Neisler, Julie},
title = {Defining Productive Struggle in ST Math: Implications for Developing Indicators of Learning Behaviors and Strategies in Digital Learning Environments},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506901},
doi = {10.1145/3506860.3506901},
abstract = {This paper describes a process for operationally defining productive struggle in a widely used digital learning environment called ST Math. The process for developing an operational definition involved examining the existing literature for ways in which researchers have previously quantified productive struggle in digital learning environments. Using prior research, we defined productive struggle as a student persisting in a digital learning task while maintaining a likelihood of future success. To develop a machine-executable definition of productive struggle, we identified the typical number of attempts learners needed to complete a level in ST Math and applied a modified Performance Factors Analysis algorithm to estimate learners’ probability of success on a subsequent puzzle attempt within a level. Using definitions that differentially combined re-attempts and predicted probabilities, we examined the proportion of level attempts that could be newly classified as instances of productive struggle. The pragmatic approach described in this paper is intended to serve as an example for other digital learning environments seeking to develop indicators of productive struggle.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {514–520},
numpages = {7},
keywords = {Performance Factors Analysis, Productive struggle, predictive modeling},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3027385.3027392,
author = {Arnold, Kimberly E. and Sclater, Niall},
title = {Student perceptions of their privacy in leaning analytics applications},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027392},
doi = {10.1145/3027385.3027392},
abstract = {Over the past five years, ethics and privacy around student data have become major topics of conversation in the learning analytics field. However, the majority of these have been theoretical in nature. The authors of this paper posit that more direct student engagement needs to be undertaken, and initial data from institutions beginning this process is shared. We find that, while the majority of respondents are accepting of the use of their data by their institutions, approval varies depending on the proposed purpose of the analytics. There also appear to be notable variations between students enrolled at United Kingdom and American institutions.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {66–69},
numpages = {4},
keywords = {ethics, higher education, learning analytics, privacy},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3375462.3375477,
author = {Cavalcanti, Anderson Pinheiro and Diego, Arthur and Mello, Rafael Ferreira and Mangaroska, Katerina and Nascimento, Andr\'{e} and Freitas, Fred and Ga\v{s}evi\'{c}, Dragan},
title = {How good is my feedback? a content analysis of written feedback},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375477},
doi = {10.1145/3375462.3375477},
abstract = {Feedback is a crucial element in helping students identify gaps and assess their learning progress. In online courses, feedback becomes even more critical as it is one of the resources where the teacher interacts directly with the student. However, with the growing number of students enrolled in online learning, it becomes a challenge for instructors to provide good quality feedback that helps the student self-regulate. In this context, this paper proposed a content analysis of feedback text provided by instructors based on different indicators of good feedback. A random forest classifier was trained and evaluated at different feedback levels. The results achieved outcomes up to 87% and 0.39 of accuracy and Cohen's κ, respectively. The paper also provides insights into the most influential textual features of feedback that predict feedback quality.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {428–437},
numpages = {10},
keywords = {content analysis, feedback, learning analytics, online learning},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3506860.3506917,
author = {Tavakoli, Mohammadreza and Faraji, Abdolali and Molavi, Mohammadreza and T. Mol, Stefan and Kismih\'{o}k, G\'{a}bor},
title = {Hybrid Human-AI Curriculum Development for Personalised Informal Learning Environments},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506917},
doi = {10.1145/3506860.3506917},
abstract = {Informal learning procedures have been changing extremely fast over the recent decades not only due to the advent of online learning, but also due to changes in what humans need to learn to meet their various life and career goals. Consequently, online, educational platforms are expected to provide personalized, up-to-date curricula to assist learners. Therefore, in this paper, we propose an Artificial Intelligence (AI) and Crowdsourcing based approach to create and update curricula for individual learners. We show the design of this curriculum development system prototype, in which contributors receive AI-based recommendations to be able to define and update high-level learning goals, skills, and learning topics together with associated learning content. This curriculum development system was also integrated into our personalized online learning platform. To evaluate our prototype we compared experts’ opinion with our system’s recommendations, and resulted in 89%, 79%, and 93% F1-scores when recommending skills, learning topics, and educational materials respectively. Also, we interviewed eight senior level experts from educational institutions and career consulting organizations. Interviewees agreed that our curriculum development method has high potential to support authoring activities in dynamic, personalized learning environments.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {563–569},
numpages = {7},
keywords = {Artificial Intelligence, Crowdsourcing, Curriculum Development, Informal Learning},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3506860.3506903,
author = {Benedict, Aileen and Al-Hossami, Erfan and Dorodchi, Mohsen and Benedict, Alexandria and Wiktor, Sandra},
title = {Pilot Recommender System Enabling Students to Indirectly Help Each Other and Foster Belonging Through Reflections},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506903},
doi = {10.1145/3506860.3506903},
abstract = {Without a sense of belonging, students may become disheartened and give up when faced with new challenges. Moreover, with the sudden growth of remote learning due to COVID-19, it may be even more difficult for students to feel connected to the course and peers in isolation. Therefore, we propose a recommendation system to build connections between students while recommending solutions to challenges. This pilot system utilizes students’ reflections from previous semesters, asking about learning challenges and potential solutions. It then generates sentence embeddings and calculates cosine similarities between the challenges of current and prior students. The possible solutions given by previous students are then recommended to present students with similar challenges. Self-reflection encourages students to think deeply about their learning experiences and benefit both learners and instructors. This system has the potential to allow reflections also to help future learners. By demonstrating that previous students encountered and overcame similar challenges, we could help improve students’ sense of belonging. We then perform user studies to evaluate this system’s potential and find that participants rated 70% of the recommended solutions as useful. Our findings suggest an increase in students’ sense of membership and acceptance, and a decrease in the desire to withdraw.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {521–527},
numpages = {7},
keywords = {educational recommender systems, semantic similarity, sense of belonging, student success},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3506860.3506918,
author = {Brennan, Riordan and Perouli, Debbie},
title = {Generating and Evaluating Collective Concept Maps},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506918},
doi = {10.1145/3506860.3506918},
abstract = {Concept maps are used in education to illustrate ideas and relationships among them. Instructors employ such maps to evaluate a student’s knowledge on a subject. Collective concept maps have been recently proposed as a tool to graphically summarize a group’s rather than an individual’s understanding on a topic. In this paper, we present a methodology that automatically generates collective concept maps, which relies on grouping similar ideas into node-clusters. We present a novel clustering algorithm that is shown to produce more informational maps compared to Markov clustering. We evaluate the collective map framework by applying it to sets of a total of 56 individual maps created by teachers (grades 2-12) and students (grades 6-11) during a week-long cybersecurity camp. Finally, we discuss how collective concept maps can support longitudinal research studies on program and student outcomes by providing a novel format for knowledge exchange. We have made our tool implementation publicly available.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {570–576},
numpages = {7},
keywords = {K-12, concept maps, cybersecurity, education, knowledge exchange formats},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3448139.3448183,
author = {Kyriacou, Harrison and Ramakrishnan, Anand and Whitehill, Jacob},
title = {Learning to Work in a Materials Recovery Facility: Can Humans and Machines Learn from Each Other?},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448183},
doi = {10.1145/3448139.3448183},
abstract = {Workplace learning often requires workers to learn new perceptual and motor skills. The future of work will increasingly feature human users who cooperate with machines, both to learn the tasks and to perform them. In this paper, we examine workplace learning in Materials Recovery Facilities (MRFs), i.e., recycling plants, where workers separate waste items on conveyer belts before they are formed into bales and reprocessed. Using a simulated MRF, we explored the benefit of machine learning assistants (MLAs) that help workers, and help train them, to sort objects efficiently by providing automated perceptual guidance. In a randomized experiment (n = 140), we found: (1) A low-accuracy MLA is worse than no MLA at all, both in terms of task performance and learning. (2) A perfect MLA led to the best task performance, but was no better in helping users to learn than having no MLA at all. (3) Users tend to follow the MLA’s judgments too often, even when they were incorrect. Finally, (4) we devised a novel learning analytics algorithm to assess the worker’s accuracy, with the goal of obtaining additional training labels that can be used for fine-tuning the machine. A simulation study illustrates how even noisy labels can increase the machine’s accuracy.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {456–461},
numpages = {6},
keywords = {MRF, automated feedback, object detection, perceptual learning},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3506860.3506867,
author = {R\"{u}dian, Sylvio and Dittmeyer, Moritz and Pinkwart, Niels},
title = {Challenges of using auto-correction tools for language learning},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506867},
doi = {10.1145/3506860.3506867},
abstract = {In language learning, getting corrective feedback for writing tasks is an essential didactical concept to improve learners' language skills. Although various tools for automatic correction do exist, open writing texts still need to be corrected manually by teachers to provide helpful feedback to learners. In this paper, we explore the usefulness of an auto-correction tool in the context of language learning. In the first step, we compare the corrections of 100 learner texts suggested by a correction tool with those done by human teachers and examine the differences. In a second step, we do a qualitative analysis, where we investigate the requirements that need to be tackled to make existing proofreading tools useful for language learning. The results reveal that the aim of enhancing texts by proofreading, in general, is quite different from the purpose of providing corrective feedback in language learning. Only one of four relevant errors (recall=.26) marked by human teachers is recorded correctly by the tool, whereas many expressions thought to be faulty by the tool are sometimes no errors at all (precision=.33). We provide and discuss the challenges that need to be addressed to adjust those tools for language learning.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {426–431},
numpages = {6},
keywords = {Language learning, automated feedback, online course, written corrective feedback},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3576050.3576078,
author = {Biedermann, Daniel and Schneider, Jan and Ciordas-Hertel, George-Petru and Eichmann, Beate and Hahnel, Carolin and Goldhammer, Frank and Drachsler, Hendrik},
title = {Detecting the Disengaged Reader - Using Scrolling Data to Predict Disengagement during Reading},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576078},
doi = {10.1145/3576050.3576078},
abstract = {When reading long and complex texts, students may disengage and miss out on relevant content. In order to prevent disengaged behavior or to counteract it by means of an intervention, it is ideally detected an early stage. In this paper, we present a method for early disengagement detection that relies only on the classification of scrolling data. The presented method transforms scrolling data into a time series representation, where each point of the series represents the vertical position of the viewport in the text document. This time series representation is then classified using time series classification algorithms. We evaluated the method on a dataset of 565 university students reading eight different texts. We compared the algorithm performance with different time series lengths, data sampling strategies, the texts that make up the training data, and classification algorithms. The method can classify disengagement early with up to 70% accuracy. However, we also observe differences in the performance depending on which of the texts are included in the training dataset. We discuss our results and propose several possible improvements to enhance the method.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {585–591},
numpages = {7},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3576050.3576153,
author = {Franco, Andrea and Holzer, Adrian},
title = {Fostering Privacy Literacy among High School Students by Leveraging Social Media Interaction and Learning Traces in the Classroom},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576153},
doi = {10.1145/3576050.3576153},
abstract = {With daily social media consumption among teens exceeding eight hours, it becomes increasingly important to raise awareness about the digital traces they leave behind. However, concepts of privacy literacy such as data and metadata can seem abstract and difficult to grasp. In this short research paper, we tackle this issue by designing, implementing and presenting an evaluation of a novel technology-enhanced pedagogical scenario for high school students. The scenario covers two main sessions. In a first session the SpeakUp social-media-like classroom interaction app is used to support a digitally mediated debate. In the second session, the actual learning traces from the digitally mediated debate are used as an object of study to enable students to reflect on the traces they leave behind on social media platforms. In order to enable this scenario we extended the existing SpeakUp app to the specifics of the context. The scenario was implemented and evaluated in real classrooms during a semester-long course on digital skills with 45 high school students. Our results show that the learning scenario is appreciated by students and even though non-STEM students might require more onboarding to be fully engaged in the digitally mediated debate, students from both STEM and non-STEM classes learn effectively. We discuss shortcomings and future research avenues.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {538–544},
numpages = {7},
keywords = {SpeakUp, classroom interaction app, learning traces, metadata, privacy literacy, social media},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3170358.3170373,
author = {Gardner, Josh and Brooks, Christopher},
title = {Coenrollment networks and their relationship to grades in undergraduate education},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170373},
doi = {10.1145/3170358.3170373},
abstract = {In this paper, we evaluate the complete undergraduate coenrollment network over a decade of education at a large American public university. We provide descriptive properties of the network, demonstrating that the coenrollment networks evaluated follow power-law degree distributions similar to many other large-scale networks; that they reveal strong performance-based assortativity; and that network-based features can significantly improve GPA-based student performance predictors. We then implement a network-based, multi-view classification model to predict students' final course grades. In particular, we adapt a structural modeling approach from [19, 34], whereby we model the university-wide undergraduate coenrollment network as an undirected graph. We compare the performance of our predictor to traditional methods used for grade prediction in undergraduate university courses, and demonstrate that a multi-view ensembling approach outperforms both prior "flat" and network-based models for grade prediction across several classification metrics. These findings demonstrate the usefulness of combining diverse approaches in models of student success, and demonstrate specific network-based modeling strategies which are likely to be most effective for grade prediction.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {295–304},
numpages = {10},
keywords = {grade prediction, learning analytics},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3576050.3576148,
author = {Galici, Roberta and Kaser, Tanja and Fenu, Gianni and Marras, Mirko},
title = {Do Not Trust a Model Because It is Confident: Uncovering and Characterizing Unknown Unknowns to Student Success Predictors in Online-Based Learning},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576148},
doi = {10.1145/3576050.3576148},
abstract = {Student success models might be prone to develop weak spots, i.e., examples hard to accurately classify due to insufficient representation during model creation. This weakness is one of the main factors undermining users’ trust, since model predictions could for instance lead an instructor to not intervene on a student in need. In this paper, we unveil the need of detecting and characterizing unknown unknowns in student success prediction in order to better understand when models may fail. Unknown unknowns include the students for which the model is highly confident in its predictions, but is actually wrong. Therefore, we cannot solely rely on the model’s confidence when evaluating the predictions quality. We first introduce a framework for the identification and characterization of unknown unknowns. We then assess its informativeness on log data collected from flipped courses and online courses using quantitative analyses and interviews with instructors. Our results show that unknown unknowns are a critical issue in this domain and that our framework can be applied to support their detection. The source code is available at https://github.com/epfl-ml4ed/unknown-unknowns.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {441–452},
numpages = {12},
keywords = {Fairness, Machine Learning, Student Success, Trust, Uncertainty, Unknown Unknowns.},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3375462.3375517,
author = {Benedetto, Luca and Cappelli, Andrea and Turrin, Roberto and Cremonesi, Paolo},
title = {R2DE: a NLP approach to estimating IRT parameters of newly generated questions},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375517},
doi = {10.1145/3375462.3375517},
abstract = {The main objective of exams consists in performing an assessment of students' expertise on a specific subject. Such expertise, also referred to as skill or knowledge level, can then be leveraged in different ways (e.g., to assign a grade to the students, to understand whether a student might need some support, etc.). Similarly, the questions appearing in the exams have to be assessed in some way before being used to evaluate students. Standard approaches to questions' assessment are either subjective (e.g., assessment by human experts) or introduce a long delay in the process of question generation (e.g., pretesting with real students). In this work we introduce R2DE (which is a Regressor for Difficulty and Discrimination Estimation), a model capable of assessing newly generated multiple-choice questions by looking at the text of the question and the text of the possible choices. In particular, it can estimate the difficulty and the discrimination of each question, as they are defined in Item Response Theory. We also present the results of extensive experiments we carried out on a real world large scale dataset coming from an e-learning platform, showing that our model can be used to perform an initial assessment of newly created questions and ease some of the problems that arise in question generation.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {412–421},
numpages = {10},
keywords = {educational data mining, item response theory, knowledge tracing, latent traits estimation, learning analytics, natural language processing},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3506860.3506874,
author = {Zavaleta Bernuy, Angela and Han, Ziwen and Shaikh, Hammad and Zheng, Qi Yin and Lim, Lisa-Angelique and Rafferty, Anna and Petersen, Andrew and Williams, Joseph Jay},
title = {How can Email Interventions Increase Students’ Completion of Online Homework? A Case Study Using A/B Comparisons},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506874},
doi = {10.1145/3506860.3506874},
abstract = {Email communication between instructors and students is ubiquitous, and it could be valuable to explore ways of testing out how to make email messages more impactful. This paper explores the design space of using emails to get students to plan and reflect on starting weekly homework earlier. We deployed a series of email reminders using randomized A/B comparisons to test alternative factors in the design of these emails, providing examples of an experimental paradigm and metrics for a broader range of interventions. We also surveyed and interviewed instructors and students to compare their predictions about the effectiveness of the reminders with their actual impact. We present our results on which seemingly obvious predictions about effective emails are not borne out, despite there being evidence for further exploring these interventions, as they can sometimes motivate students to attempt their homework more often. We also present qualitative evidence about student opinions and behaviours after receiving the emails, to guide further interventions. These findings provide insight into how to use randomized A/B comparisons in everyday channels such as emails, to provide empirical evidence to test our beliefs about the effectiveness of alternative design choices.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {107–118},
numpages = {12},
keywords = {A/B comparisons, Embedded experimentation, Procrastination, Randomized experiments, Reminder},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3506860.3506907,
author = {Yeckehzaare, Iman and Mulligan, Victoria and Ramstad, Grace and Resnick, Paul},
title = {Semester-level Spacing but Not Procrastination Affected Student Exam Performance},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506907},
doi = {10.1145/3506860.3506907},
abstract = {Spacing and procrastination are often thought of as opposites. It is possible, however, for a student to space their studying by doing something every day throughout the semester and still procrastinate by waiting until late in the semester to increase their amount of studying. To analyze the relationship between spacing and procrastination, we examined 674 students’ interactions with a course eBook over four semesters of an introductory programming course. We measured each student’s semester-level spacing as the number of days they interacted with the eBook, and each student’s semester-level procrastination as the average delay from the start of the semester for all their eBook interactions. Surprisingly, there was a small, yet positive, correlation between the two measures. Which, then, matters for course performance: studying over more days or studying earlier in the semester? When controlling for total amount of studying, as well as a number of academic and demographic characteristics in an SEM analysis, we find a strong positive effect of spacing but no significant effect of procrastination on final exam scores.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {304–314},
numpages = {11},
keywords = {computer science education, cramming, desirable difficulties, incentivized spacing, massing, procrastination, scheduling, spacing, study behavior, timing behavior},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/2883851.2883879,
author = {Brooks, Christopher A. and Thompson, Craig and Kovanovi\'{c}, Vitomir},
title = {Introduction to data mining for educational researchers},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883879},
doi = {10.1145/2883851.2883879},
abstract = {The goal of this tutorial is to share data mining tools and techniques used by computer scientists with educational social scientists. We broadly define educational social scientists as being made up of people with backgrounds in the learning sciences, cognitive psychology, and educational research. The learning analytics community is heavily populated with researchers of these backgrounds, and we believe those that find themselves at the intersection of research, theory, and practice have a particular interest in expanding their knowledge of datadriven tools and techniques.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {505–506},
numpages = {2},
keywords = {data mining, learning analytics, predictive modeling},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/3375462.3375492,
author = {Ak\c{c}apinar, G\"{o}khan and Chen, Mei-Rong Alice and Majumdar, Rwitajit and Flanagan, Brendan and Ogata, Hiroaki},
title = {Exploring student approaches to learning through sequence analysis of reading logs},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375492},
doi = {10.1145/3375462.3375492},
abstract = {In this paper, we aim to explore students' study approaches (e.g., deep, strategic, surface) from the logs collected by an electronic textbook (eBook) system. Data was collected from 89 students related to their reading activities both in and out of the class in a Freshman English course. Students are given a task to study reading materials through the eBook system, highlight the text that is related to the main or supporting ideas, and answer the questions prepared for measuring their level of comprehension. Students in and out of class reading times and their usage of the marker feature were used as a proxy to understand their study approaches. We used theory-driven and data-driven approaches together to model the study approaches of students. Our results showed that three groups of students who have different study approaches could be identified. Relationships between students' reading behaviors and their academic performance is also investigated by using association rule mining analysis. Obtained results are discussed in terms of monitoring, feedback, predicting learning outcomes, and identifying problems with the content design.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {106–111},
numpages = {6},
keywords = {association rule mining, clustering, learning analytics, reading logs, sequence analysis, study approaches},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3170358.3170381,
author = {Cicchinelli, Anal\'{\i}a and Veas, Eduardo and Pardo, Abelardo and Pammer-Schindler, Viktoria and Fessl, Angela and Barreiros, Carla and Lindst\"{a}dt, Stefanie},
title = {Finding traces of self-regulated learning in activity streams},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170381},
doi = {10.1145/3170358.3170381},
abstract = {This paper aims to identify self-regulation strategies from students' interactions with the learning management system (LMS). We used learning analytics techniques to identify metacognitive and cognitive strategies in the data. We define three research questions that guide our studies analyzing i) self-assessments of motivation and self regulation strategies using standard methods to draw a baseline, ii) interactions with the LMS to find traces of self regulation in observable indicators, and iii) self regulation behaviours over the course duration. The results show that the observable indicators can better explain self-regulatory behaviour and its influence in performance than preliminary subjective assessments.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {191–200},
numpages = {10},
keywords = {blended-learning, clickstream activity, learning analytics, learning strategies, self regulation},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3375462.3375513,
author = {Ou, Lu and Andrade, Alejandro and Alberto, Rosa and van Helden, Gitte and Bakker, Arthur},
title = {Using a cluster-based regime-switching dynamic model to understand embodied mathematical learning},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375513},
doi = {10.1145/3375462.3375513},
abstract = {Embodied learning and the design of embodied learning platforms have gained popularity in recent years due to the increasing availability of sensing technologies. In our study, we made use of the Mathematical Imagery Trainer for Proportion (MIT-P) that uses a touchscreen tablet to help students explore the concept of mathematical proportion. The use of sensing technologies provides an unprecedented amount of high-frequency data on students' behaviors. We investigated a statistical model called mixture Regime-Switching Hidden Logistic Transition Process (mixRHLP) and fit it to the students' hand motion data. Simultaneously, the model finds characteristic regimes and assigns students to clusters of regime transitions. To understand the nature of these regimes and clusters, we explore some properties in students' and tutor's verbalization associated with these different phases.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {496–501},
numpages = {6},
keywords = {dynamic models, embodied cognition, mathematical learning, multimodal learning analytics},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3303772.3303807,
author = {Andres, Juliana Ma. Alexandra L. and Ocumpaugh, Jaclyn and Baker, Ryan S. and Slater, Stefan and Paquette, Luc and Jiang, Yang and Karumbaiah, Shamya and Bosch, Nigel and Munshi, Anabil and Moore, Allison and Biswas, Gautam},
title = {Affect Sequences and Learning in Betty's Brain},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303807},
doi = {10.1145/3303772.3303807},
abstract = {Education research has explored the role of students' affective states in learning, but some evidence suggests that existing models may not fully capture the meaning or frequency of how students transition between different states. In this study we examine the patterns of educationally-relevant affective states within the context of Betty's Brain, an open-ended, computer-based learning system used to teach complex scientific processes. We examine three types of affective transitions based on similarity with the theorized D'Mello and Graesser model, transition between two affective states, and the sustained instances of certain states. We correlate of the frequency of these patterns with learning outcomes and our findings suggest that boredom is a powerful indicator of students' knowledge, but not necessarily indicative of learning. We discuss our findings within the context of both research and theory on affect dynamics and the implications for pedagogical and system design.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {383–390},
numpages = {8},
keywords = {Affect dynamics, affect, learning analytics},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3636555.3636909,
author = {Lee, Morgan and Siedahmed, Abubakir and Heffernan, Neil},
title = {Expert Features for a Student Support Recommendation Contextual Bandit Algorithm},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636909},
doi = {10.1145/3636555.3636909},
abstract = {Contextual multi-armed bandits have previously been used to personalize student support messages given to learners by supplying a model with relevant context about the user, problem, and available student supports. In this work, we propose using careful feature selection with relevant domain knowledge to improve the quality of student support recommendations. By providing Bayesian Knowledge Tracing mastery estimates to a contextual multi-armed bandit as user-level context in a simulated environment, we demonstrate that using domain knowledge to engineer contextual features results in higher average cumulative reward, and significant improvement over randomly selecting student supports. The data used to simulate sequential recommendations are available at https://osf.io/sfyzv/?view_only=351fb8781d2c4f3bbc9d7486762d563a.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {864–870},
numpages = {7},
keywords = {Feature Engineering, Multi-Armed Bandit, Personalized Learning, Reinforcement Learning},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3576050.3576113,
author = {Moulder, Robert and Booth, Brandon and Abitino, Angelina and D'Mello, Sidney},
title = {Recurrence Quantification Analysis of Eye Gaze Dynamics During Team Collaboration},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576113},
doi = {10.1145/3576050.3576113},
abstract = {Shared visual attention between team members facilitates collaborative problem solving (CPS), but little is known about how team-level eye gaze dynamics influence the quality and successfulness of CPS. To better understand the role of shared visual attention during CPS, we collected eye gaze data from 279 individuals solving computer-based physics puzzles while in teams of three. We converted eye gaze into discrete screen locations and quantified team-level gaze dynamics using recurrence quantification analysis (RQA). Specifically, we used a centroid-based auto-RQA approach, a pairwise team member cross-RQAs approach, and a multi-dimensional RQA approach to quantify team-level eye gaze dynamics from the eye gaze data of team members. We find that teams differing in composition based on prior task knowledge, gender, and race show few differences in team-level eye gaze dynamics. We also find that RQA metrics of team-level eye gaze dynamics were predictive of task success (all ps &lt; .001). However, the same metrics showed different patterns of feature importance depending on predictive model and RQA type, suggesting some redundancy in task-relevant information. These findings signify that team-level eye gaze dynamics play an important role in CPS and that different forms of RQA pick up on unique aspects of shared attention between team-members.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {430–440},
numpages = {11},
keywords = {eye gaze dynamics, recurrence quantification analysis, shared attention, team collaboration, team dynamics},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3375462.3375516,
author = {Quick, Joshua and Motz, Benjamin and Israel, Jamie and Kaetzel, Jason},
title = {What college students say, and what they do: aligning self-regulated learning theory with behavioral logs},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375516},
doi = {10.1145/3375462.3375516},
abstract = {A central concern in learning analytics specifically and educational research more generally is the alignment of robust, coherent measures to well-developed conceptual and theoretical frameworks. Capturing and representing processes of learning remains an ongoing challenge in all areas of educational inquiry and presents substantive considerations on the nature of learning, knowledge, and assessment &amp; measurement that have been continuously refined in various areas of education and pedagogical practice. Learning analytics as a still developing method of inquiry has yet to substantively navigate the alignment of measurement, capture, and representation of learning to theoretical frameworks despite being used to identify various practical concerns such as at risk students. This study seeks to address these concerns by comparing behavioral measurements from learning management systems to established measurements of components of learning as understood through self-regulated learning frameworks. Using several prominent and robustly supported self-reported survey measures designed to identify dimensions of self-regulated learning, as well as typical behavioral features extracted from a learning management system, we conducted descriptive and exploratory analyses on the relational structures of these data. With the exception of learners' self-reported time management strategies and level of motivation, the current results indicate that behavioral measures were not well correlated with survey measurements. Possibilities and recommendations for learning analytics as measurements for self-regulated learning are discussed.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {534–543},
numpages = {10},
keywords = {LMS, self-regulated learning, self-reports, trace data},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375472,
author = {Kia, Fatemeh Salehian and Teasley, Stephanie D. and Hatala, Marek and Karabenick, Stuart A. and Kay, Matthew},
title = {How patterns of students dashboard use are related to their achievement and self-regulatory engagement},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375472},
doi = {10.1145/3375462.3375472},
abstract = {The aim of student-facing dashboards is to support learning by providing students with actionable information and promoting self-regulated learning. We created a new dashboard design aligned with SRL theory, called MyLA, to better understand how students use a learning analytics tool. We conducted sequence analysis on students' interactions with three different visualizations in the dashboard, implemented in a LMS, for a large number of students (860) in ten courses representing different disciplines. To evaluate different students' experiences with the dashboard, we computed chi-squared tests of independence on dashboard users (52%) to find frequent patterns that discriminate students by their differences in academic achievement and self-regulated learning behaviors. The results revealed discriminating patterns in dashboard use among different levels of academic achievement and self-regulated learning, particularly for low achieving students and high self-regulated learners. Our findings highlight the importance of differences in students' experience with a student-facing dashboard, and emphasize that one size does not fit all in the design of learning analytics tools.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {340–349},
numpages = {10},
keywords = {academic achievement, self-regulated learning, sequential pattern mining, student-facing dashboard},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3576050.3576067,
author = {Nazaretsky, Tanya and Mikeska, Jamie N. and Beigman Klebanov, Beata},
title = {Empowering Teacher Learning with AI: Automated Evaluation of Teacher Attention to Student Ideas during Argumentation-focused Discussion},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576067},
doi = {10.1145/3576050.3576067},
abstract = {Engaging students in argument from evidence is an essential goal of science education. This is a complex skill to develop; recent research in science education proposed the use of simulated classrooms to facilitate the practice of the skill. We use data from one such simulated environment to explore whether automated analysis of the transcripts of the teacher’s interaction with the simulated students using Natural Language Processing techniques could yield an accurate evaluation of the teacher’s performance. We are especially interested in explainable models that could also support formative feedback. The results are encouraging: Not only can the models score the transcript as well as humans can, but they can also provide justifications for the scores comparable to those provided by human raters.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {122–132},
numpages = {11},
keywords = {Automated Feedback, Deep Learning, Practice-based Teacher Education, Simulated Teaching, Teacher Discourse},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/2883851.2883870,
author = {Pardo, Abelardo and Mirriahi, Negin and Martinez-Maldonado, Roberto and Jovanovic, Jelena and Dawson, Shane and Ga\v{s}evi\'{c}, Dragan},
title = {Generating actionable predictive models of academic performance},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883870},
doi = {10.1145/2883851.2883870},
abstract = {The pervasive collection of data has opened the possibility for educational institutions to use analytics methods to improve the quality of the student experience. However, the adoption of these methods faces multiple challenges particularly at the course level where instructors and students would derive the most benefit from the use of analytics and predictive models. The challenge lies in the knowledge gap between how the data is captured, processed and used to derive models of student behavior, and the subsequent interpretation and the decision to deploy pedagogical actions and interventions by instructors. Simply put, the provision of learning analytics alone has not necessarily led to changing teaching practices. In order to support pedagogical change and aid interpretation, this paper proposes a model that can enable instructors to readily identify subpopulations of students to provide specific support actions. The approach was applied to a first year course with a large number of students. The resulting model classifies students according to their predicted exam scores, based on indicators directly derived from the learning design.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {474–478},
numpages = {5},
keywords = {feedback, learning analytics, personalization, recursive partitioning},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883946,
author = {Cooper, Martyn and Ferguson, Rebecca and Wolff, Annika},
title = {What can analytics contribute to accessibility in e-learning systems and to disabled students' learning?},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883946},
doi = {10.1145/2883851.2883946},
abstract = {This paper explores the potential of analytics for improving accessibility of e-learning and supporting disabled learners in their studies. A comparative analysis of completion rates of disabled and non-disabled students in a large five-year dataset is presented and a wide variation in comparative retention rates is characterized. Learning analytics enable us to identify and understand such discrepancies and, in future, could be used to focus interventions to improve retention of disabled students. An agenda for onward research, focused on Critical Learning Paths, is outlined. This paper is intended to stimulate a wider interest in the potential benefits of learning analytics for institutions as they try to assure the accessibility of their e-learning and provision of support for disabled students.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {99–103},
numpages = {5},
keywords = {HCI, accessibility, higher education, learning analytics, metrics, technology enhanced learning},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883968,
author = {Berg, Alan and Scheffel, Maren and Drachsler, Hendrik and Ternier, Stefaan and Specht, Marcus},
title = {The dutch xAPI experience},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883968},
doi = {10.1145/2883851.2883968},
abstract = {We present the collected experiences since 2012 of the Dutch Special Interest Group (SIG) for Learning Analytics in the application of the xAPI standard. We have been experimenting and exchanging best practices around the application of xAPI in various contexts. The practices include different design patterns centered around Learning Record Stores. We present three projects that apply xAPI in very different ways and publish a consistent set of xAPI recipes.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {544–545},
numpages = {2},
keywords = {data silos, data standardization, learning analytics, learning record store, xAPI},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2536536.2536573,
author = {Conde, Miguel \'{A}. and Hern\'{a}ndez-Garc\'{\i}a, \'{A}ngel},
title = {A promised land for educational decision-making? present and future of learning analytics},
year = {2013},
isbn = {9781450323451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2536536.2536573},
doi = {10.1145/2536536.2536573},
abstract = {There is increasing interest in Learning Analytics, a concept that has evolved in the last years from the status of buzzword or trend to the generation of a discipline within the educational field. In this paper, we introduce the Track on Learning Analytics within the Technological Ecosystems for Enhancing Multiculturality 2013 Conference.The paper provides a general overview of the motivations behind the proposal of this track, including a review of the current state of Learning Analytics, from its origins and a definition of the term to some pending issues for future research, which are addressed by the papers participating in the track and which hopefully will be expanded by future studies. The paper also includes an insight on the submission management and participants' selection process, which is followed by a detailed summary of the manuscripts accepted for participation in the conference and how they are linked to the track objectives.},
booktitle = {Proceedings of the First International Conference on Technological Ecosystem for Enhancing Multiculturality},
pages = {239–243},
numpages = {5},
keywords = {academic analytics, decision making, educational data mining, educational technologies, knowledge management, learning analytics, predictive analytics, visual analytics},
location = {Salamanca, Spain},
series = {TEEM '13}
}

@inproceedings{10.1145/3170358.3170419,
author = {Broos, Tom and Verbert, Katrien and Langie, Greet and Van Soom, Carolien and De Laet, Tinne},
title = {Multi-institutional positioning test feedback dashboard for aspiring students: lessons learnt from a case study in flanders},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170419},
doi = {10.1145/3170358.3170419},
abstract = {Our work focuses on a multi-institutional implementation and evaluation of a Learning Analytics Dashboards (LAD) at scale, providing feedback to N=337 aspiring STEM (science, technology, engineering and mathematics) students participating in a region-wide positioning test before entering the study program. Study advisors were closely involved in the design and evaluation of the dashboard. The multi-institutional context of our case study requires careful consideration of external stakeholders and data ownership and portability issues, which gives shape to the technical design of the LAD. Our approach confirms students as active agents with data ownership, using an anonymous feedback code to access the LAD and to enable students to share their data with institutions at their discretion. Other distinguishing features of the LAD are the support for active content contribution by study advisors and LATEX type-setting of question item feedback to enhance visual recognizability. We present our lessons learnt from a first iteration in production.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {51–55},
numpages = {5},
keywords = {case study, feedback, higher education, learning analytics, positioning test, student dashboard},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3448139.3448176,
author = {Fernandez-Nieto, Gloria Milena and Martinez-Maldonado, Roberto and Kitto, Kirsty and Buckingham Shum, Simon},
title = {Modelling Spatial Behaviours in Clinical Team Simulations using Epistemic Network Analysis: Methodology and Teacher Evaluation},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448176},
doi = {10.1145/3448139.3448176},
abstract = {In nursing education through team simulations, students must learn to position themselves correctly in coordination with colleagues. However, with multiple student teams in action, it is difficult for teachers to give detailed, timely feedback on these spatial behaviours to each team. Indoor-positioning technologies can now capture student spatial behaviours, but relatively little work has focused on giving meaning to student activity traces, transforming low-level x/y coordinates into language that makes sense to teachers. Even less research has investigated if teachers can make sense of that feedback. This paper therefore makes two contributions. (1) Methodologically, we document the use of Epistemic Network Analysis (ENA) as an approach to model and visualise students’ movements. To our knowledge, this is the first application of ENA to analyse human movement. (2) We evaluated teachers’ responses to ENA diagrams through qualitative analysis of video-recorded sessions. Teachers constructed consistent narratives about ENA diagrams’ meaning, and valued the new insights ENA offered. However, ENA’s abstract visualisation of spatial behaviours was not intuitive, and caused some confusions. We propose, therefore, that the power of ENA modelling can be combined with other spatial representations such as a classroom map, by overlaying annotations to create a more intuitive user experience.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {386–396},
numpages = {11},
keywords = {Epistemic Network Analysis, nursing, qualitative analysis, simulation, spatial behaviour},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3506860.3506875,
author = {Chang, Xinyuan and Wang, Bingxin and Hui, Bowen},
title = {Towards an Automatic Approach for Assessing Program Competencies},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506875},
doi = {10.1145/3506860.3506875},
abstract = {Skills analysis is an interdisciplinary area that studies labor market trends and provides recommendations for developing educational standards and re-skilling efforts. We leverage techniques in this area to develop a scalable approach that identifies and evaluates educational competencies. In this work, we developed a skills extraction algorithm that uses natural language processing and machine learning techniques. We evaluated our algorithm on a labeled dataset and found its performance to be competitive with state-of-the-art methods. Using this algorithm, we analyzed student skills, university course syllabi, and online job postings. Our cross-sector analysis provides an initial landscape of skill needs for specific job titles. Additionally, we conducted a within-sector analysis based on programming jobs, computer science curriculum, and undergraduate students. Our findings suggest that students have a variety of hard skills and soft skills, but they are not necessarily the ones that employers want. The data also suggests these courses teach skills that are somewhat different from industry needs, and there is a lack of emphasis on soft skills. These results provide an initial assessment of the program competencies for a computer science program. Future work includes more data gathering, improving the algorithm, and applying our method to assess additional educational programs.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {119–129},
numpages = {11},
keywords = {Job skills, competency-based education, gap analysis, linguistic patterns, skills extraction, text classification},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/2330601.2330603,
author = {Wellman, Barry},
title = {Networked individualism: how the personalized internet, ubiquitous connectivity, and the turn to social networks can affect learning analytics},
year = {2012},
isbn = {9781450311113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2330601.2330603},
doi = {10.1145/2330601.2330603},
abstract = {The Triple Revolution---the coming together of the turn to social networks, the personalized internet, and accessible mobile connectivity---has fostered networked individualism. This has implications for learning analytics, in the need to move beyond analyzing bounded groups and aggregates of individuals to taking into account complex, partial networks of social relationships.},
booktitle = {Proceedings of the 2nd International Conference on Learning Analytics and Knowledge},
pages = {1},
numpages = {1},
keywords = {internet-revolution, mobile-revolution networked-individualism, network analysis, network analytics, social change, social-network revolution},
location = {Vancouver, British Columbia, Canada},
series = {LAK '12}
}

@inproceedings{10.1145/3448139.3448143,
author = {Khosravi, Hassan and Demartini, Gianluca and Sadiq, Shazia and Gasevic, Dragan},
title = {Charting the Design and Analytics Agenda of Learnersourcing Systems},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448143},
doi = {10.1145/3448139.3448143},
abstract = {Learnersourcing is emerging as a viable learner-centred and pedagogically justified approach for harnessing the creativity and evaluation power of learners as experts-in-training. Despite the increasing adoption of learnersourcing in higher education, understanding students’ behaviour while engaged in learnersourcing and best practices for the design and development of learnersourcing systems are still largely under-researched. This paper offers data-driven reflections and lessons learned from the development and deployment of a learnersourcing adaptive educational system called RiPPLE, which to date, has been used in more than 50-course offerings with over 12,000 students. Our reflections are categorised into examples and best practices on (1) assessing the quality of students’ contributions using accurate, explainable and fair approaches to data analysis, (2) incentivising students to develop high-quality contributions and (3) empowering instructors with actionable and explainable insights to guide student learning. We discuss the implications of these findings and how they may contribute to the growing literature on the development of effective learnersourcing systems and more broadly technological educational solutions that support learner-centred learning at scale.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {32–42},
numpages = {11},
keywords = {Learnersourcing, crowdsourcing in education, explainable AI, human-centred computing},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3375462.3375475,
author = {Prestigiacomo, Rita and Hadgraft, Roger and Hunter, Jane and Locker, Lori and Knight, Simon and van den Hoven, Elise and Martinez-Maldonado, Roberto},
title = {Learning-centred translucence: an approach to understand how teachers talk about classroom data},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375475},
doi = {10.1145/3375462.3375475},
abstract = {Teachers are increasingly being encouraged to embrace evidence-based practices. Learning analytics (LA) offer great promise in supporting these by providing evidence for teachers and learners to make informed decisions and transform the educational experience. However, LA limitations and their uptake by educators are coming under critical scrutiny. This is in part due to the lack of involvement of teachers and learners in the design of LA tools. In this paper, we propose a human-centred approach to generate understanding of teachers' data needs through the lens of three key principles of translucence: visibility, awareness and accountability. We illustrate our approach through a participatory design sprint to identify how teachers talk about classroom data. We describe teachers' perspectives on the evidence they need for making better-informed decisions and discuss the implications of our approach for the design of human-centred LA in the next years.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {100–105},
numpages = {6},
keywords = {evidence-based decision-making, human-centred design},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3576050.3576139,
author = {Marinho, Wemerson and Clua, Esteban Walter and Mart\'{\i}, Luis and Marinho, Karla},
title = {Predicting Item Response Theory Parameters Using Question Statements Texts},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576139},
doi = {10.1145/3576050.3576139},
abstract = {Recently, new Neural Language Models pre-trained on a massive corpus of texts are available. These models encode statistical features of the languages through their parameters, creating better word vector representations that allow the training of neural networks with smaller sample sets. In this context, we investigate the application of these models to predict Item Response Theory parameters in multiple choice questions. More specifically, we apply our models for the Brazilian National High School Exam (ENEM) questions using the text of their statements and propose a novel optimization target for regression: Item Characteristic Curve. The architecture employed could predict the difficulty parameter b of the ENEM 2020 and 2021 items with a mean absolute error of 70 points. Calculating the IRT score in each knowledge area of the exam for a sample of 100,000 students, we obtained a mean absolute below 40 points for all knowledge areas. Considering only the top quartile, the exam’s main target of interest, the average error was less than 30 points for all areas, being the majority lower than 15 points. Such performance allows predicting parameters on newly created questions, composing mock tests for student training, and analyzing their performance with excellent precision, dispensing with the need for costly item calibration pre-test step.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {1–10},
numpages = {10},
keywords = {ENEM, Item response theory, Question difficulty prediction, Text regression},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3303772.3303782,
author = {Jovanovi\'{c}, Jelena and Ga\v{s}evi\'{c}, Dragan and Pardo, Abelardo and Dawson, Shane and Whitelock-Wainwright, Alexander},
title = {Introducing meaning to clicks: Towards traced-measures of self-efficacy and cognitive load},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303782},
doi = {10.1145/3303772.3303782},
abstract = {The use of learning trace data together with various analytical methods has proven successful in detecting patterns in learning behaviour, identifying student profiles, and clustering learning resources. However, interpretation of the findings is often difficult and uncertain due to a lack of contextual data (e.g., data on student motivation, emotion or curriculum design). In this study we explored the integration of student self-reports about cognitive load and self-efficacy into the learning process and collection of relevant students' perceptions as learning traces. Our objective was to examine the association of traced measures of relevant learning constructs (cognitive load and self-efficacy) with i) indicators of the students' learning behaviour derived from trace data, and ii) the students' academic performance. The results indicated the presence of association between some indicators of students' engagement with learning activities and traced measures of cognitive load and self-efficacy. Correlational analysis demonstrated significant positive correlation between the students' course performance and traced measures of cognitive load and self-efficacy.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {511–520},
numpages = {10},
keywords = {cognitive load, learning analytics, perceived difficulty, self-efficacy, self-reports, trace data},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3448139.3448141,
author = {G\"{u}nther, Sebastian A.},
title = {The impact of social norms on students’ online learning behavior: Insights from two randomized controlled trials},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448141},
doi = {10.1145/3448139.3448141},
abstract = {The provision of comparative feedback is a promising approach in digital learning environments to support learners’ self-regulated learning. Yet, empirical evidence suggests that such feedback can sometimes backfire or may only help learners with relatively high self-regulated learning skills, potentially exacerbating educational inequality. In this paper, we try to overcome such drawbacks by re-evaluating a feedback system based on the social norms theory that has previously led to intriguing results: A social comparison component embedded into the learning platform of a blended learning course (elective module, 58 participants) considerably encouraged online learning during the semester. Moreover, there was no heterogeneity in the behavioral response, suggesting that all subgroups responded similarly to the feedback. To further shed light on the generalizability of these results, this paper presents a follow-up study. Specifically, we conducted a second experiment during the COVID-19 pandemic with a different university course (compulsory module, 118 participants) and a non-overlapping sample and find similar results. The feedback shifted students’ online learning from the end towards the middle of the semester. Overall, the findings suggest that our feedback system has a large impact on students’ online learning and that this desirable impact is present in all subgroup analyses.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {12–21},
numpages = {10},
keywords = {Online learning, behavioral intervention, feedback, field experiment, generalizability, replication study, social norms},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448167,
author = {Gurung, Ashish and Botelho, Anthony F. and Heffernan, Neil T.},
title = {Examining Student Effort on Help through Response Time Decomposition},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448167},
doi = {10.1145/3448139.3448167},
abstract = {Many teachers have come to rely on the affordances that computer-based learning platforms offer in regard to aiding in student assessment, supplementing instruction, and providing immediate feedback and help to students as they work through assigned content. Similarly, researchers commonly utilize the large datasets of clickstream logs describing students’ interactions with the platform to study learning. For the teachers that use this information to monitor student progress, as well as for researchers, this data provides limited insights into the learning process; this is particularly the case as it pertains to observing and understanding the effort that students are applying to their work. From the perspective of teachers, it is important for them to know which students are attending to and using computer-provided aid and which are taking advantage of the system to complete work without effectively learning the material. In this paper, we conduct a series of analyses based on response time decomposition (RTD) to explore student help-seeking behavior in the context of on-demand hints within a computer-based learning platform with particular focus on examining which students appear to be exhibiting effort to learn while engaging with the system. Our findings are then leveraged to examine how our measure of student effort correlates with later student performance measures.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {292–301},
numpages = {10},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3027385.3029443,
author = {Knight, Simon and Anderson, Theresa and Tall, Kelly},
title = {Dear learner: participatory visualisation of learning data for sensemaking},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029443},
doi = {10.1145/3027385.3029443},
abstract = {We discuss the application of a hand-drawn self-visualization approach to learner-data, to draw attention to the space of representational possibilities, the power of representation interactions, and the performativity of information representation.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {532–533},
numpages = {2},
keywords = {learning analytics, participatory, sensemaking, visualization},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3506860.3506888,
author = {Hur, Paul and Bosch, Nigel},
title = {Tracking Individuals in Classroom Videos via Post-processing OpenPose Data},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506888},
doi = {10.1145/3506860.3506888},
abstract = {Analyzing classroom video data provides valuable insights about the interactions between students and teachers, albeit often through time-consuming qualitative coding or the use of bespoke sensors to record individual movement information. We explore measuring classroom posture and movement in secondary classroom video data through computer vision methods (especially OpenPose), and introduce a simple but effective approach to automatically track movement via post-processing of OpenPose output data. Analysis of 67 videos of mathematics classes from middle school and high school levels highlighted the challenges associated with analyzing movement in typical classroom videos: occlusion from low camera angles, difficulty detecting lower body movement due to sitting, and the close proximity of students to one another and their teachers. Despite these challenges, our approach tracked person IDs across classroom videos for 93.0% of detected individuals. The tracking results were manually verified through randomly sampling 240 instances, which revealed notable OpenPose tracking inconsistencies. Finally, we discuss the implications for supporting more scalability of video data classroom movement analysis, and future potential explorations.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {465–471},
numpages = {7},
keywords = {classroom video, movement, posture, video analysis},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3027385.3029461,
author = {Hansen, Cecilie Johanne Slokvik and Wasson, Barbara and Skretting, Hans and Netteland, Grete and Hirnstein, Marina},
title = {When learning is high stake},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029461},
doi = {10.1145/3027385.3029461},
abstract = {Firefighter learning is high stake. They need to maintain certain competence levels related to physical, mental, and firefighting and rescue skills in order to provide the public with a high level of emergency service. Fire and Rescue Services need to maintain an overview of the current competences of their personnel and to react when there is a competence gap. This poster presents our approach to using competence modelling, learner models, learning analytics, and visualisations in order provide insight into competence status and development on the individual, team, and organisation level, and to provide early-alerts and automated messages to instructors responsible for planning training activities, as well as to team leaders responsible for making decisions about teams in high stakes situations.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {564–565},
numpages = {2},
keywords = {competence development, learning analytics, open learner model, visualization},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3029447,
author = {Clow, Doug and Ferguson, Rebecca and Kitto, Kirsty and Cho, Yong-Sang and Sharkey, Mike and Aguerrebere, Cecilia},
title = {Beyond failure: the 2nd LAK Failathon poster},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029447},
doi = {10.1145/3027385.3029447},
abstract = {This poster will be a chance for a wider LAK audience to engage with the 2nd LAK Failathon workshop. Both of these will build on the successful Failathon event in 2016 and extend beyond discussing individual experiences of failure to exploring how the field can improve, particularly regarding the creation and use of evidence.Failure in research is an increasingly hot topic, with high-profile crises of confidence in the published research literature in medicine and psychology. Among the major factors in this research crisis are the many incentives to report and publish only positive findings. These incentives prevent the field in general from learning from negative findings, and almost entirely preclude the publication of mistakes and errors. Thus providing an alternative forum for practitioners and researchers to learn from each other's failures can be very productive. The first LAK Failathon, held in 2016, provided just such an opportunity for researchers and practitioners to share their failures and negative findings in a lower-stakes environment, to help participants learn from each other's mistakes. It was very successful, and there was strong support for running it as an annual event. The 2nd LAK Failathon workshop will build on that success, with twin objectives to provide an environment for individuals to learn from each other's failures, and also to co-develop plans for how we as a field can better build and deploy our evidence base.This poster is an opportunity for wider feedback on the plans developed in the workshop, with interactive use of sticky notes to add new ideas and coloured dots to illustrate prioritisation. This broadens the participant base in this important work, which should improve the quality of the plans and the commitment of the community to delivering them.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {540–541},
numpages = {2},
keywords = {analytics, evidence, learning analytics, learning from failure},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3448139.3448198,
author = {Hayward, Caitlin and Schulz, Kyle and Fishman, Barry},
title = {Who wins, who learns? Exploring gameful pedagogy as a technique to support student differences},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448198},
doi = {10.1145/3448139.3448198},
abstract = {Gameful pedagogy is a novel approach to teaching that has emerged in the past decade that emphasizes intentionally designing curricula to support student motivation. In this study we investigate how five gameful courses at a large R1 university have impacted students when analyzed with an eye towards equity: are men vs women, underrepresented minorities vs majority students, and first-generation students vs traditional students able to achieve similar amounts of success in gameful courses? Results show that for both men and minority students, there is evidence to suggest they are underachieving in the courses studied as compared to women and majority students, but when we control for prior academic performance these trends disappear. For first-generation students, we see conflicting evidence, with cases of both under and over-achievement present. We again see these discrepancies disappear when we control for prior performance.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {559–564},
numpages = {6},
keywords = {Gameful, curricular design, higher education, self-determination theory},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3506860.3506892,
author = {Dood, Amber and Winograd, Blair and Finkenstaedt-Quinn, Solaire and Gere, Anne and Shultz, Ginger},
title = {PeerBERT: Automated Characterization of Peer Review Comments across Courses},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506892},
doi = {10.1145/3506860.3506892},
abstract = {Writing-to-learn pedagogies are an evidence-based practice known to aid students in constructing knowledge. Barriers exist for the implementation of such assignments; namely, instructors feel they do not have time to provide each student with feedback. To ease implementation of writing-to-learn assignments at scale, we have incorporated automated peer review, which facilitates peer review without input from the instructor. Participating in peer review can positively impact students’ learning and allow students to receive feedback on their writing. Instructors may want to monitor these peer interactions and gain insight into their students’ understanding using the feedback generated by their peers. To facilitate instructors’ use of the content from students’ peer review comments, we pre-trained a transformer model called PeerBERT. PeerBERT was fine-tuned on several downstream tasks to categorize students’ peer review comments as praise, problem/solution, or verification/summary. The model exhibits high accuracy, even across different peer review prompts, assignments, and courses. Additional downstream tasks label problem/solution peer review comments as one or more types: writing/formatting, missing content/needs elaboration, and incorrect content. This approach can help instructors pinpoint common issues in student writing by parsing out which comments are problem/solution and which type of problem/solution students identify.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {492–499},
numpages = {8},
keywords = {peer review, undergraduate education, writing-to-learn},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/2723576.2723577,
author = {Knight, Simon and Littleton, Karen},
title = {Developing a multiple-document-processing performance assessment for epistemic literacy},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723577},
doi = {10.1145/2723576.2723577},
abstract = {The LAK15 theme "shifts the focus from data to impact", noting the potential for Learning Analytics based on existing technologies to have scalable impact on learning for people of all ages. For such demand and potential in scalability to be met the challenges of addressing higher-order thinking skills should be addressed. This paper discuses one such approach--the creation of an analytic and task model to probe epistemic cognition in complex literacy tasks. The research uses existing technologies in novel ways to build a conceptually grounded model of trace-indicators for epistemic-commitments in information seeking behaviors. We argue that such an evidence centered approach is fundamental to realizing the potential of analytics, which should maintain a strong association with learning theory.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {241–245},
numpages = {5},
keywords = {discourse analytics, educational assessment, epistemic cognition, learning analytics, social learning analytics},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/3448139.3448182,
author = {Matayoshi, Jeffrey and Karumbaiah, Shamya},
title = {Using Marginal Models to Adjust for Statistical Bias in the Analysis of State Transitions},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448182},
doi = {10.1145/3448139.3448182},
abstract = {Many areas of educational research require the analysis of data that have an inherent sequential or temporal ordering. In certain cases, researchers are specifically interested in the transitions between different states—or events—in these sequences, with the goal being to understand the significance of these transitions; one notable example is the study of affect dynamics, which aims to identify important transitions between affective states. Unfortunately, a recent study has revealed a statistical bias with several metrics used to measure and compare these transitions, possibly causing these metrics to return unexpected and inflated values. This issue then causes extra difficulties when interpreting the results of these transition metrics. Building on this previous work, in this study we look in more detail at the specific mechanisms that are responsible for the bias with these metrics. After giving a theoretical explanation for the issue, we present an alternative procedure that attempts to address the problem with the use of marginal models. We then analyze the effectiveness of this procedure, both by running simulations and by applying it to actual student data. The results indicate that the marginal model procedure seemingly compensates for the bias observed in other transition metrics, thus resulting in more accurate estimates of the significance of transitions between states.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {449–455},
numpages = {7},
keywords = {L statistic, affect dynamics, marginal models, sequential data, transition metrics},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3576050.3576084,
author = {Caruso, Megan and D'Mello, Sidney},
title = {Do Associations Between Mind Wandering and Learning from Complex Texts Vary by Assessment Depth and Time?},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576084},
doi = {10.1145/3576050.3576084},
abstract = {We examined associations between mind wandering – where attention shifts from the task at hand to task-unrelated thoughts – and learning outcomes. Our data consisted of 177 students who self-reported mind wandering while reading five long, connected texts on scientific research methods and completed learning assessments targeting multiple depths of processing (rote, inference, integration) at different timescales (during and after reading each text, after reading all texts, and after a week-long delay). We found that mind wandering negatively predicted measures of factual, text-based (explicit) information and global integration of information across multiple parts of the text, but not measures requiring a local inference on a single sentence. Further, mind wandering only predicted comprehension measures assessed during the reading session and not after a week-long delay. Our findings provide important nuances to the established negative link between mind wandering and learning outcomes, which has predominantly focused on rote comprehension assessed during the learning session itself. Implications for interventions to address mind wandering during learning are discussed.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {230–239},
numpages = {10},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/2883851.2883945,
author = {McPherson, Jen and Tong, Huong Ly and Fatt, Scott J. and Liu, Danny Y. T.},
title = {Student perspectives on data provision and use: starting to unpack disciplinary differences},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883945},
doi = {10.1145/2883851.2883945},
abstract = {How can we best align learning analytics practices with disciplinary knowledge practices in order to support student learning? Although learning analytics itself is an interdisciplinary field, it tends to take a 'one-size-fits-all' approach to the collection, measurement, and reporting of data, overlooking disciplinary knowledge practices. In line with a recent trend in higher education research, this paper considers the contribution of a realist sociology of education to the field of learning analytics, drawing on findings from recent student focus groups at an Australian university. It examines what learners say about their data needs with reference to organizing principles underlying knowledge practices within their disciplines. The key contribution of this paper is a framework that could be used as the basis for aligning the provision and/or use of data in relation to curriculum, pedagogy, and assessment with disciplinary knowledge practices. The framework extends recent research in Legitimation Code Theory, which understands disciplinary differences in terms of the principles that underpin knowledge-building. The preliminary analysis presented here both provides a tool for ensuring a fit between learning analytics practices and disciplinary practices and standards for achievement, and signals disciplinarity as an important consideration in learning analytics practices.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {158–167},
numpages = {10},
keywords = {disciplinary differences, knowledge, learning analytics, legitimation code theory, sociology of education, student needs},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/3170358.3170412,
author = {Shimada, Atsushi and Taniguchi, Yuta and Okubo, Fumiya and Konomi, Shin'ichi and Ogata, Hiroaki},
title = {Online change detection for monitoring individual student behavior via clickstream data on E-book system},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170412},
doi = {10.1145/3170358.3170412},
abstract = {We propose a new change detection method using clickstream data collected through an e-Book system. Most of the prior work has focused on the batch processing of clickstream data. In contrast, the proposed method is designed for online processing, with the model parameters for change detection updated sequentially based on observations of new click events. More specifically, our method generates a model for an individual student and performs minute-by-minute change detection based on click events during a classroom lecture. We collected clickstream data from four face-to-face lectures, and conducted experiments to demonstrate how the proposed method discovered change points and how such change points correlated with the students' performances.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {446–450},
numpages = {5},
keywords = {change detection, clickstream, learning analytics, online processing},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3027385.3029469,
author = {Chen, Guanliang and Davis, Dan and Krause, Markus and Hauff, Claudia and Houben, Geert-Jan},
title = {Buying time: enabling learners to become earners with a real-world paid task recommender system},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029469},
doi = {10.1145/3027385.3029469},
abstract = {Massive Open Online Courses (MOOCs) aim to educate the world, especially learners from developing countries. While MOOCs are certainly available to the masses, they are not yet fully accessible. Although all course content is just clicks away, deeply engaging with a MOOC requires a substantial time commitment, which frequently becomes a barrier to success. To mitigate the time required to learn from a MOOC, we here introduce a design that enables learners to earn money by applying what they learn in the course to real-world marketplace tasks. We present a Paid Task Recommender System (Rec-$ys), which automatically recommends course-relevant tasks to learners as drawn from online freelance platforms. Rec-$ys has been deployed into a data analysis MOOC and is currently under evaluation.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {578–579},
numpages = {2},
keywords = {MOOCs, learning analytics, learning design},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3448139.3448195,
author = {Vanacore, Kirk and Dieter, Kevin and Hurwitz, Lisa and Studwell, Jamie},
title = {Longitudinal Clusters of Online Educator Portal Access: Connecting Educator Behavior to Student Outcomes},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448195},
doi = {10.1145/3448139.3448195},
abstract = {The rising prevalence of blended learning programs has provided educators with an abundance of information about students' specific educational needs through educator portals. Full implementation of blended learning models requires educators to utilize these data to inform their teaching practices, yet most research on blended learning programs focuses solely on student engagement with the digital learning environment. In this paper, we utilize a longitudinal clustering method to identify patterns of educator portal usage and examine the associations between these clusters and student program outcomes. The clusters of educators varied in intensity and consistency of educator portal access across a school year and were associated with significant differences in student usage and progress in the program. The analyses allowed us to identify preferable educator usage patterns based upon their associated students’ program outcomes, which provides novel information about the potential impact of educator engagement on overall implementation fidelity of blended learning programs.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {540–545},
numpages = {6},
keywords = {Educator engagement, Implementation fidelity, Longitudinal k-means clustering},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/2883851.2883930,
author = {Schwendimann, Beat A. and Rodr\'{\i}guez-Triana, Mar\'{\i}a Jes\'{u}s and Vozniuk, Andrii and Prieto, Luis P. and Boroujeni, Mina Shirvani and Holzer, Adrian and Gillet, Denis and Dillenbourg, Pierre},
title = {Understanding learning at a glance: an overview of learning dashboard studies},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883930},
doi = {10.1145/2883851.2883930},
abstract = {Research on learning dashboards aims to identify what data is meaningful to different stakeholders in education, and how data can be presented to support sense-making processes. This paper summarizes the main outcomes of a systematic literature review on learning dashboards, in the fields of Learning Analytics and Educational Data Mining. The query was run in five main academic databases and enriched with papers coming from GScholar, resulting in 346 papers out of which 55 were included in the final analysis. Our review distinguishes different kinds of research studies as well as different aspects of learning dashboards and their maturity in terms of evaluation. As the research field is still relatively young, many of the studies are exploratory and proof-of-concept. Among the main open issues and future lines of work in the area of learning dashboards, we identify the need for longitudinal research in authentic settings, as well as studies that systematically compare different dashboard design options.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {532–533},
numpages = {2},
keywords = {dashboards, educational data mining, information visualization, learning analytics, systematic review},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/3027385.3029429,
author = {Clow, Doug and Ferguson, Rebecca and Kitto, Kirsty and Cho, Yong-Sang and Sharkey, Mike and Aguerrebere, Cecilia},
title = {Beyond failure: the 2nd LAK Failathon},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029429},
doi = {10.1145/3027385.3029429},
abstract = {The 2nd LAK Failathon will build on the successful event in 2016 and extend the workshop beyond discussing individual experiences of failure to exploring how the field can improve, particularly regarding the creation and use of evidence.Failure in research is an increasingly hot topic, with high-profile crises of confidence in the published research literature in medicine and psychology. Among the major factors in this research crisis are the many incentives to report and publish only positive findings. These incentives prevent the field in general from learning from negative findings, and almost entirely preclude the publication of mistakes and errors. Thus providing an alternative forum for practitioners and researchers to learn from each other's failures can be very productive. The first LAK Failathon, held in 2016, provided just such an opportunity for researchers and practitioners to share their failures and negative findings in a lower-stakes environment, to help participants learn from each other's mistakes. It was very successful, and there was strong support for running it as an annual event. This workshop will build on that success, with twin objectives to provide an environment for individuals to learn from each other's failures, and also to co-develop plans for how we as a field can better build and deploy our evidence base.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {504–505},
numpages = {2},
keywords = {analytics, evidence, learning analytics, learning from failure},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3576050.3576082,
author = {Song, Yukyeong and Xing, Wanli and Tian, Xiaoyi and Li, Chenglu},
title = {Are We on the Same Page? Modeling Linguistic Synchrony and Math Literacy in Mathematical Discussions},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576082},
doi = {10.1145/3576050.3576082},
abstract = {Mathematical discussions have become a popular educational strategy to promote math literacy. While some studies have associated math literacy with linguistic factors such as verbal ability and phonological skills, no studies have examined the relationship between linguistic synchrony and math literacy. In this study, we modeled linguistic synchrony and students’ math literacy from 20,776 online mathematical discussion threads between students and facilitators. We conducted Cross-Recurrence Quantification Analysis (CRQA) to calculate linguistic synchrony within each thread. The statistical testing result comparing CRQA indices between high and low math literacy groups shows that students with high math literacy have a significantly higher Recurrence Rate (RR), Number of Recurrence Lines (NRLINE), and the average Length of lines (L), but lower Determinism (DET) and normalized Entropy (rENTR). This result implies that students with high math literacy are more likely to share common words with facilitators, but they would paraphrase them. On the other hand, students with low math literacy tend to repeat the exact same phrases from the facilitators. The findings provide a better understanding of mathematical discussions and can potentially guide teachers in promoting effective mathematical discussions.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {599–605},
numpages = {7},
keywords = {cross-recurrence quantification analysis, linguistic synchrony, math literacy, mathematical discussion},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/2883851.2883858,
author = {Whyte, Anthony and Nayak, Prashant and Johnston, John},
title = {LAK16 workshop: extending IMS caliper analytics™ with learning activity profiles},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883858},
doi = {10.1145/2883851.2883858},
abstract = {Educational institutions are evolving away from the one-application-fits-all learning management system to a loosely connected digital learning ecosystem comprising diverse services that increasingly leverage data analytics to drive pedagogical innovation. Yet an ecosystem rich in services but lacking a common approach to measuring learning activity will find data collection, aggregation and analysis time-consuming and costly. The IMS Caliper Analytics™ specification addresses the need for data and semantic interoperability by providing an extensible information model, controlled vocabularies and an API for instrumenting learning applications and systems that log learning events. However, many learning activities have yet to be modeled by the Caliper working group. Engaging the SoLAR community directly in this effort will help ensure that the needs of researchers and other consumers of learning analytics data will inform future versions of the specification. The LAK16 Caliper workshop is being offered with this goal in mind. The half-day session, facilitated by members of Team Caliper, will provide LAK16 participants with an opportunity to extend the Caliper specification by modeling new learning activity profiles. New profiles, new connections and new friendships are expected outcomes.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {490–491},
numpages = {2},
keywords = {IMS caliper, controlled vocabularies, information modeling, learning activity profiles, learning analytics},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/3506860.3506977,
author = {Ferreira Mello, Rafael and Fiorentino, Giuseppe and Oliveira, Hil\'{a}rio and Miranda, P\'{e}ricles and Rakovic, Mladen and Gasevic, Dragan},
title = {Towards automated content analysis of rhetorical structure of written essays using sequential content-independent features in Portuguese},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506977},
doi = {10.1145/3506860.3506977},
abstract = {Brazilian universities have included essay writing assignments in the entrance examination procedure to select prospective students. The essay scorers manually look for the presence of required Rhetorical Structure Theory (RST) categories and evaluate essay coherence. However, identifying RST categories is a time-consuming task. The literature reported several attempts to automate the identification of RST categories in essays with machine learning. Still, previous studies have focused on using machine learning algorithms trained on content-dependent features that can diminish classification performance, leading to over-fitting and hindering model generalisability. Therefore, this paper proposes: (i) the analysis of state-of-the-art classifiers and content-independent features to the task of RST rhetorical moves; (ii) a new approach that considers the sequence of the text to extract features – i.e. sequential content-independent features; (iii) an empirical study about the generalisability of the machine learning models and sequential content-independent features for this context; (iv) the identification of the most predictive features for automated identification of RST categories in essays written in Portuguese. The best performing classifier, XGBoost, based on sequential content-independent features, outperformed the classifiers used in the literature and are based on traditional content-dependent features. The XGBoost classifier based on sequential content-independent features also reached promising accuracy when tested for generalisability.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {404–414},
numpages = {11},
keywords = {Essay analysis, content analytics, context analysis, natural language processing., rhetoric structure},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3375462.3375531,
author = {Poquet, Oleksandra and Tupikina, Liubov and Santolini, Marc},
title = {Are forum networks social networks? a methodological perspective},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375531},
doi = {10.1145/3375462.3375531},
abstract = {The mission of learning analytics (LA) is to improve learner experiences using the insights from digitally collected learner data. While some areas of LA are maturing, this is not consistent across all LA specialisations. For instance, LA for social learning lack validated approaches to account for the effects of cross-course variability in learner behavior. Although the associations between network structure and learning outcomes have been examined in the context of online forums, it remains unclear whether such associations represent bona fide social effects, or merely reflect heterogeneity in individual posting behavior, leading to seemingly complex but artefactual social network structures. We argue that to start addressing this issue, posting activity should be explicitly included and modelled in forum network representations. To gain insight to what extent learner degree and edge weight are merely derivatives of learner activity, we construct random models that control for the level of posting and post properties, such as popularity and thread hierarchy level. Analysis of forum networks in twenty online courses presented in this paper demonstrates that individual posting behavior is highly predictive of both the breadth (degree) and frequency (strength) in forum communication networks. This implies that, in the context of forum-based modelling, degree and frequency may not reflect the social dynamics. However, results suggest that clustering of the network structure is not a derivative of individual posting behaviour. Hence, weighted local clustering coefficient may be a better proxy for social relationships. The empirical results are relevant to scientists interested in social interactions and learner networks in digital learning, and more generally to researchers interested in deriving informative social network models from online forums.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {366–375},
numpages = {10},
keywords = {null models, online forums, online learning, social networks},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3448139.3448190,
author = {Xu, Yang and Wilson, Kevin},
title = {Early Alert Systems During a Pandemic: A Simulation Study on the Impact of Concept Drift},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448190},
doi = {10.1145/3448139.3448190},
abstract = {Predictions from early alert systems are increasingly being used by institutions to assist decision-making and support at-risk individuals. Concept drifts caused by the 2020 SARS-CoV-2 pandemic are threatening the performance and usefulness of the machine learning models that power these systems. In this paper, we present an analytical framework that uses imputation-based simulations to perform preliminary evaluation on the extent to which data quality and availability issues impact the performance of machine learning models. Guided by this framework, we studied how these issues would impact the performance of the high school dropout prediction model implemented in the Early Warning System (EWS). Results show that despite the disruptions, this model can still be reasonably useful in assisting decision-making. We discuss the implications of these findings in more general educational contexts and recommend steps in countering the challenges of using predictions from imperfect machine learning models in early alert systems and, more broadly, learning analytic research that uses longitudinal data.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {504–510},
numpages = {7},
keywords = {concept drift, dropout prediction, early alert system, imputation, pandemic, simulation},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448204,
author = {Chockkalingam, Shruthi and Yu, Run and Pardos, Zachary A.},
title = {Which one’s more work? Predicting effective credit hours between courses},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448204},
doi = {10.1145/3448139.3448204},
abstract = {University students select courses for an upcoming term in part based on expected workload. Course credit hours is often the only metric given by the institution relevant to how much work a course will be and does not serve as a precise estimate due to the lack of granularity of the metric which can lead to student under or overestimation. We define a novel task of predicting relative effective course credit hours, or time load; essentially, determining which courses take more time than others. For this task, we draw from institutional data sources including course catalog descriptions, student enrollment histories and ratings from a popular course rating website. To validate this work, we design a personalized survey for university students to collect ground truth labels, presenting them with pairs of courses they had taken and asking which course took more time per week on average. We evaluate which sources of data using which machine representation techniques provide the best prediction of these course time load ratings. We establish a benchmark accuracy of 0.71 on this novel task and find skip-grams applied to enrollment data (i.e., course2vec), not catalog descriptions, to be most useful in predicting the time demands of a course.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {599–605},
numpages = {7},
keywords = {course load, course2vec, enrollment data, higher education, institutional data, predictive analytics, survey study, time load},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/2723576.2723600,
author = {Rienties, Bart and Toetenel, Lisette and Bryan, Annie},
title = {"Scaling up" learning design: impact of learning design activities on LMS behavior and performance},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723600},
doi = {10.1145/2723576.2723600},
abstract = {While substantial progress has been made in terms of predictive modeling in the Learning Analytics Knowledge (LAK) community, one element that is often ignored is the role of learning design. Learning design establishes the objectives and pedagogical plans which can be evaluated against the outcomes captured through learning analytics. However, no empirical study is available linking learning designs of a substantial number of courses with usage of Learning Management Systems (LMS) and learning performance. Using cluster- and correlation analyses, in this study we compared how 87 modules were designed, and how this impacted (static and dynamic) LMS behavior and learning performance. Our findings indicate that academics seem to design modules with an "invisible" blueprint in their mind. Our cluster analyses yielded four distinctive learning design patterns: constructivist, assessment-driven, balanced-variety and social constructivist modules. More importantly, learning design activities strongly influenced how students were engaging online. Finally, learning design activities seem to have an impact on learning performance, in particular when modules rely on assimilative activities. Our findings indicate that learning analytics researchers need to be aware of the impact of learning design on LMS data over time, and subsequent academic performance.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {315–319},
numpages = {5},
keywords = {academic retention, learning analytics, learning design},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/3303772.3303832,
author = {Reilly, Joseph M. and Dede, Chris},
title = {Differences in Student Trajectories via Filtered Time Series Analysis in an Immersive Virtual World},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303832},
doi = {10.1145/3303772.3303832},
abstract = {To scaffold students' investigations of an inquiry-based immersive virtual world for science education without undercutting the affordances an open-ended activity provides, this study explores ways time-stamped log files of groups' actions may enable the automatic generation of formative supports. Groups' logged actions in the virtual world are filtered via principal component analysis to provide a time series trajectory showing the rate of their investigative activities over time. This technique functions well in open-ended environments and examines the entire course of their experience in the virtual world instead of specific subsequences. Groups' trajectories are grouped via k-means clustering to identify different typical pathways taken through the immersive virtual world. These different approaches are then correlated with learning gains across several survey constructs (affective dimensions, ecosystem science content, understanding of causality, and experimental methods) to see how various trends are associated with different outcomes. Differences by teacher and school are explored to see how best to support inclusion and success of a diverse array of learners.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {130–134},
numpages = {5},
keywords = {immersive virtual world, learning analytics, log file analysis, scientific inquiry, time-series analysis},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3448139.3448170,
author = {Ruan, Sherry and Wei, Wei and Landay, James},
title = {Variational Deep Knowledge Tracing for Language Learning},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448170},
doi = {10.1145/3448139.3448170},
abstract = {Deep Knowledge Tracing (DKT), which traces a student’s knowledge change using deep recurrent neural networks, is widely adopted in student cognitive modeling. Current DKT models only predict a student’s performance based on the observed learning history. However, a student’s learning processes often contain latent events not directly observable in the learning history, such as partial understanding, making slips, and guessing answers. Current DKT models fail to model this kind of stochasticity in the learning process. To address this issue, we propose Variational Deep Knowledge Tracing (VDKT), a latent variable DKT model that incorporates stochasticity into DKT through latent variables. We show that VDKT outperforms both a sequence-to-sequence DKT baseline and previous SoTA methods on MAE, F1, and AUC by evaluating our approach on two Duolingo language learning datasets. We also draw various interpretable analyses from VDKT and offer insights into students’ stochastic behaviors in language learning.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {323–332},
numpages = {10},
keywords = {deep learning, knowledge tracing, language learning, student modeling, variational inference},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3506860.3506978,
author = {Ferreira, M\'{a}verick Andr\'{e} Dion\'{\i}sio and Ferreira Mello, Rafael and Kovanovic, Vitomir and Nascimento, Andr\'{e} and Lins, Rafael and Gasevic, Dragan},
title = {NASC: Network analytics to uncover socio-cognitive discourse of student roles},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506978},
doi = {10.1145/3506860.3506978},
abstract = {Roles that learners assume during online discussions are an important aspect of educational experience. The roles can be assigned to learners and/or can spontaneously emerge through student-student interaction. While existing research proposed several approaches for analytics of emerging roles, there is limited research in analytic methods that can i) automatically detect emerging roles that can be interpreted in terms of higher-order constructs of collaboration; ii) analyse the extent to which students complied to scripted roles and how emerging roles compare to scripted ones; and iii) track progression of roles in social knowledge progression over time. To address these gaps in the literature, this paper propose a network-analytic approach that combines techniques of cluster analysis and epistemic network analysis. The method was validated in an empirical study discovered emerging roles that were found meaningful in terms of social and cognitive dimensions of the well-known model of communities of inquiry. The study also revealed similarities and differences between emerging and script roles played by learners and identified different progression trajectories in social knowledge construction between emerging and scripted roles. The proposed analytic approach and the study results have implications that can inform teaching practice and development techniques for collaboration analytics.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {415–425},
numpages = {11},
keywords = {CSCL, Clustering Analysis, Emerging Roles, Epistemic Network Analysis},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3448139.3448164,
author = {Salehian Kia, Fatemeh and Hatala, Marek and Baker, Ryan S. and Teasley, Stephanie D.},
title = {Measuring Students’ Self-Regulatory Phases in LMS with Behavior and Real-Time Self Report},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448164},
doi = {10.1145/3448139.3448164},
abstract = {Research has emphasized that self-regulated learning (SRL) is critically important for learning. However, students have different capabilities of regulating their learning processes and individual needs. To help students improve their SRL capabilities, we need to identify students’ current behaviors. Specifically, we applied instructional design to create visible and meaningful markers of student learning at different points in time in LMS logs. We adopted knowledge engineering to develop a framework of proximal indicators representing SRL phases and evaluated them in a quasi-experiment in two different learning activities. A comparison of two sources of collected students’ SRL data, self-reported and trace data, revealed a relatively high agreement between our classifications (weighted kappa, κ = .74 and κ = .68). However, our indicators did not always discriminate adjacent SRL phases, particularly for enactment and adapting phases, compared with students’ real-time self-reported behaviors. Our behavioral indicators also were comparably successful at classifying SRL phases for different self-regulatory engagement levels. This study demonstrated how the triangulation of various sources of students’ self-regulatory data could help to unravel the complex nature of metacognitive processes.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {259–268},
numpages = {10},
keywords = {knowledge-engineered trace measures, pattern recognition, self-regulated learning, self-reported measures},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448173,
author = {Li, Zhi and Ren, Cheng and Li, Xianyou and Pardos, Zachary A.},
title = {Learning Skill Equivalencies Across Platform Taxonomies},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448173},
doi = {10.1145/3448139.3448173},
abstract = {Assessment and reporting of skills is a central feature of many digital learning platforms. With students often using multiple platforms, cross-platform assessment has emerged as a new challenge. While technologies such as Learning Tools Interoperability (LTI) have enabled communication between platforms, reconciling the different skill taxonomies they employ has not been solved at scale. In this paper, we introduce and evaluate a methodology for finding and linking equivalent skills between platforms by utilizing problem content as well as the platform’s clickstream data. We propose six models to represent skills as continuous real-valued vectors, and leverage machine translation to map between skill spaces. The methods are tested on three digital learning platforms: ASSISTments, Khan Academy, and Cognitive Tutor. Our results demonstrate reasonable accuracy in skill equivalency prediction from a fine-grained taxonomy to a coarse-grained one, achieving an average recall@5 of 0.8 between the three platforms. Our skill translation approach has implications for aiding in the tedious, manual process of taxonomy to taxonomy mapping work, also called crosswalks, within the tutoring as well as standardized testing worlds.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {354–363},
numpages = {10},
keywords = {Skill equivalencies, acknowledging prior knowledge, app hand-offs., crosswalks, digital learning platforms, interoperability, machine translation, representation learning, taxonomies, transfer models},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3375462.3375522,
author = {Kim, Yanghee and Butail, Sachit and Tscholl, Michael and Liu, Lichuan and Wang, Yunlong},
title = {An exploratory approach to measuring collaborative engagement in child robot interaction},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375522},
doi = {10.1145/3375462.3375522},
abstract = {This study explored data analytic approaches to assessing young children's engagement in robot-mediated collaborative interaction. To develop our analytic models, we took a case-study approach and looked closely into four children's behaviors during three conversational sessions. Grounded in engagement theory, three sources of multimodal behavioral data (utterances, kinesics, and vocie) were coded through human annotation and automatic speech recognition and analysis. Then, information-theoretic methods were used to uncover nonlinear dependencies (called mutual information) among the multimodal behaviors of each child. From this, we derived a model to compute a compound variable of engagement. This computation produced engagement trends of each child, the engagement relationship between two children in a pair, and the engagement relationship with the robot over time. The computed trends corresponded well with the data from human observations. This approach has implications for quantifying engagement from rich and natural multimodal behaviors.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {209–217},
numpages = {9},
keywords = {automatic speech recognition, child robot interaction, collaborative problem solving, engagement, human computer interaction, information theory, learning analytics, multimodal data analytics, mutual information, social robotics},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3448139.3448146,
author = {Ahn, June and Nguyen, Ha and Campos, Fabio and Young, William},
title = {Transforming Everyday Information into Practical Analytics with Crowdsourced Assessment Tasks},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448146},
doi = {10.1145/3448139.3448146},
abstract = {Educators use a wide variety of data to inform their practices. Examples of these data include forms of information that are commonplace in schools, such as student work and paper-based artifacts. One limitation in these situations is that there are less efficient ways to process such everyday varieties of information into analytics that are more usable and practical for educators. To explore how to address this constraint, we describe two sets of design experiments that utilize crowdsourced tasks for scoring open-ended assessments. Developing crowdsourced systems and their resulting analytics introduced a variety of challenges, such as attending to the expertise and learning of the crowd. In this paper, we describe the potential efficacy of design decisions such as screening the crowd, providing multimedia instruction, and asking the crowd to explain their answers. We also explore the potential of crowdsourcing as a learning opportunity for those participating in the collective tasks. Our work offers key design implications for leveraging crowdsourcing to process educational data in ways that are relevant to educators, while offering learning experiences for the crowd.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {66–76},
numpages = {11},
keywords = {assessment, crowdsourcing, education, worker training},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/2883851.2883883,
author = {Pardo, Abelardo and Han, Feifei and Ellis, Robert A.},
title = {Exploring the relation between self-regulation, online activities, and academic performance: a case study},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883883},
doi = {10.1145/2883851.2883883},
abstract = {The areas of educational data mining and learning analytics focus on the extraction of knowledge and actionable items from data sets containing detailed information about students. However, the potential impact from these techniques is increased when properly contextualized within a learning environment. More studies are needed to explore the connection between student interactions, approaches to learning, and academic performance. Self-regulated learning (SRL) is defined as the extent to which a student is able to motivationally, metacognitively, and cognitively engage in a learning experience. SRL has been the focus of research in traditional classroom learning and is also argued to play a vital role in the online or blended learning contexts. In this paper, we study how SRL affects students' online interactions with various learning activities and its influence in academic performance. The results derived from a naturalistic experiment among a cohort of first year engineering students showed that positive self-regulated strategies (PSRS) and negative self-regulated strategies (NSRS) affected both the interaction with online activities and academic performance. NSRS directly predicted academic outcomes, whereas PSRS only contributed indirectly to academic performance via the interactions with online activities. These results point to concrete avenues to promote self-regulation among students in this type of learning contexts.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {422–429},
numpages = {8},
keywords = {SEM, higher education, learning analytics, self-regulation},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2723576.2723638,
author = {Knight, Simon and Wise, Alyssa F. and Chen, Bodong and Cheng, Britte Haugan},
title = {It's about time: 4th international workshop on temporal analyses of learning data},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723638},
doi = {10.1145/2723576.2723638},
abstract = {Interest in analyses that probe the temporal aspects of learning continues to grow. The study of common and consequential sequences of events (such as learners accessing resources, interacting with other learners and engaging in self-regulatory activities) and how these are associated with learning outcomes, as well as the ways in which knowledge and skills grow or evolve over time are both core areas of interest. Learning analytics datasets are replete with fine-grained temporal data: click streams; chat logs; document edit histories (e.g. wikis, etherpads); motion tracking (e.g. eye-tracking, Microsoft Kinect), and so on. However, the emerging area of temporal analysis presents both technical and theoretical challenges in appropriating suitable techniques and interpreting results in the context of learning. The learning analytics community offers a productive focal ground for exploring and furthering efforts to address these challenges. This workshop, the fourth in a series on temporal analysis of learning, provides a focal point for analytics researchers to consider issues around and approaches to temporality in learning analytics.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {388–389},
numpages = {2},
keywords = {CSCL, discourse analytics, knowledge building, learning analytics, sequence mining, temporality},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/3375462.3375467,
author = {Vrzakova, Hana and Amon, Mary Jean and Stewart, Angela and Duran, Nicholas D. and D'Mello, Sidney K.},
title = {Focused or stuck together: multimodal patterns reveal triads' performance in collaborative problem solving},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375467},
doi = {10.1145/3375462.3375467},
abstract = {Collaborative problem solving (CPS) in virtual environments is an increasingly important context of 21st century learning. However, our understanding of this complex and dynamic phenomenon is still limited. Here, we examine unimodal primitives (activity on the screen, speech, and body movements), and their multimodal combinations during remote CPS. We analyze two datasets where 116 triads collaboratively engaged in a challenging visual programming task using video conferencing software. We investigate how UI-interactions, behavioral primitives, and multimodal patterns were associated with teams' subjective and objective performance outcomes. We found that idling with limited speech (i.e., silence or backchannel feedback only) and without movement was negatively correlated with task performance and with participants' subjective perceptions of the collaboration. However, being silent and focused during solution execution was positively correlated with task performance. Results illustrate that in some cases, multimodal patterns improved the predictions and improved explanatory power over the unimodal primitives. We discuss how the findings can inform the design of real-time interventions for remote CPS.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {295–304},
numpages = {10},
keywords = {CSCL, CSCW, interpretability, multimodal learning analytics},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375520,
author = {Abdi, Solmaz and Khosravi, Hassan and Sadiq, Shazia and Gasevic, Dragan},
title = {Complementing educational recommender systems with open learner models},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375520},
doi = {10.1145/3375462.3375520},
abstract = {Educational recommender systems (ERSs) aim to adaptively recommend a broad range of personalised resources and activities to students that will most meet their learning needs. Commonly, ERSs operate as a "black box" and give students no insight into the rationale of their choice. Recent contributions from the learning analytics and educational data mining communities have emphasised the importance of transparent, understandable and open learner models (OLMs) that provide insight and enhance learners' understanding of interactions with learning environments. In this paper, we aim to investigate the impact of complementing ERSs with transparent and understandable OLMs that provide justification for their recommendations. We conduct a randomised control trial experiment using an ERS with two interfaces ("Non-Complemented Interface" and "Complemented Interface") to determine the effect of our approach on student engagement and their perception of the effectiveness of the ERS. Overall, our results suggest that complementing an ERS with an OLM can have a positive effect on student engagement and their perception about the effectiveness of the system despite potentially making the system harder to navigate. In some cases, complementing an ERS with an OLM has the negative consequence of decreasing engagement, understandability and sense of fairness.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {360–365},
numpages = {6},
keywords = {educational recommender systems, open learner models, user models},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375533,
author = {Poquet, Oleksandra and Jovanovic, Jelena},
title = {Intergroup and interpersonal forum positioning in shared-thread and post-reply networks},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375533},
doi = {10.1145/3375462.3375533},
abstract = {Network analysis has become a major approach for analysing social learning, used to capture learner positioning in online forum networks. LA research investigated the association between positioning in forum networks with academic performance and discourse quality, the latter two serving as proxies for learning. However, the research findings have been inconsistent, in part due to the discrepancies in the adopted approaches to network construction. Yet, it is still unclear how online forum networks should be modelled to assure that the learners' network positioning is properly captured. To address this gap, the current study explored if some existing approaches to network construction may complement each other and thus offer richer insights. In particular, we hypothesised that the post-reply learner network could represent interpersonal positioning, whereas the network based on co-participation in discussion threads could encapsulate intergroup positioning. The study used learner social interaction data from a large edX MOOC forum to examine the relationship between these two kinds of network positioning. The results suggest that intergroup and interpersonal positioning may capture different aspects of social learning, potentially related to different learning outcomes. We find that although interpersonal and intergroup positioning indicators covary, these measures are not congruent for some 37% of forum posters. Network coevolution analysis also reveals an interdependent relationship between the intergroup and interpersonal centrality in a forum network. Co-occurrence of learners in a discussion thread prior to direct exchanges is predictive of a direct post-reply interaction at a later stage of the course, and vice-versa, suggesting that intergroup positioning is a precursor of direct communication. The study contributes to the discussion around the definition of learner forum positioning in learning analytics, and validated approaches towards measuring it.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {187–196},
numpages = {10},
keywords = {centrality, collective learning, learner networks, positioning},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3448139.3448188,
author = {Shin, Dongmin and Shim, Yugeun and Yu, Hangyeol and Lee, Seewoo and Kim, Byungsoo and Choi, Youngduck},
title = {SAINT+: Integrating Temporal Features for EdNet Correctness Prediction},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448188},
doi = {10.1145/3448139.3448188},
abstract = {We propose SAINT+, a successor of SAINT which is a Transformer based knowledge tracing model that separately processes exercise information and student response information. Following the architecture of SAINT, SAINT+ has an encoder-decoder structure where the encoder applies self-attention layers to a stream of exercise embeddings, and the decoder alternately applies self-attention layers and encoder-decoder attention layers to streams of response embeddings and encoder output. Moreover, SAINT+ incorporates two temporal feature embeddings into the response embeddings: elapsed time, the time taken for a student to answer, and lag time, the time interval between adjacent learning activities. We empirically evaluate the effectiveness of SAINT+ on EdNet, the largest publicly available benchmark dataset in the education domain. Experimental results show that SAINT+ achieves state-of-the-art performance in knowledge tracing with an improvement of 1.25% in area under receiver operating characteristic curve compared to SAINT, the current state-of-the-art model in EdNet dataset.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {490–496},
numpages = {7},
keywords = {Deep Learning, Education, Knowledge Tracing, Personalized Learning, Transformer},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448189,
author = {Abdi, Solmaz and Khosravi, Hassan and Sadiq, Shazia},
title = {Modelling Learners in Adaptive Educational Systems: A Multivariate Glicko-based Approach},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448189},
doi = {10.1145/3448139.3448189},
abstract = {The Elo rating system has been recognised as an effective method for modelling students and items within adaptive educational systems. A common characteristic across Elo-based learner models is that they are not sensitive to the lag time between two consecutive interactions of a student within the system. Implicitly, this characteristic assumes that students do not learn or forget between two consecutive interactions. However, this assumption seems insufficient in the context of adaptive learning systems where students could have improved their mastery through practising outside of the system or that their mastery may be declined due to forgetting. In this paper, we extend the existing works on the use of rating systems for modelling learners in adaptive educational systems by proposing a new learner model called MV-Glicko that builds on the Glicko rating system. MV-Glicko is sensitive to the lag time between two consecutive interactions of a student within the system and models it as a parameter that captures the confidence of the system in the current inferred rating. We apply MV-Glicko on three public data sets and three data sets obtained from an adaptive learning system and provide evidence that MV-Glicko outperforms other conventional models in estimating students’ knowledge mastery.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {497–503},
numpages = {7},
keywords = {Adaptive learning, Glicko rating system, knowledge tracing, learner modelling},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448177,
author = {Loh, Hyunbin and Shin, Dongmin and Lee, Seewoo and Baek, Jineon and Hwang, Chanyou and Lee, Youngnam and Cha, Yeongmin and Kwon, Soonwoo and Park, Juneyoung and Choi, Youngduck},
title = {Recommendation for Effective Standardized Exam Preparation},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448177},
doi = {10.1145/3448139.3448177},
abstract = {Finding an optimal learning trajectory is an important question in educational systems. Existing Artificial Intelligence in Education (AiEd) technologies mostly used indirect methods to make the learning process efficient such as recommending contents based on difficulty adjustment, weakness analysis, learning theory, psychometric analysis, or domain specific rules. In this study, we propose a recommender system that optimizes the learning trajectory of a student preparing for a standardized exam by recommending the learning content(question) which directly maximizes the expected score after the consumption of the content. In particular, the proposed RCES model computes the expected score of a user by effectively capturing educational effects. To validate the proposed model in an end-to-end system, we conduct an A/B test on 1713 real students by deploying 4 recommenders to a real mobile application. Result shows that RCES has better educational efficiencies than traditional methods such as expert designed models and item response theory based models.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {397–404},
numpages = {8},
keywords = {Education, Intelligent tutoring system, Personalized learning, Recommender Systems},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3170358.3170422,
author = {Liaqat, Amna and Munteanu, Cosmin},
title = {Towards a writing analytics framework for adult english language learners},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170422},
doi = {10.1145/3170358.3170422},
abstract = {Improving the written literacy of newcomers to English-speaking countries can lead to better education, employment, or social integration opportunities. However, this remains a challenge in traditional classrooms where providing frequent, timely, and personalized feedback is not always possible. Analytics can scaffold the writing development of English Language Learners (ELLs) by providing such feedback. To design these analytics, we conducted a field study analyzing essay samples from immigrant adult ELLs (a group often overlooked in writing analytics research) and identifying their epistemic beliefs and learning motivations. We identified common themes across individual learner differences and patterns of errors in the writing samples. The study revealed strong associations between epistemic writing beliefs and learning strategies. The results are used to develop guidelines for designing writing analytics for adult ELLs, and to propose ideas for analytics that scaffold writing development for this group.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {121–125},
numpages = {5},
keywords = {adult learners, immigrant, learning analytics, writing},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/2883851.2883956,
author = {Milligan, Sandra and He, Jiazhen and Bailey, James and Zhang, Rui and Rubinstein, Benjamin I. P},
title = {Validity: a framework for cross-disciplinary collaboration in mining indicators of learning from MOOC forums},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883956},
doi = {10.1145/2883851.2883956},
abstract = {Two research teams from the University of Melbourne's Learning Analytics Research Group used validation as applied in educational measurement to provide a framework for collaboration. One team was focussed on defining and building measures of learning capability of MOOCs participants, and the other on using topic modelling to discover topics in MOOC forums. The collaboration explored the suitability of items discovered from MOOC forums using topic modelling as measures of learning capability of participants in MOOCs.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {546–547},
numpages = {2},
keywords = {MOOC, collaborative learning, crowd-sourced learning, learner performance, learning analytics, learning progression, measurement theory, non-negative matrix factorisation, rasch analysis, topic modelling, validity},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2723576.2723625,
author = {Pardo, Abelardo and Ellis, Robert A. and Calvo, Rafael A.},
title = {Combining observational and experiential data to inform the redesign of learning activities},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723625},
doi = {10.1145/2723576.2723625},
abstract = {A main goal for learning analytics is to inform the design of a learning experience to improve its quality. The increasing presence of solutions based on big data has even questioned the validity of current scientific methods. Is this going to happen in the area of learning analytics? In this paper we postulate that if changes are driven solely by a digital footprint, there is a risk of focusing only on factors that are directly connected to numeric methods. However, if the changes are complemented with an understanding about how students approach their learning, the quality of the evidence used in the redesign is significantly increased. This reasoning is illustrated with a case study in which an initial set of activities for a first year engineering course were shaped based only on the student's digital footprint. These activities were significantly modified after collecting qualitative data about the students approach to learning. We conclude the paper arguing that the interpretation of the meaning of learning analytics is improved when combined with qualitative data which reveals how and why students engaged with the learning tasks in qualitatively different ways, which together provide a more informed basis for designing learning activities.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {305–309},
numpages = {5},
keywords = {active learning, approaches to learning, interventions, learning analytics, mixed methods analysis},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2883851.2883903,
author = {Freeman, J. D.},
title = {Demonstration of the Unizin sentiment visualizer},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883903},
doi = {10.1145/2883851.2883903},
abstract = {While much promise has been demonstrated in the learning analytics field with sentiment analysis, the analyses are typically post hoc. The Unizin Sentiment Visualizer demonstrates that the application of sentiment analysis in real-time provides a powerful new tool to support students in complex learning environments.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {552–553},
numpages = {2},
keywords = {Unizin, discussion, intervention, learning analytics, natural language processing, real time, real-time, sentiment analysis, student, text mining},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/3506860.3506896,
author = {Hunkins, Nicholas and Kelly, Sean and D'Mello, Sidney},
title = {“Beautiful work, you're rock stars!”: Teacher Analytics to Uncover Discourse that Supports or Undermines Student Motivation, Identity, and Belonging in Classrooms},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506896},
doi = {10.1145/3506860.3506896},
abstract = {From carefully crafted messages to flippant remarks, warm expressions to unfriendly grunts, teachers’ behaviors set the tone, expectations, and attitudes of the classroom. Thus, it is prudent to identify the ways in which teachers foster motivation, positive identity, and a strong sense of belonging through inclusive messaging and other interactions. We leveraged a new coding of teacher supportive discourse in 156 video clips from 73 6th to 8th grade math teachers from the archival Measures of Effective Teaching (MET) project. We trained Random Forest classifiers using verbal (words used) and paraverbal (acoustic-prosodic cues, e.g., speech rate) features to detect seven features of teacher discourse (e.g., public admonishment, autonomy supportive messages) from transcripts and audio, respectively. While both modalities performed over chance guessing, the specific language content was more predictive than paraverbal cues (mean correlation = .546 vs. .276); combining the two yielded no improvement. We examined the most predictive cues in order to gain a deeper understanding of the underlying messages in teacher talk. We discuss implications of our work for teacher analytics tools that aim to provide educators and researchers with insight into supportive discourse.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {230–238},
numpages = {9},
keywords = {Automated Feedback, Discourse Analysis, Natural Language Processing, Teacher Analytics},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3448139.3448169,
author = {Rodriguez, Fernando and Lee, Hye Rin and Rutherford, Teomara and Fischer, Christian and Potma, Eric and Warschauer, Mark},
title = {Using Clickstream Data Mining Techniques to Understand and Support First-Generation College Students in an Online Chemistry Course},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448169},
doi = {10.1145/3448139.3448169},
abstract = {Although online courses can provide students with a high-quality and flexible learning experience, one of the caveats is that they require high levels of self-regulation. This added hurdle may have negative consequences for first-generation college students. In order to better understand and support students’ self-regulated learning, we examined a fully online Chemistry course with high enrollment (N = 312) and a high percentage of first-generation college students (65.70%). Using students’ lecture video clickstream data, we created two indicators of self-regulated learning: lecture video completion and time management. Performing a k-means clustering on these indicators uncovered four distinct self-regulated learning patterns: (1) Early Planning, (2) Planning, (3) Procrastination, and (4) Low Engagement. Early Planning behaviors were especially important for course success—they consistently predicted higher final course grades, even after controlling for important demographic variables. Interestingly, first-generation college students classified as Early Planners achieved at similar levels as their non-first-generation peers, but first-generation students in the Low Engagement group had the lowest average grades among students. Overall, our results show that self-regulation may be an important skill for determining first-generation students’ STEM achievement, and targeting these skills may serve as a useful way to support their specific learning needs.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {313–322},
numpages = {10},
keywords = {Clickstream Data Mining, College Students, Online Learning, STEM, Self-Regulation, Underrepresented Students},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/2883851.2883895,
author = {Taraghi, Behnam and Saranti, Anna and Legenstein, Robert and Ebner, Martin},
title = {Bayesian modelling of student misconceptions in the one-digit multiplication with probabilistic programming},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883895},
doi = {10.1145/2883851.2883895},
abstract = {One-digit multiplication errors are one of the most extensively analysed mathematical problems. Research work primarily emphasises the use of statistics whereas learning analytics can go one step further and use machine learning techniques to model simple learning misconceptions. Probabilistic programming techniques ease the development of probabilistic graphical models (bayesian networks) and their use for prediction of student behaviour that can ultimately influence learning decision processes.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {449–453},
numpages = {5},
keywords = {Bayesian modelling, learning analytics, one-digit multiplication, probabilistic programming},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/3027385.3029441,
author = {Pardo, Abelardo and Mart\'{\i}nez-Maldonado, Roberto and Buckingham Shum, Simon and Schulte, Jurgen and McIntyre, Simon and Ga\v{s}evi\'{c}, Dragan and Gao, Jing and Siemens, George},
title = {Connecting data with student support actions in a course: a hands-on tutorial},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029441},
doi = {10.1145/3027385.3029441},
abstract = {The amount of data extracted from learning experiences has grown at an astonishing pace both in depth due to the increasing variety of data sources, and in breath with courses now being offered to massive student cohorts. However, in this emerging scenario instructors are now facing the challenge of connecting the knowledge emerging from data analysis with the provision of meaningful support actions to students within the context of an instructional design.The objective of this tutorial is to give attendees a set of hypothetical scenarios in which the knowledge extracted from a learning experience needs to be used to provide frequent personalized feedback to students.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {522–523},
numpages = {2},
keywords = {feedback, instructional design, learning analytics, predictive analytics, student support},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3448139.3448196,
author = {Sun, Zhiru and Chiarandini, Marco},
title = {An Exact Algorithm for Group Formation to Promote Collaborative Learning},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448196},
doi = {10.1145/3448139.3448196},
abstract = {Collaborative learning has been widely used to foster students’ communication and joint knowledge construction. However, the classification of learners into well-structured groups is one of the most challenging tasks in the field. The aim of this study is to propose a novel method to form intra-heterogeneous and inter-homogeneous groups based on relevant student characteristics. Such a method allows for the consideration of multiple student characteristics and can handle both numerical and categorical characteristic types simultaneously. It assumes that the teacher provides an order of importance of the characteristics, then it solves the grouping problem as a lexicographic optimization problem in the given order. We formulate the problem in mixed integer linear programming (MILP) terms and solve it to optimality. A pilot experiment was conducted with 29 college freshmen considering three general characteristics (i.e., 13 specific features) including knowledge level, demographic information, and motivation. Results of such an experiment demonstrate the validity and computational feasibility of the algorithmic approach. Large-scale studies are needed to assess the impact of the proposed grouping method on students’ learning experience and academic achievement.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {546–552},
numpages = {7},
keywords = {Computer-supported collaborative learning (CSCL), Group formation, Mixed integer linear programming (MILP), student-project assignment},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3170358.3170406,
author = {Ochoa, Xavier and Dom\'{\i}nguez, Federico and Guam\'{a}n, Bruno and Maya, Ricardo and Falcones, Gabriel and Castells, Jaime},
title = {The RAP system: automatic feedback of oral presentation skills using multimodal analysis and low-cost sensors},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170406},
doi = {10.1145/3170358.3170406},
abstract = {Developing communication skills in higher education students could be a challenge to professors due to the time needed to provide formative feedback. This work presents RAP, a scalable system to provide automatic feedback to entry-level students to develop basic oral presentation skills. The system improves the state-of-the-art by analyzing posture, gaze, volume, filled pauses and the slides of the presenters through data captured by very low-cost sensors. The system also provides an off-line feedback report with multimodal recordings of their performance. An initial evaluation of the system indicates that the system's feedback highly agrees with human feedback and that students considered that feedback useful to develop their oral presentation skills.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {360–364},
numpages = {5},
keywords = {filled-pauses, gaze, multimodal learning analytics, posture},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3027385.3027386,
author = {Lee, Alwyn Vwen Yen and Tan, Seng Chee},
title = {Temporal analytics with discourse analysis: tracing ideas and impact on communal discourse},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027386},
doi = {10.1145/3027385.3027386},
abstract = {This paper presents a study of temporal analytics and discourse analysis of an online discussion, through investigation of a group of 13 in-service teachers and 2 instructors. A discussion forum consisting of 281 posts on an online collaborative learning environment was investigated. A text-mining tool was used to discover keywords from the discourse, and through social network analysis based on these keywords, a significant presence of relevant and promising ideas within discourse was revealed. However, uncovering the key ideas alone is insufficient to clearly explain students' level of understanding regarding the discussed topics. A more thorough analysis was thus performed by using temporal analytics with step-wise discourse analysis to trace the ideas and determine their impact on communal discourse. The results indicated that most ideas within the discourse could be traced to the origin of a set of improvable ideas, which impacted and also increased the community's level of interest in sharing and discussing ideas through discourse.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {120–127},
numpages = {8},
keywords = {discourse analysis, idea measurement, learning analytics, social network analysis, temporality},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3027447,
author = {Di Mitri, Daniele and Scheffel, Maren and Drachsler, Hendrik and B\"{o}rner, Dirk and Ternier, Stefaan and Specht, Marcus},
title = {Learning pulse: a machine learning approach for predicting performance in self-regulated learning using multimodal data},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027447},
doi = {10.1145/3027385.3027447},
abstract = {Learning Pulse explores whether using a machine learning approach on multimodal data such as heart rate, step count, weather condition and learning activity can be used to predict learning performance in self-regulated learning settings. An experiment was carried out lasting eight weeks involving PhD students as participants, each of them wearing a Fitbit HR wristband and having their application on their computer recorded during their learning and working activities throughout the day. A software infrastructure for collecting multimodal learning experiences was implemented. As part of this infrastructure a Data Processing Application was developed to pre-process, analyse and generate predictions to provide feedback to the users about their learning performance. Data from different sources were stored using the xAPI standard into a cloud-based Learning Record Store. The participants of the experiment were asked to rate their learning experience through an Activity Rating Tool indicating their perceived level of productivity, stress, challenge and abilities. These self-reported performance indicators were used as markers to train a Linear Mixed Effect Model to generate learner-specific predictions of the learning performance. We discuss the advantages and the limitations of the used approach, highlighting further development points.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {188–197},
numpages = {10},
keywords = {biosensors, learning analytics, machine learning, multimodal data, wearable enhanced learning},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/2723576.2723593,
author = {Kennedy, Gregor and Coffrin, Carleton and de Barba, Paula and Corrin, Linda},
title = {Predicting success: how learners' prior knowledge, skills and activities predict MOOC performance},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723593},
doi = {10.1145/2723576.2723593},
abstract = {While MOOCs have taken the world by storm, questions remain about their pedagogical value and high rates of attrition. In this paper we argue that MOOCs which have open entry and open curriculum structures, place pressure on learners to not only have the requisite knowledge and skills to complete the course, but also the skills to traverse the course in adaptive ways that lead to success. The empirical study presented in the paper investigated the degree to which students' prior knowledge and skills, and their engagement with the MOOC as measured through learning analytics, predict end-of-MOOC performance. The findings indicate that prior knowledge is the most significant predictor of MOOC success followed by students' ability to revise and revisit their previous work.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {136–140},
numpages = {5},
keywords = {MOOCs, engagement, learning analytics, prior knowledge},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/3448139.3448180,
author = {Lin, Yiwen and Dowell, Nia and Godfrey, Andrew},
title = {Skills Matter: Modeling the relationship between decision making processes and collaborative problem-solving skills during Hidden Profile Tasks},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448180},
doi = {10.1145/3448139.3448180},
abstract = {Collaborative problem-solving (CPS) is one of the most essential 21st century skills for success across educational and professional settings. The hidden-profile paradigm is one of the most prominent avenues of studying group decision making and underlying issues in information sharing. Previous research on the hidden-profile paradigm has primarily focused on static constructs (e.g., group size, group expertise), or on the information itself (whether certain pieces of information is being shared). In the current study, we propose a lens on individual and group’s collaborative problem-solving skills, to explore the relationships between dynamic discourse processes and decision making in a distributed information environment. Specifically, we sought to examine CPS skills in association with decision change and productive decision-making. Our results suggest that while sharing information has significantly positive association with decision change and effective decision-making, other aspects of social processes appear to be negatively correlated with these outcomes. Cognitive CPS skills, however, exhibit a strong positive relationship with making a (productive) change in students final decisions. We also find that these results are more pronounced at the group level, particularly with cognitive CPS skills. Our study shed lights on a more nuanced picture of how social and cognitive CPS interactions are related to effective information sharing and decision making in collaborative problem-solving interactions.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {428–437},
numpages = {10},
keywords = {Hidden-profile paradigm, collaborative problem solving, decision making, group processes},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448152,
author = {Schlotterbeck, Danner and Uribe, Pablo and Araya, Roberto and Jimenez, Abelino and Caballero, Daniela},
title = {What Classroom Audio Tells About Teaching: A Cost-effective Approach for Detection of Teaching Practices Using Spectral Audio Features},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448152},
doi = {10.1145/3448139.3448152},
abstract = {Acoustic features and machine learning models have been recently proposed as promising tools to analyze lessons. Furthermore, acoustic patterns, both in the time and spectral domain, have been found to be related to teacher pedagogical practices. Nonetheless, most of previous work relies on expensive or third party equipment, limiting its scalability, and additionally, it is mainly used for diarization. Instead, in this work we present a cost-effective approach to identify teachers’ practices according to three categories (Presenting, Administration, and Guiding) which are compiled from the Classroom Observation Protocol for Undergraduate STEM. Particularly, we record teachers’ lessons using low-cost microphones connected to their smartphones. We then compute the mean and standard deviation of the amplitude, Mel spectrogram, and Mel Frequency Cepstral coefficients of the recordings to train supervised models for the task of predicting three categories compiled from the Classroom Observation Protocol for Undergraduate STEM. We found that spectral features perform better at the task of predicting teachers’ activities along the lessons and that our models can predict the presence of the two most common teaching practices with over 80% of accuracy and good discriminative power. Finally, with these models, we found that using audio obtained from the teachers’ smartphones it is also possible to automatically discriminate between sessions where students are using or not an online platform. This approach is important for teachers and other stakeholders who could use an automatic and cost-effective tool for analyzing teaching practices.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {132–140},
numpages = {9},
keywords = {COPUS, Classroom Sound, Spectral Audio Features, Teacher Activity Detection},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448209,
author = {Nguyen, Huy and Lim, Michelle and Moore, Steven and Nyberg, Eric and Sakr, Majd and Stamper, John},
title = {Exploring Metrics for the Analysis of Code Submissions in an Introductory Data Science Course},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448209},
doi = {10.1145/3448139.3448209},
abstract = {While data science education has gained increased recognition in both academic institutions and industry, there has been a lack of research on automated coding assessment for novice students. Our work presents a first step in this direction, by leveraging the coding metrics from traditional software engineering (Halstead Volume and Cyclomatic Complexity) in combination with those that reflect a data science project’s learning objectives (number of library calls and number of common library calls with the solution code). Through these metrics, we examined the code submissions of 97 students across two semesters of an introductory data science course. Our results indicated that the metrics can identify cases where students had overly complicated codes and would benefit from scaffolding feedback. The number of library calls, in particular, was also a significant predictor of changes in submission score and submission runtime, which highlights the distinctive nature of data science programming. We conclude with suggestions for extending our analyses towards more actionable intervention strategies, for example by tracking the fine-grained submission grading outputs throughout a student’s submission history, to better model and support them in their data science learning process.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {632–638},
numpages = {7},
keywords = {Coding Metrics, Data Science Education, Linear Mixed Model, Programming Analysis},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448208,
author = {Tavakoli, Mohammadreza and Elias, Mirette and Kismih\'{o}k, G\'{a}bor and Auer, S\"{o}ren},
title = {Metadata Analysis of Open Educational Resources},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448208},
doi = {10.1145/3448139.3448208},
abstract = {Open Educational Resources (OERs) are openly licensed educational materials that are widely used for learning. Nowadays, many online learning repositories provide millions of OERs. Therefore, it is exceedingly difficult for learners to find the most appropriate OER among these resources. Subsequently, the precise OER metadata is critical for providing high-quality services such as search and recommendation. Moreover, metadata facilitates the process of automatic OER quality control as the continuously increasing number of OERs makes manual quality control extremely difficult. This work uses the metadata of 8,887 OERs to perform an exploratory data analysis on OER metadata. Accordingly, this work proposes metadata-based scoring and prediction models to anticipate the quality of OERs. Based on the results, our analysis demonstrated that OER metadata and OER content qualities are closely related, as we could detect high-quality OERs with an accuracy of 94.6%. Our model was also evaluated on 884 educational videos from Youtube to show its applicability on other educational repositories.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {626–631},
numpages = {6},
keywords = {Exploratory Analysis, Machine Learning, Metadata Analysis, OER, Open Educational Resources, Prediction Models},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448197,
author = {Chen, Lujie Karen},
title = {Timing of Support in One-on-one Math Problem Solving Coaching: A Survival Analysis Approach with Multimodal Data},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448197},
doi = {10.1145/3448139.3448197},
abstract = {In this paper, we explore a kind of teaching-oriented temporal analytics on the timing of support in the context of one-on-one math problem-solving coaching. We build the analytical framework upon the human-human multimodal interaction data collected from the naturalist environments. We demonstrated the potential utility of leveraging survival analysis, a class of statistical methods to model time-to-event data, to gain insights into the timing decisions. We shed light on the heterogeneity of coaching decisions as to when to render support in connection to the problem-solving stages, coaching dyads, as well as the pre-intervention event characteristics. This work opens future avenues into a different type of human tutoring study supported by multimodal data, computational models, and statistical frameworks. This model framework may yield useful reflective teaching analytics to tutors, coaches, or teachers when further developed. We also envision that those analyses may ultimately inform the design of AI-supported autonomous agents that could learn the tutorial interaction logic from empirical data.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {553–558},
numpages = {6},
keywords = {human tutoring studies, learning analtyics, multimodal},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448205,
author = {Shi, Yang and Shah, Krupal and Wang, Wengran and Marwan, Samiha and Penmetsa, Poorvaja and Price, Thomas},
title = {Toward Semi-Automatic Misconception Discovery Using Code Embeddings},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448205},
doi = {10.1145/3448139.3448205},
abstract = {Understanding students’ misconceptions is important for effective teaching and assessment. However, discovering such misconceptions manually can be time-consuming and laborious. Automated misconception discovery can address these challenges by highlighting patterns in student data, which domain experts can then inspect to identify misconceptions. In this work, we present a novel method for the semi-automated discovery of problem-specific misconceptions from students’ program code in computing courses, using a state-of-the-art code classification model. We trained the model on a block-based programming dataset and used the learned embedding to cluster incorrect student submissions. We found these clusters correspond to specific misconceptions about the problem and would not have been easily discovered with existing approaches. We also discuss potential applications of our approach and how these misconceptions inform domain-specific insights into students’ learning processes.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {606–612},
numpages = {7},
keywords = {Automatic Assessment, Code Analysis, Learning Representation, Neural Network},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3506860.3506889,
author = {Li, Jiayu and Li, Huiyong and Majumdar, Rwitajit and Yang, Yuanyuan and Ogata, Hiroaki},
title = {Self-directed Extensive Reading Supported with GOAL System: Mining Sequential Patterns of Learning Behavior and Predicting Academic Performance},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506889},
doi = {10.1145/3506860.3506889},
abstract = {Self-directed learning (SDL) is an important skill in the 21st century, while the understanding of its process in behavior has not been well explored. Analysis of the sequential behavior patterns in SDL and the relations with students’ academic performance could help to advance our understanding of SDL in theory and practice. In this study, we mined the behavioral sequences of self-directed extensive reading from students’ learning and self-directed behavioral logs using differential pattern mining technique. Furthermore, we built models to predict students’ academic performance using the conventional behavior frequency features and the behavior sequence features. Experimental results identified 14 sequential patterns of SDL behaviors in the high-performance student group. The prediction model revealed the importance of sequential patterns in SDL behavior, which was built with an acceptable AUC. These findings suggested that several SDL strategies in behavior contribute to students’ academic performance, such as analysis learning status before planning, planning before learning, monitoring after learning.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {472–477},
numpages = {6},
keywords = {Machine learning, Self-directed learning, Sequence mining, Teaching/learning strategies},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3170358.3170388,
author = {Boroujeni, Mina Shirvani and Dillenbourg, Pierre},
title = {Discovery and temporal analysis of latent study patterns in MOOC interaction sequences},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170388},
doi = {10.1145/3170358.3170388},
abstract = {Capturing students' behavioral patterns through analysis of sequential interaction logs is an important task in educational data mining and could enable more effective and personalized support during the learning processes. This study aims at discovery and temporal analysis of learners' study patterns in MOOC assessment periods. We propose two different methods to achieve this goal. First, following a hypothesis-driven approach, we identify learners' study patterns based on their interaction with lectures and assignments. Through clustering of study pattern sequences, we capture different longitudinal activity profiles among learners and describe their properties. Second, we propose a temporal clustering pipeline for unsupervised discovery of latent patterns in learners' interaction data. We model and cluster activity sequences at each time step and perform cluster matching to enable tracking learning behaviours over time. Our proposed pipeline is general and applicable in different learning environments such as MOOC and ITS. Moreover, it allows for modeling and temporal analysis of interaction data at different levels of actions granularity and time resolution. We demonstrate the application of this method for detecting latent study patterns in a MOOC course.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {206–215},
numpages = {10},
keywords = {EDM, LA, MOOCs, clustering, learning analytics, markov model, sequence mining, study pattern, temporal analysis},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/2567574.2567591,
author = {M\'{e}ndez, Gonzalo and Ochoa, Xavier and Chiluiza, Katherine},
title = {Techniques for data-driven curriculum analysis},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567591},
doi = {10.1145/2567574.2567591},
abstract = {One of the key promises of Learning Analytics research is to create tools that could help educational institutions to gain a better insight of the inner workings of their programs, in order to tune or correct them. This work presents a set of simple techniques that applied to readily available historical academic data could provide such insights. The techniques described are real course difficulty estimation, dependance estimation, curriculum coherence, dropout paths and load/performance graph. The description of these techniques is accompanied by its application to real academic data from a Computer Science program. The results of the analysis are used to obtain recommendations for curriculum re-design.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {148–157},
numpages = {10},
keywords = {curriculum design, learning analytics},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/3613905.3650936,
author = {Anthraper, Nisha and Javiya, Prachee and Iluru, Sai and Chen, Lujie Karen and Kleinsmith, Andrea},
title = {PeerConnect: Co-Designing a Peer-Mentoring Support System with Computing Transfer Students},
year = {2024},
isbn = {9798400703317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613905.3650936},
doi = {10.1145/3613905.3650936},
abstract = {In the US, nearly half of the STEM undergraduates begin their academic careers at community colleges. Transferring to four-year institutions can be challenging. Evidence suggests that mentoring can help by increasing a sense of belonging and retention. We engaged mentors and mentees from a pilot mentoring program for new transfer students in computing majors at a minority-serving institution in the Northeastern US in a co-design workshop to understand their needs and requirements for a peer-mentoring system, PeerConnect. PeerConnect aims to foster transfer students’ academic and social engagement, increase self-efficacy and belonging, and develop students’ self-regulated learning skills. Preliminary results show that students want features that push the system beyond merely measuring engagement to actively promoting it. This study contributes to HCI and CSCW work in designing support systems for mentoring and peer support programs in educational settings and to the emerging literature on student-centered learning analytics systems.},
booktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {262},
numpages = {7},
keywords = {co-design, learning analytics, peer mentoring, sense of belonging},
location = {Honolulu, HI, USA},
series = {CHI EA '24}
}

@inproceedings{10.1145/3649217.3653586,
author = {Pozzan, Gabriele and Padova, Costanza and Montuori, Chiara and Arf\'{e}, Barbara and Vardanega, Tullio},
title = {Experimental Analysis of First-Grade Students' Block-Based Programming Problem Solving Processes},
year = {2024},
isbn = {9798400706004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649217.3653586},
doi = {10.1145/3649217.3653586},
abstract = {This work presents an experimental analysis of first-grade students' block-based programming trajectories. These trajectories consist of edit-level program snapshots that capture learners' problem-solving processes in a navigational microworld. Our results highlight the potential of this fine-grained data capture. Snapshot frequencies in trajectories collected before and after a coding intervention showcase the collective progress of the learners. Graph visualizations, in which nodes represent snapshots and directed edges code edits, highlight strategies, pitfalls and debugging procedures. Individual programming trajectories shed light on details of learners' problem-solving processes that less granular analysis would conceal. Various works in the field of Learning Analytics research show the usefulness of collecting fine-grained process data that proceed from programming activities. However, how to analyze this data is still an open question and research on the subject is in an experimental phase. We contribute to this experimentation by analyzing and discussing results collected from 30 first-grade students in a pretest-posttest study.},
booktitle = {Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 1},
pages = {143–149},
numpages = {7},
keywords = {block-based programming, computational thinking, learning analytics, programming trajectories},
location = {Milan, Italy},
series = {ITiCSE 2024}
}

@inproceedings{10.1145/3027385.3029431,
author = {Ringtved, Ulla and Milligan, Sandra and Corrin, Linda and Littlejohn, Allison and Law, Nancy},
title = {DesignLAK17: quality metrics and indicators for analytics of assessment design at scale},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029431},
doi = {10.1145/3027385.3029431},
abstract = {Notions of what constitutes quality in design in traditional on-campus or online teaching and learning may not always translate into scaled digital environments. The DesignLAK17 workshop builds on the DesignLAK16 workshop to explore one aspect of this theme, namely the opportunities arising from the use of analytics in scaled assessment design. New paradigms for learning design are exploiting the distinctive characteristics and potentials of analytics, trace data and newer kinds of sensory data usable on digital platforms to transform assessment. But, characteristics of quality assessment design need to be reconsidered, and new metrics for capturing quality are required. This symposium and workshop focuses on what might be appropriate quality metrics and indicators for assessment design in scaled learning. It aims to build a community of interest round the topic, to share perspectives, and to generate design and research ideas.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {508–509},
numpages = {2},
keywords = {assessment, feedback, learning analytics, learning at scale, learning design, scaled courses},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/2883851.2883897,
author = {Pijeira-D\'{\i}az, H\'{e}ctor J. and Drachsler, Hendrik and J\"{a}rvel\"{a}, Sanna and Kirschner, Paul A.},
title = {Investigating collaborative learning success with physiological coupling indices based on electrodermal activity},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883897},
doi = {10.1145/2883851.2883897},
abstract = {Collaborative learning is considered a critical 21st century skill. Much is known about its contribution to learning, but still investigating a process of collaboration remains a challenge. This paper approaches the investigation on collaborative learning from a psychophysiological perspective. An experiment was set up to explore whether biosensors can play a role in analysing collaborative learning. On the one hand, we identified five physiological coupling indices (PCIs) found in the literature: 1) Signal Matching (SM), 2) Instantaneous Derivative Matching (IDM), 3) Directional Agreement (DA), 4) Pearson's correlation coefficient (PCC) and the 5) Fisher's z-transform (FZT) of the PCC. On the other hand, three collaborative learning measurements were used: 1) collaborative will (CW), 2) collaborative learning product (CLP) and 3) dual learning gain (DLG). Regression analyses showed that out of the five PCIs, IDM related the most to CW and was the best predictor of the CLP. Meanwhile, DA predicted DLG the best. These results play a role in determining informative collaboration measures for designing a learning analytics, biofeedback dashboard.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {64–73},
numpages = {10},
keywords = {biosensors, collaborative learning, electrodermal activity, learning analytics, physiological coupling indices},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/3170358.3170393,
author = {Nacu, Denise and Baltes, Jennifer and Hamid, Taha and Gemmell, Jonathan and Pinkard, Nichole},
title = {A framework for developing metrics of youth engagement in informal learning environments},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170393},
doi = {10.1145/3170358.3170393},
abstract = {This paper proposes a framework which aims to leverage data from informal learning environments to provide insights about youth engagement for various stakeholders. To explore the framework, we created metrics to examine the engagement of 98 middle school-aged girls during a 20-week STEM program using attendance records and log data from an online learning platform coded to reflect 21st century learning activities. We present preliminary analyses using the metrics, focusing on how they can help stakeholders understand engagement and equity of participation.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {310–314},
numpages = {5},
keywords = {engagement, informal learning, learning analytics, log analysis},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3027385.3027426,
author = {Huptych, Michal and Bohuslavek, Michal and Hlosta, Martin and Zdrahal, Zdenek},
title = {Measures for recommendations based on past students' activity},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027426},
doi = {10.1145/3027385.3027426},
abstract = {This paper introduces two measures for the recommendation of study materials based on students' past study activity. We use records from the Virtual Learning Environment (VLE) and analyse the activity of previous students. We assume that the activity of past students represents patterns, which can be used as a basis for recommendations to current students.The measures we define are Relevance, for description of a supposed VLE activity derived from previous students of the course, and Effort, that represents the actual effort of individual current students. Based on these measures, we propose a composite measure, which we call Importance.We use data from the previous course presentations to evaluate of the consistency of students' behaviour. We use correlation of the defined measures Relevance and Average Effort to evaluate the behaviour of two different student cohorts and the Root Mean Square Error to measure the deviation of Average Effort and individual student Effort.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {404–408},
numpages = {5},
keywords = {effort, learning analytics, learning strategy, recommendation, relevance, student retention},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3506860.3506915,
author = {Li, Boyi and Minematsu, Tsubasa and Taniguchi, Yuta and Okubo, Fumiya and Shimada, Atsushi},
title = {How Does Analysis of Handwritten Notes Provide Better Insights for Learning Behavior?},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506915},
doi = {10.1145/3506860.3506915},
abstract = {Handwritten notes are one important component of students’ learning process, which is used to record what they have learned in class or tease out knowledge after class for reflection and further strengthen the learning effect. It also helps a lot during review. We hope to divide handwritten notes (Japanese) into different parts, such as text, mathematical expressions, charts, etc., and quantify them to evaluate the condition of the notes and compare them among students. At the same time, data on students’ learning behaviors in the course are collected through the online education platform, such as the use time of textbook and attendance, as well as the scores of the online quiz and course grade. In this paper, the analysis of the relationship between the segmentation results of handwritten notes and learning behavior are reported, as well as the research on automatic page segmentation based on deep learning.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {549–555},
numpages = {7},
keywords = {handwritten note, learning behavior, page segmentation},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3303772.3303786,
author = {Lee, Jinseok and Yeung, Dit-Yan},
title = {Knowledge Query Network for Knowledge Tracing: How Knowledge Interacts with Skills},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303786},
doi = {10.1145/3303772.3303786},
abstract = {Knowledge Tracing (KT) is to trace the knowledge of students as they solve a sequence of problems represented by their related skills. This involves abstract concepts of students' states of knowledge and the interactions between those states and skills. Therefore, a KT model is designed to predict whether students will give correct answers and to describe such abstract concepts. However, existing methods either give relatively low prediction accuracy or fail to explain those concepts intuitively. In this paper, we propose a new model called Knowledge Query Network (KQN) to solve these problems. KQN uses neural networks to encode student learning activities into knowledge state and skill vectors, and models the interactions between the two types of vectors with the dot product. Through this, we introduce a novel concept called probabilistic skill similarity that relates the pairwise cosine and Euclidean distances between skill vectors to the odds ratios of the corresponding skills, which makes KQN interpretable and intuitive.On four public datasets, we have carried out experiments to show the following: 1. KQN outperforms all the existing KT models based on prediction accuracy. 2. The interaction between the knowledge state and skills can be visualized for interpretation. 3. Based on probabilistic skill similarity, a skill domain can be analyzed with clustering using the distances between the skill vectors of KQN. 4. For different values of the vector space dimensionality, KQN consistently exhibits high prediction accuracy and a strong positive correlation between the distance matrices of the skill vectors.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {491–500},
numpages = {10},
keywords = {Deep Learning, Domain Modeling, Educational Data Mining, Intelligent Tutoring Systems, Knowledge Modeling, Knowledge Tracing, Learner Modeling, Learning Analytics, Massive Open Online Courses},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3027385.3027395,
author = {Mutahi, Juliet and Kinai, Andrew and Bore, Nelson and Diriye, Abdigani and Weldemariam, Komminist},
title = {Studying engagement and performance with learning technology in an African classroom},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027395},
doi = {10.1145/3027385.3027395},
abstract = {In this paper, we study the engagement and performance of students in a classroom using a system the Cognitive Learning Companion (CLC). CLC is designed to keep track of the relationship between the student, content interaction and learning progression. It also provides evidence-based engagement-oriented actionable insights to teachers by assessing information from a sensor-rich instrumented learning environment in order to infer a learner's cognitive and affective states. Data captured from the instrumented environment is aggregated and analyzed to create interlinked insights helping teachers identify how students engage with learning content and view their performance records on selected assignments. We conducted a 1 month pilot with 27 learners in a primary school in Nairobi, Kenya during their maths and science instructional periods. We present our primary analysis of content-level interactions and engagement at the individual student and classroom level.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {148–152},
numpages = {5},
keywords = {developing countries, education, engagement, learning analytics, mobile development},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/2723576.2723662,
author = {Corrin, Linda and de Barba, Paula},
title = {How do students interpret feedback delivered via dashboards?},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723662},
doi = {10.1145/2723576.2723662},
abstract = {Providing feedback directly to students on their engagement and performance in educational activities is important to supporting students' learning. However, questions have been raised whether such data representations are adequate to inform reflection, planning and monitoring of students' learning strategies. In this poster we present an investigation of how students interpret feedback delivered via learning analytics dashboards. The findings indicated that most students were able to articulate an interpretation of the feedback presented through the dashboard to identify gaps between their expected and actual performance to inform changes to their study strategies. However, there was also evidence of uncertain interpretation both in terms of the format of the visualization of the feedback and their inability to understand the connection between the feedback and their current strategies. The findings have been used to inform recommendations for ways to enhance the effectiveness of the delivery of feedback through dashboards to provide value to students in developing effective learning strategies to meet their educational goals.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {430–431},
numpages = {2},
keywords = {dashboards, feedback, learning analytics, self-regulated learning},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2883851.2883932,
author = {Robinson, Carly and Yeomans, Michael and Reich, Justin and Hulleman, Chris and Gehlbach, Hunter},
title = {Forecasting student achievement in MOOCs with natural language processing},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883932},
doi = {10.1145/2883851.2883932},
abstract = {Student intention and motivation are among the strongest predictors of persistence and completion in Massive Open Online Courses (MOOCs), but these factors are typically measured through fixed-response items that constrain student expression. We use natural language processing techniques to evaluate whether text analysis of open responses questions about motivation and utility value can offer additional capacity to predict persistence and completion over and above information obtained from fixed-response items. Compared to simple benchmarks based on demographics, we find that a machine learning prediction model can learn from unstructured text to predict which students will complete an online course. We show that the model performs well out-of-sample, compared to a standard array of demographics. These results demonstrate the potential for natural language processing to contribute to predicting student success in MOOCs and other forms of open online learning.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {383–387},
numpages = {5},
keywords = {MOOCS, learning analytics, motivation},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/3027385.3027411,
author = {Davis, Dan and Jivet, Ioana and Kizilcec, Ren\'{e} F. and Chen, Guanliang and Hauff, Claudia and Houben, Geert-Jan},
title = {Follow the successful crowd: raising MOOC completion rates through social comparison at scale},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027411},
doi = {10.1145/3027385.3027411},
abstract = {Social comparison theory asserts that we establish our social and personal worth by comparing ourselves to others. In in-person learning environments, social comparison offers students critical feedback on how to behave and be successful. By contrast, online learning environments afford fewer social cues to facilitate social comparison. Can increased availability of such cues promote effective self-regulatory behavior and achievement in Massive Open Online Courses (MOOCs)? We developed a personalized feedback system that facilitates social comparison with previously successful learners based on an interactive visualization of multiple behavioral indicators. Across four randomized controlled trials in MOOCs (overall N = 33, 726), we find: (1) the availability of social comparison cues significantly increases completion rates, (2) this type of feedback benefits highly educated learners, and (3) learners' cultural context plays a significant role in their course engagement and achievement.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {454–463},
numpages = {10},
keywords = {cultural differences, feedback, framing, learning analytics, massive open online course, social comparison},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3576050.3576109,
author = {Gurung, Ashish and Baral, Sami and Vanacore, Kirk P. and Mcreynolds, Andrew A. and Kreisberg, Hilary and Botelho, Anthony F. and Shaw, Stacy T. and Hefferna, Neil T.},
title = {Identification, Exploration, and Remediation: Can Teachers Predict Common Wrong Answers?},
year = {2023},
isbn = {9781450398657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576050.3576109},
doi = {10.1145/3576050.3576109},
abstract = {Prior work analyzing tutoring sessions provided evidence that highly effective tutors, through their interaction with students and their experience, can perceptively recognize incorrect processes or “bugs” when students incorrectly answer problems. Researchers have studied these tutoring interactions examining instructional approaches to address incorrect processes and observed that the format of the feedback can influence learning outcomes. In this work, we recognize the incorrect answers caused by these buggy processes as Common Wrong Answers (CWAs). We examine the ability of teachers and instructional designers to identify CWAs proactively. As teachers and instructional designers deeply understand the common approaches and mistakes students make when solving mathematical problems, we examine the feasibility of proactively identifying CWAs and generating Common Wrong Answer Feedback (CWAFs) as a formative feedback intervention for addressing student learning needs. As such, we analyze CWAFs in three sets of analyses. We first report on the accuracy of the CWAs predicted by the teachers and instructional designers on the problems across two activities. We then measure the effectiveness of the CWAFs using an intent-to-treat analysis. Finally, we explore the existence of personalization effects of the CWAFs for the students working on the two mathematics activities.},
booktitle = {LAK23: 13th International Learning Analytics and Knowledge Conference},
pages = {399–410},
numpages = {12},
keywords = {Automated Feedback, Buggy Messages, Causal Inference, Common Wrong Answers},
location = {Arlington, TX, USA},
series = {LAK2023}
}

@inproceedings{10.1145/3506860.3506906,
author = {L. Leite, Walter and Roy, Samrat and Chakraborty, Nilanjana and Michailidis, George and Huggins-Manley, A. Corinne and D'Mello, Sidney and Shirani Faradonbeh, Mohamad Kazem and Jensen, Emily and Kuang, Huan and Jing, Zeyuan},
title = {A novel video recommendation system for algebra: An effectiveness evaluation study},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506906},
doi = {10.1145/3506860.3506906},
abstract = {This study presents a novel video recommendation system for an algebra virtual learning environment (VLE) that leverages ideas and methods from engagement measurement, item response theory, and reinforcement learning. Following Vygotsky's Zone of Proximal Development (ZPD) theory, but considering low affect and high affect students separately, we developed a system of five categories of video recommendations: 1) Watch new video; 2) Review current topic video with a new tutor; 3) Review segment of current video with current tutor; 4) Review segment of current video with a new tutor; 5) Watch next video in curriculum sequence. The category of recommendation was determined by student scores on a quiz and a sensor-free engagement detection model. New video recommendations (i.e., category 1) were selected based on a novel reinforcement learning algorithm that takes input from an item response theory model. The recommendation system was evaluated in a large field experiment, both before and after school closures due to the COVID-19 pandemic. The results show evidence of effectiveness of the video recommendation algorithm during the period of normal school operations, but the effect disappears after school closures. Implications for teacher orchestration of technology for normal classroom use and periods of school closure are discussed.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {294–303},
numpages = {10},
keywords = {algebra, effectiveness study, engagement detection, item response theory, recommender system},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3027385.3029455,
author = {Edwards, Rebecca L. and Davis, Sarah K. and Hadwin, Allyson F. and Milford, Todd M.},
title = {Using predictive analytics in a self-regulated learning university course to promote student success},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029455},
doi = {10.1145/3027385.3029455},
abstract = {Prior research offers evidence that differing levels of student engagement are associated with different outcomes in terms of performance. In this study, we investigating the efficacy of a model of behavioural and agentic engagement to predict student performance (low, middle, high) at four timepoints in a semester. The model was significant at all four timepoints. Measures of behavioural and agentic engagement predicted membership across the three groups differently. With a few exceptions, these differences were consistent across timepoints. Looking at variations in student engagement across time can be used to target interventions to support student success at the undergraduate level.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {556–557},
numpages = {2},
keywords = {higher education, learning analytics, predictive modeling, self-regulated learning, student engagement},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3448139.3448175,
author = {Lall\'{e}, S\'{e}bastien and Yal\c{c}\i{}n, \"{O}zge Nilay and Conati, Cristina},
title = {Combining Data-Driven Models and Expert Knowledge for Personalized Support to Foster Computational Thinking Skills},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448175},
doi = {10.1145/3448139.3448175},
abstract = {Game-Design (GD) environments show promise in fostering Computational Thinking (CT) skills at a young age. However, such environments can be challenging to some students due to their highly open-ended nature. We propose to alleviate this difficulty by learning interpretable student models from data that can drive personalization of a real-world GD learning environment to the student’s needs. We apply our approach on a dataset collected in ecological settings and evaluate the ability of the generated student models at predicting ineffective learning behaviors over the course of the interaction. We then discuss how these behaviors can be used to define personalized support in GD learning activities, by conducting extensive interviews with experienced instructors.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {375–385},
numpages = {11},
keywords = {Computational Thinking, Educational Data Mining, Game Design, Open-Ended Learning Environments, Student Modeling},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/2723576.2723641,
author = {Drachsler, Hendrik and Dietze, Stefan and Herder, Eelco and d'Aquin, Mathieu and Taibi, Davide and Scheffel, Maren},
title = {The 3rd LAK data competition},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723641},
doi = {10.1145/2723576.2723641},
abstract = {The LAK Data Challenge 2015 continues the research efforts of the previous data competitions in 2013 and 2014 by stimulating research on the evolving fields Learning Analytics (LA) and Educational Data Mining (EDM). Building on a series of activities of the LinkedUp project, the challenge aims to generate new insights and analysis on the LA &amp; EDM disciplines and is supported through the LAK Dataset - a unique corpus of LA &amp; EDM literature, exposed in structured and machine-readable formats.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {396–397},
numpages = {2},
keywords = {data mining, learning analytics, linked data, visualization},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2883851.2883859,
author = {Mol, Stefan and Kobayashi, Vladimer and Kismih\'{o}k, G\'{a}bor and Zhao, Catherine},
title = {Learning through goal setting},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883859},
doi = {10.1145/2883851.2883859},
abstract = {Despite the mounting evidence supporting the role that goal setting has on the learning process, there seems to be only a handful of studies that directly investigate goal setting in the context of Learning Analytics (LA). Although investigations have incorporated elements of goal setting, the attention afforded to theory and operationalization have been modest. In this workshop we plan to position goal setting at the forefront of LA research. The workshop will serve as a venue to bring together researchers interested in advancing Goal Setting (GS) research in the LA field. Topics include: (1) GS theory and measurement; (2) analysis and visualization of GS data; (3) strategies for integrating GS in the learning experience; and (4) implementation of GS technologies. Participants who need tools to execute their GS ideas and those who already have tools and are exploring better ways to integrate a goal setting feature can gain a lot from this workshop. Moreover, participants will have the opportunity to contribute to the conceptualization and staging of GS ideas in LA research.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {512–513},
numpages = {2},
keywords = {goal setting, learning analytics, learning record store},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/3027385.3027449,
author = {Hlosta, Martin and Zdrahal, Zdenek and Zendulka, Jaroslav},
title = {Ouroboros: early identification of at-risk students without models based on legacy data},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027449},
doi = {10.1145/3027385.3027449},
abstract = {This paper focuses on the problem of identifying students, who are at risk of failing their course. The presented method proposes a solution in the absence of data from previous courses, which are usually used for training machine learning models. This situation typically occurs in new courses. We present the concept of a "self-learner" that builds the machine learning models from the data generated during the current course. The approach utilises information about already submitted assessments, which introduces the problem of imbalanced data for training and testing the classification models.There are three main contributions of this paper: (1) the concept of training the models for identifying at-risk students using data from the current course, (2) specifying the problem as a classification task, and (3) tackling the challenge of imbalanced data, which appears both in training and testing data.The results show the comparison with the traditional approach of learning the models from the legacy course data, validating the proposed concept.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {6–15},
numpages = {10},
keywords = {imbalanced data, learning analytics, predictive analytics, self-learning, student retention},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/2567574.2567603,
author = {Clow, Doug},
title = {Data wranglers: human interpreters to help close the feedback loop},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567603},
doi = {10.1145/2567574.2567603},
abstract = {Closing the feedback loop to improve learning is at the heart of good learning analytics practice. However, the quantity of data, and the range of different data sources, can make it difficult to take systematic action on that data. Previous work in the literature has emphasised the need for and value of human meaning-making in the process of interpretation of data to transform it in to actionable intelligence.This paper describes a programme of human Data Wranglers deployed at the Open University, UK, charged with making sense of a range of data sources related to learning, analysing that data in the light of their understanding of practice in individual faculties/departments, and producing reports that summarise the key points and make actionable recommendations.The evaluation of and experience in this programme of work strongly supports the value of human meaning-makers in the learning analytics process, and suggests that barriers to organisational change in this area can be mitigated by embedding learning analytics work within strategic contexts, and working at an appropriate level and granularity of analysis.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {49–53},
numpages = {5},
keywords = {data wrangling, interpretation, learning analytics, meaning-making, organizational learning, systems},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/3448139.3448178,
author = {Park, Kyungjin and Sohn, Hyunwoo and Mott, Bradford and Min, Wookhee and Saleh, Asmalina and Glazewski, Krista and Hmelo-Silver, Cindy and Lester, James},
title = {Detecting Disruptive Talk in Student Chat-Based Discussion within Collaborative Game-Based Learning Environments},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448178},
doi = {10.1145/3448139.3448178},
abstract = {Collaborative game-based learning environments offer significant promise for creating engaging group learning experiences. Online chat plays a pivotal role in these environments by providing students with a means to freely communicate during problem solving. These chat-based discussions and negotiations support the coordination of students’ in-game learning activities. However, this freedom of expression comes with the possibility that some students might engage in undesirable communicative behavior. A key challenge posed by collaborative game-based learning environments is how to reliably detect disruptive talk that purposefully disrupt team dynamics and problem-solving interactions. Detecting disruptive talk during collaborative game-based learning is particularly important because if it is allowed to persist, it can generate frustration and significantly impede the learning process for students. This paper analyzes disruptive talk in a collaborative game-based learning environment for middle school science education to investigate how such behaviors influence students’ learning outcomes and varies across gender and students’ prior knowledge. We present a disruptive talk detection framework that automatically detects disruptive talk in chat-based group conversations. We further investigate both classic machine learning and deep learning models for the framework utilizing a range of dialogue representations as well as supplementary information such as student gender. Findings show that long short-term memory network (LSTM)-based disruptive talk detection models outperform competitive baseline models, indicating that the LSTM-based disruptive talk detection framework offers significant potential for supporting effective collaborative game-based learning through the identification of disruptive talk.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {405–415},
numpages = {11},
keywords = {Collaborative Game-Based Learning, Disruptive Talk Detection, Text Analytics},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3303772.3303811,
author = {Chua, Yi Han Victoria and Dauwels, Justin and Tan, Seng Chee},
title = {Technologies for automated analysis of co-located, real-life, physical learning spaces: Where are we now?},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303811},
doi = {10.1145/3303772.3303811},
abstract = {The motivation for this paper is derived from the fact that there has been increasing interest among researchers and practitioners in developing technologies that capture, model and analyze learning and teaching experiences that take place beyond computer-based learning environments. In this paper, we review case studies of tools and technologies developed to collect and analyze data in educational settings, quantify learning and teaching processes and support assessment of learning and teaching in an automated fashion. We focus on pipelines that leverage information and data harnessed from physical spaces and/or integrates collected data across physical and digital spaces. Our review reveals a promising field of physical classroom analysis. We describe some trends and suggest potential future directions. Specifically, more research should be geared towards a) deployable and sustainable data collection set-ups in physical learning environments, b) teacher assessment, c) developing feedback and visualization systems and d) promoting inclusivity and generalizability of models across populations.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {11–20},
numpages = {10},
keywords = {Face-to-face classroom analysis, co-located learning, educational data mining, educational technologies, physical learning analytics},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/2723576.2723623,
author = {Kovanovi\'{c}, Vitomir and Ga\v{s}evi\'{c}, Dragan and Dawson, Shane and Joksimovi\'{c}, Sre\'{c}ko and Baker, Ryan S. and Hatala, Marek},
title = {Penetrating the black box of time-on-task estimation},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723623},
doi = {10.1145/2723576.2723623},
abstract = {All forms of learning take time. There is a large body of research suggesting that the amount of time spent on learning can improve the quality of learning, as represented by academic performance. The wide-spread adoption of learning technologies such as learning management systems (LMSs), has resulted in large amounts of data about student learning being readily accessible to educational researchers. One common use of this data is to measure time that students have spent on different learning tasks (i.e., time-on-task). Given that LMS systems typically only capture times when students executed various actions, time-on-task measures are estimated based on the recorded trace data. LMS trace data has been extensively used in many studies in the field of learning analytics, yet the problem of time-on-task estimation is rarely described in detail and the consequences that it entails are not fully examined.This paper presents the results of a study that examined the effects of different time-on-task estimation methods on the results of commonly adopted analytical models. The primary goal of this paper is to raise awareness of the issue of accuracy and appropriateness surrounding time-estimation within the broader learning analytics community, and to initiate a debate about the challenges of this process. Furthermore, the paper provides an overview of time-on-task estimation methods in educational and related research fields.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {184–193},
numpages = {10},
keywords = {higher education, learning analytics, learning management systems (LMS), measurement, moodle, time on task},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/3448139.3448142,
author = {Shusterman, Einat and Kim, Hyunsoo Gloria and Facciotti, Marc and Igo, Michele and Sripathi, Kamali and Karger, David and Segal, Avi and Gal, Kobi},
title = {Seeding Course Forums using the Teacher-in-the-Loop},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448142},
doi = {10.1145/3448139.3448142},
abstract = {Online forums are an integral part of modern day courses, but motivating students to participate in educationally beneficial discussions can be challenging. Our proposed solution is to initialize (or “seed”) a new course forum with comments from past instances of the same course that are intended to trigger discussion that is beneficial to learning. In this work, we develop methods for selecting high-quality seeds and evaluate their impact over one course instance of a 186-student biology class. We designed a scale for measuring the “seeding suitability” score of a given thread (an opening comment and its ensuing discussion). We then constructed a supervised machine learning (ML) model for predicting the seeding suitability score of a given thread. This model was evaluated in two ways: first, by comparing its performance to the expert opinion of the course instructors on test/holdout data; and second, by embedding it in a live course, where it was actively used to facilitate seeding by the course instructors. For each reading assignment in the course, we presented a ranked list of seeding recommendations to the course instructors, who could review the list and filter out seeds with inconsistent or malformed content. We then ran a randomized controlled study, in which one group of students was shown seeds that were recommended by the ML model, and another group was shown seeds that were recommended by an alternative model that ranked seeds purely by the length of discussion that was generated in previous course instances. We found that the group of students that received posts from either seeding model generated more discussion than a control group in the course that did not get seeded posts. Furthermore, students who received seeds selected by the ML-based model showed higher levels of engagement, as well as greater learning gains, than those who received seeds ranked by length of discussion.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {22–31},
numpages = {10},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3027385.3027416,
author = {Yeomans, Michael and Reich, Justin},
title = {Planning prompts increase and forecast course completion in massive open online courses},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027416},
doi = {10.1145/3027385.3027416},
abstract = {Among all of the learners in Massive Open Online Courses (MOOCs) who intend to complete a course, the majority fail to do so. This intention-action gap is found in many domains of human experience, and research in similar goal pursuit domains suggests that plan-making is a cheap and effective nudge to encourage follow-through. In a natural field experiment in three HarvardX courses, some students received open-ended planning prompts at the beginning of a course. These prompts increased course completion by 29%, and payment for certificates by 40%. This effect was largest for students enrolled in traditional schools. Furthermore, the contents of students' plans could predict which students were least likely to succeed - in particular, students whose plans focused on specific times were unlikely to complete the course. Our results suggest that planning prompts can help learners adopted productive frames of mind at the outset of a learning goal that encourage and forecast student success.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {464–473},
numpages = {10},
keywords = {MOOCs, decision-making, goal pursuit, learning analytics, motivation, natural language processing},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3303772.3303791,
author = {Gardner, Josh and Brooks, Christopher and Baker, Ryan},
title = {Evaluating the Fairness of Predictive Student Models Through Slicing Analysis},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303791},
doi = {10.1145/3303772.3303791},
abstract = {Predictive modeling has been a core area of learning analytics research over the past decade, with such models currently deployed in a variety of educational contexts from MOOCs to K-12. However, analyses of the differential effectiveness of these models across demographic, identity, or other groups has been scarce. In this paper, we present a method for evaluating unfairness in predictive student models. We define this in terms of differential accuracy between subgroups, and measure it using a new metric we term the Absolute Between-ROC Area (ABROCA). We demonstrate the proposed method through a gender-based "slicing analysis" using five different models replicated from other works and a dataset of 44 unique MOOCs and over four million learners. Our results demonstrate (1) significant differences in model fairness according to (a) statistical algorithm and (b) feature set used; (2) that the gender imbalance ratio, curricular area, and specific course used for a model all display significant association with the value of the ABROCA statistic; and (3) that there is not evidence of a strict tradeoff between performance and fairness. This work provides a framework for quantifying and understanding how predictive models might inadvertently privilege, or disparately impact, different student subgroups. Furthermore, our results suggest that learning analytics researchers and practitioners can use slicing analysis to improve model fairness without necessarily sacrificing performance.1},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {225–234},
numpages = {10},
keywords = {Fairness, MOOCs, machine learning},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/2567574.2567625,
author = {Aguilar, Stephen and Lonn, Steven and Teasley, Stephanie D.},
title = {Perceptions and use of an early warning system during a higher education transition program},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567625},
doi = {10.1145/2567574.2567625},
abstract = {This paper reports findings from the implementation of a learning analytics-powered Early Warning System (EWS) by academic advisors who were novice users of data-driven learning analytics tools. The information collected from these users sheds new light on how student analytic data might be incorporated into the work practices of advisors working with university students. Our results indicate that advisors predominantly used the EWS during their meetings with students---despite it being designed as a tool to provide information to prepare for meetings and identify students who are struggling academically. This introduction of an unintended audience brings significant design implications to bear that are relevant for learning analytics innovations.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {113–117},
numpages = {5},
keywords = {academic advising, design-research, higher education, learning analytics},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/3170358.3170408,
author = {Lebis, Alexis and Lefevre, Marie and Luengo, Vanda and Guin, Nathalie},
title = {Capitalisation of analysis processes: enabling reproducibility, openness and adaptability thanks to narration},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170408},
doi = {10.1145/3170358.3170408},
abstract = {Analysis processes of learning traces, used to gain important pedagogical insights, are yet to be easily shared and reused. They face what is commonly called a reproducibility crisis. From our observations, we identify two important factors that may be the cause of this crisis: technical constraints due to runnable necessities, and context dependencies. Moreover, the meaning of the reproducibility itself is ambiguous and a source of misunderstanding. In this paper, we present an ontological framework dedicated to taking full advantage of already implemented educational analyses. This framework shifts the actual paradigm of analysis processes by representing them from a narrative point of view, instead of a technical one. This enables a formal description of analysis processes with high-level concepts. We show how this description is performed, and how it can help analysts. The goal is to empower both expert and non-expert analysis stakeholders with the possibility to be involved in the elaboration of analysis processes and their reuse in different contexts, by improving both human and machine understanding of these analyses. This possibility is known as the capitalisation of analysis processes of learning traces.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {245–254},
numpages = {10},
keywords = {adaptability, analysis processes of learning traces, capitalization, context, learning analytics, ontology, openness, reproducibility, reuse},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3027385.3029446,
author = {Schulte, Jurgen and Fernandez de Mendonca, Pedro and Martinez-Maldonado, Roberto and Buckingham Shum, Simon},
title = {Large scale predictive process mining and analytics of university degree course data},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029446},
doi = {10.1145/3027385.3029446},
abstract = {For students, in particular freshmen, the degree pathway from semester to semester is not that transparent, although students have a reasonable idea what courses are expected to be taken each semester. An often-pondered question by students is: "what can I expect in the next semester?" More precisely, given the commitment and engagement I presented in this particular course and the respective performance I achieved, can I expect a similar outcome in the next semester in the particular course I selected? Are the demands and expectations in this course much higher so that I need to adjust my commitment and engagement and overall workload if I expect a similar outcome? Is it better to drop a course to manage expectations rather than to (predictably) fail, and perhaps have to leave the degree altogether? Degree and course advisors and student support units find it challenging to provide evidence based advise to students. This paper presents research into educational process mining and student data analytics in a whole university scale approach with the aim of providing insight into the degree pathway questions raised above. The beta-version of our course level degree pathway tool has been used to shed light for university staff and students alike into our university's 1,300 degrees and associated 6 million course enrolments over the past 20 years.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {538–539},
numpages = {2},
keywords = {educational data mining, educational process visualization, learning analytics, predictive modeling, process mining},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3027441,
author = {Diana, Nicholas and Eagle, Michael and Stamper, John and Grover, Shuchi and Bienkowski, Marie and Basu, Satabdi},
title = {An instructor dashboard for real-time analytics in interactive programming assignments},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027441},
doi = {10.1145/3027385.3027441},
abstract = {Many introductory programming environments generate a large amount of log data, but making insights from these data accessible to instructors remains a challenge. This research demonstrates that student outcomes can be accurately predicted from student program states at various time points throughout the course, and integrates the resulting predictive models into an instructor dashboard. The effectiveness of the dashboard is evaluated by measuring how well the dashboard analytics correctly suggest that the instructor help students classified as most in need. Finally, we describe a method of matching low-performing students with high-performing peer tutors, and show that the inclusion of peer tutors not only increases the amount of help given, but the consistency of help availability as well.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {272–279},
numpages = {8},
keywords = {dashboards, introductory programming, learning analytics, machine learning, peer tutors},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3027436,
author = {Gibson, Andrew and Aitken, Adam and S\'{a}ndor, \'{A}gnes and Buckingham Shum, Simon and Tsingos-Lucas, Cherie and Knight, Simon},
title = {Reflective writing analytics for actionable feedback},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027436},
doi = {10.1145/3027385.3027436},
abstract = {Reflective writing can provide a powerful way for students to integrate professional experience and academic learning. However, writing reflectively requires high quality actionable feedback, which is time-consuming to provide at scale. This paper reports progress on the design, implementation, and validation of a Reflective Writing Analytics platform to provide actionable feedback within a tertiary authentic assessment context. The contributions are: (1) a new conceptual framework for reflective writing; (2) a computational approach to modelling reflective writing, deriving analytics, and providing feedback; (3) the pedagogical and user experience rationale for platform design decisions; and (4) a pilot in a student learning context, with preliminary data on educator and student acceptance, and the extent to which we can evidence that the software provided actionable feedback for reflective writing.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {153–162},
numpages = {10},
keywords = {formative feedback, learning analytics, reflective writing analytics, reflective writing theory},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3506860.3506894,
author = {Pugh, Samuel L. and Rao, Arjun and Stewart, Angela E.B. and D'Mello, Sidney K.},
title = {Do Speech-Based Collaboration Analytics Generalize Across Task Contexts?},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506894},
doi = {10.1145/3506860.3506894},
abstract = {We investigated the generalizability of language-based analytics models across two collaborative problem solving (CPS) tasks: an educational physics game and a block programming challenge. We analyzed a dataset of 95 triads (N=285) who used videoconferencing to collaborate on both tasks for an hour. We trained supervised natural language processing classifiers on automatic speech recognition transcripts to predict the human-coded CPS facets (skills) of constructing shared knowledge, negotiation / coordination, and maintaining team function. We tested three methods for representing collaborative discourse: (1) deep transfer learning (using BERT), (2) n-grams (counts of words/phrases), and (3) word categories (using the Linguistic Inquiry Word Count [LIWC] dictionary). We found that the BERT and LIWC methods generalized across tasks with only a small degradation in performance (Transfer Ratio of .93 with 1 indicating perfect transfer), while the n-grams had limited generalizability (Transfer Ratio of .86), suggesting overfitting to task-specific language. We discuss the implications of our findings for deploying language-based collaboration analytics in authentic educational environments.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {208–218},
numpages = {11},
keywords = {Collaboration analytics, Collaborative problem solving, Natural language processing},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3448139.3448150,
author = {Zhang, Tom and Taub, Michelle and Chen, Zhongzhou},
title = {Measuring the Impact of COVID-19 Induced Campus Closure on Student Self-Regulated Learning in Physics Online Learning Modules},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448150},
doi = {10.1145/3448139.3448150},
abstract = {This paper examines the impact of COVID-19 induced campus closure on university students’ self-regulated learning behavior by analyzing click-stream data collected from student interactions with 70 online learning modules in a university physics course. To do so, we compared the trend of six types of actions related to the three phases of self-regulated learning before and after campus closure and between two semesters. We found that campus closure changed students’ planning and goal setting strategies for completing the assignments, but didn’t have a detectable impact on the outcome or the time of completion, nor did it change students’ self-reflection behavior. The results suggest that most students still manage to complete assignments on time during the pandemic, while the design of online learning modules might have provided the flexibility and support for them to do so.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {110–120},
numpages = {11},
keywords = {Click-stream data, Online learning environments, Self-regulated learning},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448193,
author = {Wang, Karen and Nair, Krishnan and Wieman, Carl},
title = {Examining the Links between Log Data and Reflective Problem-solving Practices in An Interactive Task},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448193},
doi = {10.1145/3448139.3448193},
abstract = {Learning how to solve authentic problems is an important goal of education, yet how to assess and teach problem solving are research topics to be further explored. This study examines how interaction log data from a computerized task environment could be used to extract meaningful features in order to automate the assessment of reflective problem-solving practices. We collected survey responses and interaction log data of 40 college students working to solve the mass of a ”mystery object” in an interactive physics simulation. The log data was parsed to reveal both the test trials conducted to solve the problem and the pauses in-between test trials, where potential monitoring and reflection of the problem-solving process took place. The results show that reflective problem-solving practices, as indicated by meaningful pauses, can predict problem-solving performance above and beyond participants’ application of physics knowledge. Our approach to log data processing has implications for how we study problem solving using interactive simulations.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {525–532},
numpages = {8},
keywords = {log data processing, problem-solving practices, reflection, science simulation},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448156,
author = {Diederich, Morgan and Kang, Jina and Kim, Taehyun and Lindgren, Robb},
title = {Developing an In-Application Shared View Metric to Capture Collaborative Learning in a Multi-Platform Astronomy Simulation},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448156},
doi = {10.1145/3448139.3448156},
abstract = {There has been recent interest in the design of collaborative learning activities that are distributed across multiple technology devices for students to engage in scientific inquiry. Emerging research has begun to investigate students’ collaborative behaviors across different device types and students’ shared attention by tracking eye gaze, body posture, and their interactions with the digital environment. Using a 3D astronomy simulation that leverages a VR headset and tablet computers, this paper builds on the ideas described in eye-gaze studies by developing and implementing a metric of shared viewing across multiple devices. Preliminary findings suggest that a higher level of shared view could be related to increased conceptual discussion, as well as point to an early-stage pattern of behavior of decreased SV to prompt facilitator intervention to refocus collaborative efforts. We hope this metric will be a promising first step in further understanding and assessing the quality of collaboration across multiple device platforms in a single shared space. This paper provides an in depth look at a highly exploratory stage of a broader research trajectory to establish a robust, effective way to track screen views, including providing resources to teachers when students engage in similar learning environments, and providing insight from log data to understand how students effectively collaborate.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {173–183},
numpages = {11},
keywords = {Astronomy education, Immersive virtual reality, Log data, Science education, Shared view},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/2883851.2883865,
author = {Chen, Bodong and Wise, Alyssa F. and Knight, Simon and Cheng, Britte Haugan},
title = {Putting temporal analytics into practice: the 5th international workshop on temporality in learning data},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883865},
doi = {10.1145/2883851.2883865},
abstract = {Interest in temporal analytics---analytics that probe temporal aspects of learning so as to gain insights into the processes through which learning occurs---continues to grow. The relationships of temporal patterns to learning outcomes is a central area of interest. However, while the literature on temporal analyses is developing, there has been less consideration of the methods by which temporal analyses might be translated to actionable insights and thus, put into use in educational practice. Emerging temporal analysis techniques present both theoretical and practical challenges for producing and interpreting results. Synergetic actions are needed in order to support practitioners.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {488–489},
numpages = {2},
keywords = {CSCL, analytics for action, learning analytics, practitioner knowledge, temporality},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/3027385.3027433,
author = {Knight, Simon and Martinez-Maldonado, Roberto and Gibson, Andrew and Buckingham Shum, Simon},
title = {Towards mining sequences and dispersion of rhetorical moves in student written texts},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027433},
doi = {10.1145/3027385.3027433},
abstract = {There is an increasing interest in the analysis of both student's writing and the temporal aspects of learning data. The analysis of higher-level learning features in writing contexts requires analyses of data that could be characterised in terms of the sequences and processes of textual features present. This paper (1) discusses the extant literature on sequential and process analyses of writing; and, based on this and our own first-hand experience on sequential analysis, (2) proposes a number of approaches to both pre-process and analyse sequences in whole-texts. We illustrate how the approaches could be applied to examples drawn from our own datasets of 'rhetorical moves' in written texts, and the potential each approach holds for providing insight into that data. Work is in progress to apply this model to provide empirical insights. Although, similar sequence or process mining techniques have not yet been applied to student writing, techniques applied to event data could readily be operationalised to undercover patterns in texts.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {228–232},
numpages = {5},
keywords = {academic writing, learning analytics, process mining, rhetorical moves, sequence mining, temporal analysis, text mining, writing analytics},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3639478.3639804,
author = {Mei\ss{}ner, Niklas},
title = {MEITREX - Gamified and Adaptive Intelligent Tutoring in Software Engineering Education},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3639804},
doi = {10.1145/3639478.3639804},
abstract = {Nowadays, learning management systems (LMSs) are established tools in higher education, especially in the domain of software engineering (SE). However, the potential of such educational technologies has not been fully exploited, as student performance in SE education is still strongly dependent on feedback from time-constrained lecturers and tutors. Moreover, current LMSs are not designed for SE courses, as external SE tools are required to fulfill the requirements of lecturers such as programming and UML modeling features. Evolving these LMSs in the direction of intelligent tutoring could assist students in receiving automatic, individual feedback from the LMSs on their learning performance at any time. Also, gamified learning elements can serve to motivate students to engage with SE materials. Therefore, this paper presents an approach combining learning analytics, feedback, and interactive learning such as gamification in one LMS designed for SE education. The system could thus address diverse students with different backgrounds and motivational aspects and provide appropriate individual support to ensure effective SE education.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {198–200},
numpages = {3},
keywords = {software engineering education, student motivation, intelligent tutoring system, learning analytics, gamification, feedback},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3448139.3448166,
author = {Carpenter, Dan and Cloude, Elizabeth and Rowe, Jonathan and Azevedo, Roger and Lester, James},
title = {Investigating Student Reflection during Game-Based Learning in Middle Grades Science},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448166},
doi = {10.1145/3448139.3448166},
abstract = {Reflection plays a critical role in learning by encouraging students to contemplate their knowledge and previous learning experiences to inform their future actions and higher-order thinking, such as reasoning and problem solving. Reflection is particularly important in inquiry-driven learning scenarios where students have the freedom to set goals and regulate their own learning. However, despite the importance of reflection in learning, there are significant theoretical, methodological, and analytical challenges posed by measuring, modeling, and supporting reflection. This paper presents results from a classroom study to investigate middle-school students’ reflection during inquiry-driven learning with Crystal Island, a game-based learning environment for middle-school microbiology. To collect evidence of reflection during game-based learning, we used embedded reflection prompts to elicit written reflections during students’ interactions with Crystal Island. Results from analysis of data from 105 students highlight relationships between features of students’ reflections and learning outcomes related to both science content knowledge and problem solving. We consider implications for building adaptive support in game-based learning environments to foster deep reflection and enhance learning, and we identify key features in students’ problem-solving actions and reflections that are predictive of reflection depth. These findings present a foundation for providing adaptive support for reflection during game-based learning.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {280–291},
numpages = {12},
keywords = {Game-Based Learning, Reflection, Self-Regulated Learning},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3027385.3029466,
author = {Schweighart, M.},
title = {Using item response theory to generate an item pool for an e-learning-system},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029466},
doi = {10.1145/3027385.3029466},
abstract = {This paper1 demonstrates how the application of item response theory yields useful item characteristics, which further can be the foundation of item pools and therefore adaptive educational software to come.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {574–575},
numpages = {2},
keywords = {adaptive learning, big data analysis, evaluation, item pool, item response theory, learning analytics},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3375462.3375512,
author = {Faucon, Louis and Olsen, Jennifer K. and Dillenbourg, Pierre},
title = {A bayesian model of individual differences and flexibility in inductive reasoning for categorization of examples},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375512},
doi = {10.1145/3375462.3375512},
abstract = {Inductive reasoning is an important educational practice but can be difficult for teachers to support in the classroom due to the high level of preparation and classroom time needed to choose the teaching materials that challenge students' current views. Intelligent tutoring systems can potentially facilitate this work for teachers by supporting the automatic adaptation of examples based on a student model of the induction process. However, current models of inductive reasoning usually lack two main characteristics helpful to adaptive learning environments, individual differences of students and tracing of students' learning as they receive feedback. In this paper, we describe a model to predict and simulate inductive reasoning of students for a categorization task. Our approach uses a Bayesian model for describing the reasoning processes of students. This model allows us to predict students' choices in categorization questions by accounting for their feature biases. Using data gathered from 222 students categorizing three topics, we find that our model has a 75% accuracy, which is 10% greater than a baseline model. Our model is a contribution to learning analytics by enabling us to assign different bias profiles to individual students and tracking these profile changes over time through which we can gain a better understanding of students' learning processes. This model may be relevant for systematically analysing students' differences and evolution in inductive reasoning strategies while supporting the design of adaptive inductive learning environments.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {285–294},
numpages = {10},
keywords = {adaptive learning environment, inductive reasoning, process mining, student modeling},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3448139.3448202,
author = {Winograd, Blair A. and Dood, Amber J and Moeller, Robert and Moon, Alena and Gere, Anne and Shultz, Ginger},
title = {Detecting High Orders of Cognitive Complexity in Students’ Reasoning in Argumentative Writing About Ocean Acidification},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448202},
doi = {10.1145/3448139.3448202},
abstract = {Providing students in STEM courses the opportunity to write about scientific content can be beneficial to the learning process. However, it is a logistical challenge to provide feedback to students’ written work in large-enrollment courses. Motivated by these reasons, the study presented herein considers a method to identify the depth of students’ scientific reasoning in their written work. A writing-to-learn (WTL) activity was implemented in a large undergraduate general chemistry class. An analytical framework of cognitive operations that characterizes students’ scientific reasoning evidenced in their writing was applied. Engagement in some of the more complex cognitive operations, such as causal reasoning and argumentation, was a sign that students were properly engaging in meaning making activities. This work considers a method to automate coaching of students in using more complex reasoning in their writing with the desired outcome that it may help students better engage with the science content. We consider a series of new natural language processing models to discern types of reasoning in student essays from the WTL activity.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {586–591},
numpages = {6},
keywords = {Scientific reasoning, peer review, undergraduate education, writing-to-learn},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/2723576.2723657,
author = {Dodge, Bernie and Whitmer, John and Frazee, James P.},
title = {Improving undergraduate student achievement in large blended courses through data-driven interventions},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723657},
doi = {10.1145/2723576.2723657},
abstract = {This pilot study applied Learning Analytics methods to identify students at-risk of not succeeding in two high enrollment courses with historically low pass rates at San Diego State University: PSY 101 and STAT 119. With input from instructors, targeted interventions were developed and sent to participating students (n=882) suggesting ways to improve their performance. An experimental design was used with half of the students randomly assigned to receive these interventions via email and the other half being analyzed for at-risk triggers but receiving no intervention. Pre-course surveys on student motivation [4] and prior subject matter knowledge were conducted, and students were asked to maintain weekly logs of their activity online and offline connected to the courses. Regression analyses, incorporating feature selection methods to account for student demographic data, were used to compare the impact of the interventions between the control and experimental groups. Results showed that the interventions were associated with a higher final grade in one course, but only for a particular demographic group.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {412–413},
numpages = {2},
keywords = {at-risk student prediction, blended learning, interventions, large enrollment courses, learning analytics, learning management systems, motivation, time logs},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/3375462.3375509,
author = {Harrak, Fatima and Bouchet, Fran\c{c}ois and Luengo, Vanda and Gillois, Pierre},
title = {Evaluating teachers' perceptions of students' questions organization},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375509},
doi = {10.1145/3375462.3375509},
abstract = {Students' questions are essential to help teachers in assessing their understanding and adapting their pedagogy. However, in a flipped classroom context where many questions are asked online to be addressed in class, selecting questions can be difficult for teachers. To help them in this task, we present here three alternative ways of organizing questions: one based on pedagogical needs, one based on estimated students' profiles and one mixing both approaches. Results of a survey filled by 37 teachers in a flipped classroom pedagogy show no consensus over a single organization. A cluster analysis based on teachers' flipped classroom experience allowed us to distinguish two profiles, but they were not associated with any particular question organization preference. Qualitative results suggest the need for different organizations may rely more on a pedagogical philosophy and advocates for differentiated dashboards.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {11–16},
numpages = {6},
keywords = {pedagogical interest, question organization, student's need, student's profile, student's question, teacher's perception},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3027385.3029434,
author = {Arnold, Kimberly E. and Karcher, Brandon and Wright, Casey V. and McKay, James},
title = {Student empowerment, awareness, and self-regulation through a quantified-self student tool},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029434},
doi = {10.1145/3027385.3029434},
abstract = {The purpose of this paper is to examine the cross institutional use of a quantified-self application called Pattern, which is designed to promote self-regulation and reflective learning in learners. This paper provides a brief look into how learners report spending their time and react to in-app recommendations. Initial data is encouraging; however, there are limitations of Pattern, and additional research and development must be undertaken.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {526–527},
numpages = {2},
keywords = {higher education, learning analytics, mobile application, quantified-self student, real-time feedback, recommendation engine, reflective learning practices, self-regulated learning},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/2883851.2883955,
author = {Buckingham Shum, Simon and S\'{a}ndor, \'{A}gnes and Goldsmith, Rosalie and Wang, Xiaolong and Bass, Randall and McWilliams, Mindy},
title = {Reflecting on reflective writing analytics: assessment challenges and iterative evaluation of a prototype tool},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883955},
doi = {10.1145/2883851.2883955},
abstract = {When used effectively, reflective writing tasks can deepen learners' understanding of key concepts, help them critically appraise their developing professional identity, and build qualities for lifelong learning. As such, reflecting writing is attracting substantial interest from universities concerned with experiential learning, reflective practice, and developing a holistic conception of the learner. However, reflective writing is for many students a novel genre to compose in, and tutors may be inexperienced in its assessment. While these conditions set a challenging context for automated solutions, natural language processing may also help address the challenge of providing real time, formative feedback on draft writing. This paper reports progress in designing a writing analytics application, detailing the methodology by which informally expressed rubrics are modelled as formal rhetorical patterns, a capability delivered by a novel web application. This has been through iterative evaluation on an independently human-annotated corpus, showing improvements from the first to second version. We conclude by discussing the reasons why classifying reflective writing has proven complex, and reflect on the design processes enabling work across disciplinary boundaries to develop the prototype to its current state.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {213–222},
numpages = {10},
keywords = {education, learning analytics, metadiscourse, natural language processing, reflection, rhetoric, writing analytics},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/3448139.3448147,
author = {Barbosa, Arthur and Ferreira, M\'{a}verick and Ferreira Mello, Rafael and Dueire Lins, Rafael and Gasevic, Dragan},
title = {The impact of automatic text translation on classification of online discussions for social and cognitive presences},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448147},
doi = {10.1145/3448139.3448147},
abstract = {This paper reports the findings of a study that measured the effectiveness of employing automatic text translation methods in automated classification of online discussion messages according to the categories of social and cognitive presences. Specifically, we examined the classification of 1,500 Portuguese and 1,747 English discussion messages using classifiers trained on the datasets before and after the application of text translation. While the English model generated, with the original and translated texts, achieved results (accuracy and Cohen’s κ) similar to those of the previously reported studies, the translation to Portuguese led to a decrease in the performance. The indicates the general viability of the proposed approach when converting the text to English. Moreover, this study highlighted the importance of different features and resources, and the limitations of the resources for Portuguese as reasons of the results obtained.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {77–87},
numpages = {11},
keywords = {Community of Inquiry Model, Content Analytics, Online Discussion, Text Translation},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3448139.3448168,
author = {Jensen, Emily and L. Pugh, Samuel and K. D'Mello, Sidney},
title = {A Deep Transfer Learning Approach to Modeling Teacher Discourse in the Classroom},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448168},
doi = {10.1145/3448139.3448168},
abstract = {Teachers, like everyone else, need objective reliable feedback in order to improve their effectiveness. However, developing a system for automated teacher feedback entails many decisions regarding data collection procedures, automated analysis, and presentation of feedback for reflection. We address the latter two questions by comparing two different machine learning approaches to automatically model seven features of teacher discourse (e.g., use of questions, elaborated evaluations). We compared a traditional open-vocabulary approach using n-grams and Random Forest classifiers with a state-of-the-art deep transfer learning approach for natural language processing (BERT). We found a tradeoff between data quantity and accuracy, where deep models had an advantage on larger datasets, but not for smaller datasets, particularly for variables with low incidence rates. We also compared the models based on the level of feedback granularity: utterance-level (e.g., whether an utterance is a question or a statement), class session-level proportions by averaging across utterances (e.g., question incidence score of 48%), and session-level ordinal feedback based on pre-determined thresholds (e.g., question asking score is medium [vs. low or high]) and found that BERT generally provided more accurate feedback at all levels of granularity. Thus, BERT appears to be the most viable approach to providing automatic feedback on teacher discourse provided there is sufficient data to fine tune the model.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {302–312},
numpages = {11},
keywords = {Automated Feedback, Deep Learning, Natural Language Processing, Teacher Discourse, Teaching Analytics},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3375462.3375480,
author = {Seitlinger, Paul and Bibi, Abida and Uus, \~{O}nne and Ley, Tobias},
title = {How working memory capacity limits success in self-directed learning: a cognitive model of search and concept formation},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375480},
doi = {10.1145/3375462.3375480},
abstract = {With this work we intend to develop cognitive modules for learning analytics solutions used in inquiry learning environments that can monitor and assess mental abilities involved in self-directed learning activities. We realize this idea by drawing on models from mathematical psychology, which specify assumptions about the human mind algorithmically and thereby automate a theory-driven data analysis.We report a study to exemplify this approach in which N=105 15-year-old high school students perform a self-determined navigation in a taxonomy of dinosaur concepts. We analyze their search and learning traces through the lens of a connectionist network model of working memory (WM). The results are encouraging in three ways. First, the model predicts students' average progress (as well as difficulties) in forming new concepts at high accuracy. Second, a simple (1-parameter) extension, which we derive from a meta-cognitive learning framework, is sufficient to also predict aggregated search patterns. Third, our initial attempt to fit the model to individual data offers some promising results: estimates of a free parameter correlate significantly with a measure of WM capacity.Together, we believe that these results help demonstrate a novel and promising way towards extending learner models by cognitive variables. We also discuss current limitations in the light of our future work on cognitive-computational scaffolding techniques in inquiry learning scenarios.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {53–62},
numpages = {10},
keywords = {cognitive-computational modeling, concept formation, self-directed learning, working memory capacity},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3170358.3170394,
author = {Herder, Tiffany and Swiecki, Zachari and Fougt, Simon Skov and Tamborg, Andreas Lindenskov and Allsopp, Benjamin Brink and Shaffer, David Williamson and Misfeldt, Morten},
title = {Supporting teachers' intervention in students' virtual collaboration using a network based model},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170394},
doi = {10.1145/3170358.3170394},
abstract = {This paper reports a Design-Based Research project developing a tool (the Process Tab) that supports teachers' interventions with students in virtual internships. The tool uses a networked approach and allows insights into the discourse of groups and individuals based on contributions in chat fora and assignments.In the paper, we present the tool and reports from interviews with three teachers who used the tool. The interviews provide insights about the teachers' hopes, actual use, and difficulties with the tool. The main insight is that even though the teachers genuinely liked the idea and specific representations of the Process Tab, their lack of ability to teach and look at the tool at the same time hindered their use. In the final part of the paper, we discuss how to address this issue.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {21–25},
numpages = {5},
keywords = {ACM proceedings, immersive learning environments, learning analytics, teacher intervention, teacher practices, virtual collaboration},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/2883851.2883947,
author = {Ruip\'{e}rez-Valiente, Jos\'{e} A. and Mu\~{n}oz-Merino, Pedro J. and Kloos, Carlos Delgado},
title = {Analyzing students' intentionality towards badges within a case study using Khan academy},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883947},
doi = {10.1145/2883851.2883947},
abstract = {One of the most common gamification techniques in education is the use of badges as a reward for making specific student actions. We propose two indicators to gain insight about students' intentionality towards earning badges and use them with data from 291 students interacting with Khan Academy courses. The intentionality to earn badges was greater for repetitive badges, and this can be related to the fact that these are easier to achieve. We provide the general distribution of students depending on these badge indicators, obtaining different profiles of students which can be used for adaptation purposes.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {536–537},
numpages = {2},
keywords = {Khan academy, badges, learning analytics, modelling behavior},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/3170358.3170390,
author = {Alhadad, Sakinah S. J. and Thompson, Kate and Knight, Simon and Lewis, Melinda and Lodge, Jason M.},
title = {Analytics-enabled teaching as design: reconceptualisation and call for research},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170390},
doi = {10.1145/3170358.3170390},
abstract = {As a human-centred educational practice and field of research, learning analytics must account for key stakeholders in teaching and learning. The focus of this paper is on the role of institutions to support teachers to incorporate learning analytics into their practice by understanding the confluence of internal and external factors that influence what they do. In this paper, we reconceptualise `teaching as design' for `analytics-enabled teaching as design' to shape this discussion to allow for the consideration of external factors, such as professional learning or ethical considerations of student data, as well as personal considerations, such as data literacy and teacher beliefs and identities. In order to address the real-world challenges of progressing teachers' efficacy and capacity toward analytics-enabled teaching as design, we have placed the teacher - as a cognitive, social, and emotional being - at the center. In so doing, we discuss potential directions towards research for practice in elucidating underpinning factors of teacher inquiry in the process of authentic design.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {427–435},
numpages = {9},
keywords = {analytics, design cognition, ethics, institutional culture, professional learning, teacher identity, teaching as design, team teaching},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3448139.3448153,
author = {Zylich, Brian and Lan, Andrew},
title = {Linguistic Skill Modeling for Second Language Acquisition},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448153},
doi = {10.1145/3448139.3448153},
abstract = {To adapt materials for an individual learner, intelligent tutoring systems must estimate their knowledge or abilities. Depending on the content taught by the tutor, there have historically been different approaches to student modeling. Unlike common skill-based models used by math and science tutors, second language acquisition (SLA) tutors use memory-based models since there are many tasks involving memorization and retrieval, such as learning the meaning of a word in a second language. Based on estimated memory strengths provided by these memory-based models, SLA tutors are able to identify the optimal timing and content of retrieval practices for each learner to improve retention. In this work, we seek to determine whether skill-based models can be combined with memory-based models to improve student modeling and especially retrieval practice performance for SLA. In order to define skills in the context of SLA, we develop methods that can automatically extract multiple types of linguistic features from words. Using these features as skills, we apply skill-based models to a real-world SLA dataset. Our main findings are as follows. First, incorporating lexical features to represent individual words as skills in skill-based models outperforms existing memory-based models in terms of recall probability prediction. Second, incorporating additional morphological and syntactic features of each word via multiple-skill tagging of each word further improves the skill-based models. Third, incorporating semantic features, like word embeddings, to model similarities between words in a learner’s practice history and their effects on memory also improves the models and appears to be a promising direction for future research.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {141–150},
numpages = {10},
keywords = {memory decay, second language acquisition, student modeling},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3027385.3029442,
author = {Koedinger, Ken and Liu, Ran and Stamper, John and Thille, Candace and Pavlik, Phil},
title = {Community based educational data repositories and analysis tools},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029442},
doi = {10.1145/3027385.3029442},
abstract = {This workshop will explore community based repositories for educational data and analytic tools that are used to connect researchers and reduce the barriers to data sharing. Leading innovators in the field, as well as attendees, will identify and report on bottlenecks that remain toward our goal of a unified repository. We will discuss these as well as possible solutions. We will present LearnSphere, an NSF funded system that supports collaborating on and sharing a wide variety of educational data, learning analytics methods, and visualizations while maintaining confidentiality. We will then have hands-on sessions in which attendees have the opportunity to apply existing learning analytics workflows to their choice of educational datasets in the repository (using a simple drag-and-drop interface), add their own learning analytics workflows (requires very basic coding experience), or both. Leaders and attendees will then jointly discuss the unique benefits as well as the limitations of these solutions. Our goal is to create building blocks to allow researchers to integrate their data and analysis methods with others, in order to advance the future of learning science.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {524–525},
numpages = {2},
keywords = {data storage and sharing, data-informed efforts, data-informed learning theories, learning metrics, modeling, scalability},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3027385.3029435,
author = {Cooper, Adam and Berg, Alan and Sclater, Niall and Dorey-Elias, Tanya and Kitto, Kirsty},
title = {LAK17 hackathon: getting the right information to the right people so they can take the right action},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029435},
doi = {10.1145/3027385.3029435},
abstract = {The hackathon is intended to be a practical hands-on workshop involving participants from academia and commercial organizations with both technical and practitioner expertise. It will consider the outstanding challenge of visualizations which are effective for the intended audience: informing action, not likely to be misinterpreted, and embodying contextual appropriacy, etc. It will surface particular issues as workshop challenges and explore responses to these challenges as visualizations resting upon interoperability standards and API-oriented open architectures.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {514–515},
numpages = {2},
keywords = {actionable insights, contextual appropriacy, hackathon, interoperability, open learning analytics, visualization},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3448139.3448163,
author = {Farrow, Elaine and Moore, Johanna and Gasevic, Dragan},
title = {A network analytic approach to integrating multiple quality measures for asynchronous online discussions},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448163},
doi = {10.1145/3448139.3448163},
abstract = {Asynchronous online discussions within a community of learners can improve learning outcomes through social knowledge construction, but the depth and quality of student contributions often varies widely. Approaches to assessing critical discourse typically use content analysis to identify indicators that correspond to framework constructs, that in turn serve as measures of depth and quality. Often only a single construct is addressed for performing content analysis in the literature, although recent work has used both social presence and cognitive presence constructs from the Community of Inquiry (CoI) framework. Nevertheless, there is no effective, commonly used, analytic approach to combining insights from multiple perspectives about quality and depth of online discussions. This paper addresses the gap by proposing the combined use of cognitive engagement (the ICAP framework) and cognitive presence (CoI); and by proposing a network analytic approach that quantifies the associations between the two frameworks and measures the moderation effects of two instructional interventions on those associations. The present study found that these associations were moderated by one intervention but not the other; and that messages labelled with the most common phase of cognitive presence could be usefully assigned to smaller meaningful subgroups by also considering the mode of cognitive engagement.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {248–258},
numpages = {11},
keywords = {Community of Inquiry, Epistemic Network Analysis, ICAP, cognitive presence, critical thinking, discussion forum},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3027385.3027425,
author = {Peffer, Melanie E. and Kyle, Kristopher},
title = {Assessment of language in authentic science inquiry reveals putative differences in epistemology},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027425},
doi = {10.1145/3027385.3027425},
abstract = {Science epistemology, or beliefs about what it means to do science and how science knowledge is generated, is an integral part of authentic science inquiry. Although the development of a sophisticated science epistemology is critical for attaining science literacy, epistemology remains an elusive construct to precisely and quantitatively evaluate. Previous work has suggested that analysis of student practices in science inquiry, such as their use of language, may be reflective of their underlying epistemologies. Here we describe the usage of a learning analytics tool, TAALES, and keyness analysis to analyze the concluding statements made by students at the end of a computer-based authentic science inquiry experience. Preliminary results indicate that linguistic analysis reveals differences in domain-general lexical sophistication and in domain-specific verb usage that are consistent with the expertise level of the participant. For example, experts tend to use more hedging language such as "may" and "support" during conclusions whereas novices use stronger language such as "cause." Using these differences, a simple, rule-based prediction algorithm with LOOCV achieved prediction accuracies of greater than 80%. These data underscore the potential for the use of learning analytics in simulated authentic inquiry to provide a novel and valuable method of assessing inquiry practices and related epistemologies.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {138–142},
numpages = {5},
keywords = {TAALES, assessment, authentic science inquiry, lexical sophistication, science classroom inquiry simulations, science epistemology, science practices},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3448139.3448159,
author = {Williams-Dobosz, Destiny and Azevedo, Renato Ferreira Leit\~{a}o and Jeng, Amos and Thakkar, Vyom and Bhat, Suma and Bosch, Nigel and Perry, Michelle},
title = {A Social Network Analysis of Online Engagement for College Students Traditionally Underrepresented in STEM},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448159},
doi = {10.1145/3448139.3448159},
abstract = {Little is known about the online learning behaviors of students traditionally underrepresented in STEM fields (i.e., UR-STEM students), as well as how those behaviors impact important learning outcomes. The present study examined the relationship between online discussion forum engagement and success for UR-STEM and non-UR-STEM students, using the Community of Inquiry (CoI) model as our theoretical framework. Social network analysis and nested regression models were used to explore how three different measures of forum engagement—1) total number of posts written, 2) number of help-seeking posts written and replied to, and 3) level of connectivity—were related to improvement (i.e., relative performance gains) for 70 undergraduate students enrolled in an online introductory STEM course. We found a significant positive relationship between help-seeking and improvement and nonsignificant effects of general posting and connectivity; these results held for UR-STEM and non-UR-STEM students alike. Our findings suggest that online help-seeking has benefits for course improvement beyond what can be predicted by posting alone and that one need not be well connected in a class network to achieve positive learning outcomes. Finally, UR-STEM students demonstrated greater grade improvement than their non-UR-STEM counterparts, which suggests that the online environment has the potential to combat barriers to success that disproportionately affect underrepresented students.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {207–215},
numpages = {9},
keywords = {Help-seeking, Online discussion forums, Social network connectivity, Underrepresentation in STEM},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3375462.3375473,
author = {Hu, Yuanyuan and Donald, Claire and Giacaman, Nasser and Zhu, Zexuan},
title = {Towards automated analysis of cognitive presence in MOOC discussions: a manual classification study},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375473},
doi = {10.1145/3375462.3375473},
abstract = {This paper reports on early stages of a machine learning research project, where phases of cognitive presence in MOOC discussions were manually coded in preparation for training automated cognitive classifiers. We present a manual-classification rubric combining Garrison, Anderson and Archer's [11] coding scheme with Park's [25] revised version for a target MOOC. The inter-rater reliability between two raters achieved 95.4% agreement with a Cohen's weighted kappa of 0.96, demonstrating our classification rubric is plausible for the target MOOC dataset. The classification rubric, originally intended for for-credit, undergraduate courses, can be applied to a MOOC context. We found that the main disagreements between two raters lay on adjacent cognitive phases, implying that additional categories may exist between cognitive phases in such MOOC discussion messages. Overall, our results suggest a reliable rubric for classifying cognitive phases in discussion messages of the target MOOC by two raters. This indicates we are in a position to apply machine learning algorithms which can also cater for data with inter-rater disagreements in future automated classification studies.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {135–140},
numpages = {6},
keywords = {MOOC, cognitive presence, online discussions, text classification},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/2567574.2567586,
author = {Coffrin, Carleton and Corrin, Linda and de Barba, Paula and Kennedy, Gregor},
title = {Visualizing patterns of student engagement and performance in MOOCs},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567586},
doi = {10.1145/2567574.2567586},
abstract = {In the last five years, the world has seen a remarkable level of interest in Massive Open Online Courses, or MOOCs. A consistent message from universities participating in MOOC delivery is their eagerness to understand students' online learning processes. This paper reports on an exploratory investigation of students' learning processes in two MOOCs which have different curriculum and assessment designs. When viewed through the lens of common MOOC learning analytics, the high level of initial student interest and, ultimately, the high level of attrition, makes these two courses appear very similar to each other, and to MOOCs in general. With the goal of developing a greater understanding of students' patterns of learning behavior in these courses, we investigated alternative learning analytic approaches and visual representations of the output of these analyses. Using these approaches we were able to meaningfully classify student types and visualize patterns of student engagement which were previously unclear. The findings from this research contribute to the educational community's understanding of students' engagement and performance in MOOCs, and also provide the broader learning analytics community with suggestions of new ways to approach learning analytic data analysis and visualization.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {83–92},
numpages = {10},
keywords = {MOOC, completion rate, learner engagement patterns, learning analytics, online learning, prior knowledge, visualization, visualizations},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/3631700.3664899,
author = {Akhuseyinoglu, Kamil and McDonald, Emma and Klasnja Milicevic, Aleksandra and Demmans Epp, Carrie and Brusilovsky, Peter},
title = {Exploring Adaptive Social Comparison for Online Practice},
year = {2024},
isbn = {9798400704666},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3631700.3664899},
doi = {10.1145/3631700.3664899},
abstract = {Students experience motivational issues during online learning which has led to explorations of how to better support their self-regulated learning. One way to support students uses social reference frames or social comparison in student-facing learning analytics dashboards (LADs) and open learner models (OLMs). Usually, the social reference frame communicates class averages. Despite the positive effects of class-average-based social comparison on students’ activity levels and learning behaviors, comparison to class average can be misleading for some students and offer an irrelevant reference frame, motivating only low or high performers. Such conflicting findings highlight a need for an investigation of social reference frames that are not based on the “average” student. We extend the research on social comparison in education by conducting two complementary classroom studies. The first explores the effects of different fixed social reference frames in a non-mandatory practice system, while the second introduces an adaptive social reference frame that dynamically selects the peers who serve as a comparison group when students are engaged in online programming practice. We reported our analyses from both studies and shared students’ subjective evaluations of the system and its adaptive comparison functionality.},
booktitle = {Adjunct Proceedings of the 32nd ACM Conference on User Modeling, Adaptation and Personalization},
pages = {374–379},
numpages = {6},
keywords = {OLM, adaptive educational systems, classroom experiment, computer science education, learning analytics, self-regulated learning},
location = {Cagliari, Italy},
series = {UMAP Adjunct '24}
}

@inproceedings{10.1145/3649217.3653561,
author = {Karaka\v{s}, Aleksandar and Helic, Denis},
title = {Combining Local Testing with Automatic Commits: Benefits for Progress Tracking and CS2 Students' Learning Experience},
year = {2024},
isbn = {9798400706004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649217.3653561},
doi = {10.1145/3649217.3653561},
abstract = {Many instructors in introductory programming courses experience high dropout and failure rates. Identifying struggling students early is a prerequisite to target this problem. To this end, instructors and learning analytics researchers may leverage version control by analyzing the students' commit histories. This approach relies on frequent pushes to the version control platform, which many instructors incentivize by offering test results each time a student pushes a commit. However, instructors who provide test cases that can be run locally (i.e., without creating a commit) may face coarse-grained commit histories.In this study, we analyze a CS2 course which offers both local and remote testing. Students were provided with tools that automatically create and push a commit on each local test run. We investigate to what extent these automatically created snapshots contribute to obtaining fine-grained commit histories and early initial push events. Our analysis uncovers distinct commit patterns among high- and low-performing students. Furthermore, we find that despite the commit automation and encouraging students to start early, many students pushed their first commit late. We triangulate this observation with survey results which confirm the late start of many students. The survey also identified reasons for students to opt out of automatic commit creation. Moreover, many students expressed a positive attitude towards testing their programs locally. Thus, our survey results underline that instructors should strive for providing students with comprehensive feedback that students can conveniently obtain.},
booktitle = {Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 1},
pages = {108–114},
numpages = {7},
keywords = {computer science education, cs2, educational data mining, git, learning analytics, version control},
location = {Milan, Italy},
series = {ITiCSE 2024}
}

@inproceedings{10.1145/3375462.3375488,
author = {Whitelock-Wainwright, Alexander and Tsai, Yi-Shan and Lyons, Kayley and Kaliff, Svetlana and Bryant, Mike and Ryan, Kris and Ga\v{s}evi\'{c}, Dragan},
title = {Disciplinary differences in blended learning design: a network analytic study},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375488},
doi = {10.1145/3375462.3375488},
abstract = {Learning design research has predominately relied upon survey- and interview-based methodologies, both of which are subject to limitations of social desirability and recall. An alternative approach is offered in this manuscript, whereby physical and online learning activity data is analysed using Epistemic Network Analysis. Using a sample of 6,040 course offerings from 10 faculties across a four year period (2016--2019), the utility of networks to understand learning design is illustrated. Specifically, through the adoption of a network analytic approach, the following was found: universities are clearly committed to blended learning, but there are considerable differences both between and within disciplines.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {579–588},
numpages = {10},
keywords = {epistemic network analysis, faculty, learning activity types},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3448139.3448200,
author = {Li, Chenglu and Xing, Wanli and Leite, Walter},
title = {Yet Another Predictive Model? Fair Predictions of Students’ Learning Outcomes in an Online Math Learning Platform},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448200},
doi = {10.1145/3448139.3448200},
abstract = {To support online learners at a large scale, extensive studies have adopted machine learning (ML) techniques to analyze students’ artifacts and predict their learning outcomes automatically. However, limited attention has been paid to the fairness of prediction with ML in educational settings. This study intends to fill the gap by introducing a generic algorithm that can orchestrate with existing ML algorithms while yielding fairer results. Specifically, we have implemented logistic regression with the Seldonian algorithm and compared the fairness-aware model with fairness-unaware ML models. The results show that the Seldonian algorithm can achieve comparable predictive performance while producing notably higher fairness.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {572–578},
numpages = {7},
keywords = {fair machine learning, online math learning, predictive analytics},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/2522848.2533792,
author = {Thompson, Kate},
title = {Using micro-patterns of speech to predict the correctness of answers to mathematics problems: an exercise in multimodal learning analytics},
year = {2013},
isbn = {9781450321297},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2522848.2533792},
doi = {10.1145/2522848.2533792},
abstract = {Learning analytics techniques are traditionally used on the "big data" collected at the course or university level. The application of such techniques to the data sets generated in complex learning environments can provide insights into the relationships between the design of learning environments, the processes of learning, and learning outcomes. In this paper, two of the codes described as part of the Collaborative Process Analysis Coding Scheme (CPACS) were extracted from the Math Data Corpus. The codes selected were tense and pronouns, both of which have been found to indicate phases of group work and the action associated with collaboration. Rather than examine these measures of social interactions in isolation, a framework for the analysis of complex learning environments was applied. This facilitated an analysis of the relationships between the social interactions, the task design and learning outcomes, as well as tool use. The generation of a successful problem solution of one expert and one non-expert group was accurately predicted (75%-94%). The examination of interactions between the social, epistemic and tool elements of the learning environment for one group showed that successful role differentiation and participation were related to successful problem solutions in the first meeting. In the second meeting, these were less important. The relationship between the discourse related to problem resolution through action and the correctness of a problems solution was found to be less reliable measures, however further analysis is needed at a finer grain to investigate this finding. A rich description of the processes of learning (with regards to social interaction, generation of knowledge, and discourse related to action) was generated for one group.},
booktitle = {Proceedings of the 15th ACM on International Conference on Multimodal Interaction},
pages = {591–598},
numpages = {8},
keywords = {complex learning environment, discourse analysis, multimodal learning analytics},
location = {Sydney, Australia},
series = {ICMI '13}
}

@inproceedings{10.1145/2883851.2883853,
author = {Bull, Susan and Ginon, Blandine and Boscolo, Clelia and Johnson, Matthew},
title = {Introduction of learning visualisations and metacognitive support in a persuadable open learner model},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883853},
doi = {10.1145/2883851.2883853},
abstract = {This paper describes open learner models as visualisations of learning for learners, with a particular focus on how information about their learning can be used to help them reflect on their skills, identify gaps in their skills, and plan their future learning. We offer an approach that, in addition to providing visualisations of their learning, allows learners to propose changes to their learner model. This aims to help improve the accuracy of the learner model by taking into account student viewpoints on their learning, while also promoting learner reflection on their learning as part of a discussion of the content of their learner model. This aligns well with recent calls for learning analytics for learners. Building on previous research showing that learners will use open learner models, we here investigate their initial reactions to open learner model features to identify the likelihood of uptake in contexts where an open learner model is offered on an optional basis. We focus on university students' perceptions of a range of visualisations and their stated preferences for a facility to view evidence for the learner model data and to propose changes to the values.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {30–39},
numpages = {10},
keywords = {learning analytics for learners, open learner models, persuading the learner model, visual learning analytics},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/3448139.3448184,
author = {Zhang, Yupei and An, Rui and Cui, Jiaqi and Shang, Xuequn},
title = {Undergraduate Grade Prediction in Chinese Higher Education Using Convolutional Neural Networks},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448184},
doi = {10.1145/3448139.3448184},
abstract = {Prediction of undergraduate grades before their course enrollments is beneficial to the student’s learning plan on selective courses and failure warnings to compulsory courses in Chinese higher education. This study proposed to use a deep learning-based model composed of sparse attention layers, convolutional neural layers, and a fully connected layer, called Sparse Attention Convolutional Neural Networks (SACNN), to predict undergraduate grades. Concretely, sparse attention layers response to the fact that courses have different contributions to the grade prediction of the target course; convolutional neural layers aim to capture the one-dimensional temporal feature on these courses organized in terms; the fully connected layer is to complete the final classification based on achieved features. We collected a dataset including grade records, student’s demographics and course descriptions from our institution in the past five years. The dataset contained about 54k grade records from 1307 students and 137 courses, where all mentioned methods were evaluated by the hold-out evaluation. The result shows SACNN achieves 81% prediction precision and 85% accuracy on the failure prediction, which is more effective than those compared methods. Besides, SACNN delivers a potential explanation to the reason of the predicted result, thanks to the sparse attention layer. This study provides a useful technique for personalized learning and course relationship discovery in undergraduate education.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {462–468},
numpages = {7},
keywords = {convolutional neural networks, grade prediction, personalized learning, sparse attention},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/2723576.2723606,
author = {Ferguson, Rebecca and Clow, Doug},
title = {Examining engagement: analysing learner subpopulations in massive open online courses (MOOCs)},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723606},
doi = {10.1145/2723576.2723606},
abstract = {Massive open online courses (MOOCs) are now being used across the world to provide millions of learners with access to education. Many learners complete these courses successfully, or to their own satisfaction, but the high numbers who do not finish remain a subject of concern for platform providers and educators. In 2013, a team from Stanford University analysed engagement patterns on three MOOCs run on the Coursera platform. They found four distinct patterns of engagement that emerged from MOOCs based on videos and assessments. However, not all platforms take this approach to learning design. Courses on the FutureLearn platform are underpinned by a social-constructivist pedagogy, which includes discussion as an important element. In this paper, we analyse engagement patterns on four FutureLearn MOOCs and find that only two clusters identified previously apply in this case. Instead, we see seven distinct patterns of engagement: Samplers, Strong Starters, Returners, Mid-way Dropouts, Nearly There, Late Completers and Keen Completers. This suggests that patterns of engagement in these massive learning environments are influenced by decisions about pedagogy. We also make some observations about approaches to clustering in this context.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {51–58},
numpages = {8},
keywords = {MOOCs, learner engagement patterns, learning analytics},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2883851.2883898,
author = {Giannakos, Michail N. and Sampson, Demetrios G. and Kidzi\'{n}ski, \L{}ukasz and Pardo, Abelardo},
title = {Smart environments and analytics on video-based learning},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883898},
doi = {10.1145/2883851.2883898},
abstract = {The International Workshop of Smart Environments and Analytics on Video-Based Learning (SE@VBL) aims to connect research efforts on Video-Based Learning with Smart Environments and Analytics to create synergies between these fields. The main objective is to build a research community around the intersection of these topical areas. In particular, SE@VBL aims to develop a critical discussion about the next generation of video-based learning environments and their analytics, the form of these analytics and the way they can be analyzed in order to help us to better understand and improve the value of educational videos to support teaching and learning. SE@VBL is based on the rationale that combining and analyzing learners' interactions with other available data obtained from learners, new avenues for research on video-based learning have emerged. This can have a significant impact in current educational trends such as Massive Open Online Courses (MOOCs) and Flipped Classroom.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {502–504},
numpages = {3},
keywords = {interaction design, learning analytics, smart environments, video-based learning, visual analytics},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2567574.2567582,
author = {Piety, Philip J. and Hickey, Daniel T. and Bishop, M. J.},
title = {Educational data sciences: framing emergent practices for analytics of learning, organizations, and systems},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567582},
doi = {10.1145/2567574.2567582},
abstract = {In this paper, we develop a conceptual framework for organizing emerging analytic activities involving educational data that can fall under broad and often loosely defined categories, including Academic/Institutional Analytics, Learning Analytics/Educational Data Mining, Learner Analytics/Personalization, and Systemic Instructional Improvement. While our approach is substantially informed by both higher education and K-12 settings, this framework is developed to apply across all educational contexts where digital data are used to inform learners and the management of learning. Although we can identify movements that are relatively independent of each other today, we believe they will in all cases expand from their current margins to encompass larger domains and increasingly overlap. The growth in these analytic activities leads to the need to find ways to synthesize understandings, find common language, and develop frames of reference to help these movements develop into a field.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {193–202},
numpages = {10},
keywords = {analytic approaches, big data, data-driven decisions, educational data mining, educational data science, learner analytics, learning analytics, methods, theories and theoretical concepts for understanding learning, tools for sense-making in learning analytics},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/3536221.3557031,
author = {Anderson, Khalil},
title = {Real-time Feedback for Developing Conversation Literacy},
year = {2022},
isbn = {9781450393904},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3536221.3557031},
doi = {10.1145/3536221.3557031},
abstract = {Learning Analytics (LA) has exploded with the growth in Machine Learning applications. LA, while not new, is always changing, and now allows for automated analyses that used to seem impossible. There is still a need for real-time systems while also centering the design around students and data. By using multiple data streams to provide real-time Learning Analytics, this paper aims to describe a plan for creating a system that aids teachers managing classes and improve the collaboration literacy of students while taking into account ethical, privacy, and user concerns. By surveying and engaging with students and teachers, general guidelines in terms of ethics and privacy in LA will be designed. Along with the data, there will be a way to best present this information in a useful and engaging method to students and teachers that would be determined through studies on the efficacy of different User Interfaces (UIs) and data views along with the surveying.},
booktitle = {Proceedings of the 2022 International Conference on Multimodal Interaction},
pages = {701–704},
numpages = {4},
keywords = {HCI, Learning Analytics, Multi-modal, audio processing, video processing},
location = {Bengaluru, India},
series = {ICMI '22}
}

@inproceedings{10.1145/2883851.2883941,
author = {Kevan, Jonathan M. and Menchaca, Michael P. and Hoffman, Ellen S.},
title = {Designing MOOCs for success: a student motivation-oriented framework},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883941},
doi = {10.1145/2883851.2883941},
abstract = {Considerable literature exists regarding MOOCs. Evaluations of MOOCs range from ringing endorsements to its vilification as a delivery model. Much evaluation focuses on completion rates and/or participant satisfaction. Overall, MOOCs are ill-defined and researchers struggle with appropriate evaluation criteria beyond attrition rates. In this paper, we provide a brief history of MOOCs, a summary of some evaluation research, and we propose a new model for evaluation with an example from a previously-delivered MOOC. Measurement of the MOOC success framework through four student satisfaction types is proposed in this paper with a model for informal learning satisfaction, one of the proposed types, theorized and tested. Results indicated theoretical underpinnings, while intended to improve instruction, might not have influenced the same satisfaction construct. Therefore, future research into alternative satisfaction factor models is needed.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {274–278},
numpages = {5},
keywords = {MOOC, confirmatory factor analysis, framework, learning analytics, motivation, structural equation modeling},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/3303772.3303805,
author = {Aguilar, Stephen J. and Baek, Clare},
title = {Motivated Information Seeking and Graph Comprehension Among College Students},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303805},
doi = {10.1145/3303772.3303805},
abstract = {Learning Analytics Dashboards (LADs) are predicated on the notion that access to more academic information can help students regulate their academic behaviors, but what is the association between information seeking preferences and help-seeking practices among college students? If given access to more information, what might college students do with it?We investigated these questions in a series of two studies. Study 1 validates a measure of information-seeking preferences---the Motivated Information-Seeking Questionnaire (MISQ)----using a college student sample drawn from across the country (n = 551). In a second study, we used the MISQ to measure college students' (n=210) performance-avoid (i.e., avoiding seeming incompetent in relation to one's peers) and performance-approach (i.e., wishing to outperform one's peers) information seeking preferences, their help-seeking behaviors, and their ability to comprehend line graphs and bar graphs---two common graphs types for LADs.Results point to a negative relationship between graph comprehension and help-seeking strategies, such as attending office hours, emailing one's professor for help, or visiting a study center---even after controlling for academic performance and demographic characteristics. This suggests that students more capable of readings graphs might not seek help when needed. Further results suggest a positive relationship between performance-approach information-seeking preferences, and how often students compare themselves to their peers.This study contributes to our understanding of the motivational implications of academic data visualizations in academic settings, and increases our knowledge of the way students interpret visualizations. It uncovers tensions between what students want to see, versus what it might be more motivationally appropriate for them to see. Importantly, the MISQ and graph comprehension measure can be used in future studies to better understand the role of students' information seeking tendencies with regard to their interpretation of various kinds of feedback present in LADs.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {280–289},
numpages = {10},
keywords = {Higher Education, Instrument Validation, Motivation, Non-cognitive factors, Visualizations},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3170358.3170411,
author = {Long, Yanjin and Holstein, Kenneth and Aleven, Vincent},
title = {What exactly do students learn when they practice equation solving? refining knowledge components with the additive factors model},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170411},
doi = {10.1145/3170358.3170411},
abstract = {Accurately modeling individual students' knowledge growth is important in many applications of learning analytics. A key step is to decompose the knowledge targeted in the instruction into detailed knowledge components (KCs). We search for an accurate KC model for basic equation solving skills, using data from an intelligent tutoring system (ITS), Lynnette. Key criteria are data fit and predictive accuracy based on a standard logistic model called the Additive Factors Model (AFM). We focus on three difficulty factors for equation solving: understanding of variables, the negative sign, and the complexity of the equation. Fine-grained KC models were found to have greater fit and predictive accuracy than an "ideal," more abstract model, indicating that there is substantial under-generalization in students' equation-solving skill related to all three difficulty factors. The work enhances scientific understanding of the challenges students face in learning equation solving. It illustrates how learning analytics could inform the improvement of technology-enhanced learning environments.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {399–408},
numpages = {10},
keywords = {K-12, educational data mining, equation solving, intelligent tutoring systems, knowledge components, student modeling},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3303772.3303813,
author = {Azcona, David and Arora, Piyush and Hsiao, I-Han and Smeaton, Alan},
title = {user2code2vec: Embeddings for Profiling Students Based on Distributional Representations of Source Code},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303813},
doi = {10.1145/3303772.3303813},
abstract = {In this work, we propose a new methodology to profile individual students of computer science based on their programming design using a technique called embeddings. We investigate different approaches to analyze user source code submissions in the Python language. We compare the performances of different source code vectorization techniques to predict the correctness of a code submission. In addition, we propose a new mechanism to represent students based on their code submissions for a given set of laboratory tasks on a particular course. This way, we can make deeper recommendations for programming solutions and pathways to support student learning and progression in computer programming modules effectively at a Higher Education Institution. Recent work using Deep Learning tends to work better when more and more data is provided. However, in Learning Analytics, the number of students in a course is an unavoidable limit. Thus we cannot simply generate more data as is done in other domains such as FinTech or Social Network Analysis. Our findings indicate there is a need to learn and develop better mechanisms to extract and learn effective data features from students so as to analyze the students' progression and performance effectively.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {86–95},
numpages = {10},
keywords = {Code Embeddings, Computer Science Education, Distributed Representations, Machine Learning, Representation Learning for Source Code, code2vec, user2code2vec},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/2883851.2883856,
author = {Ringtved, Ulla and Milligan, Sandra and Corrin, Linda},
title = {Learning design and feedback processes at scale: stocktaking emergent theory and practice},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883856},
doi = {10.1145/2883851.2883856},
abstract = {Design for learning in scaled courses is shifting away from replication of traditional on-campus or online teaching towards exploiting the distinctive characteristic and potentials of scale to transform both teaching and learning. Scaled learning environments such as MOOCs may represent a new paradigm for teaching. This workshop involves consideration of the how learning occurs in scaled environments, and how learning designers and analysts can assist. It will explore questions at the heart of effective learning design, using expert panelists and collaborative knowledge-building techniques to arrive at a stocktake of thinking.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {479–480},
numpages = {2},
keywords = {crowd-sourced learning, feedback, learning analytics, learning at scale, learning design, management performance, scaled courses},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/3375462.3375506,
author = {Lang, Charles and Woo, Charlotte and Sinclair, Jeanne},
title = {Quantifying data sensitivity: precise demonstration of care when building student prediction models},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375506},
doi = {10.1145/3375462.3375506},
abstract = {Until recently an assumption within the predictive modelling community has been that collecting more student data is always better. But in reaction to recent high profile data privacy scandals, many educators, scholars, students and administrators have been questioning the ethics of such a strategy. Suggestions are growing that the minimum amount of data should be collected to aid the function for which a prediction is being made. Yet, machine learning algorithms are primarily judged on metrics derived from prediction accuracy or whether they meet probabilistic criteria for significance. They are not routinely judged on whether they utilize the minimum number of the least sensitive features, preserving what we name here as data collection parsimony. We believe the ability to assess data collection parsimony would be a valuable addition to the suite of evaluations for any prediction strategy and to that end, the following paper provides an introduction to data collection parsimony, describes a novel method for quantifying the concept using empirical Bayes estimates and then tests the metric on real world data. Both theoretical and empirical benefits and limitations of this method are discussed. We conclude that for the purpose of model building this metric is superior to others in several ways, but there are some hurdles to effective implementation.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {655–664},
numpages = {10},
keywords = {data collection parsimony, data privacy, data sensitivity, empirical bayes, ethics, prediction models, student models},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3303772.3303812,
author = {NeCamp, Timothy and Gardner, Josh and Brooks, Christopher},
title = {Beyond A/B Testing: Sequential Randomization for Developing Interventions in Scaled Digital Learning Environments},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303812},
doi = {10.1145/3303772.3303812},
abstract = {Randomized experiments ensure robust causal inference that is critical to effective learning analytics research and practice. However, traditional randomized experiments, like A/B tests, are limiting in large scale digital learning environments. While traditional experiments can accurately compare two treatment options, they are less able to inform how to adapt interventions to continually meet learners' diverse needs. In this work, we introduce a trial design for developing adaptive interventions in scaled digital learning environments -- the sequential randomized trial (SRT). With the goal of improving learner experience and developing interventions that benefit all learners at all times, SRTs inform how to sequence, time, and personalize interventions. In this paper, we provide an overview of SRTs, and we illustrate the advantages they hold compared to traditional experiments. We describe a novel SRT run in a large scale data science MOOC. The trial results contextualize how learner engagement can be addressed through culturally-targeted reminder emails. We also provide practical advice for researchers who aim to run their own SRTs to develop adaptive interventions in scaled digital learning environments.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {539–548},
numpages = {10},
keywords = {Experimental design, MOOCs, sequential randomization},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/3027385.3027413,
author = {Avila, Cecilia and Baldiris, Silvia and Fabregat, Ramon and Graf, Sabine},
title = {ATCE: an analytics tool to trace the creation and evaluation of inclusive and accessible open educational resources},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027413},
doi = {10.1145/3027385.3027413},
abstract = {The creation of Inclusive and Accessible Open Educational Resources (IA-OERs) is a challenge for teachers because they have to invest time and effort to create learning contents considering students' learning needs and preferences. An IA-OER is characterized by its alignment with the Universal Design Learning (UDL) principles, the quality on its contents and the web accessibility as a way to address the diversity of students. Creating an IA-OER with these characteristics is not a straightforward task, especially when teachers do not have enough information/feedback to make decisions on how to improve the learning contents. In this paper we introduce ATCE - an Analytics Tool to Trace the Creation and Evaluation of IA-OERs. This tool focuses in particular on the accessibility and quality of the IA-OERs. ATCE was developed as a module within the ATutor Learning Management System (LMS). An analytics dashboard with visualizations related to the teachers' competences in the creation and evaluation of IA-OERs was included as part of the tool. This paper also presents a use case of the visualizations obtained from the creation and evaluation of one IA-OER after using our analytics tool.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {183–187},
numpages = {5},
keywords = {competences, learning analytics, open educational resources, quality, teachers, web accessibility},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/2460296.2460332,
author = {Clow, Doug},
title = {MOOCs and the funnel of participation},
year = {2013},
isbn = {9781450317856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2460296.2460332},
doi = {10.1145/2460296.2460332},
abstract = {Massive Online Open Courses (MOOCs) are growing substantially in numbers, and also in interest from the educational community. MOOCs offer particular challenges for what is becoming accepted as mainstream practice in learning analytics.Partly for this reason, and partly because of the relative newness of MOOCs as a widespread phenomenon, there is not yet a substantial body of literature on the learning analytics of MOOCs. However, one clear finding is that drop-out/non-completion rates are substantially higher than in more traditional education.This paper explores these issues, and introduces the metaphor of a 'funnel of participation' to reconceptualise the steep drop-off in activity, and the pattern of steeply unequal participation, which appear to be characteristic of MOOCs and similar learning environments. Empirical data to support this funnel of participation are presented from three online learning sites: iSpot (observations of nature), Cloudworks ('a place to share, find and discuss learning and teaching ideas and experiences'), and openED 2.0, a MOOC on business and management that ran between 2010--2012. Implications of the funnel for MOOCs, formal education, and learning analytics practice are discussed.},
booktitle = {Proceedings of the Third International Conference on Learning Analytics and Knowledge},
pages = {185–189},
numpages = {5},
keywords = {MOOCs, learning analytics, participation},
location = {Leuven, Belgium},
series = {LAK '13}
}

@inproceedings{10.1145/3170358.3170417,
author = {Millecamp, Martijn and Guti\'{e}rrez, Francisco and Charleer, Sven and Verbert, Katrien and De Laet, Tinne},
title = {A qualitative evaluation of a learning dashboard to support advisor-student dialogues},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170417},
doi = {10.1145/3170358.3170417},
abstract = {This paper presents an evaluation of a learning dashboard that supports the dialogue between a student and a study advisor. The dashboard was designed, developed, and evaluated in collaboration with study advisers. To ensure scalability to other contexts, the dashboard uses data that is commonly available at any higher education institute. It visualizes the grades of the student, an overview of the progress through the year, his/her position in comparison with peers, sliders to plan the next years and a prediction of the length of the bachelor program for this student in years based on historic data. The dashboard was deployed at KU Leuven, Belgium and used in September 2017 to support 224 sessions between students and study advisers. We observed twenty of these conversations. We also collected feedback from 101 students with questionnaires. Results of our observations indicate that the dashboard primarily triggers insights at the beginning of a conversation. The number of insights and the level of these insights (factual, interpretative and reflective) depends on the context of the conversation. Most insights were triggered in conversations with students doubting to continue the program, indicating that our dashboard is useful to support difficult decision-making processes.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {56–60},
numpages = {5},
keywords = {information visualization, insights, learning analytics dashboards, learning technologies},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3448139.3448206,
author = {Li, Fanjie and Wang, Zuo and Tzi Dong Ng, Jeremy and Hu, Xiao},
title = {Studying with Learners’ Own Music: Preliminary Findings on Concentration and Task Load},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448206},
doi = {10.1145/3448139.3448206},
abstract = {Through profiling learners’ music usage in everyday learning settings and depicting their learning experience when studying with a music app powered by a large-scale and real-world music library, this study revealed preliminary observations on how background music impacts learning under varying task load, and manifested intriguing patterns of learners’ music usage and music preferences in various task load conditions. Specifically, we piloted a three-day field experiment in students’ everyday learning environment. During the experiment, participants performed learning tasks with music in the background and completed a set of online surveys before and after each learning session. Our results suggested that learners’ self-selected, real-life background music could enhance their learning effectiveness, while the beneficial effect of background music was more apparent when the learning task was less mentally or temporally demanding. Towards a closer look at the characteristics of preferable music pieces under various task load conditions, our findings showed that music preferred by participants under high versus low temporal demand differs in a number of characteristics, including speechiness, acousticness, danceability, and energy. This study further reveals the effects of background music on learning under varying task load levels and provides implications for context-aware background music selection when designing musically enriched learning environments.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {613–619},
numpages = {7},
keywords = {Background music, Field experiment, Learning environment, Learning experience, Task load},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3303772.3303838,
author = {Doroudi, Shayan and Brunskill, Emma},
title = {Fairer but Not Fair Enough On the Equitability of Knowledge Tracing},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303838},
doi = {10.1145/3303772.3303838},
abstract = {Adaptive educational technologies have the capacity to meet the needs of individual students in theory, but in some cases, the degree of personalization might be less than desired, which could lead to inequitable outcomes for students. In this paper, we use simulations to demonstrate that while knowledge tracing algorithms are substantially more equitable than giving all students the same amount of practice, such algorithms can still be inequitable when they rely on inaccurate models. This can arise as a result of two factors: (1) using student models that are fit to aggregate populations of students, and (2) using student models that make incorrect assumptions about student learning. In particular, we demonstrate that both the Bayesian knowledge tracing algorithm and the N-Consecutive Correct Responses heuristic are susceptible to these concerns, but that knowledge tracing with the additive factor model may be more equitable. The broader message of this paper is that when designing learning analytics algorithms, we need to explicitly consider whether the algorithms act fairly with respect to different populations of students, and if not, how we can make our algorithms more equitable.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {335–339},
numpages = {5},
keywords = {equity, fairness, knowledge tracing, model misspecification},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/2883851.2883908,
author = {Feng, Mingyu and Krumm, Andrew E. and Bowers, Alex J. and Podkul, Timothy},
title = {Elaborating data intensive research methods through researcher-practitioner partnerships},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883908},
doi = {10.1145/2883851.2883908},
abstract = {Technologies used by teachers and students generate vast amounts of data that can be analyzed to provide insights into improving teaching and learning. However, practitioners are left out of the process. We describe the development of an approach by which researchers and practitioners can work together to use data intensive research methods to launch improvement efforts within schools. This paper describes elements of the first year of a researcher-practitioner partnership, highlighting initial findings, challenges, and strategies for overcoming these challenges.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {540–541},
numpages = {2},
keywords = {data intensive research, learning analytics, researcher-practitioner partnership},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883927,
author = {Prieto, Luis P. and Sharma, Kshitij and Dillenbourg, Pierre and Jes\'{u}s, Mar\'{\i}a},
title = {Teaching analytics: towards automatic extraction of orchestration graphs using wearable sensors},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883927},
doi = {10.1145/2883851.2883927},
abstract = { 'Teaching analytics' is the application of learning analytics techniques to understand teaching and learning processes, and eventually enable supportive interventions. However, in the case of (often, half-improvised) teaching in face-to-face classrooms, such interventions would require first an understanding of what the teacher actually did, as the starting point for teacher reflection and inquiry. Currently, such teacher enactment characterization requires costly manual coding by researchers. This paper presents a case study exploring the potential of machine learning techniques to automatically extract teaching actions during classroom enactment, from five data sources collected using wearable sensors (eye-tracking, EEG, accelerometer, audio and video). Our results highlight the feasibility of this approach, with high levels of accuracy in determining the social plane of interaction (90%, κ=0.8). The reliable detection of concrete teaching activity (e.g., explanation vs. questioning) accurately still remains challenging (67%, κ=0.56), a fact that will prompt further research on multimodal features and models for teaching activity extraction, as well as the collection of a larger multimodal dataset to improve the accuracy and generalizability of these methods.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {148–157},
numpages = {10},
keywords = {activity detection, multimodal learning analytics, teacher reflection, teaching analytics, wearable sensors},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2883851.2883918,
author = {Clow, Doug and Ferguson, Rebecca and Macfadyen, Leah and Prinsloo, Paul and Slade, Sharon},
title = {LAK failathon},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883918},
doi = {10.1145/2883851.2883918},
abstract = {As in many fields, most papers in the learning analytics literature report success or, at least, read as if they are reporting success. This is almost certainly not because learning analytics research and activity are always successful. Generally, we report our successes widely, but keep our failures to ourselves. As Bismarck is alleged to have said: it is wise to learn from the mistakes of others. This workshop offers an opportunity for researchers and practitioners to share their failures in a lower-stakes environment, to help them learn from each other's mistakes.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {509–511},
numpages = {3},
keywords = {failure, negative results, positive results, publication bias},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2723576.2723611,
author = {Pardo, Abelardo and Mirriahi, Negin and Dawson, Shane and Zhao, Yu and Zhao, An and Ga\v{s}evi\'{c}, Dragan},
title = {Identifying learning strategies associated with active use of video annotation software},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723611},
doi = {10.1145/2723576.2723611},
abstract = {The higher education sector has seen a shift in teaching approaches over the past decade with an increase in the use of video for delivering lecture content as part of a flipped classroom or blended learning model. Advances in video technologies have provided opportunities for students to now annotate videos as a strategy to support their achievement of the intended learning outcomes. However, there are few studies exploring the relationship between video annotations, student approaches to learning, and academic performance. This study seeks to narrow this gap by investigating the impact of students' use of video annotation software coupled with their approaches to learning and academic performance in the context of a flipped learning environment. Preliminary findings reveal a significant positive relationship between annotating videos and exam results. However, negative effects of surface approaches to learning, cognitive strategy use and test anxiety on midterm grades were also noted. This indicates a need to better promote and scaffold higher order cognitive strategies and deeper learning with the use of video annotation software.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {255–259},
numpages = {5},
keywords = {learning analytics, learning strategies, video annotation software},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/3573051.3593375,
author = {Buckingham Shum, Simon},
title = {Trust, Sustainability and Learning@Scale},
year = {2023},
isbn = {9798400700255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573051.3593375},
doi = {10.1145/3573051.3593375},
abstract = {It is not overstating matters to say that humanity finds itself at an inflection point. The interlocking crises can feel overwhelming (ecological; political; financial; medical; technological; educational...), with recent leaps in AI closing the gap between human and machine cognition, raising many issues, including of course, educational questions. If a plausible diagnosis for our predicament as a species is "failure to learn", praxis questions assail us. What qualities should we cultivate most urgently, in which contexts? What literacies equip teachers and learners to engage critically with AI? How do we track progress meaningfully, at scale? And very pragmatically, when and why do people deem our tools trustworthy enough to trial, and if robust, embed sustainably into their teaching and learning practices?Taking a complex organisation as a microcosm of these challenges, I offer some reflections through the prism of nine years running a university learning analytics innovation centre. We invent and evaluate analytics and AI targeting student qualities that transcend the disciplines, such as critical and reflective writing, teamwork, dispositions, and sense of belonging. Without trust we cannot deploy at scale, motivating our use of methods from human-centered design and deliberative democracy to build common ground among diverse stakeholders.Perhaps the approaches and lessons learnt in this small but nonetheless complex system can scale fractally, offering insights for our wider challenges. Whatever the scale, it seems that trust, sustainability and learning reinforce each other, and must shape how we conceive, design and deploy our learning infrastructures.},
booktitle = {Proceedings of the Tenth ACM Conference on Learning @ Scale},
pages = {1–2},
numpages = {2},
keywords = {artificial intelligence, education, learning analytics, sustainability, trust},
location = {Copenhagen, Denmark},
series = {L@S '23}
}

@inproceedings{10.1145/3375462.3375466,
author = {Lang, David and Chen, Guanling and Mirzaei, Kathy and Paepcke, Andreas},
title = {Is faster better? a study of video playback speed},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375466},
doi = {10.1145/3375462.3375466},
abstract = {We explore the relationship between video playback speed and student learning outcomes. Using an experimental design, we present the results of a pre-registered study that assigns users to watch videos at either 1.0x or 1.25x speed. We find that students who consume sped content are more likely to get better grades in a course, attempt more content, and obtain more certificates. We also find that when videos are sped up, students spend less time consuming videos and are marginally more likely to complete more video content. These findings suggest that future study of playback speed as a tool for optimizing video content for MOOCs is warranted. Applications for reinforcement learning and adaptive content are discussed.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {260–269},
numpages = {10},
keywords = {MOOCs, clickstreams, playback speed, randomized controlled trials, video analytics},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/2883851.2883940,
author = {Jayaprakash, Sandeep M. and Laur\'{\i}a, Eitel J. M. and Gandhi, Pritesh and Mendhe, Dinesh},
title = {Benchmarking student performance and engagement in an early alert predictive system using interactive radar charts},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883940},
doi = {10.1145/2883851.2883940},
abstract = {This poster synthesizes the design features of a visualization layer applied on the Open Academic Analytics Initiative (OAAI), an open source academic early alert system based on predictive analytics. The poster explores ways to convey the predictive model outputs and benchmark student performances using visually intuitive radar plots.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {526–527},
numpages = {2},
keywords = {benchmarking, data mining, information visualizations, instructional assessment, intervention, interventions, learning analytics, open source, predictive analytics, visualization},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/3657604.3664666,
author = {Li, Hai and Xing, Wanli and Li, Chenglu and Zhu, Wangda and Heffernan, Neil},
title = {Positive Affective Feedback Mechanisms in an Online Mathematics Learning Platform},
year = {2024},
isbn = {9798400706332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3657604.3664666},
doi = {10.1145/3657604.3664666},
abstract = {This research aims to investigate the allocation mechanisms of written positive affective feedback (PAF) in online mathematics assignments provided by teachers, employing multimodal learning analytics. We extract mathematical text features and readability indicators from teacher comments, utilizing collinearity matrices for linear feature selection. Student multimodal response patterns are obtained through clustering. To analyze the teacher comment strategies under different student response patterns, Mann-Whitney U tests were employed to investigate differences in student scores and feedback readability between scenarios with and without PAF. Our findings uncover the linguistic characteristics of teacher-provided PAF and the corresponding strategies they adopt. Teachers are more inclined to offer PAF to K-12 students with higher scores, challenging assignments, and younger ages. The study points out potential imbalances in the allocation of teacher PAF and emphasizes key factors that teachers need to consider when providing PAF. The findings offer new insights for educators to contemplate on designing and implementing more effective PAF strategies.},
booktitle = {Proceedings of the Eleventh ACM Conference on Learning @ Scale},
pages = {371–375},
numpages = {5},
keywords = {multimodal learning analytics, positive affective feedback, response pattern},
location = {Atlanta, GA, USA},
series = {L@S '24}
}

@inproceedings{10.1145/3303772.3303808,
author = {Fiacco, James and Cotos, Elena and Ros\'{e}, Carolyn},
title = {Towards Enabling Feedback on Rhetorical Structure with Neural Sequence Models},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303808},
doi = {10.1145/3303772.3303808},
abstract = {Analysis of student writing, both for assessment and for enabling feedback have been of interest to the field of learning analytics. While much progress can be made through detection of local cues in writing, structured prediction approaches offer capabilities that are particularly well tailored to the needs of models aiming to offer substantive feedback on rhetorical structure. We thus cast the analysis of rhetorical structure in academic writing as a structured prediction task in which we employ models that leverage both local and global cues in writing. In particular, this paper presents a hierarchical neural architecture that performs this task. The evaluation demonstrates that the architecture achieves near-human performance while significantly surpassing state-of-the-art baselines. A multifaceted approach to model interpretation offers insights into the inner workings of the model.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {310–319},
numpages = {10},
keywords = {Rhetorical structure, automatic essay evaluation, bidirectional LSTM, conditional random field, hierarchical, neural network interpretation, neural sequence model, writing feedback},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/2883851.2883951,
author = {Chen, Ye and Yu, Bei and Zhang, Xuewei and Yu, Yihan},
title = {Topic modeling for evaluating students' reflective writing: a case study of pre-service teachers' journals},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883951},
doi = {10.1145/2883851.2883951},
abstract = {Journal writing is an important and common reflective practice in education. Students' reflection journals also offer a rich source of data for formative assessment. However, the analysis of the textual reflections in class of large size presents challenges. Automatic analysis of students' reflective writing holds great promise for providing adaptive real time support for students. This paper proposes a method based on topic modeling techniques for the task of themes exploration and reflection grade prediction. We evaluated this method on a sample of journal writings from pre-service teachers. The topic modeling method was able to discover the important themes and patterns emerged in students' reflection journals. Weekly topic relevance and word count were identified as important indicators of their journal grades. Based on the patterns discovered by topic modeling, prediction models were developed to automate the assessing and grading of reflection journals. The findings indicate the potential of topic modeling in serving as an analytic tool for teachers to explore and assess students' reflective thoughts in written journals.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {1–5},
numpages = {5},
keywords = {LDA, automated grading, education, journal writing, learning analytics, reflection, text mining, topic modeling},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/3375462.3375528,
author = {Jung, Yeonji and Wise, Alyssa Friend},
title = {How and how well do students reflect? multi-dimensional automated reflection assessment in health professions education},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375528},
doi = {10.1145/3375462.3375528},
abstract = {Reflection assessment is a critical component of health professions education that can be used for personalized learning support. However, reflection assessment at scale remains a challenge due to the demanding nature of tasks and the common use of simplified criteria of quality. This study addressed this issue by developing a multi-dimensional automated assessment that uses linguistic models to classify reflections by overall quality (depth) and the presence of six constituent elements denoting quality (description, analysis, feeling, perspective, evaluation, and outcome). 1500 reflections from 369 dental students were manually coded to establish ground truth. Classifiers for each of the six elements were trained and tested based on linguistic features extracted using the LIWC tool applying both single-label and multi-label classification approaches. Classifiers for depth were built both directly from linguistic features and based on the presence of the six elements. Results showed that linguistic modeling can be used to reliably detect the presence of reflection elements and the level of depth. However, the depth classifier showed a heavy reliance on cognitive elements (description, analysis, and evaluation) rather than the others. These findings indicate the feasibility of implementing multidimensional automated assessment in health professions education and the need to reconsider how quality of reflection is conceptualized.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {595–604},
numpages = {10},
keywords = {classification, content analysis, health professions education, natural language processing, reflection, reflection assessment},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375486,
author = {Niemeijer, Koen and Feskens, Remco and Krempl, Georg and Koops, Jesse and Brinkhuis, Matthieu J. S.},
title = {Constructing and predicting school advice for academic achievement: a comparison of item response theory and machine learning techniques},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375486},
doi = {10.1145/3375462.3375486},
abstract = {Educational tests can be used to estimate pupils' abilities and thereby give an indication of whether their school type is suitable for them. However, tests in education are usually conducted for each content area separately which makes it difficult to combine these results into one single school advice. To help with school advice, we provide a comparison between both domain-specific and domain-agnostic methods for predicting school types. Both use data from a pupil monitoring system in the Netherlands, a system that keeps track of pupils' educational progress over several years by a series of tests measuring multiple skills.A domain-specific item response theory (IRT) model is calibrated from which an ability score is extracted and is subsequently plugged into a multinomial log-linear regression model. Second, we train domain-agnostic machine learning (ML) models. These are a random forest (RF) and a shallow neural network (NN). Furthermore, we apply case weighting to give extra attention to pupils who switched between school types.When considering the performance of all pupils, RFs provided the most accurate predictions followed by NNs and IRT respectively. When only looking at the performance of pupils who switched school type, IRT performed best followed by NNs and RFs. Case weighting proved to provide a major improvement for this group. Lastly, IRT was found to be much easier to explain in comparison to the other models. Thus, while ML provided more accurate results, this comes at the cost of a lower explainability in comparison to IRT.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {462–471},
numpages = {10},
keywords = {e-learning, explainable AI, item response theory, machine learning, neural networks, random forests},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3303772.3303795,
author = {Ding, Mucong and Yang, Kai and Yeung, Dit-Yan and Pong, Ting-Chuen},
title = {Effective Feature Learning with Unsupervised Learning for Improving the Predictive Models in Massive Open Online Courses},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303795},
doi = {10.1145/3303772.3303795},
abstract = {The effectiveness of learning in massive open online courses (MOOCs) can be significantly enhanced by introducing personalized intervention schemes which rely on building predictive models of student learning behaviors such as some engagement or performance indicators. A major challenge that has to be addressed when building such models is to design handcrafted features that are effective for the prediction task at hand. In this paper, we make the first attempt to solve the feature learning problem by taking the unsupervised learning approach to learn a compact representation of the raw features with a large degree of redundancy. Specifically, in order to capture the underlying learning patterns in the content domain and the temporal nature of the clickstream data, we train a modified auto-encoder (AE) combined with the long short-term memory (LSTM) network to obtain a fixed-length embedding for each input sequence. When compared with the original features, the new features that correspond to the embedding obtained by the modified LSTM-AE are not only more parsimonious but also more discriminative for our prediction task. Using simple supervised learning models, the learned features can improve the prediction accuracy by up to 17% compared with the supervised neural networks and reduce overfitting to the dominant low-performing group of students, specifically in the task of predicting students' performance. Our approach is generic in the sense that it is not restricted to a specific supervised learning model nor a specific prediction task for MOOC learning analytics.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {135–144},
numpages = {10},
keywords = {Autoencoder, Dimensionality Reduction, Feature Learning, Learning Behavior, Long Short-Term Memory, Unsupervised Learning},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.1145/2883851.2883942,
author = {Manai, Ouajdi and Yamada, Hiroyuki and Thorn, Christopher},
title = {Real-time indicators and targeted supports: using online platform data to accelerate student learning},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883942},
doi = {10.1145/2883851.2883942},
abstract = {Statway® is one of the Community College Pathways initiatives designed to promote students' success in their developmental math sequence and reduce the time required to earn college credit. A recent causal analysis confirmed that Statway dramatically increased students' success rates in half the time across two different cohorts. These impressive results were also obtained across gender and race/ethnicity groups. However, there is still room for improvement. Students who did not succeed in Statway often did not complete the first of the two-course sequence. Therefore, the objective of this study is to formulate a series of indicators from self-report and online learning system data, alerting instructors to students' progress during the first weeks of the first course in the Statway sequence.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {183–187},
numpages = {5},
keywords = {cognitive and non-cognitive factors, community college developmental mathematics, hierarchical linear modeling, learning analytics, networked improvement community, online engagement},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/3375462.3375496,
author = {Barbosa, Gian and Camelo, Raissa and Cavalcanti, Anderson Pinheiro and Miranda, P\'{e}ricles and Mello, Rafael Ferreira and Kovanovi\'{c}, Vitomir and Ga\v{s}evi\'{c}, Dragan},
title = {Towards automatic cross-language classification of cognitive presence in online discussions},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375496},
doi = {10.1145/3375462.3375496},
abstract = {This paper presents a study that examined automated cross-language classification of online discussion messages for the levels of cognitive presence, a key construct from the widely used Community of Inquiry (CoI) model of online learning. Specifically, we examined the classification of 1,500 Portuguese language discussion messages using a classifier trained on a corpus of the 1,747 English language discussion messages. In the study, a random forest classifier was developed using a small set of 108 validated indicators of psychological processes, linguistic coherence, and online discussion structure. The classifier obtained 67% accuracy and Cohen's κ of 0.32, showing a moderate level of inter-rater agreement above chance and the general viability of the proposed approach. Most importantly, the findings suggest that certain aspects of cognitive presence construct are highly generalizable and transfer across different languages. Finally, the paper also presents a novel method for addressing class imbalance problem using a generic algorithm heuristic technique, which provided substantial improvements over the use of imbalanced dataset. Results and practical implications are further discussed.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {605–614},
numpages = {10},
keywords = {community of inquiry model, content analytics, cross-language classification, online discussion, optimization},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/2723576.2723632,
author = {Holman, Caitlin and Aguilar, Stephen J. and Levick, Adam and Stern, Jeff and Plummer, Benjamin and Fishman, Barry},
title = {Planning for success: how students use a grade prediction tool to win their classes},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723632},
doi = {10.1145/2723576.2723632},
abstract = {Gameful course designs require a significant shift in approach for both students and instructors. Transforming a standard course into a good game involves fundamentally altering how the course functions, most notably by giving students greater control over their work. We have developed an application, GradeCraft, to support this shift in pedagogy. A key feature of the application is the Grade Predictor, where students can explore coursework options and plan pathways to success. We observed students in two gameful courses with differing designs using the Grade Predictor in similar ways: they spent similar amounts of time per session, increased usage when assignments were due and before making significant course decisions, predicted different types of assignments at different rates, and made more predictions in preparation for the end of semester. This study describes how students plan their coursework using the GradeCraft Grade Predictor tool.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {260–264},
numpages = {5},
keywords = {design-research, gameful instruction, higher education, learning analytics},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/3375462.3375497,
author = {Duff, Andrea and Zamecnik, Andrew and Pardo, Abelardo and Smith, Elizabeth},
title = {The SEIRA approach: course embedded activities to promote academic integrity and literacies in first year engineering},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375497},
doi = {10.1145/3375462.3375497},
abstract = {Students enrol into STEM programs with varying degrees of confidence with citing and referencing texts in their written work. Students often have an inclination to choose numbers over written language throughout schooling which means less opportunity to practice referencing and citation. This is compounded by large numbers of students for whom English is an additional language or who articulate from different cultural ways-of-doing. The Search, Evaluate, Integrate, Reference and Act Ethically (SEIRA) modules were developed to provide discipline-relevance to a confounding task. Data Analysis looking at the student engagement with the SEIRA site and subsequent student success provides an indication of the value of this approach to developing academic literacy across the STEM disciplines.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {218–223},
numpages = {6},
keywords = {academic integrity, academic literacy, plagiarism},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/2883851.2883928,
author = {Joksimovi\'{c}, Sre\'{c}ko and Manataki, Areti and Ga\v{s}evi\'{c}, Dragan and Dawson, Shane and Kovanovi\'{c}, Vitomir and de Kereki, In\'{e}s Friss},
title = {Translating network position into performance: importance of centrality in different network configurations},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883928},
doi = {10.1145/2883851.2883928},
abstract = {As the field of learning analytics continues to mature, there is a corresponding evolution and sophistication of the associated analytical methods and techniques. In this regard social network analysis (SNA) has emerged as one of the cornerstones of learning analytics methodologies. However, despite the noted importance of social networks for facilitating the learning process, it remains unclear how and to what extent such network measures are associated with specific learning outcomes. Motivated by Simmel's theory of social interactions and building on the argument that social centrality does not always imply benefits, this study aimed to further contribute to the understanding of the association between students' social centrality and their academic performance. The study reveals that learning analytics research drawing on SNA should incorporate both - descriptive and statistical methods to provide a more comprehensive and holistic understanding of a students' network position. In so doing researchers can undertake more nuanced and contextually salient inferences about learning in network settings. Specifically, we show how differences in the factors framing students' interactions within two instances of a MOOC affect the association between the three social network centrality measures (i.e., degree, closeness, and betweenness) and the final course outcome.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {314–323},
numpages = {10},
keywords = {ERGM, MOOC, academic achievement, learning, social network analysis, social processes},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2723576.2723634,
author = {Hickey, Daniel T. and Quick, Joshua D. and Shen, Xinyi},
title = {Formative and summative analyses of disciplinary engagement and learning in a big open online course},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723634},
doi = {10.1145/2723576.2723634},
abstract = {Situative theories of knowing and participatory approaches to learning and assessment were used to offer a big open online course on Educational Assessment using Google CourseBuilder in 2013. The course was started by 160 students and completed by 60, with relatively extensive instructor interaction with individual learners. This yielded much higher levels of engagement and learning than are typical of open or conventional online courses. The course was further refined and offered a second time in 2014, where it was started by 76 students and completed by 22, with a much lower level of support. Comparable levels of engagement and learning were obtained, suggesting that this participatory approach to learning and assessment can indeed be managed with more typical instructor support. Nonetheless, additional automation and streamlining is called for if the model is to eventually be used in massive online courses with thousands of students or as an autonomous self-paced open course.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {310–314},
numpages = {5},
keywords = {analytic approaches, assessment, learning analytics, personalized learning, social learning analysis},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/3448139.3448151,
author = {Jensen, Emily and Umada, Tetsumichi and Hunkins, Nicholas C. and Hutt, Stephen and Huggins-Manley, A. Corinne and D'Mello, Sidney K.},
title = {What You Do Predicts How You Do: Prospectively Modeling Student Quiz Performance Using Activity Features in an Online Learning Environment},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448151},
doi = {10.1145/3448139.3448151},
abstract = {Students using online learning environments need to effectively self-regulate their learning. However, with an absence of teacher-provided structure, students often resort to less effective, passive learning strategies versus constructive ones. We consider the potential benefits of interventions that promote retrieval practice – retrieving learned content from memory – which is an effective strategy for learning and retention. The goal is to nudge students towards completing short, formative quizzes when they are likely to succeed on those assessments. Towards this goal, we developed a machine-learning model using data from 32,685 students who used an online mathematics platform over an entire school year to prospectively predict scores on three-item assessments (N = 210,020) from interaction patterns up to 9 minutes before the assessment as well as Item Response Theory (IRT) estimates of student ability and quiz difficulty. These models achieved a student-independent correlation of 0.55 between predicted and actual scores on the assessments and outperformed IRT-only predictions (r = 0.34). Model performance was largely independent of the length of the analyzed window preceding a quiz. We discuss potential for future applications of the models to trigger dynamic interventions that aim to encourage students to engage with formative assessments rather than more passive learning strategies.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {121–131},
numpages = {11},
keywords = {Formative assessment, Item Response Theory, Machine Learning, Online Learning, Predicting Student Performance, Retrieval Practice},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3506860.3506870,
author = {Zhu, Mengxia and Han, Siqi and Yuan, Peisen and Lu, Xuesong},
title = {Enhancing Programming Knowledge Tracing by Interacting Programming Skills and Student Code},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506870},
doi = {10.1145/3506860.3506870},
abstract = {Programming education has received extensive attention in recent years due to the increasing demand for programming ability in almost all industries. Educational institutions have widely employed online judges for programming training, which can help teachers automatically assess programming assignments by executing students’ code with test cases. However, a more important teaching process with online judges should be to evaluate how students master each of the programming skills such as strings or pointers, so that teachers may give personalized feedback and help them proceed to the success more efficiently. Previous studies have adopted deep models of knowledge tracing to evaluate a student’s mastery level of skills during the interaction with programming exercises. However, existing models generally follow the conventional assumption of knowledge tracing that each programming exercise requires only one skill, whereas in practice a programming exercise usually inspects the comprehensive use of multiple skills. Moreover, the feature of student code is often simply concatenated with other input features without the consideration of its relationship with the inspected programming skills. To bridge the gap, we propose a simple attention-based approach to learn from student code the features reflecting the multiple programming skills inspected by each programming exercise. In particular, we first use a program embedding method to obtain the representations of student code. Then we use the skill embeddings of each programming exercise to query the embeddings of student code and form an aggregated hidden state representing how the inspected skills are used in the student code. We combine the learned hidden state with DKT (Deep Knowledge Tracing), an LSTM (Long Short-Term Memory)-based knowledge tracing model, and show the improvements over baseline model. We point out some possible directions to improve the current work.},
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {438–443},
numpages = {6},
keywords = {attention mechanism, code representation, intelligent education, knowledge tracing, programming education},
location = {Online, USA},
series = {LAK22}
}

@inproceedings{10.1145/3027385.3027415,
author = {Hsiao, I-Han and Huang, Po-Kai and Murphy, Hannah},
title = {Uncovering reviewing and reflecting behaviors from paper-based formal assessment},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027415},
doi = {10.1145/3027385.3027415},
abstract = {In this paper, we study students' learning effectiveness through their use of a homegrown educational technology, Web Programming Grading Assistant (WPGA), which facilitates grading and feedback delivery of paper-based assessments. We designed a classroom study and collected data from a lower-division blended-instruction computer science class. We tracked and modeled students' reviewing and reflecting behaviors from WPGA. The results show that students demonstrated an effort and desire to review assessments regardless of if they were graded for academic performance or for attendance. Hardworking students achieved higher exam scores on average and were found to review their exams and the correct questions frequently. Additionally, student cohorts exhibited similar initial reviewing patterns, but different in-depth reviewing and reflecting strategies. Ultimately, this work contributes to the aggregation of multidimensional learning analytics across the physical and cybersphere.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {319–328},
numpages = {10},
keywords = {blended instruction classes, computing education, cross LAK, feedback, orchestration technology, programming learning, reflection},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/2460296.2460358,
author = {Giannakos, Michail N. and Chorianopoulos, Konstantinos and Ronchetti, Marco and Szegedi, Peter and Teasley, Stephanie D.},
title = {Analytics on video-based learning},
year = {2013},
isbn = {9781450317856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2460296.2460358},
doi = {10.1145/2460296.2460358},
abstract = {The International Workshop on Analytics on Video-based Learning (WAVe2013) aims to connect research efforts on Video-based Learning with Learning Analytics to create visionary ideas and foster synergies between the two fields. The main objective of WAVe is to build a research community around the topical area of Analytics on video-based learning. In particular, WAVe aims to develop a critical discussion about the next generation of analytics employed on video learning tools, the form of these analytics and the way they can be analyzed in order to help us to better understand and improve the value of video-based learning. WAVe is based on the rationale that combining and analyzing learners' interactions with other available data obtained from learners, new avenues for research on video-based learning have emerged.},
booktitle = {Proceedings of the Third International Conference on Learning Analytics and Knowledge},
pages = {283–284},
numpages = {2},
keywords = {MOOCs, interaction design, learning analytics, video based learning},
location = {Leuven, Belgium},
series = {LAK '13}
}

@inproceedings{10.1145/3375462.3375542,
author = {Agrawal, Sweety and Lalwani, Amar},
title = {Decoding the performance in an out-of-context problem during blocked practice},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375542},
doi = {10.1145/3375462.3375542},
abstract = {To master a skill, students generally practice the content of the skill in a blocking manner. While practicing in a blocked fashion, students know the context of the problems and also which strategy is needed to arrive at a solution. However, in real life standardized tests, where problems from various skills are grouped together, students often find it challenging to identify the correct strategy to solve the problems. This is because, during learning, students often practice the content in isolation. It hinders their ability to discriminate among the contexts of the problem. In this work, using tutor funtoot, we present students working on the topic Addition Word Problems with a subtraction word problem and investigate how they perform in the out-of-context subtraction word problem. We find that students' performance in the topic Addition Word Problems is a strong predictor of their performance in this out-of-context problem. Our results suggest that it is a stronger predictor for higher grades (4th and 5th) compared to the lower (2nd and 3rd) grades.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {118–123},
numpages = {6},
keywords = {ITS, K-12 education, blocking, interleaving, massed practice, mathematics},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375526,
author = {Kitto, Kirsty and Sarathy, Nikhil and Gromov, Aleksandr and Liu, Ming and Musial, Katarzyna and Buckingham Shum, Simon},
title = {Towards skills-based curriculum analytics: can we automate the recognition of prior learning?},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375526},
doi = {10.1145/3375462.3375526},
abstract = {In an era that will increasingly depend upon lifelong learning, the LA community will need to facilitate the movement and sharing of data and information across institutional and geographic boundaries. This will help us to recognise prior learning (RPL) and to personalise the learner experience. Here, we explore the utility of skills-based curriculum analytics and how it might facilitate the process of awarding RPL between two institutions. We explore the potential utility of combining natural language processing and skills taxonomies to map between subject descriptions for these two different institutions, presenting two algorithms we have developed to facilitate RPL and evaluating their performance. We draw attention to some of the issues that arise, listing areas that we consider ripe for future work in a surprisingly underexplored area.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {171–180},
numpages = {10},
keywords = {curriculum analytics, lifelong learning, recognition of prior learning, semantic spaces, skills ontologies},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@article{10.1145/3157086,
author = {Tomkin, Jonathan H. and West, Matthew and Herman, Geoffrey L.},
title = {An Improved Grade Point Average, With Applications to CS Undergraduate Education Analytics},
year = {2018},
issue_date = {December 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {4},
url = {https://doi.org/10.1145/3157086},
doi = {10.1145/3157086},
abstract = {We present a methodological improvement for calculating Grade Point Averages (GPAs). Heterogeneity in grading between courses systematically biases observed GPAs for individual students: the GPA observed depends on course selection. We show how a logistic model can account for course selection by simulating how every student in a sample would perform if they took all available courses, giving a new “modeled GPA.” We then use 10 years of grade data from a large university to demonstrate that this modeled GPA is a more accurate predictor of student performance in individual courses than the observed GPA. Using Computer Science (CS) as an example learning analytics application, it is found that required CS courses give significantly lower grades than average courses. This depresses the recorded GPAs of CS majors: modeled GPAs are 0.25 points higher than those that are observed. The modeled GPA also correlates much more closely with standardized test scores than the observed GPA: the correlation with Math ACT is 0.37 for the modeled GPA and is 0.20 for the observed GPA. This implies that standardized test scores are much better predictors of student performance than might otherwise be assumed.},
journal = {ACM Trans. Comput. Educ.},
month = sep,
articleno = {17},
numpages = {16},
keywords = {GPA, Learning analytics, gender disparity, women in computing}
}

@inproceedings{10.1145/3027385.3029483,
author = {Hu, Xiao and Hou, Xiangyu and Lei, Chi-Un and Yang, Chengrui and Ng, Jeremy},
title = {An outcome-based dashboard for moodle and Open edX},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029483},
doi = {10.1145/3027385.3029483},
abstract = {This poster presents a cross-platform learning analytics dashboard on Moodle and Open edX for monitoring outcome-based learning progress. The dashboard visualizes students' interactions with the platforms in near real-time, aiming to help teachers and students monitor students' learning progress. The dashboard has been used in four large-size general education courses in a comprehensive university in Hong Kong, undergoing evaluation and improvement.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {604–605},
numpages = {2},
keywords = {Open edX, dashboard, moodle, outcome-based learning},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/2883851.2883964,
author = {Wang, Xu and Wen, Miaomiao and Ros\'{e}, Carolyn P.},
title = {Towards triggering higher-order thinking behaviors in MOOCs},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883964},
doi = {10.1145/2883851.2883964},
abstract = {With the aim of better scaffolding discussion to improve learning in a MOOC context, this work investigates what kinds of discussion behaviors contribute to learning. We explored whether engaging in higher-order thinking behaviors results in more learning than paying general or focused attention to course materials. In order to evaluate whether to attribute the effect to engagement in the associated behaviors versus persistent characteristics of the students, we adopted two approaches. First, we used propensity score matching to pair students who exhibit a similar level of involvement in other course activities. Second, we explored individual variation in engagement in higher-order thinking behaviors across weeks. The results of both analyses support the attribution of the effect to the behavioral interpretation. A further analysis using LDA applied to course materials suggests that more social oriented topics triggered richer discussion than more biopsychology oriented topics.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {398–407},
numpages = {10},
keywords = {LDA topic modeling, coding manual, discussion, learning analytics, propensity score matching, regression analysis},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2567574.2567576,
author = {Samson, Perry J.},
title = {Analyzing student notes and questions to create personalized study guides},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567576},
doi = {10.1145/2567574.2567576},
abstract = {In the foreseeable future it will be technically possible for instructors, advisors and other delegated representatives of a college or university to access student participation and performance data in near-real time. One potential benefit of this increased data flow could include an improved ability to identify students at risk of academic failure or withdrawal. The availability of these data could also lead to creation of new adaptive learning measures that can automatically provide students personalized guidance.This demonstration will describe how the student notes and questions are being mined to provide student study guides that automatically link to outside resources. The demonstration will also report on how these new study guides have been received by the students and how they are at least partially responsible for a significant increase in student outcomes.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {263–264},
numpages = {2},
keywords = {data mining, learning analytics, student engagement},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/3375462.3375469,
author = {Kuzilek, Jakub and Zdrahal, Zdenek and Vaclavek, Jonas and Fuglik, Viktor and Skocilas, Jan},
title = {Exploring exam strategies of successful first year engineering students},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375469},
doi = {10.1145/3375462.3375469},
abstract = {At present, universities collect study-related data about their students. This information can be used to support students at risk of failing their studies. At the Faculty of Mechanical Engineering (FME), Czech Technical University in Prague (CTU), the group of the first-year students is the most vulnerable. The most critical part of the first year is the winter exam period when students usually divide into those who will pass and fail. One of the most important abilities, students need to learn, is exam planning, and our research aims at the exploration of the exam strategies of successful students. These strategies can be used for improving first-year students retention. The outgoing research on the analysis of exam strategies of the first-year students in the academic year 2017/2018 is reported. From a total of 361 first-year students, successful students have been selected. The successful student is the one who finished all three mandatory exams before the end of the first exam period. From the exam sequences of 153 selected students, a "layered" Markov chain probabilistic model has been constructed. It uncovered the most common exam strategies taken by those students.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {124–128},
numpages = {5},
keywords = {Markov chains, exam strategies, modelling},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/2883851.2883953,
author = {Hicks, Drew and Eagle, Michael and Rowe, Elizabeth and Asbell-Clarke, Jodi and Edwards, Teon and Barnes, Tiffany},
title = {Using game analytics to evaluate puzzle design and level progression in a serious game},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883953},
doi = {10.1145/2883851.2883953},
abstract = {Our previous work has demonstrated that players who perceive a game as more challenging are likely to perceive greater learning from that game [8]. However, this may not be the case for all sources of challenge. In this study of a Science learning game called Quantum Spectre, we found that students' progress through the first zone of the game seemed to encounter a "roadblock" during gameplay, dropping out when they cannot (or do not want to) progress further. Previously we had identified two primary types of errors in the learning game, Quantum Spectre: Science Errors related to the game's core educational content; and Puzzle Errors related to rules of the game but not to science knowledge. Using this prior analysis, alongside Survival Analysis techniques for analyzing time-series data and drop-out rates, we explored players' gameplay patterns to help us understand player dropout in Quantum Spectre. These results demonstrate that modeling player behavior can be useful for both assessing learning and for designing complex problem solving content for learning environments.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {440–448},
numpages = {9},
keywords = {complex problem solving, educational data mining, learning analytics, serious games, survival analysis},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2723576.2723647,
author = {Dascalu, Mihai and Stavarache, Larise L. and Trausan-Matu, Stefan and Dessus, Philippe and Bianco, Maryse and McNamara, Danielle S.},
title = {ReaderBench: An Integrated Tool Supporting both Individual and Collaborative Learning},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723647},
doi = {10.1145/2723576.2723647},
abstract = {The core of our ReaderBench software framework exposes a unified vision for predicting and assessing comprehension in both individual and collaborative learning scenarios. ReaderBench aims to improve both the quality and the classification of the analyzed documents by using an expanded range of criteria such as: morphology, semantics, discourse analysis with emphasis on polyphony and dialogism, thus providing reliable support for both tutors and students across a range of educational settings. ReaderBench uses a unitary cohesion-based representation of discourse applied into three major directions, all tightly connected by the underlying model and the Natural Language Processing (NLP) computations: reading strategies, textual complexity, and collaboration evaluation in Computer Supported Collaborative Learning (CSCL) conversations.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {436–437},
numpages = {2},
keywords = {Discourse Analysis, Learning Analytics, Participation/Collaboration Assessment, Reading Strategies, Textual Complexity},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2883851.2883907,
author = {Brown, Michael Geoffrey and DeMonbrun, R. Matthew and Lonn, Steven and Aguilar, Stephen J. and Teasley, Stephanie D.},
title = {What and when: the role of course type and timing in students' academic performance},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883907},
doi = {10.1145/2883851.2883907},
abstract = {In this paper we discuss the results of a study of students' academic performance in first year general education courses. Using data from 566 students who received intensive academic advising as part of their enrollment in the institution's pre-major/general education program, we investigate individual student, organizational, and disciplinary factors that might predict a students' potential classification in an Early Warning System as well as factors that predict improvement and decline in their academic performance. Disciplinary course type (based on Biglan's [7] typology) was significantly related to a student's likelihood to enter below average performance classifications. Students were the most likely to enter a classification in fields like the natural science, mathematics, and engineering in comparison to humanities courses. We attribute these disparities in academic performance to disciplinary norms around teaching and assessment. In particular, the timing of assessments played a major role in students' ability to exit a classification. Implications for the design of Early Warning analytics systems as well as academic course planning in higher education are offered.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {459–468},
numpages = {10},
keywords = {disciplinary fields, early warning system, time based learning analytics, undergraduate education},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2330601.2330647,
author = {Lonn, Steven and Krumm, Andrew E. and Waddington, R. Joseph and Teasley, Stephanie D.},
title = {Bridging the gap from knowledge to action: putting analytics in the hands of academic advisors},
year = {2012},
isbn = {9781450311113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2330601.2330647},
doi = {10.1145/2330601.2330647},
abstract = {This paper presents current findings from an ongoing design-based research project aimed at developing an early warning system (EWS) for academic mentors in an undergraduate engineering mentoring program. This paper details our progress in mining Learning Management System data and translating these data into an EWS for academic mentors. We focus on the role of mentors and advisors, and elaborate on their importance in learning analytics-based interventions developed for higher education.},
booktitle = {Proceedings of the 2nd International Conference on Learning Analytics and Knowledge},
pages = {184–187},
numpages = {4},
keywords = {design-research, higher education, learning analytics, undergraduate engineering},
location = {Vancouver, British Columbia, Canada},
series = {LAK '12}
}

@inproceedings{10.1145/2460296.2460350,
author = {Holman, Caitlin and Aguilar, Stephen and Fishman, Barry},
title = {GradeCraft: what can we learn from a game-inspired learning management system?},
year = {2013},
isbn = {9781450317856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2460296.2460350},
doi = {10.1145/2460296.2460350},
abstract = {The "gamification" of courses (i.e., designing courses that leverage motivational mechanisms found in videogames) is a movement that is gaining traction in educational research communities and universities. Two game-inspired courses were developed at a high-enrollment public university in an effort to increase student engagement, and to provide students with more personalized learning experiences. We designed a learning management system, GradeCraft, to foreground the affordances of these grading systems, and to enhance the "game-like" experience for students. Along with serving as a translation layer for the grading systems of these courses, GradeCraft is also designed with an eye towards learning analytics, and captures information that can be described as student "process" data. Currently this data includes what types of assignments students choose to complete; how students assign percentage weights to their chosen assignments; how often and how accurately students check or model their course grades; and how successfully assignments are completed by students individually and the class as a whole across a structured grading rubric. We hope GradeCraft will give instructors new insight into student engagement, and provide data-driven ideas about how to tailor courses to student needs.},
booktitle = {Proceedings of the Third International Conference on Learning Analytics and Knowledge},
pages = {260–264},
numpages = {5},
keywords = {game-inspired instruction, gamification, learning analytics, syllabus design},
location = {Leuven, Belgium},
series = {LAK '13}
}

@inproceedings{10.1145/3375462.3375508,
author = {Eagan, Brendan and Brohinsky, Jais and Wang, Jingyi and Shaffer, David Williamson},
title = {Testing the reliability of inter-rater reliability},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375508},
doi = {10.1145/3375462.3375508},
abstract = {Analyses of learning often rely on coded data. One important aspect of coding is establishing reliability. Previous research has shown that the common approach for establishing coding reliability is seriously flawed in that it produces unacceptably high Type I error rates. This paper focuses on testing whether or not these error rates correspond to specific reliability metrics or a larger methodological problem. Our results show that the method for establishing reliability is not metric specific, and we suggest the adoption of new practices to control Type I error rates associated with establishing coding reliability.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {454–461},
numpages = {8},
keywords = {coding, interrater reliability, reliability, statistical analysis, validity},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375491,
author = {Effenberger, Tom\'{a}\v{s} and Pel\'{a}nek, Radek and \v{C}ech\'{a}k, Jaroslav},
title = {Exploration of the robustness and generalizability of the additive factors model},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375491},
doi = {10.1145/3375462.3375491},
abstract = {Additive Factors Model is a widely used student model, which is primarily used for refining knowledge component models (Q-matrices). We explore the robustness and generalizability of the model. We explicitly formulate simplifying assumptions that the model makes and we discuss methods for visualizing learning curves based on the model. We also report on an application of the model to data from a learning system for introductory programming; these experiments illustrate possibly misleading interpretation of model results due to differences in item difficulty. Overall, our results show that greater care has to be taken in the application of the model and in the interpretation of results obtained with the model.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {472–479},
numpages = {8},
keywords = {introductory programming, knowledge components, learning curves, student modeling},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/2883851.2883922,
author = {Koile, Kimberle and Rubin, Andee and Chapman, Steve and Kliman, Marlene and Ko, Lily},
title = {Using machine analysis to make elementary students' mathematical thinking visible},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883922},
doi = {10.1145/2883851.2883922},
abstract = {The INK-12: Teaching and Learning Using Interactive Ink Inscriptions in K-12 project has been developing and investigating the use of pen-based technology in elementary math classes. This paper reports on progress made on machine analysis of students' visual representations created using digital tools developed to support learning multiplication and division. The goal of the analysis is to make student thinking visible in order to (a) better understand how students learn multiplication and division, and (b) provide feedback to teachers, e.g., about strategies students use to solve problems. Student work from a five-week trial in a third grade class provides a corpus for development and evaluation of the machine analysis routines. Preliminary findings indicate that the routines can reproduce human analyses.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {524–525},
numpages = {2},
keywords = {elementary education, formative assessment, learning analytics, mathematics, pen-based computing, visual representations},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2723576.2723603,
author = {Simsek, Duygu and S\'{a}ndor, \'{A}gnes and Buckingham Shum, Simon and Ferguson, Rebecca and De Liddo, Anna and Whitelock, Denise},
title = {Correlations between automated rhetorical analysis and tutors' grades on student essays},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723603},
doi = {10.1145/2723576.2723603},
abstract = {When assessing student essays, educators look for the students' ability to present and pursue well-reasoned and strong arguments. Such scholarly argumentation is often articulated by rhetorical metadiscourse. Educators will be necessarily examining metadiscourse in students' writing as signals of the intellectual moves that make their reasoning visible. Therefore students and educators could benefit from available powerful automated textual analysis that is able to detect rhetorical metadiscourse. However, there is a need to validate such technologies in higher education contexts, since they were originally developed in non-educational applications. This paper describes an evaluation study of a particular language analysis tool, the Xerox Incremental Parser (XIP), on undergraduate social science student essays, using the mark awarded as a measure of the quality of the writing. As part of this exploration, the study presented in this paper seeks to assess the quality of the XIP through correlational studies and multiple regression analysis.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {355–359},
numpages = {5},
keywords = {academic writing, academic writing analytics, argumentation, learning analytics, metadiscourse, natural language processing, rhetorical parsing, writing analytics},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/3448139.3448181,
author = {van der Graaf, Joep and Lim, Lyn and Fan, Yizhou and Kilgour, Jonathan and Moore, Johanna and Bannert, Maria and Gasevic, Dragan and Molenaar, Inge},
title = {Do Instrumentation Tools Capture Self-Regulated Learning?},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448181},
doi = {10.1145/3448139.3448181},
abstract = {Researchers have been struggling with the measurement of Self-Regulated Learning (SRL) for decades. Instrumentation tools have been proposed to help capture SRL processes that are difficult to capture. The aim of the present study was to improve measurement of SRL by embedding instrumentation tools in a learning environment and validating the measurement of SRL with these instrumentation tools using think aloud. Synchronizing log data and concurrent think aloud data helped identify which SRL processes were captured by particular instrumentation tools. One tool was associated with a single SRL process: the timer co-occurred with monitoring. Other tools co-occurred with a number of SRL processes, i.e., the highlighter and note taker captured superficial writing down, organizing, and monitoring, whereas the search and planner tools revealed planning and monitoring. When specific learner actions with the tool were analyzed, a clearer picture emerged of the relation between the highlighter and note taker and SRL processes. By aligning log data with think aloud data, we showed that instrumentation tool use indeed reflects SRL processes. The main contribution is that this paper is the first to show that SRL processes that are difficult to measure by trace data can indeed be captured by instrumentation tools such as high cognition and metacognition. Future challenges are to collect and process log data real time with learning analytic techniques to measure ongoing SRL processes and support learners during learning with personalized SRL scaffolds.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {438–448},
numpages = {11},
keywords = {Instrumentation tools, Self-Regulated Learning},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/2723576.2723661,
author = {Shehata, Shady and Arnold, Kimberly E.},
title = {Measuring student success using predictive engine},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723661},
doi = {10.1145/2723576.2723661},
abstract = {A basic challenge in delivering global education is improving student success. Institutions of education are increasingly focused on improving graduation and retention rates of their students. In this poster, we describe Student Success System (S3) that can measure student performance starting from the first weeks of the semester and the adoption process for S3 by University of Wisconsin System (UWS).},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {416–417},
numpages = {2},
keywords = {algorithms, data mining, learning analytics, machine learning, predictive modeling, regression analysis, student success},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/3027385.3027393,
author = {Brown, Michael Geoffrey and DeMonbrun, R. Matthew and Teasley, Stephanie D.},
title = {Don't call it a comeback: academic recovery and the timing of educational technology adoption},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027393},
doi = {10.1145/3027385.3027393},
abstract = {Recent research using learning analytics data to explore student performance over the course of a term suggests that a substantial percentage of students who are classified as academically struggling manage to recover. In this study, we report the result of a hazard analysis based on students' behavioral engagement with different digital instructional technologies over the course of a semester. We observe substantially different adoption and use behavior between students who did and did not experience academic difficulty in the course. Students who experienced moderate academic difficulty benefited the most from using tools that helped them plan their study behaviors. Students who experienced more severe academic difficulty benefited from tools that helped them prepare for exams. We observed that students adopted most tools and system features before they experienced academic difficulty, and students who adopted early were more likely to recover.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {489–493},
numpages = {5},
keywords = {early warning systems, educational technology, technology adoption, undergraduate education},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3375462.3375514,
author = {Srivastava, Namrata and Nawaz, Sadia and Lodge, Jason M. and Velloso, Eduardo and Erfani, Sarah and Bailey, James},
title = {Exploring the usage of thermal imaging for understanding video lecture designs and students' experiences},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375514},
doi = {10.1145/3375462.3375514},
abstract = {Video is becoming a dominant medium for the delivery of educational material. Despite the widespread use of video for learning, there is still a lack of understanding about how best to help people learn in this medium. This study demonstrates the use of thermal camera as compared to traditional self-reported methods for assessing learners' cognitive load while watching video lectures of different styles. We evaluated our approach in a study with 78 university students viewing two variants of short video lectures on two different topics. To incorporate subjective measures, the students reported on mental effort, interest, prior knowledge, confidence, and challenge. Moreover, through a physical slider device, the students could continuously report on their perceived level of difficulty. Lastly, we used thermal sensor as an additional indicator of students' level of difficulty and associated cognitive load. This was achieved through, continuous real-time monitoring of students by using a thermal imaging camera. This study aims to address the following: firstly, to analyze if video styles differ in terms of the associated cognitive load. Secondly, to assess the effects of cognitive load on learning outcomes; could an increase in the cognitive load be associated with poorer learning outcomes? Third, to see if there is a match between students' perceived difficulty levels and a biological indicator. The results suggest that thermal imaging could be an effective tool to assess learners' cognitive load, and an increased cognitive load could lead to poorer performance. Moreover, in terms of the lecture styles, the animated video lectures appear to be a better tool than the text-only lectures (in the content areas tested here). The results of this study may guide future works on effective video designs, especially those that consider the cognitive load.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {250–259},
numpages = {10},
keywords = {cognitive load, instructional design, thermal imaging, video lectures},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/2567574.2567622,
author = {Grann, Jeff and Bushway, Deborah},
title = {Competency map: visualizing student learning to promote student success},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567622},
doi = {10.1145/2567574.2567622},
abstract = {Adult students often struggle to appreciate the relevance of their higher educational experiences to their careers. Capella University's competency map is a dashboard that visually indicates each student's status relative to specific assessed competencies. MBA students who utilize their competency map demonstrate competencies at slightly higher levels and persist in their program at greater rates, even after statistically controlling for powerful covariates, such as course engagement.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {168–172},
numpages = {5},
keywords = {competency, evaluation, learning analytics, visualization},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/3706599.3720153,
author = {Srivastava, Namrata and Healey, Jennifer and Jain, Rajiv and Liu, Guanli and Ma, Ying and Llana, Borano and Gasevic, Dragan and Dingler, Tilman and Wallace, Shaun},
title = {Priming at Scale: An Evaluation of Using AI to Generate Primes for Mobile Readers},
year = {2025},
isbn = {9798400713958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706599.3720153},
doi = {10.1145/3706599.3720153},
abstract = {Text summaries, images, and mind maps are well-known methods for priming readers to better engage with content. Previously, these “primes” needed to be hand-crafted, limiting their use. The advent of generative technologies makes the automatic creation of custom primes for any passage a realistic possibility. Here, we evaluate the efficacy of primes generated using AI on reading comprehension, reading speed, and re-engagement during mobile reading, which is notorious for its frequent interruptions. We used a mobile platform to present a reading task with an interruption to 44 readers (21 with English as a first language). We found that AI primes increased reading speed by an average of 7% for all readers in the initial reading task with no loss of comprehension and that visual primes had a significant interruption recovery effect for people whose first language was not English. Across all readers, text primes had both the initial reading speed increase and were overall most preferred.},
booktitle = {Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {447},
numpages = {10},
keywords = {Reading interfaces, interruptions, mobile reading, generative AI, priming},
location = {
},
series = {CHI EA '25}
}

@inproceedings{10.1145/3375462.3375479,
author = {Tuti, Timothy and Paton, Chris and Winters, Niall},
title = {Learning to represent healthcare providers knowledge of neonatal emergency care: findings from a smartphone-based learning intervention targeting clinicians from LMICs},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375479},
doi = {10.1145/3375462.3375479},
abstract = {Modelling healthcare providers' knowledge while they are gaining new concepts is an important step towards supporting self-regulated personalised learning at scale. This is especially important if we are to address health workforce skills development and enhance the subsequent quality of care patients receive in the Global South, where a huge skills gap exists. Rich data about healthcare providers' learning can be captured by their responses to close-ended problems within conjunctive solution space -such as clinical training scenarios for emergency care delivery- on smartphone-based learning interventions which are being proposed as a solution for reducing the healthcare skills gap in this context. Together with sequential data detailing a learner's progress while they are solving a learning task, this provides useful insights into their learning behaviour. Predicting learning or forgetting curves from representations of healthcare providers knowledge is a difficult task, but recent promising machine learning advances have produced techniques capable of learning knowledge representations and overcoming this challenge. In this study, we train a Long Short-Term Memory neural network for predicting learners' future performance and forgetting curves by feeding it sequence embeddings of learning task attempts from healthcare providers from Global South. From this training, the model captures nuanced representations of a healthcare provider's clinical knowledge and their patterns of learning behaviours, predicting their future performance with high accuracy. More significantly, by differentiating reduced performance based on spaced learning, the model can help provide timely warning that helps support healthcare providers to reinforce their self-regulated learning while providing a basis for personalised instructional support to aid improved clinical outcomes from their professional practices.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {320–329},
numpages = {10},
keywords = {clinical training, deep knowledge tracing, emergency care, forgetting curves, global health, neonatal care, smartphones},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375503,
author = {Peffer, Melanie and Quigley, David and Brusman, Liza and Avena, Jennifer and Knight, Jennifer},
title = {Trace data from student solutions to genetics problems reveals variance in the processes related to different course outcomes},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375503},
doi = {10.1145/3375462.3375503},
abstract = {Problem solving, particularly in disciplines such as genetics, is an essential but difficult competency for students to master. Prior work indicated that trace data can be leveraged to measure the invisible cognitive processes that undergird learning activities such as problem solving. Building on prior work and given the importance and difficulties associated with genetics problem solving, we used unsupervised statistical methods (k-means clustering and feature selection) to characterize the patterns of processes students use during genetics problem solving and the relationship to proximal and distal outcomes. At the level of the individual problem, we found that conclusion processes, such as making claims and eliminating possible solutions, was an important interim step and associated with getting a particular problem correct. Surprisingly, we noted that a different set of processes was associated with course outcomes. Students who performed multiple metacognitive steps (e.g. monitoring, checking, planning) in a row or who engaged in execution steps (e.g. using information, drawing a picture, restating the process) as part of problem solving during the semester performed better on final assessments. We found a third set of practices, making consecutive conclusion processes, metacognitive processes preceding reasoning and reasoning preceding conclusions to be important for success at both the problem level and on final assessments. This suggests that different problem-solving processes are associated with success on different course benchmarks. This work raises provocative questions regarding best practices for teaching problem solving in genetics classrooms.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {47–52},
numpages = {6},
keywords = {clustering, genetics, metacognition, problem solving},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/2723576.2723619,
author = {Aguiar, Everaldo and Lakkaraju, Himabindu and Bhanpuri, Nasir and Miller, David and Yuhas, Ben and Addison, Kecia L.},
title = {Who, when, and why: a machine learning approach to prioritizing students at risk of not graduating high school on time},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723619},
doi = {10.1145/2723576.2723619},
abstract = {Several hundred thousand students drop out of high school every year in the United States. Interventions can help those who are falling behind in their educational goals, but given limited resources, such programs must focus on the right students, at the right time, and with the right message. In this paper, we describe an incremental approach that can be used to select and prioritize students who may be at risk of not graduating high school on time, and to suggest what may be the predictors of particular students going off-track. These predictions can then be used to inform targeted interventions for these students, hopefully leading to better outcomes.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {93–102},
numpages = {10},
keywords = {early intervention, learning analytics, predictive analytics, secondary education, student retention},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/3170358.3170386,
author = {Mangaroska, Katerina and Sharma, Kshitij and Giannakos, Michail and Tr\ae{}tteberg, Hallvard and Dillenbourg, Pierre},
title = {Gaze insights into debugging behavior using learner-centred analysis},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170386},
doi = {10.1145/3170358.3170386},
abstract = {The presented study tries to tackle an intriguing question of how user-generated data from current technologies can be used to reinforce learners' reflections, improve teaching practices, and close the learning analytics loop. In particular, the aim of the study is to utilize users' gaze to examine the role of a mirroring tool (i.e. Exercise View in Eclipse) in orchestrating basic behavioral regulation of participants engaged in a debugging task. The results demonstrated that students who processed the information presented in the Exercise View and acted upon it, improved their performance and achieved higher level of success than those who failed to do it. The findings shed a light how to capture what constitute relevant data within a particular context using gaze patterns, that could guide collection of essential learner-centred analytics for the purpose of designing usable and modular learning environments based on data-driven approaches.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {350–359},
numpages = {10},
keywords = {behaviour regulation, debugging, eye-tracking, learner-centred analysis, mirroring tools},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/3375462.3375490,
author = {Klebanov, Beata Beigman and Loukina, Anastassia and Lockwood, John and Liceralde, Van Rynald T. and Sabatini, John and Madnani, Nitin and Gyawali, Binod and Wang, Zuowei and Lentini, Jennifer},
title = {Detecting learning in noisy data: the case of oral reading fluency},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375490},
doi = {10.1145/3375462.3375490},
abstract = {In a school context, learning is usually detected by repeated measurements of the skill of interest through a sequence of specially designed tests; in particular, this is the case with tracking improvement in oral reading fluency in elementary school children in the U.S. Results presented in this paper suggest that it is possible and feasible to detect improvement in oral reading fluency using data collected during children's independent reading of a book using the Relay Reader™ app. We are thus a step closer to the vision of having a child read for the story, not for a test, yet being able to unobtrusively assess their progress in oral reading fluency.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {490–495},
numpages = {6},
keywords = {book reading, children's reading, fluency, oral reading fluency, reading analytics, reading app},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375532,
author = {Khosravi, Hassan and Gyamfi, George and Hanna, Barbara E. and Lodge, Jason},
title = {Fostering and supporting empirical research on evaluative judgement via a crowdsourced adaptive learning system},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375532},
doi = {10.1145/3375462.3375532},
abstract = {The value of students developing the capacity to make accurate judgements about the quality of their work and that of others has been widely recognised in higher education literature. However, despite this recognition, little attention has been paid to the development of tools and strategies with the potential both to foster evaluative judgement and to support empirical research into its growth. This paper provides a demonstration of how educational technologies may be used to fill this gap. In particular, we introduce the adaptive learning system RiPPLE and describe how it aims to (1) develop evaluative judgement in large-class settings through suggested strategies from the literature such as the use of rubrics, exemplars and peer review and (2) enable large empirical studies at low cost to determine the effect-size of such strategies. A case study demonstrating how RiPPLE has been used to achieve these goals in a specific context is presented.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {83–88},
numpages = {6},
keywords = {crowd-sourcing, educational technologies, evaluative judgement, student-authored materials},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375524,
author = {Pardos, Zachary A. and Jiang, Weijie},
title = {Designing for serendipity in a university course recommendation system},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375524},
doi = {10.1145/3375462.3375524},
abstract = {Collaborative filtering based algorithms, including Recurrent Neural Networks (RNN), tend towards predicting a perpetuation of past observed behavior. In a recommendation context, this can lead to an overly narrow set of suggestions lacking in serendipity and inadvertently placing the user in what is known as a "filter bubble." In this paper, we grapple with the issue of the filter bubble in the context of a course recommendation system in production at a public university. Our approach is to present course results that are novel or unexpected to the student but still relevant to their interests. We build one set of models based on course catalog descriptions (BOW) and another set informed by enrollment histories (course2vec). We compare the performance of these models on off-line validation sets and against the system's existing RNN-based recommendation engine in an online user study of undergraduates (N = 70) who rated their course recommendations along six characteristics related to serendipity. Results of the user study show a dramatic lack of novelty in RNN recommendations and depict the characteristic trade-offs that make serendipity difficult to achieve. While the machine learned course2vec models performed best on off-line validation tasks, it was the simple bag-of-words based recommendations that students rated as more serendipitous. We discuss the role of the kind of information presented by the system in a student's decision to accept a recommendation from either algorithm.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {350–359},
numpages = {10},
keywords = {course guidance, filter bubble, higher education, neural networks},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/2883851.2883890,
author = {Bos, Nynke and Brand-Gruwel, Saskia},
title = {Student differences in regulation strategies and their use of learning resources: implications for educational design},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883890},
doi = {10.1145/2883851.2883890},
abstract = {The majority of the learning analytics research focuses on the prediction of course performance and modeling student behaviors with a focus on identifying students who are at risk of failing the course. Learning analytics should have a stronger focus on improving the quality of learning for all students, not only identifying at risk students. In order to do so, we need to understand what successful patterns look like when reflected in data and subsequently adjust the course design to avoid unsuccessful patterns and facilitate successful patterns.However, when establishing these successful patterns, it is important to account for individual differences among students since previous research has shown that not all students engage with learning resources to the same extent. Regulation strategies seem to play an important role in explaining the different usage patterns students' display when using digital learning recourses. When learning analytics research incorporates contextualized data about student regulation strategies we are able to differentiate between students at a more granular level.The current study examined if regulation strategies could account for differences in the use of various learning resources. It examines how students regulated their learning process and subsequently used the different learning resources throughout the course and established how this use contributes to course performance.The results show that students with different regulation strategies use the learning resources to the same extent. However, the use of learning resources influences course performance differently for different groups of students. This paper recognizes the importance of contextualization of learning data resources with a broader set of indicators to understand the learning process. With our focus on differences between students, we strive for a shift within learning analytics from identifying at risk students towards a contribution of learning analytics in the educational design process and enhance the quality of learning; for all students.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {344–353},
numpages = {10},
keywords = {blended learning, cluster analysis, individual differences, learning dispositions, regulation strategies},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2723576.2723613,
author = {Hsiao, I-Han and Awasthi, Piyush},
title = {Topic facet modeling: semantic visual analytics for online discussion forums},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723613},
doi = {10.1145/2723576.2723613},
abstract = {In this paper, we propose a novel Topic Facet Model (TFM), a probabilistic topic model that assumes all words in single sentence are generated from one topic facet. The model is applied to automatically extract forum posts semantics for uncovering the content latent structures. We further prototype a visual analytics interface to present online discussion forum semantics. We hypothesize that the semantic modeling through analytics on open online discussion forums can help users examine the post content by viewing the summarized topic facets. Our preliminary results demonstrated that TFM can be a promising method to extract topic specificity from conversational and relatively short texts in online programming discussion forums.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {231–235},
numpages = {5},
keywords = {LDA, SLDA, TFM, automated assessment, discourse analytics, discussion forums, learning analytics, programming, topic modeling},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2090116.2090119,
author = {Haythornthwaite, Caroline},
title = {Learning networks, crowds and communities},
year = {2011},
isbn = {9781450309448},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2090116.2090119},
doi = {10.1145/2090116.2090119},
abstract = {Who we learn from, where and when is dramatically affected by the reach of the Internet. From learning for formal education to learning for pleasure, we look to the web early and often for our data and knowledge needs, but also for places and spaces where we can collaborate, contribute to, and create learning and knowledge communities. Based on the keynote presentation given at the first Learning Analytics and Knowledge Conference held in 2011 in Banff, Alberta, this paper explores a social network perspective on learning with reference to social network principles and studies by the author. The paper explores the ways a social network perspective can be used to examine learning, with attention to the structure and dynamics of online learning networks, and emerging configurations such as online crowds and communities.},
booktitle = {Proceedings of the 1st International Conference on Learning Analytics and Knowledge},
pages = {18–22},
numpages = {5},
keywords = {education, learning analytics, online learning, social networks},
location = {Banff, Alberta, Canada},
series = {LAK '11}
}

@inproceedings{10.1145/3375462.3375521,
author = {Wei, Huan and Li, Haotian and Xia, Meng and Wang, Yong and Qu, Huamin},
title = {Predicting student performance in interactive online question pools using mouse interaction features},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375521},
doi = {10.1145/3375462.3375521},
abstract = {Modeling student learning and further predicting the performance is a well-established task in online learning and is crucial to personalized education by recommending different learning resources to different students based on their needs. Interactive online question pools (e.g., educational game platforms), an important component of online education, have become increasingly popular in recent years. However, most existing work on student performance prediction targets at online learning platforms with a well-structured curriculum, predefined question order and accurate knowledge tags provided by domain experts. It remains unclear how to conduct student performance prediction in interactive online question pools without such well-organized question orders or knowledge tags by experts. In this paper, we propose a novel approach to boost student performance prediction in interactive online question pools by further considering student interaction features and the similarity between questions. Specifically, we introduce new features (e.g., think time, first attempt, and first drag-and-drop) based on student mouse movement trajectories to delineate students' problem-solving details. In addition, heterogeneous information network is applied to integrating students' historical problem-solving information on similar questions, enhancing student performance predictions on a new question. We evaluate the proposed approach on the dataset from a real-world interactive question pool using four typical machine learning models. The result shows that our approach can achieve a much higher accuracy for student performance prediction in interactive online question pools than the traditional way of only using the statistical features (e.g., students' historical question scores) in various models. We further discuss the performance consistency of our approach across different prediction models and question classes, as well as the importance of the proposed interaction features in detail.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {645–654},
numpages = {10},
keywords = {heterogeneous information network, mouse movement trajectory, question pool, student performance prediction},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/2883851.2883915,
author = {Hsiao, I-Han and Pandhalkudi Govindarajan, Sesha Kumar and Lin, Yi-Ling},
title = {Semantic visual analytics for today's programming courses},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883915},
doi = {10.1145/2883851.2883915},
abstract = {We designed and studied an innovative semantic visual learning analytics for orchestrating today's programming classes. The visual analytics integrates sources of learning activities by their content semantics. It automatically processs paper-based exams by associating sets of concepts to the exam questions. Results indicated the automatic concept extraction from exams were promising and could be a potential technological solution to address a real world issue. We also discovered that indexing effectiveness was especially prevalent for complex content by covering more comprehensive semantics. Subjective evaluation revealed that the dynamic concept indexing provided teachers with immediate feedback on producing more balanced exams.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {48–53},
numpages = {6},
keywords = {auto grading, dashboard, intelligent authoring, orchestration technology, programming, semantic analytics, visual analytics},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/3027385.3027435,
author = {Ocumpaugh, Jaclyn and Baker, Ryan S. and San Pedro, Maria O. C. Z. and Hawn, M. Aaron and Heffernan, Cristina and Heffernan, Neil and Slater, Stefan A.},
title = {Guidance counselor reports of the ASSISTments college prediction model (ACPM)},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3027435},
doi = {10.1145/3027385.3027435},
abstract = {Advances in the learning analytics community have created opportunities to deliver early warnings that alert teachers and instructors when a student is at risk of not meeting academic goals [6], [71]. Alert systems have also been developed for school district leaders [33] and for academic advisors in higher education [39], but other professionals in the K-12 system, namely guidance counselors, have not been widely served by these systems. In this study, we use college enrollment models created for the ASSISTments learning system [55] to develop reports that target the needs of these professionals, who often work directly with students, but usually not in classroom settings. These reports are designed to facilitate guidance counselors' efforts to help students to set long term academic and career goals. As such, they provide the calculated likelihood that a student will attend college (the ASSISTments College Prediction Model or ACPM), alongside student engagement and learning measures. Using design principles from risk communication research and student feedback theories to inform a co-design process, we developed reports that can inform guidance counselor efforts to support student achievement.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {479–488},
numpages = {10},
keywords = {college attendance, guidance counselors, intelligent tutoring systems, predictive analytics, stakeholder reports, student engagement},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/3170358.3170414,
author = {Fougt, Simon Skov and Siebert-Evenstone, Amanda and Eagan, Brendan and Tabatabai, Sara and Misfeldt, Morten},
title = {Epistemic network analysis of students' longer written assignments as formative/summative evaluation},
year = {2018},
isbn = {9781450364003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170358.3170414},
doi = {10.1145/3170358.3170414},
abstract = {This paper reports on an exploratory trial of developing pedagogical visualizations of 16 students' written assignments on literary analysis using two sets of keywords and Epistemic Network Analysis (ENA). The visualizations are aimed at summative evaluation as a tool for the professor to support assessment and understanding of subject learning. Results show that ENA can visually distinguish low, middle and high performing students, but not statistically significantly. Thus, our trial provides a tool for the professor that supports understanding of subject learning and formative assessment.},
booktitle = {Proceedings of the 8th International Conference on Learning Analytics and Knowledge},
pages = {126–130},
numpages = {5},
keywords = {epistemic network analysis, formative and summative assessment, pedagogical learning analytics, student assignment},
location = {Sydney, New South Wales, Australia},
series = {LAK '18}
}

@inproceedings{10.1145/2567574.2567575,
author = {Kuo, Chin-Hwa and Peng, Jian-Wen and Chang, Wen-Chen},
title = {Hanzi handwriting acquisition with automatic feedback},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567575},
doi = {10.1145/2567574.2567575},
abstract = {One of the most crucial distinctions between Chinese and Western languages is that the former is based on ideograms, whereas the latter is based on phonograms. Due to this distinction, Western learners of Chinese often experience more difficulties in grasping correct character stroke sequence and/or stroke direction relative to native Chinese speakers. In this paper, we designed a HanZi writing environment with automatic feedback to address the above issue. Before the collection of HanZi characters on a massive scale, we conducted a pilot study to collect handwritten Chinese samples from 160 college students in the U.S. The findings from this study enabled us to further refine the learning environment and design optimal learning and teaching strategies for learners and teachers.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {261–262},
numpages = {2},
keywords = {chinese character handwriting, learning analytics, personalization},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/2567574.2567584,
author = {Hecking, Tobias and Ziebarth, Sabrina and Hoppe, H. Ulrich},
title = {Analysis of dynamic resource access patterns in a blended learning course},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567584},
doi = {10.1145/2567574.2567584},
abstract = {This paper presents an analysis of resource access patterns in a recently conducted master level university course. The specialty of the course was that it followed a new teaching approach by providing additional learning resources such as wikis, self-tests and videos. To gain deeper insights into the usage of the provided learning material we have built dynamic bipartite student -- resource networks based on event logs of resource access. These networks are analysed using methods adapted from social network analysis. In particular we uncover bipartite clusters of students and resources in those networks and propose a method to identify patterns and traces of their evolution over time.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {173–182},
numpages = {10},
keywords = {MOOCs, learning analytics, learning resources, social network analysis},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/3375462.3375465,
author = {Molenaar, Inge and Horvers, Anne and Dijkstra, Rick and Baker, Ryan S.},
title = {Personalized visualizations to promote young learners' SRL: the learning path app},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375465},
doi = {10.1145/3375462.3375465},
abstract = {This paper describes the design and evaluation of personalized visualizations to support young learners' Self-Regulated Learning (SRL) in Adaptive Learning Technologies (ALTs). Our learning path app combines three Personalized Visualizations (PV) that are designed as an external reference to support learners' internal regulation process. The personalized visualizations are based on three pillars: grounding in SRL theory, the usage of trace data and the provision of clear actionable recommendations for learners to improve regulation. This quasi-experimental pre-posttest study finds that learners in the personalized visualization condition improved the regulation of their practice behavior, as indicated by higher accuracy and less complex moment-by-moment learning curves compared to learners in the control group. Learners in the PV condition showed better transfer on learning. Finally, students in the personalized visualizations condition were more likely to under-estimate instead of over-estimate their performance. Overall, these findings indicates that the personalized visualizations improved regulation of practice behavior, transfer of learning and changed the bias in relative monitoring accuracy.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {330–339},
numpages = {10},
keywords = {adaptive learning technologies, hybrid human-system intelligence, learner-faced dashboards, self-regulated learning},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3587103.3594185,
author = {Scott, Michael James and Mitchell, Alexander and Brown, Douglas},
title = {Retention in First Stage Undergraduate Computing: Lessons Learned from a Collaborative Learning Intervention},
year = {2023},
isbn = {9798400701399},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587103.3594185},
doi = {10.1145/3587103.3594185},
abstract = {It is challenging to retain computing students through their first stage of undergraduate education. Attrition is high, with many transferring courses or dropping out. This poster explores preliminary findings from an action research project improving continuation in first-stage undergraduate computing. Five years of data from Falmouth University's Games Academy in the UK suggest improvement in first-stage retention from 66.6% in 2017-18 to 91.2% in 2021-22. Findings support prior work on pair programming, media computation, and peer instruction. However, they also highlight the benefits of collaborative learning facilitated by faculty and informed by learning analytics. Peer reviews and pre-submission clinics, student advisor follow-ups, and retrieval via synoptic assessment also contributed to the improvement.},
booktitle = {Proceedings of the 2023 Conference on Innovation and Technology in Computer Science Education V. 2},
pages = {631},
numpages = {1},
keywords = {action research, collaborative learning, completion, continuation, formative feedback, learning analytics, programming tutors, progression, retention, student advisors, synoptic assessment},
location = {Turku, Finland},
series = {ITiCSE 2023}
}

@inproceedings{10.1145/3027385.3029428,
author = {Mojarad, Shirin and Lewkow, Nicholas and Essa, Alfred and Zhang, Jie and Feild, Jacqueline},
title = {Quasi-experimental design for causal inference using Python and Apache Spark: a hands-on tutorial},
year = {2017},
isbn = {9781450348706},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027385.3029428},
doi = {10.1145/3027385.3029428},
abstract = {Educational practitioners and policy makers require evidence supporting claims about educational efficacy. Evidence is often found using causal relationships between education inputs and student learning outcomes. Causal inference covers a wide range of topics in education research, including efficacy studies to prove if a new policy, software, curriculum or intervention is effective in improving student learning outcomes. Randomized controlled trials (RCT) are considered a gold standard method to demonstrate causality. However, these studies are expensive, timely and costly, as well as not being ethical to conduct in many educational contexts. Causality can also be deducted purely from observational data. In this tutorial, we will review methodologies for estimating the causal effects of education inputs on student learning outcomes using observational data. This is an inherently complex task due to many hidden variables and their interrelationships in educational research. In this tutorial, we discuss causal inference in the context of educational research with big data. This is the first tutorial of its kind at Learning Analytics and Knowledge Conference (LAK) that provides a hands-on experience with Python and Apache Spark as a practical tool for educational researchers to conduct causal inference. As a prerequisite, attendees are required to have familiarity with Python.},
booktitle = {Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference},
pages = {502–503},
numpages = {2},
keywords = {Apache Spark, Python, big data, causal inference, parallel computing, quasiexperiment design},
location = {Vancouver, British Columbia, Canada},
series = {LAK '17}
}

@inproceedings{10.1145/2723576.2723601,
author = {Gross, Eric and Wshah, Safwan and Simmons, Isaiah and Skinner, Gary},
title = {A handwriting recognition system for the classroom},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723601},
doi = {10.1145/2723576.2723601},
abstract = {The Xerox Ignite™ Educator Support System (henceforth referred to simply as Ignite™) is a data collection, analysis, and visualization workflow and software solution to assist K-12 educators. To illustrate, suppose a third-grade teacher wants to know how well her class has grasped a lesson on fractions. She would first scan her students' homework and/or exams into the Ignite system via a range of multifunctional input devices. Xerox Ignite™ reads, interprets, and analyzes the students' work in minutes. Then the teacher can select how to view the data by choosing from numerous reports. Examples are; an "at a glance" class summary that shows who needs extra help in what areas and who is ready to move on; a "context" report showing how each skill for each student is progressing over time; a grade-level performance report that helps third-grade teachers share best practices and cluster students into learning groups; and a student feedback report that tells each student what he/she needs to improve upon. Ignite™ intent is also to make it easier for districts to administer, score and evaluate content based on academic goals set for schools and students. The scanning and 'mark lifting' technology embedded into Ignite™ reduces the time needed to correct papers and frees time for the teacher to apply detailed insights to their day-to-day instruction tasks. Critical to this function is the automated reading of student marks, including handwriting, to enable the digitization of student performance at a detailed level. In this paper we present a system level description of the Ignite™ handwriting recognition module and describe the challenges and opportunities presented in an educational environment.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {218–222},
numpages = {5},
keywords = {Xerox Ignite™, handwriting, intelligent character recognition, learning analytics},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/2567574.2567614,
author = {Taraghi, Behnam and Ebner, Martin and Saranti, Anna and Sch\"{o}n, Martin},
title = {On using markov chain to evidence the learning structures and difficulty levels of one digit multiplication},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567614},
doi = {10.1145/2567574.2567614},
abstract = {Understanding the behavior of learners within learning applications and analyzing the factors that may influence the learning process play a key role in designing and optimizing learning applications. In this work we focus on a specific application named "1x1 trainer" that has been designed for primary school children to learn one digit multiplications. We investigate the database of learners' answers to the asked questions (N &gt; 440000) by applying the Markov chains. We want to understand whether the learners' answers to the already asked questions can affect the way they will answer the subsequent asked questions and if so, to what extent. Through our analysis we first identify the most difficult and easiest multiplications for the target learners by observing the probabilities of the different answer types. Next we try to identify influential structures in the history of learners' answers considering the Markov chain of different orders. The results are used to identify pupils who have difficulties with multiplications very soon (after couple of steps) and to optimize the way questions are asked for each pupil individually.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {68–72},
numpages = {5},
keywords = {difficulty level, elearning, learning analytics, markov chain, math, one digit multiplication, primary school},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/3375462.3375470,
author = {Sher, Varshita and Hatala, Marek and Ga\v{s}evi\'{c}, Dragan},
title = {Analyzing the consistency in within-activity learning patterns in blended learning},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375470},
doi = {10.1145/3375462.3375470},
abstract = {Performance and consistency play a large role in learning. This study analyzes the relation between consistency in students' online work habits and academic performance in a blended course. We utilize the data from logs recorded by a learning management system (LMS) in two information technology courses. The two courses required the completion of monthly asynchronous online discussion tasks and weekly assignments, respectively. We measure consistency by using Data Time Warping (DTW) distance for two successive tasks (assignments or discussions), as an appropriate measure to assess similarity of time series, over 11-day timeline starting 10 days before and up to the submission deadline. We found meaningful clusters of students exhibiting similar behavior and we use these to identify three distinct consistency patterns: highly consistent, incrementally consistent, and inconsistent users. We also found evidence of significant associations between these patterns and learner's academic performance.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {1–10},
numpages = {10},
keywords = {learner performance and consistency, regularity, student persistence, time management, time-series analysis, work habits},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375485,
author = {Geller, Shay A. and Hoernle, Nicholas and Gal, Kobi and Segal, Avi and Zhang, Amy X. and Karger, David and Facciotti, Marc T. and Igo, Michele},
title = {#Confused and beyond: detecting confusion in course forums using students' hashtags},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375485},
doi = {10.1145/3375462.3375485},
abstract = {Students' confusion is a barrier for learning, contributing to loss of motivation and to disengagement with course materials. However, detecting students' confusion in large-scale courses is both time and resource intensive. This paper provides a new approach for confusion detection in online forums that is based on harnessing the power of students' self-reported affective states (reported using a set of pre-defined hashtags). It presents a rule for labeling confusion, based on students' hashtags in their posts, that is shown to align with teachers' judgement. We use this labeling rule to inform the design of an automated classifier for confusion detection for the case when there are no self-reported hashtags present in the test set. We demonstrate this approach in a large scale Biology course using the Nota Bene annotation platform. This work lays the foundation to empower teachers with better support tools for detecting and alleviating confusion in online courses.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {589–594},
numpages = {6},
keywords = {confusion detection, emojis, hashtags, online discussion forum, self-reported affect, text classification},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/2330601.2330649,
author = {Sherin, Bruce},
title = {Using computational methods to discover student science conceptions in interview data},
year = {2012},
isbn = {9781450311113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2330601.2330649},
doi = {10.1145/2330601.2330649},
abstract = {A large body of research in the learning sciences has focused on students' commonsense science knowledge---the everyday knowledge of the natural world that is gained outside of formal instruction. Although researchers studying commonsense science have employed a variety of methods, one-on-one clinical interviews have played a unique and central role. The data that result from these interviews take the form of video recordings, which in turn are often compiled into written transcripts, and coded by human analysts. In my team's work on learning analytics, we draw on this same type of data, but we attempt to automate its analysis. In this paper, I describe the success we have had using extremely simple methods from computational linguistics---methods that are based on rudimentary vector space models and simple clustering algorithms. These automated analyses are employed in an exploratory mode, as a way to discover student conceptions in the data. The aims of this paper are primarily methodological in nature; I will attempt to show that it is possible to use techniques from computational linguistics to analyze data from commonsense science interviews. As a test bed, I draw on transcripts of a corpus of interviews in which 54 middle school students were asked to explain the seasons.},
booktitle = {Proceedings of the 2nd International Conference on Learning Analytics and Knowledge},
pages = {188–197},
numpages = {10},
keywords = {conceptual change, learning analytics},
location = {Vancouver, British Columbia, Canada},
series = {LAK '12}
}

@inproceedings{10.1145/3448139.3448207,
author = {\"{O}ncel, P\"{u}ren and Flynn, Lauren E. and Sonia, Allison N. and Barker, Kennis E. and Lindsay, Grace C. and McClure, Caleb M. and McNamara, Danielle S. and Allen, Laura K.},
title = {Automatic Student Writing Evaluation: Investigating the Impact of Individual Differences on Source-Based Writing},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448207},
doi = {10.1145/3448139.3448207},
abstract = {Automated Writing Evaluation systems have been developed to help students improve their writing skills through the automated delivery of both summative and formative feedback. These systems have demonstrated strong potential in a variety of educational contexts; however, they remain limited in their personalization and scope. The purpose of the current study was to begin to address this gap by examining whether individual differences could be modeled in a source-based writing context. Undergraduate students (n=106) wrote essays in response to multiple sources and then completed an assessment of their vocabulary knowledge. Natural language processing tools were used to characterize the linguistic properties of the source-based essays at four levels: descriptive, lexical, syntax, and cohesion. Finally, machine learning models were used to predict students’ vocabulary scores from these linguistic features. The models accounted for approximately 29% of the variance in vocabulary scores, suggesting that the linguistic features of source-based essays are reflective of individual differences in vocabulary knowledge. Overall, this work suggests that automated text analyses can help to understand the role of individual differences in the writing process, which may ultimately help to improve personalization in computer-based learning environments.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {620–625},
numpages = {6},
keywords = {individual differences, machine-learning models, source-based writing, vocabulary knowledge},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/2460296.2460347,
author = {Renzel, Dominik and Klamma, Ralf},
title = {From micro to macro: analyzing activity in the ROLE Sandbox},
year = {2013},
isbn = {9781450317856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2460296.2460347},
doi = {10.1145/2460296.2460347},
abstract = {Current learning services are increasingly based on standard Web technologies and concepts. As by-product of service operation, Web logs capture and contextualize user interactions in a generic manner, in high detail, and on a massive scale. At the same time, we face inventions of data standards for capturing and encoding learner interactions tailored to learning analytics purposes. However, such standards are often focused on institutional and management perspectives or biased by their intended use. In this paper, we argue for Web logs as valuable data sources for learning analytics on all levels of Bronfenbrenner's Ecological System Theory and introduce a simple framework for Web log data enrichment, processing and further analysis. Based on an example data set from a management service for widget-based Personal Learning Environments, we illustrate our approach and discuss the applicability of different analysis techniques along with their particular benefits for learners.},
booktitle = {Proceedings of the Third International Conference on Learning Analytics and Knowledge},
pages = {250–254},
numpages = {5},
keywords = {ecological systems theory, learning analytics, personal learning environment, processing pipeline, web logs},
location = {Leuven, Belgium},
series = {LAK '13}
}

@inproceedings{10.1145/2567574.2567604,
author = {Bogar\'{\i}n, Alejandro and Romero, Crist\'{o}bal and Cerezo, Rebeca and S\'{a}nchez-Santill\'{a}n, Miguel},
title = {Clustering for improving educational process mining},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567604},
doi = {10.1145/2567574.2567604},
abstract = {In this paper, we propose to use clustering to improve educational process mining. We want to improve both the performance and comprehensibility of the models obtained. We have used data from 84 undergraduate students who followed an online course using Moodle 2.0. We propose to group students firstly starting from data about Moodle's usage summary and/or the students' final marks in the course. Then, we propose to use data from Moodle's logs about each cluster/group of students separately in order to be able to obtain more specific and accurate models of students' behaviour. The results show that the fitness of the specific models is greater than the general model obtained using all the data, and the comprehensibility of the models can be also improved in some cases.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {11–15},
numpages = {5},
keywords = {clustering, educational data mining, learning analytics, process mining},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/3375462.3375530,
author = {Akintunde, Ruth Okoilu and Shabrina, Preya and Catete, Veronica and Barnes, Tiffany and Lynch, Collin and Rutherford, Teomara},
title = {Data-informed curriculum sequences for a curriculum-integrated game},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375530},
doi = {10.1145/3375462.3375530},
abstract = {In this paper, we perform a predictive analysis of a curriculum-integrated math game, ST Math, to suggest a partial ordering for the game's curriculum sequence. We analyzed the sequence of ST Math objectives played by elementary school students in 5 U.S. districts and grouped each objective into difficult and easy categories according to how many retries were needed for students to master an objective. We observed that retries on some objectives were high in one district and low in another district where the objectives are played in a different order. Motivated by this observation, we investigated what makes an effective curriculum sequence. To infer a new partially-ordered sequence, we performed an expanded replication study of a novel predictive analysis by a prior study to find predictive relationships between 15 objectives played in different sequences by 3,328 students from 5 districts. Based on the predictive abilities of objectives in these districts, we found 17 suggested objective orderings. After deriving these orderings, we confirmed the validity of the order by evaluating the impact of the suggested sequence on changes in rates of retries and corresponding performance. We observed that when the objectives were played in the suggested sequence, we record a drastic reduction in retries, implying that these objectives are easier for students. This indicates that objectives that come earlier can provide prerequisite knowledge for later objectives. We believe that data-informed sequences, such as the ones we suggest, may improve efficiency of instruction and increase content learning and performance.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {635–644},
numpages = {10},
keywords = {curricular sequencing, educational games, retries, serious game analytics},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3503823.3503850,
author = {Tsoni, Rozita and Zorkadis, Vasilios and S. Verykios, Vassilios},
title = {A Data Pipeline to Preserve Privacy in Educational Settings},
year = {2022},
isbn = {9781450395557},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503823.3503850},
doi = {10.1145/3503823.3503850},
abstract = {In the sensitive field of distance learning data handling should lead to actionable knowledge and, at the same time, ought to respect the privacy of the students. The hype of online learning led to a plethora of data but also raised ethical issues regarding privacy protection which is mainly addressed in the GDPR. There is an optimum equilibrium between making out the most of available data and protecting the individual freedom of the participants. In this paper, we propose a data pipeline that could be incorporated in a Learning Analytics cycle and provide anonymous, low-risk data.},
booktitle = {Proceedings of the 25th Pan-Hellenic Conference on Informatics},
pages = {138–142},
numpages = {5},
keywords = {Anonymization, Data Pipelines, Distance Learning, Learning Analytics},
location = {Volos, Greece},
series = {PCI '21}
}

@inproceedings{10.1145/3375462.3375471,
author = {van Leeuwen, Anouschka and Rummel, Nikol},
title = {Comparing teachers' use of mirroring and advising dashboards},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375471},
doi = {10.1145/3375462.3375471},
abstract = {Teachers play an essential role during collaborative learning. To provide effective support, teachers have to be constantly aware of students' activities and make fast decisions about which group to offer support, without disrupting students' collaborative process. Teacher dashboards are visual displays that provide analytics about learners to help teachers increase their awareness of the situation. However, if teachers are not able to efficiently and effectively distill information from the dashboard, the dashboard can become an obstacle instead of an aid. In the present study, we compared dashboards that provide information (mirroring) to dashboards that provide information and alert the teacher to groups that are in need of support (advising). Teachers were shown standardized, fictitious collaborative situations on one of the types of dashboards and were asked to detect the group that was in need of support. The results showed that teachers in the advising condition more often detected the problematic group, needed less effort to do so, and were more confident of their decisions. The teacher-dashboard interaction patterns showed that teachers in the advising condition generally started by checking the given alert, but also that they tried to look at as much information about other groups as they could. In the mirroring condition, teachers generally started by examining information from class overviews, but did not always have time to check information for individual groups. These findings are discussed in light of the role of a teacher dashboard in teachers' decision making in the context of student collaboration.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {26–34},
numpages = {9},
keywords = {cooperative/collaborative learning, elementary education, human-computer interface, improving classroom teaching, teaching/learning strategies},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/2883851.2883857,
author = {Agnihotri, Lalitha and Mojarad, Shirin and Lewkow, Nicholas and Essa, Alfred},
title = {Educational data mining with Python and Apache spark: a hands-on tutorial},
year = {2016},
isbn = {9781450341905},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2883851.2883857},
doi = {10.1145/2883851.2883857},
abstract = {Enormous amount of educational data has been accumulated through Massive Open Online Courses (MOOCs), as well as commercial and non-commercial learning platforms. This is in addition to the educational data released by US government since 2012 to facilitate disruption in education by making data freely available. The high volume, variety and velocity of collected data necessitate use of big data tools and storage systems such as distributed databases for storage and Apache Spark for analysis.This tutorial will introduce researchers and faculty to real-world applications involving data mining and predictive analytics in learning sciences. In addition, the tutorial will introduce statistics required to validate and accurately report results. Topics will cover how big data is being used to transform education. Specifically, we will demonstrate how exploratory data analysis, data mining, predictive analytics, machine learning, and visualization techniques are being applied to educational big data to improve learning and scale insights driven from millions of student's records.The tutorial will be held over a half day and will be hands on with pre-posted material. Due to the interdisciplinary nature of work, the tutorial appeals to researchers from a wide range of backgrounds including big data, predictive analytics, learning sciences, educational data mining, and in general, those interested in how big data analytics can transform learning. As a prerequisite, attendees are required to have familiarity with at least one programming language.},
booktitle = {Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge},
pages = {507–508},
numpages = {2},
keywords = {big data, data mining, educational data mining, exploratory data analysis, learning analytics, machine learning, parallel computing, predictive analytics, python, simulation, spark, visualization},
location = {Edinburgh, United Kingdom},
series = {LAK '16}
}

@inproceedings{10.1145/2567574.2567583,
author = {Aguiar, Everaldo and Chawla, Nitesh V. and Brockman, Jay and Ambrose, G. Alex and Goodrich, Victoria},
title = {Engagement vs performance: using electronic portfolios to predict first semester engineering student retention},
year = {2014},
isbn = {9781450326643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567574.2567583},
doi = {10.1145/2567574.2567583},
abstract = {As providers of higher education begin to harness the power of big data analytics, one very fitting application for these new techniques is that of predicting student attrition. The ability to pinpoint students who might soon decide to drop out of a given academic program allows those in charge to not only understand the causes for this undesired outcome, but it also provides room for the development of early intervention systems. While making such inferences based on academic performance data alone is certainly possible, we claim that in many cases there is no substantial correlation between how well a student performs and his or her decision to withdraw. This is specially true when the overall set of students has a relatively similar academic performance. To address this issue, we derive measurements of engagement from students' electronic portfolios and show how these features can be effectively used to augment the quality of predictions.},
booktitle = {Proceedings of the Fourth International Conference on Learning Analytics And Knowledge},
pages = {103–112},
numpages = {10},
keywords = {data fusion, early intervention, electronic portfolios, learning analytics, predictive analytics, student retention},
location = {Indianapolis, Indiana, USA},
series = {LAK '14}
}

@inproceedings{10.1145/2723576.2723583,
author = {Martinez-Maldonado, Roberto and Pardo, Abelardo and Mirriahi, Negin and Yacef, Kalina and Kay, Judy and Clayphan, Andrew},
title = {The LATUX workflow: designing and deploying awareness tools in technology-enabled learning settings},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723583},
doi = {10.1145/2723576.2723583},
abstract = {Designing, deploying and validating learning analytics tools for instructors or students is a challenge requiring techniques and methods from different disciplines, such as software engineering, human-computer interaction, educational design and psychology. Whilst each of these disciplines has consolidated design methodologies, there is a need for more specific methodological frameworks within the cross-disciplinary space defined by learning analytics. In particular there is no systematic workflow for producing learning analytics tools that are both technologically feasible and truly underpin the learning experience. In this paper, we present the LATUX workflow, a five-stage workflow to design, deploy and validate awareness tools in technology-enabled learning environments. LATUX is grounded on a well-established design process for creating, testing and re-designing user interfaces. We extend this process by integrating the pedagogical requirements to generate visual analytics to inform instructors' pedagogical decisions or intervention strategies. The workflow is illustrated with a case study in which collaborative activities were deployed in a real classroom.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {1–10},
numpages = {10},
keywords = {awareness, dashboard, design, groupware, visualisations},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/3375462.3375525,
author = {Shabrina, Preya and Akintunde, Ruth Okoilu and Maniktala, Mehak and Barnes, Tiffany and Lynch, Collin and Rutherford, Teomara},
title = {Peeking through the classroom window: a detailed data-driven analysis on the usage of a curriculum integrated math game in authentic classrooms},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375525},
doi = {10.1145/3375462.3375525},
abstract = {We present a data-driven analysis that provides generalized insights of how a curriculum integrated educational math game gets used as a routinized classroom activity throughout the year in authentic primary school classrooms. Our study relates observations from a field study on Spatial Temporal Math (ST Math) to our findings mined from ST Math students' sequential game play data. We identified features that vary across game play sessions and modeled their relationship with session performance. We also derived data-informed suggestions that may provide teachers with insights into how to design classroom game play sessions to facilitate more effective learning.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {625–634},
numpages = {10},
keywords = {curriculum integrated math games, game analytics, integration},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/2723576.2723578,
author = {Dascalu, Mihai and Trausan-Matu, Stefan and Dessus, Philippe and McNamara, Danielle S.},
title = {Discourse cohesion: a signature of collaboration},
year = {2015},
isbn = {9781450334174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723576.2723578},
doi = {10.1145/2723576.2723578},
abstract = {As Computer Supported Collaborative Learning (CSCL) becomes increasingly adopted as an alternative to classic educational scenarios, we face an increasing need for automatic tools designed to support tutors in the time consuming process of analyzing conversations and interactions among students. Therefore, building upon a cohesion-based model of the discourse, we have validated ReaderBench, a system capable of evaluating collaboration based on a social knowledge-building perspective. Through the inter-twining of different participants' points of view, collaboration emerges and this process is reflected in the identified cohesive links between different speakers. Overall, the current experiments indicate that textual cohesion successfully detects collaboration between participants as ideas are shared and exchanged within an ongoing conversation.},
booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
pages = {350–354},
numpages = {5},
keywords = {cohesion, collaboration assessment, computer supported collaborative learning, discourse analysis, learning analytics},
location = {Poughkeepsie, New York},
series = {LAK '15}
}

@inproceedings{10.1145/3375462.3375535,
author = {Chen, Bodong and Poquet, Oleksandra},
title = {Socio-temporal dynamics in peer interaction events},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375535},
doi = {10.1145/3375462.3375535},
abstract = {Asynchronous online discussions are broadly used to support peer interaction in online and hybrid courses. In this paper, we argue that the analysis of online peer interactions would benefit from the focus on relational events that are temporal and occur due to a range of factors. To demonstrate the possibility, we applied Relational Event Modeling (REM) to a dataset from online discussions in seven online classes. Informed by a conceptual model of social interaction in online discussions, this modeling included (a) a learner attribute capturing aspects of temporal participation, (b) social dynamics factors such as preferential attachment and reciprocity, and (c) turn-by-turn sequential patterns. Results showed that learner activity and familiarity from recent interactions affected their propensity to form ties. Turn-by-turn sequential patterns, that capture individual posting in bursts, explain how two-star network patterns form. Since two-star network patterns could further facilitate small group formation in the network, we expected the models to also capture communication in triads (i.e. triadic closure). Yet, models, devoid of the content of exchanges, did not capture the social dynamics well, and failed to predict patterns for communication across triads. By bringing in discourse features, future work can investigate the role of knowledge building behaviours in triadic closure of digital networks. This study contributes fresh insights into social interaction in online discussions, calls for attention to micro-level temporal patterns, and motivates future work to scaffold learner participation in similar contexts.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {203–208},
numpages = {6},
keywords = {digital peer networks, relational event modelling, temporality},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3375462.3375495,
author = {Ferreira, M\'{a}verick and Rolim, Vitor and Mello, Rafael Ferreira and Lins, Rafael Dueire and Chen, Guanliang and Ga\v{s}evi\'{c}, Dragan},
title = {Towards automatic content analysis of social presence in transcripts of online discussions},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375495},
doi = {10.1145/3375462.3375495},
abstract = {This paper presents an approach to automatic labeling of the content of messages in online discussion according to the categories of social presence. To achieve this goal, the proposed approach is based on a combination of traditional text mining features and word counts extracted with the use of established linguistic frameworks (i.e., LIWC and Coh-metrix). The best performing classifier obtained 0.95 and 0.88 for accuracy and Cohen's kappa, respectively. This paper also provides some theoretical insights into the nature of social presence by looking at the classification features that were most relevant for distinguishing between the different categories. Finally, this study adopted epistemic network analysis to investigate the structural construct validity of the automatic classification approach. Namely, the analysis showed that the epistemic networks produced based on messages manually and automatically coded produced nearly identical results. This finding thus produced evidence of the structural validity of the automatic approach.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge},
pages = {141–150},
numpages = {10},
keywords = {community of inquiry model, content analytics, epistemic network analysis, online discussion, text classification},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3303772.3303831,
author = {Peffer, Melanie and Quigley, David and Mostowfi, Mehrgan},
title = {Clustering Analysis Reveals Authentic Science Inquiry Trajectories Among Undergraduates},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303831},
doi = {10.1145/3303772.3303831},
abstract = {Science education reforms in the United States call for an emphasis on teaching of scientific practices, such as inquiry. Previous work examined expert versus novice practices in authentic science inquiry and found although experts have fairly consistent inquiry strategies, novices exist on a continuum. In this paper, we extend our previous qualitative work to quantitatively analyze differences in inquiry practices among novices. Using clustering analysis, we found that non-science majors who performed simple investigations tended to cluster together and biology majors who performed complex investigations also tended to cluster together. We observed two additional clusters that contain both non-science majors and biology majors, but who performed distinct inquiry strategies. This raises some critical questions about how to pedagogically target students within each cluster.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics &amp; Knowledge},
pages = {96–100},
numpages = {5},
keywords = {Authentic science inquiry, clustering, science classroom inquiry simulations, science practices},
location = {Tempe, AZ, USA},
series = {LAK19}
}

